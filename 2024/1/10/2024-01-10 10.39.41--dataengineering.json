{"kind": "Listing", "data": {"after": "t3_19345cj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.\n\nWe all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern\n\nBecause users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors \n\nI agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python\n\nWhy don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we are ignoring Julia for data engineering and going for rust??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192fhgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704809199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.&lt;/p&gt;\n\n&lt;p&gt;We all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern&lt;/p&gt;\n\n&lt;p&gt;Because users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors &lt;/p&gt;\n\n&lt;p&gt;I agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python&lt;/p&gt;\n\n&lt;p&gt;Why don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192fhgg", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "subreddit_subscribers": 151629, "created_utc": 1704809199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If so, how do you deal with it?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you feel the tension between \"data democratization\" and the business need for high quality curated data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192f47n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704808143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so, how do you deal with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192f47n", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "subreddit_subscribers": 151629, "created_utc": 1704808143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All of us **want to move fast**, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don't use them.\n\nWe built a quick tool that infers tests/documents for you. \n\nWhat are people's **thoughts on a tool that infers tests and docs and helps you add them to your schema definition**? \n\n&amp;#x200B;\n\n[Schema inference](https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player)", "author_fullname": "t2_q5jcx79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT test and documentation generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mv0k6icgofbc1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/DASHPlaylist.mpd?a=1707475180%2CNTU0ZjkyNTBjMDkzZmQ0YjI4Mjg3OTdmNTA2NDg1ZGZhZWFkZGJiYWNjMDIzNDY3YWEwY2YwY2MzNmJhMDg2Zg%3D%3D&amp;v=1&amp;f=sd", "x": 1730, "y": 1080, "hlsUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/HLSPlaylist.m3u8?a=1707475180%2CMzQzZjI5ZTMxMjk1ZmIxYThjZjI0NTNmNDc1MWM0Mzc2OGM1MWMxNzBhNjVlMTNmYTMwNDcxMGYyMDkxMWI4NQ%3D%3D&amp;v=1&amp;f=sd", "id": "mv0k6icgofbc1", "isGif": false}}, "name": "t3_192he2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704814481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All of us &lt;strong&gt;want to move fast&lt;/strong&gt;, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don&amp;#39;t use them.&lt;/p&gt;\n\n&lt;p&gt;We built a quick tool that infers tests/documents for you. &lt;/p&gt;\n\n&lt;p&gt;What are people&amp;#39;s &lt;strong&gt;thoughts on a tool that infers tests and docs and helps you add them to your schema definition&lt;/strong&gt;? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player\"&gt;Schema inference&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192he2z", "is_robot_indexable": true, "report_reasons": null, "author": "bk1007", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "subreddit_subscribers": 151629, "created_utc": 1704814481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is probably a stupid question to anyone with a deeper background in data engineering, but I'm new and learned dbt first using dbt cloud which makes orchestrating transformation pretty easy. \n\nIf I have a data warehouse and I can't use dbt, how do I go about orchestrating transformation of data after landing it in raw form in the data warehouse? BigQuery seems to have Dataform and I'm guessing the other cloud providers have something similar, but what if I'm just using an on-prem database? Do I just have a bunch of scheduled stored procedures?", "author_fullname": "t2_550fb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people orchestrating the T in ELT for their data warehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192ou1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704832493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is probably a stupid question to anyone with a deeper background in data engineering, but I&amp;#39;m new and learned dbt first using dbt cloud which makes orchestrating transformation pretty easy. &lt;/p&gt;\n\n&lt;p&gt;If I have a data warehouse and I can&amp;#39;t use dbt, how do I go about orchestrating transformation of data after landing it in raw form in the data warehouse? BigQuery seems to have Dataform and I&amp;#39;m guessing the other cloud providers have something similar, but what if I&amp;#39;m just using an on-prem database? Do I just have a bunch of scheduled stored procedures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192ou1k", "is_robot_indexable": true, "report_reasons": null, "author": "PureOhms", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192ou1k/how_are_people_orchestrating_the_t_in_elt_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192ou1k/how_are_people_orchestrating_the_t_in_elt_for/", "subreddit_subscribers": 151629, "created_utc": 1704832493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After seeing the same basic question again, I was like \u201cI think it\u2019s enough. It\u2019s time to create a new subreddit\u201d and when I tried to create r/learndataengineering, it said community already exists. Voila!\n\n\nMy nominations for questions that should go there:\n- What makes a good Data Engineer?\n- Do you need &lt;tool/skill&gt; to be a Data Engineer?\n- What do you all think about &lt;tool/skill&gt;?\n- What &lt;project/skills&gt; should I work on if I want to be a Data Engineer?\n- Is Data Engineering a good role for me to transition to from &lt;current role&gt;?\n- Read my blog post about DuckDB/Polars/\u2026\n\n\nSince I had to choose a flair, I went with discussion. What questions/topics do folks think belong in that subreddit?\n\n\nEdit: I guess it\u2019s an old subreddit (3+ years). Regardless, since it exists, I think we should utilize it", "author_fullname": "t2_16b1dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Someone finally did it! r/learndataengineering is live!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1933ach", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704874234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After seeing the same basic question again, I was like \u201cI think it\u2019s enough. It\u2019s time to create a new subreddit\u201d and when I tried to create &lt;a href=\"/r/learndataengineering\"&gt;r/learndataengineering&lt;/a&gt;, it said community already exists. Voila!&lt;/p&gt;\n\n&lt;p&gt;My nominations for questions that should go there:\n- What makes a good Data Engineer?\n- Do you need &amp;lt;tool/skill&amp;gt; to be a Data Engineer?\n- What do you all think about &amp;lt;tool/skill&amp;gt;?\n- What &amp;lt;project/skills&amp;gt; should I work on if I want to be a Data Engineer?\n- Is Data Engineering a good role for me to transition to from &amp;lt;current role&amp;gt;?\n- Read my blog post about DuckDB/Polars/\u2026&lt;/p&gt;\n\n&lt;p&gt;Since I had to choose a flair, I went with discussion. What questions/topics do folks think belong in that subreddit?&lt;/p&gt;\n\n&lt;p&gt;Edit: I guess it\u2019s an old subreddit (3+ years). Regardless, since it exists, I think we should utilize it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1933ach", "is_robot_indexable": true, "report_reasons": null, "author": "jawabdey", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1933ach/someone_finally_did_it_rlearndataengineering_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1933ach/someone_finally_did_it_rlearndataengineering_is/", "subreddit_subscribers": 151629, "created_utc": 1704874234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nI'm currently working in a new open source project called DataFlint .([https://github.com/dataflint/spark](https://github.com/dataflint/spark)) to do performance monitoring better in Apache Spark.\n\nI'm working on creating a new feature for monitoring resource allocation ( i.e. how many executors do you need, and how many CPU/Memory each executor needs), so it would be easier to tune your Apache Spark resource allocation.\n\n&amp;#x200B;\n\nSo while making this feature I was wondering - how do you usually decide how many resources to give a job? on what metrics do you look? which configs do you play with?\n\n&amp;#x200B;\n\nI released an initial version of this feature and looks like this:\n\nhttps://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45\n\nit contains a graph of the number of executors, the min/max executors (if using dynamic allocation), which query ran, and some relevant configs.\n\nWDYT?", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you decide how many resources to give an Apache Spark job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"41qbfiv6qgbc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63e154617bee6f9d91a697c94a56112a9c355c78"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7133befe2d3f988c7d01619b33ee57255e467ea6"}, {"y": 191, "x": 320, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ea4dd00f02057a058d7229e44aef26a8b171944"}, {"y": 383, "x": 640, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca309ecc3ce66dddcb2b7377076ba46d0a5e53dd"}, {"y": 575, "x": 960, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a58fd54e378fb9fb6e4899cbb22854c87288ef99"}, {"y": 647, "x": 1080, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a82815521f8704e06249340feeb5f035020dcdd"}], "s": {"y": 1760, "x": 2936, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45"}, "id": "41qbfiv6qgbc1"}}, "name": "t3_192muug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AezvY7fMP8doLXTsi_G8Z8F4Tn4-jS7gnbLgMIJwhBA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704827778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working in a new open source project called DataFlint .(&lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;) to do performance monitoring better in Apache Spark.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on creating a new feature for monitoring resource allocation ( i.e. how many executors do you need, and how many CPU/Memory each executor needs), so it would be easier to tune your Apache Spark resource allocation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So while making this feature I was wondering - how do you usually decide how many resources to give a job? on what metrics do you look? which configs do you play with?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I released an initial version of this feature and looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45\"&gt;https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;it contains a graph of the number of executors, the min/max executors (if using dynamic allocation), which query ran, and some relevant configs.&lt;/p&gt;\n\n&lt;p&gt;WDYT?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?auto=webp&amp;s=a1663328cfda62d121dd3c35d545c7154f598f94", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4e8132c91b474e33b64e9468995d4cc1462f092", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7010b91ae5f970f49e95f0f17ab12305a316fa41", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea61b0ba3918f27a1e06a272145364b2a29116b2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=674e0a743cab1aebdda9c72a1ca77e724b8445d9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5e24b2de2c1f04042e397985e6b67f560bb6091", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d77199ece2473fe34677bac5cdf8dfb606a4037f", "width": 1080, "height": 540}], "variants": {}, "id": "fo-5dKHfVMLxEc3zQneZtZO1K1S_XEWh5A5Sl5GzQDY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192muug", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192muug/how_do_you_decide_how_many_resources_to_give_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192muug/how_do_you_decide_how_many_resources_to_give_an/", "subreddit_subscribers": 151629, "created_utc": 1704827778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We're sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &gt; Meltano &gt; DBT &gt; Openmetadata &gt; Tableau with custom data models.  Our head of infrastructure is a \"Microsoft can solve everything\" guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We're now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the \"wild west\" approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We're dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?", "author_fullname": "t2_1hb8tav0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Fabric - cost and viability in a large org with shrinking budget and limited usage of MS stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192epn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704806924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We&amp;#39;re sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &amp;gt; Meltano &amp;gt; DBT &amp;gt; Openmetadata &amp;gt; Tableau with custom data models.  Our head of infrastructure is a &amp;quot;Microsoft can solve everything&amp;quot; guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We&amp;#39;re now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the &amp;quot;wild west&amp;quot; approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We&amp;#39;re dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192epn6", "is_robot_indexable": true, "report_reasons": null, "author": "FrebTheRat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "subreddit_subscribers": 151629, "created_utc": 1704806924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the products my company offers requires customers to submit export files from various platforms to us. From there, our current process is to transform the data in excel and import into a MySQL database. Yuck.\n\nEach system exports similar data in the same shape, but of course, headers differ, customers use different values to mean the same thing, etc. Super simple transformations. However, it doesn\u2019t make any sense to make an ETL for each one because these files are used only once.\n\nWhat I am looking for is a tool that would allow someone who isn\u2019t super technical to upload one of these files, select a preset target, select what each header means, make any necessary transformations, and check for errors/missing values.\n\nI haven\u2019t been able to find anything that does this short of building it myself. If it doesn\u2019t, I\u2019m thinking about building it on my own.\n\nWe have a long term solution planned for this whole process so it doesn\u2019t make sense to build it out for the company.", "author_fullname": "t2_m0ki4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this tool exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192sv8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704842154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the products my company offers requires customers to submit export files from various platforms to us. From there, our current process is to transform the data in excel and import into a MySQL database. Yuck.&lt;/p&gt;\n\n&lt;p&gt;Each system exports similar data in the same shape, but of course, headers differ, customers use different values to mean the same thing, etc. Super simple transformations. However, it doesn\u2019t make any sense to make an ETL for each one because these files are used only once.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is a tool that would allow someone who isn\u2019t super technical to upload one of these files, select a preset target, select what each header means, make any necessary transformations, and check for errors/missing values.&lt;/p&gt;\n\n&lt;p&gt;I haven\u2019t been able to find anything that does this short of building it myself. If it doesn\u2019t, I\u2019m thinking about building it on my own.&lt;/p&gt;\n\n&lt;p&gt;We have a long term solution planned for this whole process so it doesn\u2019t make sense to build it out for the company.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192sv8t", "is_robot_indexable": true, "report_reasons": null, "author": "phonyfakeorreal", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192sv8t/does_this_tool_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192sv8t/does_this_tool_exist/", "subreddit_subscribers": 151629, "created_utc": 1704842154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently helped launch the Airflow 2023 User Survey, and thought some of you may find the results interesting, especially those that use Airflow :)\n\nI even had a cool infographic made. Included a sneak peek below, but for the full thing- [check it out here](https://airflow.apache.org/survey/)\n\nHope this is useful!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;format=png&amp;auto=webp&amp;s=9a7db044ca109cd37398102034f0e54d39ae365a\n\n&amp;#x200B;", "author_fullname": "t2_l5gu8nhgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow User Survey Infographic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hciz2m67whbc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3e09454778390c7ce578aab35d0fcaea35f497a"}, {"y": 166, "x": 216, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a94f97ab4f80708fef7d14b983d4cbf92154e040"}, {"y": 246, "x": 320, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9c42dd627f9fe78177c9ac6a9dbf52f0919481"}, {"y": 493, "x": 640, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94f3180b991467baad0827a1845c6e243a6dc2d5"}, {"y": 740, "x": 960, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=775fd2b4c9c52e4fb41dda044b67843649186407"}, {"y": 832, "x": 1080, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bebaaa3731d90e613fce69e064e975d399b67118"}], "s": {"y": 2442, "x": 3168, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;format=png&amp;auto=webp&amp;s=9a7db044ca109cd37398102034f0e54d39ae365a"}, "id": "hciz2m67whbc1"}}, "name": "t3_192sgnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S6G9CqW9lzwaTz1q98tNVJY5QdgfeIMzDv_T0jxdeOQ.jpg", "edited": 1704841382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704841179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently helped launch the Airflow 2023 User Survey, and thought some of you may find the results interesting, especially those that use Airflow :)&lt;/p&gt;\n\n&lt;p&gt;I even had a cool infographic made. Included a sneak peek below, but for the full thing- &lt;a href=\"https://airflow.apache.org/survey/\"&gt;check it out here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hope this is useful!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a7db044ca109cd37398102034f0e54d39ae365a\"&gt;https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a7db044ca109cd37398102034f0e54d39ae365a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "192sgnv", "is_robot_indexable": true, "report_reasons": null, "author": "BrianaGraceOkyere", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192sgnv/airflow_user_survey_infographic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192sgnv/airflow_user_survey_infographic/", "subreddit_subscribers": 151629, "created_utc": 1704841179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's use a simple use case of no windowing or anything like that. Kafka/Flink transformations are done and then data flows into a DB (such as Snowflake) to be queried. \n\nSuppose that you find out at some point that there is a bug with the streaming data transformation, and that it needs to be changed and past data needs to be re-processed. How is that handled?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do your teams handle backfill with streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192r24x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704837803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s use a simple use case of no windowing or anything like that. Kafka/Flink transformations are done and then data flows into a DB (such as Snowflake) to be queried. &lt;/p&gt;\n\n&lt;p&gt;Suppose that you find out at some point that there is a bug with the streaming data transformation, and that it needs to be changed and past data needs to be re-processed. How is that handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192r24x", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192r24x/how_do_your_teams_handle_backfill_with_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192r24x/how_do_your_teams_handle_backfill_with_streaming/", "subreddit_subscribers": 151629, "created_utc": 1704837803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, I have some experience with dbt and just wanted to know if anyone knows of any use cases for integrating the two tools together for an ELT.  In my last organization dbt was used to treat our transformation pipeline like software development to make sure it had good documentation, testing, snapshotting etc.  Would it ever make sense to use dbt and clickhouse together?", "author_fullname": "t2_maega", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt + ClickHouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1933kce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704875400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I have some experience with dbt and just wanted to know if anyone knows of any use cases for integrating the two tools together for an ELT.  In my last organization dbt was used to treat our transformation pipeline like software development to make sure it had good documentation, testing, snapshotting etc.  Would it ever make sense to use dbt and clickhouse together?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1933kce", "is_robot_indexable": true, "report_reasons": null, "author": "TheAnglerfish1616", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1933kce/dbt_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1933kce/dbt_clickhouse/", "subreddit_subscribers": 151629, "created_utc": 1704875400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI'm a data storage guy (22 years in IT. age 48). I've done some bash and Python scripting. Started playing with Pandas for some data analysis. How easy/tough would it be to get into data engineering? How's the ageism in data engineering? Are they ok with senior folks?", "author_fullname": "t2_2lbw4pfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's data engineering for a senior storage guy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1932tvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704872319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;m a data storage guy (22 years in IT. age 48). I&amp;#39;ve done some bash and Python scripting. Started playing with Pandas for some data analysis. How easy/tough would it be to get into data engineering? How&amp;#39;s the ageism in data engineering? Are they ok with senior folks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1932tvf", "is_robot_indexable": true, "report_reasons": null, "author": "pritesh_ugrankar", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1932tvf/hows_data_engineering_for_a_senior_storage_guy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1932tvf/hows_data_engineering_for_a_senior_storage_guy/", "subreddit_subscribers": 151629, "created_utc": 1704872319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tables vs int in dbt\n\nWhat are the differences between a regular table and intermediate table in dbt? \n\nI\u2019m learning by doing and while creating my tables I realised that I don\u2019t need to have all tables to be either DIMs or FCTs. \n\nThen I started searching in the internet and I felt overwhelmed tbh, specially after I came across int page and they said its not meant to be used in dashboards or exposed to end user. \n\nthen, what about the tables that I need them to study my data, build analytics around it and generate ad hoc reports?", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Types of tables in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192z6bh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704859757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tables vs int in dbt&lt;/p&gt;\n\n&lt;p&gt;What are the differences between a regular table and intermediate table in dbt? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m learning by doing and while creating my tables I realised that I don\u2019t need to have all tables to be either DIMs or FCTs. &lt;/p&gt;\n\n&lt;p&gt;Then I started searching in the internet and I felt overwhelmed tbh, specially after I came across int page and they said its not meant to be used in dashboards or exposed to end user. &lt;/p&gt;\n\n&lt;p&gt;then, what about the tables that I need them to study my data, build analytics around it and generate ad hoc reports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192z6bh", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192z6bh/types_of_tables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192z6bh/types_of_tables_in_dbt/", "subreddit_subscribers": 151629, "created_utc": 1704859757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I am looking for some help or suggestions.\n\nI have ssis script that do some calculations, while and for loops etc for each row of table and need to make the same thing in snowflake.\n\nI managed to create a bunch of procedures and function that together do the thing(as a result it inserts into temp table so i can join to it later) but only for predefined values and I need it to be executed for each row in the table.\n\n\nI also tried using dbt jinja for this purpose, but because jinja doesn't have while loop(i still have to use procedure for one part) i didn't manage to make script work with columns because it didn't render values, only column names.\n\nDo you have any ideas? I am out of them.\nThanks in advance.", "author_fullname": "t2_v3orytfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make ssis script variant in snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192q0bi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704835282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I am looking for some help or suggestions.&lt;/p&gt;\n\n&lt;p&gt;I have ssis script that do some calculations, while and for loops etc for each row of table and need to make the same thing in snowflake.&lt;/p&gt;\n\n&lt;p&gt;I managed to create a bunch of procedures and function that together do the thing(as a result it inserts into temp table so i can join to it later) but only for predefined values and I need it to be executed for each row in the table.&lt;/p&gt;\n\n&lt;p&gt;I also tried using dbt jinja for this purpose, but because jinja doesn&amp;#39;t have while loop(i still have to use procedure for one part) i didn&amp;#39;t manage to make script work with columns because it didn&amp;#39;t render values, only column names.&lt;/p&gt;\n\n&lt;p&gt;Do you have any ideas? I am out of them.\nThanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192q0bi", "is_robot_indexable": true, "report_reasons": null, "author": "awkward_period", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192q0bi/make_ssis_script_variant_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192q0bi/make_ssis_script_variant_in_snowflake/", "subreddit_subscribers": 151629, "created_utc": 1704835282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been looking at using Iceberg with Trino, but I see you need a metastore/Iceberg catalog to work with Iceberg tables. I have data in Dell ECS (can't move it to something like Amazon S3).\n\nCan anyone with Iceberg or Trino experience walk me through their thought process on choosing their Iceberg catalog? Why shouldn't I just use Hive Metastore?\n\nAny comparison on experiences with Hive Metastore or another Iceberg catalog like JDBC Catalog would be great. Thank you", "author_fullname": "t2_rl9jnw10q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg catalog for Trino?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192l2hm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704823480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking at using Iceberg with Trino, but I see you need a metastore/Iceberg catalog to work with Iceberg tables. I have data in Dell ECS (can&amp;#39;t move it to something like Amazon S3).&lt;/p&gt;\n\n&lt;p&gt;Can anyone with Iceberg or Trino experience walk me through their thought process on choosing their Iceberg catalog? Why shouldn&amp;#39;t I just use Hive Metastore?&lt;/p&gt;\n\n&lt;p&gt;Any comparison on experiences with Hive Metastore or another Iceberg catalog like JDBC Catalog would be great. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192l2hm", "is_robot_indexable": true, "report_reasons": null, "author": "Relative_Unit_7640", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192l2hm/iceberg_catalog_for_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192l2hm/iceberg_catalog_for_trino/", "subreddit_subscribers": 151629, "created_utc": 1704823480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a question wrt Meltano, as I'm considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api's on a daily basis with full overwrites to the target. Based on what I'm reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?", "author_fullname": "t2_9knk666s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch with Meltano", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192gc7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704811623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a question wrt Meltano, as I&amp;#39;m considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api&amp;#39;s on a daily basis with full overwrites to the target. Based on what I&amp;#39;m reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192gc7y", "is_robot_indexable": true, "report_reasons": null, "author": "limartje", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192gc7y/batch_with_meltano/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192gc7y/batch_with_meltano/", "subreddit_subscribers": 151629, "created_utc": 1704811623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n* I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.\n* I then download these files from GCS and perform data transformations locally.\n* Once data is cleaned, I upload it to a BigQuery table.\n* After the transformation, the local file is deleted, and the file in GCS is moved to a \"processed\" folder.\n\n**The Issue**:\n\nWhen multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)\n\n**Potential Cause**:\n\nThrough my research, it seems like this might be related to a \"race condition,\" although I\u2019m not entirely certain.\n\n**Request for Assistance**:\n\nI'm seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.\n\nCould anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?", "author_fullname": "t2_47qmqn8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Resolving File Handling Issue in Python ETL Process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192eege", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704805948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.&lt;/li&gt;\n&lt;li&gt;I then download these files from GCS and perform data transformations locally.&lt;/li&gt;\n&lt;li&gt;Once data is cleaned, I upload it to a BigQuery table.&lt;/li&gt;\n&lt;li&gt;After the transformation, the local file is deleted, and the file in GCS is moved to a &amp;quot;processed&amp;quot; folder.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Issue&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;When multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Potential Cause&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Through my research, it seems like this might be related to a &amp;quot;race condition,&amp;quot; although I\u2019m not entirely certain.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Request for Assistance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.&lt;/p&gt;\n\n&lt;p&gt;Could anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192eege", "is_robot_indexable": true, "report_reasons": null, "author": "polonium_biscuit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "subreddit_subscribers": 151629, "created_utc": 1704805948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question to all the tech leads/hiring managers/engineering managers here who work with modern data engineering stacks- \n\nQ1. What are the top 5 must know tech skills one should know to get a job in Data Engineering in 2024?\n\nQ2. What projects do you want to see in someone's resume to hire them in 2024?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Top 5 must know skills a Data Engineer should know?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1931tqo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704868394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question to all the tech leads/hiring managers/engineering managers here who work with modern data engineering stacks- &lt;/p&gt;\n\n&lt;p&gt;Q1. What are the top 5 must know tech skills one should know to get a job in Data Engineering in 2024?&lt;/p&gt;\n\n&lt;p&gt;Q2. What projects do you want to see in someone&amp;#39;s resume to hire them in 2024?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1931tqo", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1931tqo/top_5_must_know_skills_a_data_engineer_should_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1931tqo/top_5_must_know_skills_a_data_engineer_should_know/", "subreddit_subscribers": 151629, "created_utc": 1704868394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some help with dbt\n\nHi everyone I have started my learning journey in dbt. I have understood the basic concepts plus how to work with a standard dataset. But I m really confused when it comes to higher level when I m dealing with lots of table and huge data. \n\nHow can I make my models generic? \nDo I need to purely use macros for this? \nHow does a dbt project structure look like? \n\nWould be grateful if someone could guide me through this.", "author_fullname": "t2_sa16pkrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19317lt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704866275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some help with dbt&lt;/p&gt;\n\n&lt;p&gt;Hi everyone I have started my learning journey in dbt. I have understood the basic concepts plus how to work with a standard dataset. But I m really confused when it comes to higher level when I m dealing with lots of table and huge data. &lt;/p&gt;\n\n&lt;p&gt;How can I make my models generic? \nDo I need to purely use macros for this? \nHow does a dbt project structure look like? &lt;/p&gt;\n\n&lt;p&gt;Would be grateful if someone could guide me through this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19317lt", "is_robot_indexable": true, "report_reasons": null, "author": "misaaaa18", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19317lt/need_some_help_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19317lt/need_some_help_with_dbt/", "subreddit_subscribers": 151629, "created_utc": 1704866275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I originally posted this in /r/dataanalysis, but in hindsight it likely makes more sense for this audience, so asking it here as well.\n\nI work in higher education as a senior data analyst. As we have been adopting more and more external data sources (APIs, cloud-based databases, SFTP dumps), it has become clear that we need a formal ETL solution. We already have an on-premise data warehouse and staff to support it. As we start to look into whether we should buy a tool or train staff on writing custom python scripts for everything, I was hoping others at organizations might share what they do.", "author_fullname": "t2_gcxbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle ETL/ELT processes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192yp9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704858240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I originally posted this in &lt;a href=\"/r/dataanalysis\"&gt;/r/dataanalysis&lt;/a&gt;, but in hindsight it likely makes more sense for this audience, so asking it here as well.&lt;/p&gt;\n\n&lt;p&gt;I work in higher education as a senior data analyst. As we have been adopting more and more external data sources (APIs, cloud-based databases, SFTP dumps), it has become clear that we need a formal ETL solution. We already have an on-premise data warehouse and staff to support it. As we start to look into whether we should buy a tool or train staff on writing custom python scripts for everything, I was hoping others at organizations might share what they do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192yp9u", "is_robot_indexable": true, "report_reasons": null, "author": "farm3rb0b", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192yp9u/how_does_your_company_handle_etlelt_processes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192yp9u/how_does_your_company_handle_etlelt_processes/", "subreddit_subscribers": 151629, "created_utc": 1704858240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?\n\nAlso, do you guys prefer multicloud solution or does your service provide a good enough SLA?\n\nMy case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.", "author_fullname": "t2_12ff33zm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys approach High Availability and DR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192c93r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704798302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?&lt;/p&gt;\n\n&lt;p&gt;Also, do you guys prefer multicloud solution or does your service provide a good enough SLA?&lt;/p&gt;\n\n&lt;p&gt;My case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192c93r", "is_robot_indexable": true, "report_reasons": null, "author": "RobvicRJ", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "subreddit_subscribers": 151629, "created_utc": 1704798302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a junior and will be senior undergrad , my major is computer engineering, so my final thesis should be hardware related. However, I aim to be a data engineer, so I want to build a IoT streaming project. I will receive the data from iot devices via MQTT.   \nI am familiar with spark and kafka, any advice about tools and ideas that i can use as a beginner in this project and may it could benefit me in my future job as well.  \nThis [medium](https://sanchezsanchezsergio418.medium.com/iot-event-streaming-architecture-fb790c634c2f) inspired me to come up with the idea.", "author_fullname": "t2_hk3r67cz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for data engineering final thesis project with IOT application", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1934svt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704880655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a junior and will be senior undergrad , my major is computer engineering, so my final thesis should be hardware related. However, I aim to be a data engineer, so I want to build a IoT streaming project. I will receive the data from iot devices via MQTT.&lt;br/&gt;\nI am familiar with spark and kafka, any advice about tools and ideas that i can use as a beginner in this project and may it could benefit me in my future job as well.&lt;br/&gt;\nThis &lt;a href=\"https://sanchezsanchezsergio418.medium.com/iot-event-streaming-architecture-fb790c634c2f\"&gt;medium&lt;/a&gt; inspired me to come up with the idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?auto=webp&amp;s=ab083d9c859c7be494cec550975a52f31565fca4", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c09e5dc0565ec8444f41bc916eb6120b0ba94a50", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=829e4283f268b6c8dc3cb421b26d080998758841", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8ce5aacc01f74d647475d60667e0d7daa9b9534", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=52fd0134be9907dea56e8e147cc60dc1797b1b15", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=758a120d93fb317e78ba74a9e6a9dd17843cdc3e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/mlxDMcJSWs5Kv8FGtb1JJmHCTsi4nFAZH_S6XfG2uR0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e18c486e95a962cf9bf21858865c2d5c80da5520", "width": 1080, "height": 720}], "variants": {}, "id": "MGInCxRLlJU201TnP6xpsL6W1zFiurJTR2G2B9roPiU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1934svt", "is_robot_indexable": true, "report_reasons": null, "author": "FriendshipEastern291", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1934svt/looking_for_data_engineering_final_thesis_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1934svt/looking_for_data_engineering_final_thesis_project/", "subreddit_subscribers": 151629, "created_utc": 1704880655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am transitioning from data analyst to data engineer career. Usually in the technical interview process, we are asked to perform a technical assessment(live or offline) to evaluate our tech skills. \n\nWhen I was interviewing for data analyst roles, I was usually given a dataset and the following pattern-\n1. a data set and few questions to find out certain patterns or trends. \n2. A dashboard in power bi/tableau to find how I am thinking of the kpis and metrics \n3. Few questions to be done in sql from the dataset. \n4. If live coding, some basic python and sql questions \n5. A PowerPoint deck with key points and to be presented to the interview online. \n\nSimilarly, what\u2019s the pattern of technical assessments for a data engineer? I am mostly applying for roles which focus on ETL/ELT processes, big data processing like spark and Databricks, data warehousing etc. \n\nCan anyone shed any light from their previous interview experience?", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the common pattern for technical assessment for data engineer interviews?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1934jw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704879596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am transitioning from data analyst to data engineer career. Usually in the technical interview process, we are asked to perform a technical assessment(live or offline) to evaluate our tech skills. &lt;/p&gt;\n\n&lt;p&gt;When I was interviewing for data analyst roles, I was usually given a dataset and the following pattern-\n1. a data set and few questions to find out certain patterns or trends. \n2. A dashboard in power bi/tableau to find how I am thinking of the kpis and metrics \n3. Few questions to be done in sql from the dataset. \n4. If live coding, some basic python and sql questions \n5. A PowerPoint deck with key points and to be presented to the interview online. &lt;/p&gt;\n\n&lt;p&gt;Similarly, what\u2019s the pattern of technical assessments for a data engineer? I am mostly applying for roles which focus on ETL/ELT processes, big data processing like spark and Databricks, data warehousing etc. &lt;/p&gt;\n\n&lt;p&gt;Can anyone shed any light from their previous interview experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1934jw1", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1934jw1/what_is_the_common_pattern_for_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1934jw1/what_is_the_common_pattern_for_technical/", "subreddit_subscribers": 151629, "created_utc": 1704879596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm excited to share that my latest course, \"Azure Functions: Building Data-Driven Solutions With Python,\" is now live on Udemy! \ud83c\udf1f Whether you're starting your cloud journey or leveling up your skills, this course has got you covered.\n\n\ud83c\udf81 First 100 enrollments get FREE access: [Your Free Course Link](https://www.udemy.com/course/azure-functions-building-data-driven-solutions-with-python/?couponCode=EFBFF73C1572653E4597)\n\nI'd be super thankful if you support me and leave a 5-star review. Thank you!", "author_fullname": "t2_2lt1q4pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Azure Function Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1934iew", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704879417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m excited to share that my latest course, &amp;quot;Azure Functions: Building Data-Driven Solutions With Python,&amp;quot; is now live on Udemy! \ud83c\udf1f Whether you&amp;#39;re starting your cloud journey or leveling up your skills, this course has got you covered.&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf81 First 100 enrollments get FREE access: &lt;a href=\"https://www.udemy.com/course/azure-functions-building-data-driven-solutions-with-python/?couponCode=EFBFF73C1572653E4597\"&gt;Your Free Course Link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be super thankful if you support me and leave a 5-star review. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1934iew", "is_robot_indexable": true, "report_reasons": null, "author": "Kairo1004", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1934iew/free_azure_function_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1934iew/free_azure_function_course/", "subreddit_subscribers": 151629, "created_utc": 1704879417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey pairs,  \n\n\nhope you're doing well. I'm integrating dataform in a CI/CD with terraform handling provisioning.  \nI have some questions tho if you're using it in your data stack:  \n\\- How do you handle CI/CD?   \n\\- Is there a way to now have to go in the UI and apply the commits from the third party repo to the dataform GCP repo? Automatizing it?  \n\n\nThank you lots", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataform &amp; CI/CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19345cj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704877847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey pairs,  &lt;/p&gt;\n\n&lt;p&gt;hope you&amp;#39;re doing well. I&amp;#39;m integrating dataform in a CI/CD with terraform handling provisioning.&lt;br/&gt;\nI have some questions tho if you&amp;#39;re using it in your data stack:&lt;br/&gt;\n- How do you handle CI/CD?&lt;br/&gt;\n- Is there a way to now have to go in the UI and apply the commits from the third party repo to the dataform GCP repo? Automatizing it?  &lt;/p&gt;\n\n&lt;p&gt;Thank you lots&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19345cj", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19345cj/dataform_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19345cj/dataform_cicd/", "subreddit_subscribers": 151629, "created_utc": 1704877847.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}