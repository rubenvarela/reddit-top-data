{"kind": "Listing", "data": {"after": "t3_192c93r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.\n\nWe all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern\n\nBecause users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors \n\nI agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python\n\nWhy don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we are ignoring Julia for data engineering and going for rust??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192fhgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704809199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.&lt;/p&gt;\n\n&lt;p&gt;We all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern&lt;/p&gt;\n\n&lt;p&gt;Because users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors &lt;/p&gt;\n\n&lt;p&gt;I agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python&lt;/p&gt;\n\n&lt;p&gt;Why don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192fhgg", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 81, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "subreddit_subscribers": 151568, "created_utc": 1704809199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If so, how do you deal with it?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you feel the tension between \"data democratization\" and the business need for high quality curated data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192f47n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704808143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so, how do you deal with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192f47n", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "subreddit_subscribers": 151568, "created_utc": 1704808143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All of us **want to move fast**, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don't use them.\n\nWe built a quick tool that infers tests/documents for you. \n\nWhat are people's **thoughts on a tool that infers tests and docs and helps you add them to your schema definition**? \n\n&amp;#x200B;\n\n[Schema inference](https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player)", "author_fullname": "t2_q5jcx79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT test and documentation generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mv0k6icgofbc1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/DASHPlaylist.mpd?a=1707449912%2CZmVkYTNiZTI0NGQ5NjQwYjRjZTdkNDk4MGFmNzc5NThkODNhOTc3Nzk2ZmUzOWQ5MzFmODQwMGE2MWJjMTQ2Zg%3D%3D&amp;v=1&amp;f=sd", "x": 1730, "y": 1080, "hlsUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/HLSPlaylist.m3u8?a=1707449912%2CMjg1MzUyZDhjYTVlMGY5Mjc1MjQyMzQ1ZGM2ZGU4ZTkwOWI5ODljMDU3ZjBiMDFhYzc2YTkxZDgyZmQzZGI4OA%3D%3D&amp;v=1&amp;f=sd", "id": "mv0k6icgofbc1", "isGif": false}}, "name": "t3_192he2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704814481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All of us &lt;strong&gt;want to move fast&lt;/strong&gt;, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don&amp;#39;t use them.&lt;/p&gt;\n\n&lt;p&gt;We built a quick tool that infers tests/documents for you. &lt;/p&gt;\n\n&lt;p&gt;What are people&amp;#39;s &lt;strong&gt;thoughts on a tool that infers tests and docs and helps you add them to your schema definition&lt;/strong&gt;? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player\"&gt;Schema inference&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192he2z", "is_robot_indexable": true, "report_reasons": null, "author": "bk1007", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "subreddit_subscribers": 151568, "created_utc": 1704814481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was zombie-scrolling this sub after not coming here for a while and saw a post of some guy in the US who's gonna get 120K USD as entry level after watching some Udemy Courses on SQL and Python\n\nEuro poverty hitting hard\nI'm gonna need some time to recover again. See you in 10-12 months", "author_fullname": "t2_ehpb7yoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Came back here after some time just to suffer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 137, "top_awarded_type": null, "hide_score": true, "name": "t3_192we4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/5dZDfP-0UrheMEEyeieBjzGhvqYU5NNCAZwY8PSwzy8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704851470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was zombie-scrolling this sub after not coming here for a while and saw a post of some guy in the US who&amp;#39;s gonna get 120K USD as entry level after watching some Udemy Courses on SQL and Python&lt;/p&gt;\n\n&lt;p&gt;Euro poverty hitting hard\nI&amp;#39;m gonna need some time to recover again. See you in 10-12 months&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ada41lypqibc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ada41lypqibc1.jpeg?auto=webp&amp;s=d9fb9c171c6416da770940c3e62e96f941a5f141", "width": 391, "height": 385}, "resolutions": [{"url": "https://preview.redd.it/ada41lypqibc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=990e3c26e982ea65a8562b82b06dd47e9940eb84", "width": 108, "height": 106}, {"url": "https://preview.redd.it/ada41lypqibc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b8267669b6256c16ef709de5d8b7098cc4edb44", "width": 216, "height": 212}, {"url": "https://preview.redd.it/ada41lypqibc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9a75d8873cbb792918f28da415167b03055c0fe", "width": 320, "height": 315}], "variants": {}, "id": "EnqMe7IeMiiws6e_SFImMyPJ-e-2tE-dj-zAmGWU804"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "192we4k", "is_robot_indexable": true, "report_reasons": null, "author": "schizo_coder", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192we4k/came_back_here_after_some_time_just_to_suffer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ada41lypqibc1.jpeg", "subreddit_subscribers": 151568, "created_utc": 1704851470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did you know that? I'll just add that to the \"why I prefer SQL to Pandas\" points' list.  ", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You can't enforce a schema when creating a df in Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192iwrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704818275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you know that? I&amp;#39;ll just add that to the &amp;quot;why I prefer SQL to Pandas&amp;quot; points&amp;#39; list.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192iwrh", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192iwrh/you_cant_enforce_a_schema_when_creating_a_df_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192iwrh/you_cant_enforce_a_schema_when_creating_a_df_in/", "subreddit_subscribers": 151568, "created_utc": 1704818275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. \n\n&amp;#x200B;\n\nMy question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn't a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them \"Are you happy here? If not, what can we do to make it better?\" questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. \n\n&amp;#x200B;", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Data) Engineering Managers - What Makes You Stay At A Company Long", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1927hdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704779296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn&amp;#39;t a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them &amp;quot;Are you happy here? If not, what can we do to make it better?&amp;quot; questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1927hdr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "subreddit_subscribers": 151568, "created_utc": 1704779296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We're sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &gt; Meltano &gt; DBT &gt; Openmetadata &gt; Tableau with custom data models.  Our head of infrastructure is a \"Microsoft can solve everything\" guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We're now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the \"wild west\" approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We're dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?", "author_fullname": "t2_1hb8tav0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Fabric - cost and viability in a large org with shrinking budget and limited usage of MS stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192epn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704806924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We&amp;#39;re sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &amp;gt; Meltano &amp;gt; DBT &amp;gt; Openmetadata &amp;gt; Tableau with custom data models.  Our head of infrastructure is a &amp;quot;Microsoft can solve everything&amp;quot; guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We&amp;#39;re now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the &amp;quot;wild west&amp;quot; approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We&amp;#39;re dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192epn6", "is_robot_indexable": true, "report_reasons": null, "author": "FrebTheRat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "subreddit_subscribers": 151568, "created_utc": 1704806924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?  \nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using the DBT semantic layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192b61u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704793915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?&lt;br/&gt;\nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192b61u", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "subreddit_subscribers": 151568, "created_utc": 1704793915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is probably a stupid question to anyone with a deeper background in data engineering, but I'm new and learned dbt first using dbt cloud which makes orchestrating transformation pretty easy. \n\nIf I have a data warehouse and I can't use dbt, how do I go about orchestrating transformation of data after landing it in raw form in the data warehouse? BigQuery seems to have Dataform and I'm guessing the other cloud providers have something similar, but what if I'm just using an on-prem database? Do I just have a bunch of scheduled stored procedures?", "author_fullname": "t2_550fb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people orchestrating the T in ELT for their data warehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192ou1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704832493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is probably a stupid question to anyone with a deeper background in data engineering, but I&amp;#39;m new and learned dbt first using dbt cloud which makes orchestrating transformation pretty easy. &lt;/p&gt;\n\n&lt;p&gt;If I have a data warehouse and I can&amp;#39;t use dbt, how do I go about orchestrating transformation of data after landing it in raw form in the data warehouse? BigQuery seems to have Dataform and I&amp;#39;m guessing the other cloud providers have something similar, but what if I&amp;#39;m just using an on-prem database? Do I just have a bunch of scheduled stored procedures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192ou1k", "is_robot_indexable": true, "report_reasons": null, "author": "PureOhms", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192ou1k/how_are_people_orchestrating_the_t_in_elt_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192ou1k/how_are_people_orchestrating_the_t_in_elt_for/", "subreddit_subscribers": 151568, "created_utc": 1704832493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nI'm currently working in a new open source project called DataFlint .([https://github.com/dataflint/spark](https://github.com/dataflint/spark)) to do performance monitoring better in Apache Spark.\n\nI'm working on creating a new feature for monitoring resource allocation ( i.e. how many executors do you need, and how many CPU/Memory each executor needs), so it would be easier to tune your Apache Spark resource allocation.\n\n&amp;#x200B;\n\nSo while making this feature I was wondering - how do you usually decide how many resources to give a job? on what metrics do you look? which configs do you play with?\n\n&amp;#x200B;\n\nI released an initial version of this feature and looks like this:\n\nhttps://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45\n\nit contains a graph of the number of executors, the min/max executors (if using dynamic allocation), which query ran, and some relevant configs.\n\nWDYT?", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you decide how many resources to give an Apache Spark job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"41qbfiv6qgbc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63e154617bee6f9d91a697c94a56112a9c355c78"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7133befe2d3f988c7d01619b33ee57255e467ea6"}, {"y": 191, "x": 320, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ea4dd00f02057a058d7229e44aef26a8b171944"}, {"y": 383, "x": 640, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca309ecc3ce66dddcb2b7377076ba46d0a5e53dd"}, {"y": 575, "x": 960, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a58fd54e378fb9fb6e4899cbb22854c87288ef99"}, {"y": 647, "x": 1080, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a82815521f8704e06249340feeb5f035020dcdd"}], "s": {"y": 1760, "x": 2936, "u": "https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45"}, "id": "41qbfiv6qgbc1"}}, "name": "t3_192muug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AezvY7fMP8doLXTsi_G8Z8F4Tn4-jS7gnbLgMIJwhBA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704827778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working in a new open source project called DataFlint .(&lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;) to do performance monitoring better in Apache Spark.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on creating a new feature for monitoring resource allocation ( i.e. how many executors do you need, and how many CPU/Memory each executor needs), so it would be easier to tune your Apache Spark resource allocation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So while making this feature I was wondering - how do you usually decide how many resources to give a job? on what metrics do you look? which configs do you play with?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I released an initial version of this feature and looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45\"&gt;https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;it contains a graph of the number of executors, the min/max executors (if using dynamic allocation), which query ran, and some relevant configs.&lt;/p&gt;\n\n&lt;p&gt;WDYT?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?auto=webp&amp;s=a1663328cfda62d121dd3c35d545c7154f598f94", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4e8132c91b474e33b64e9468995d4cc1462f092", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7010b91ae5f970f49e95f0f17ab12305a316fa41", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea61b0ba3918f27a1e06a272145364b2a29116b2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=674e0a743cab1aebdda9c72a1ca77e724b8445d9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5e24b2de2c1f04042e397985e6b67f560bb6091", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Sv50pQCTIQ0Rf8MO8FwWxzhAHyj4iVOKP09PBXUlicU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d77199ece2473fe34677bac5cdf8dfb606a4037f", "width": 1080, "height": 540}], "variants": {}, "id": "fo-5dKHfVMLxEc3zQneZtZO1K1S_XEWh5A5Sl5GzQDY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192muug", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192muug/how_do_you_decide_how_many_resources_to_give_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192muug/how_do_you_decide_how_many_resources_to_give_an/", "subreddit_subscribers": 151568, "created_utc": 1704827778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.\n\nThis will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.", "author_fullname": "t2_vlvq8b3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what graph DB to use for knowledge graphs for our LLM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192abwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704790358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.&lt;/p&gt;\n\n&lt;p&gt;This will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192abwa", "is_robot_indexable": true, "report_reasons": null, "author": "0xkiichiro", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "subreddit_subscribers": 151568, "created_utc": 1704790358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some excel files and I need to do some data quality checks using python\n\n1) Null count and Treatment\n    if a particular column has &gt;25% , it should throw an email\n\n2) Duplicate Treatment\n\n3) Accepted values in Column \n   e.g State column should have only 50 states\n\n4) Expected Datatype check for columns\n\n5) Variance check for Numeric column compared to previous month \n\netc\n\nFor now I have inplemented using pandas inbuilt functions.\n\n\nI thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.", "author_fullname": "t2_l6b7kbp7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality python Modules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19253mf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704771759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some excel files and I need to do some data quality checks using python&lt;/p&gt;\n\n&lt;p&gt;1) Null count and Treatment\n    if a particular column has &amp;gt;25% , it should throw an email&lt;/p&gt;\n\n&lt;p&gt;2) Duplicate Treatment&lt;/p&gt;\n\n&lt;p&gt;3) Accepted values in Column \n   e.g State column should have only 50 states&lt;/p&gt;\n\n&lt;p&gt;4) Expected Datatype check for columns&lt;/p&gt;\n\n&lt;p&gt;5) Variance check for Numeric column compared to previous month &lt;/p&gt;\n\n&lt;p&gt;etc&lt;/p&gt;\n\n&lt;p&gt;For now I have inplemented using pandas inbuilt functions.&lt;/p&gt;\n\n&lt;p&gt;I thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19253mf", "is_robot_indexable": true, "report_reasons": null, "author": "vainothisside", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19253mf/data_quality_python_modules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19253mf/data_quality_python_modules/", "subreddit_subscribers": 151568, "created_utc": 1704771759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently helped launch the Airflow 2023 User Survey, and thought some of you may find the results interesting, especially those that use Airflow :)\n\nI even had a cool infographic made. Included a sneak peek below, but for the full thing- [check it out here](https://airflow.apache.org/survey/)\n\nHope this is useful!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;format=png&amp;auto=webp&amp;s=9a7db044ca109cd37398102034f0e54d39ae365a\n\n&amp;#x200B;", "author_fullname": "t2_l5gu8nhgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow User Survey Infographic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hciz2m67whbc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3e09454778390c7ce578aab35d0fcaea35f497a"}, {"y": 166, "x": 216, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a94f97ab4f80708fef7d14b983d4cbf92154e040"}, {"y": 246, "x": 320, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9c42dd627f9fe78177c9ac6a9dbf52f0919481"}, {"y": 493, "x": 640, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94f3180b991467baad0827a1845c6e243a6dc2d5"}, {"y": 740, "x": 960, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=775fd2b4c9c52e4fb41dda044b67843649186407"}, {"y": 832, "x": 1080, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bebaaa3731d90e613fce69e064e975d399b67118"}], "s": {"y": 2442, "x": 3168, "u": "https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;format=png&amp;auto=webp&amp;s=9a7db044ca109cd37398102034f0e54d39ae365a"}, "id": "hciz2m67whbc1"}}, "name": "t3_192sgnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S6G9CqW9lzwaTz1q98tNVJY5QdgfeIMzDv_T0jxdeOQ.jpg", "edited": 1704841382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704841179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently helped launch the Airflow 2023 User Survey, and thought some of you may find the results interesting, especially those that use Airflow :)&lt;/p&gt;\n\n&lt;p&gt;I even had a cool infographic made. Included a sneak peek below, but for the full thing- &lt;a href=\"https://airflow.apache.org/survey/\"&gt;check it out here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hope this is useful!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a7db044ca109cd37398102034f0e54d39ae365a\"&gt;https://preview.redd.it/hciz2m67whbc1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a7db044ca109cd37398102034f0e54d39ae365a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "192sgnv", "is_robot_indexable": true, "report_reasons": null, "author": "BrianaGraceOkyere", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192sgnv/airflow_user_survey_infographic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192sgnv/airflow_user_survey_infographic/", "subreddit_subscribers": 151568, "created_utc": 1704841179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n* I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.\n* I then download these files from GCS and perform data transformations locally.\n* Once data is cleaned, I upload it to a BigQuery table.\n* After the transformation, the local file is deleted, and the file in GCS is moved to a \"processed\" folder.\n\n**The Issue**:\n\nWhen multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)\n\n**Potential Cause**:\n\nThrough my research, it seems like this might be related to a \"race condition,\" although I\u2019m not entirely certain.\n\n**Request for Assistance**:\n\nI'm seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.\n\nCould anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?", "author_fullname": "t2_47qmqn8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Resolving File Handling Issue in Python ETL Process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192eege", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704805948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.&lt;/li&gt;\n&lt;li&gt;I then download these files from GCS and perform data transformations locally.&lt;/li&gt;\n&lt;li&gt;Once data is cleaned, I upload it to a BigQuery table.&lt;/li&gt;\n&lt;li&gt;After the transformation, the local file is deleted, and the file in GCS is moved to a &amp;quot;processed&amp;quot; folder.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Issue&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;When multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Potential Cause&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Through my research, it seems like this might be related to a &amp;quot;race condition,&amp;quot; although I\u2019m not entirely certain.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Request for Assistance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.&lt;/p&gt;\n\n&lt;p&gt;Could anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192eege", "is_robot_indexable": true, "report_reasons": null, "author": "polonium_biscuit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "subreddit_subscribers": 151568, "created_utc": 1704805948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the products my company offers requires customers to submit export files from various platforms to us. From there, our current process is to transform the data in excel and import into a MySQL database. Yuck.\n\nEach system exports similar data in the same shape, but of course, headers differ, customers use different values to mean the same thing, etc. Super simple transformations. However, it doesn\u2019t make any sense to make an ETL for each one because these files are used only once.\n\nWhat I am looking for is a tool that would allow someone who isn\u2019t super technical to upload one of these files, select a preset target, select what each header means, make any necessary transformations, and check for errors/missing values.\n\nI haven\u2019t been able to find anything that does this short of building it myself. If it doesn\u2019t, I\u2019m thinking about building it on my own.\n\nWe have a long term solution planned for this whole process so it doesn\u2019t make sense to build it out for the company.", "author_fullname": "t2_m0ki4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this tool exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192sv8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704842154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the products my company offers requires customers to submit export files from various platforms to us. From there, our current process is to transform the data in excel and import into a MySQL database. Yuck.&lt;/p&gt;\n\n&lt;p&gt;Each system exports similar data in the same shape, but of course, headers differ, customers use different values to mean the same thing, etc. Super simple transformations. However, it doesn\u2019t make any sense to make an ETL for each one because these files are used only once.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is a tool that would allow someone who isn\u2019t super technical to upload one of these files, select a preset target, select what each header means, make any necessary transformations, and check for errors/missing values.&lt;/p&gt;\n\n&lt;p&gt;I haven\u2019t been able to find anything that does this short of building it myself. If it doesn\u2019t, I\u2019m thinking about building it on my own.&lt;/p&gt;\n\n&lt;p&gt;We have a long term solution planned for this whole process so it doesn\u2019t make sense to build it out for the company.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192sv8t", "is_robot_indexable": true, "report_reasons": null, "author": "phonyfakeorreal", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192sv8t/does_this_tool_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192sv8t/does_this_tool_exist/", "subreddit_subscribers": 151568, "created_utc": 1704842154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's use a simple use case of no windowing or anything like that. Kafka/Flink transformations are done and then data flows into a DB (such as Snowflake) to be queried. \n\nSuppose that you find out at some point that there is a bug with the streaming data transformation, and that it needs to be changed and past data needs to be re-processed. How is that handled?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do your teams handle backfill with streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192r24x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704837803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s use a simple use case of no windowing or anything like that. Kafka/Flink transformations are done and then data flows into a DB (such as Snowflake) to be queried. &lt;/p&gt;\n\n&lt;p&gt;Suppose that you find out at some point that there is a bug with the streaming data transformation, and that it needs to be changed and past data needs to be re-processed. How is that handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192r24x", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192r24x/how_do_your_teams_handle_backfill_with_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192r24x/how_do_your_teams_handle_backfill_with_streaming/", "subreddit_subscribers": 151568, "created_utc": 1704837803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I am looking for some help or suggestions.\n\nI have ssis script that do some calculations, while and for loops etc for each row of table and need to make the same thing in snowflake.\n\nI managed to create a bunch of procedures and function that together do the thing(as a result it inserts into temp table so i can join to it later) but only for predefined values and I need it to be executed for each row in the table.\n\n\nI also tried using dbt jinja for this purpose, but because jinja doesn't have while loop(i still have to use procedure for one part) i didn't manage to make script work with columns because it didn't render values, only column names.\n\nDo you have any ideas? I am out of them.\nThanks in advance.", "author_fullname": "t2_v3orytfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make ssis script variant in snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192q0bi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704835282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I am looking for some help or suggestions.&lt;/p&gt;\n\n&lt;p&gt;I have ssis script that do some calculations, while and for loops etc for each row of table and need to make the same thing in snowflake.&lt;/p&gt;\n\n&lt;p&gt;I managed to create a bunch of procedures and function that together do the thing(as a result it inserts into temp table so i can join to it later) but only for predefined values and I need it to be executed for each row in the table.&lt;/p&gt;\n\n&lt;p&gt;I also tried using dbt jinja for this purpose, but because jinja doesn&amp;#39;t have while loop(i still have to use procedure for one part) i didn&amp;#39;t manage to make script work with columns because it didn&amp;#39;t render values, only column names.&lt;/p&gt;\n\n&lt;p&gt;Do you have any ideas? I am out of them.\nThanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192q0bi", "is_robot_indexable": true, "report_reasons": null, "author": "awkward_period", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192q0bi/make_ssis_script_variant_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192q0bi/make_ssis_script_variant_in_snowflake/", "subreddit_subscribers": 151568, "created_utc": 1704835282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been looking at using Iceberg with Trino, but I see you need a metastore/Iceberg catalog to work with Iceberg tables. I have data in Dell ECS (can't move it to something like Amazon S3).\n\nCan anyone with Iceberg or Trino experience walk me through their thought process on choosing their Iceberg catalog? Why shouldn't I just use Hive Metastore?\n\nAny comparison on experiences with Hive Metastore or another Iceberg catalog like JDBC Catalog would be great. Thank you", "author_fullname": "t2_rl9jnw10q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg catalog for Trino?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192l2hm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704823480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking at using Iceberg with Trino, but I see you need a metastore/Iceberg catalog to work with Iceberg tables. I have data in Dell ECS (can&amp;#39;t move it to something like Amazon S3).&lt;/p&gt;\n\n&lt;p&gt;Can anyone with Iceberg or Trino experience walk me through their thought process on choosing their Iceberg catalog? Why shouldn&amp;#39;t I just use Hive Metastore?&lt;/p&gt;\n\n&lt;p&gt;Any comparison on experiences with Hive Metastore or another Iceberg catalog like JDBC Catalog would be great. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192l2hm", "is_robot_indexable": true, "report_reasons": null, "author": "Relative_Unit_7640", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192l2hm/iceberg_catalog_for_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192l2hm/iceberg_catalog_for_trino/", "subreddit_subscribers": 151568, "created_utc": 1704823480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a question wrt Meltano, as I'm considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api's on a daily basis with full overwrites to the target. Based on what I'm reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?", "author_fullname": "t2_9knk666s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch with Meltano", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192gc7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704811623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a question wrt Meltano, as I&amp;#39;m considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api&amp;#39;s on a daily basis with full overwrites to the target. Based on what I&amp;#39;m reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192gc7y", "is_robot_indexable": true, "report_reasons": null, "author": "limartje", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192gc7y/batch_with_meltano/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192gc7y/batch_with_meltano/", "subreddit_subscribers": 151568, "created_utc": 1704811623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI am currently a 3rd year College Student with a keen interest in pursuing a career in data engineering. I've been researching about the field and its prospects in India, especially for someone entering as a fresher.\n\nI would greatly appreciate insights and advice from the **experienced members** of this community. a few questions:\n\n1. **Current Scenario in India:** What is the current state of the data engineering job market in India? Are there ample opportunities for entry-level positions?\n2. **In-Demand Skills:** As a fresher, what specific skills or technologies should I focus on to make myself more marketable in the field?\n3. **Industry Preferences:** Are there specific industries or sectors in India that have a higher demand for data engineers? Any emerging trends worth noting?\n4. **Certifications:** Are there any certifications that are highly regarded in the Indian job market for data engineering roles?\n5. **Advice for Freshers:** What advice would you give to someone like me who is just starting out in their data engineering journey in India?\n6. **Networking Opportunities:** Are there any local or online communities, meetups, or events in India that you recommend for networking and staying updated on industry trends?", "author_fullname": "t2_qf3p8az34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring the Scope of Data Engineering in India as a Fresher: Seeking Advice and Insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_192wo0c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704852236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a 3rd year College Student with a keen interest in pursuing a career in data engineering. I&amp;#39;ve been researching about the field and its prospects in India, especially for someone entering as a fresher.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate insights and advice from the &lt;strong&gt;experienced members&lt;/strong&gt; of this community. a few questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Current Scenario in India:&lt;/strong&gt; What is the current state of the data engineering job market in India? Are there ample opportunities for entry-level positions?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;In-Demand Skills:&lt;/strong&gt; As a fresher, what specific skills or technologies should I focus on to make myself more marketable in the field?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Industry Preferences:&lt;/strong&gt; Are there specific industries or sectors in India that have a higher demand for data engineers? Any emerging trends worth noting?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Certifications:&lt;/strong&gt; Are there any certifications that are highly regarded in the Indian job market for data engineering roles?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Advice for Freshers:&lt;/strong&gt; What advice would you give to someone like me who is just starting out in their data engineering journey in India?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Networking Opportunities:&lt;/strong&gt; Are there any local or online communities, meetups, or events in India that you recommend for networking and staying updated on industry trends?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192wo0c", "is_robot_indexable": true, "report_reasons": null, "author": "Aj412803", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192wo0c/exploring_the_scope_of_data_engineering_in_india/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192wo0c/exploring_the_scope_of_data_engineering_in_india/", "subreddit_subscribers": 151568, "created_utc": 1704852236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm torn between acquiring an AWS or GCP certification. I've looked through past posts of similar discussion topics and I understand that cloud skills are transferrable, but in this challenging job market I'd like to make myself stand out to recruiters while trying to break into a DE role.\n\n**GCP**: I have the most experience working with it at work and through personal projects. Any experience I gain can potentially be transferable to my current job (hybrid on-prem &amp; cloud). Seems to have the smallest market share in North America, and thus less job opportunities.\n\n**AWS**: has the largest market share in NA and more job opportunities. I have no hands on experience working with it.\n\n&amp;#x200B;\n\nSome background about me: I have an MEng with a focus on Data Science/Analytics, with almost 3 years of DA experience working as the only data specialist at a small org based in Canada. I've been reading/studying DE and worked on DE pet projects outside of work for the past 1.5 years. Due to limited growth opportunities in my current role, I've been applying for DE positions the past 6 months with limited callback success for interviews, but I did make it to the final round for one company. Currently, looking to grab some DE certifications to boost my credentials.  \n\n\nI'd appreciate any advice related to certifications or career expectations!", "author_fullname": "t2_hs2u9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS or GCP certification - NA job market value?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192v7q3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704848199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m torn between acquiring an AWS or GCP certification. I&amp;#39;ve looked through past posts of similar discussion topics and I understand that cloud skills are transferrable, but in this challenging job market I&amp;#39;d like to make myself stand out to recruiters while trying to break into a DE role.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GCP&lt;/strong&gt;: I have the most experience working with it at work and through personal projects. Any experience I gain can potentially be transferable to my current job (hybrid on-prem &amp;amp; cloud). Seems to have the smallest market share in North America, and thus less job opportunities.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AWS&lt;/strong&gt;: has the largest market share in NA and more job opportunities. I have no hands on experience working with it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some background about me: I have an MEng with a focus on Data Science/Analytics, with almost 3 years of DA experience working as the only data specialist at a small org based in Canada. I&amp;#39;ve been reading/studying DE and worked on DE pet projects outside of work for the past 1.5 years. Due to limited growth opportunities in my current role, I&amp;#39;ve been applying for DE positions the past 6 months with limited callback success for interviews, but I did make it to the final round for one company. Currently, looking to grab some DE certifications to boost my credentials.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any advice related to certifications or career expectations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "192v7q3", "is_robot_indexable": true, "report_reasons": null, "author": "SpacExclamationmark", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192v7q3/aws_or_gcp_certification_na_job_market_value/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192v7q3/aws_or_gcp_certification_na_job_market_value/", "subreddit_subscribers": 151568, "created_utc": 1704848199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software engineer working in a company that is a online store with more than 100k users. The company wants to implement a page of all the payments for the sellers, so they can view in a paginated table format and they can export everything to CSV or excel. They can also filter payments in the table.\n\nThe infrastructure of the company is based on Mongodb and Nodejs. This page would make a lot of lookups in Mongodb to gather collections of payments, products, users, and so on. In the total it would be 4 lookups on collections with millions of documents. I think that should cause problems in production database because of the heavy queries and we will have a lot of sellers trying to export everything or to consult payments in the page.\n\nAre data warehouses or data lakes suitable for this specific use case? What would you recommend? My understanding, from studying data warehousing, is that they are primarily used by a company's internal employees for business intelligence or data science purposes. However, I haven't come across information about whether system users, as opposed to internal employees, also utilize these resources.", "author_fullname": "t2_tp9d9zor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does data warehouse serve my system end-users?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192ui49", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704846308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software engineer working in a company that is a online store with more than 100k users. The company wants to implement a page of all the payments for the sellers, so they can view in a paginated table format and they can export everything to CSV or excel. They can also filter payments in the table.&lt;/p&gt;\n\n&lt;p&gt;The infrastructure of the company is based on Mongodb and Nodejs. This page would make a lot of lookups in Mongodb to gather collections of payments, products, users, and so on. In the total it would be 4 lookups on collections with millions of documents. I think that should cause problems in production database because of the heavy queries and we will have a lot of sellers trying to export everything or to consult payments in the page.&lt;/p&gt;\n\n&lt;p&gt;Are data warehouses or data lakes suitable for this specific use case? What would you recommend? My understanding, from studying data warehousing, is that they are primarily used by a company&amp;#39;s internal employees for business intelligence or data science purposes. However, I haven&amp;#39;t come across information about whether system users, as opposed to internal employees, also utilize these resources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192ui49", "is_robot_indexable": true, "report_reasons": null, "author": "gsharpchord", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192ui49/does_data_warehouse_serve_my_system_endusers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192ui49/does_data_warehouse_serve_my_system_endusers/", "subreddit_subscribers": 151568, "created_utc": 1704846308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings,  \n\n\nThis is my first attempt at messing around with graph databases/NEO4J, so excuse my noob questions. \n\n  \nI am working on a project where I am loading blockchain data into a Neo4J database. Specifically, I have one node:  \n\\- Wallets \n\nAnd the following relationships:  \n\\- Transfer (indicates a wallet sent to another wallet).  \n\n\nBecause of the sheer size of the blockchain, and the growing list of transactions, I am need of some more expertise in the graph database space to help model my database more efficiently and overcome some challenges I'm facing..   \n\n\nCurrently, I have approx. 350 million Wallet nodes (distinct 'accounts'), and over 3.5 million Token nodes (UNUSED). The relationships on the other hand are in the billions, and the relationships simply dictate \\[wallet\\] -&gt; transfer-&gt; \\[wallet\\] where the transfer has properties/\"metadata\" that includes information such as the amount sent, and what what was sent (the token, BUT it is not linked to any token nodes to keep things simple), as well as other blockchain details specifically related to the transfer (hash, etc)..  \n\n\nThe problem I am encountering is 'supernodes' and extremely deep traversals when executing cypher queries for specific wallets. This is especially prevalent in blockchain data where you may have one wallet that many other wallets send/receive from - for example, a crypto currency exchange wallet, which has millions of in/out transfers. Despite my dedicated machine with very performant specs (newest gen intel server CPU, 128gb mem, nvme), queries - especially for active wallets - are extremely non performant due to the sheer number of relationships.   \n\n\nIdeally, I need a way to optimize my dataset or refactor my data model, or maybe even a new database, but I'm honestly kind of stumped. I am particularly interested in the relationships on how wallets interact with others, in order to identify fraudulent activity and other features in which graph databases shine. Moreover, I want to keep my data model simple as to make it easy to traverse relationships.   \n\n\nAppreciate any insight or help! ", "author_fullname": "t2_tnp9l7hm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data &amp; Graph Database Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192s3r3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704840328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings,  &lt;/p&gt;\n\n&lt;p&gt;This is my first attempt at messing around with graph databases/NEO4J, so excuse my noob questions. &lt;/p&gt;\n\n&lt;p&gt;I am working on a project where I am loading blockchain data into a Neo4J database. Specifically, I have one node:&lt;br/&gt;\n- Wallets &lt;/p&gt;\n\n&lt;p&gt;And the following relationships:&lt;br/&gt;\n- Transfer (indicates a wallet sent to another wallet).  &lt;/p&gt;\n\n&lt;p&gt;Because of the sheer size of the blockchain, and the growing list of transactions, I am need of some more expertise in the graph database space to help model my database more efficiently and overcome some challenges I&amp;#39;m facing..   &lt;/p&gt;\n\n&lt;p&gt;Currently, I have approx. 350 million Wallet nodes (distinct &amp;#39;accounts&amp;#39;), and over 3.5 million Token nodes (UNUSED). The relationships on the other hand are in the billions, and the relationships simply dictate [wallet] -&amp;gt; transfer-&amp;gt; [wallet] where the transfer has properties/&amp;quot;metadata&amp;quot; that includes information such as the amount sent, and what what was sent (the token, BUT it is not linked to any token nodes to keep things simple), as well as other blockchain details specifically related to the transfer (hash, etc)..  &lt;/p&gt;\n\n&lt;p&gt;The problem I am encountering is &amp;#39;supernodes&amp;#39; and extremely deep traversals when executing cypher queries for specific wallets. This is especially prevalent in blockchain data where you may have one wallet that many other wallets send/receive from - for example, a crypto currency exchange wallet, which has millions of in/out transfers. Despite my dedicated machine with very performant specs (newest gen intel server CPU, 128gb mem, nvme), queries - especially for active wallets - are extremely non performant due to the sheer number of relationships.   &lt;/p&gt;\n\n&lt;p&gt;Ideally, I need a way to optimize my dataset or refactor my data model, or maybe even a new database, but I&amp;#39;m honestly kind of stumped. I am particularly interested in the relationships on how wallets interact with others, in order to identify fraudulent activity and other features in which graph databases shine. Moreover, I want to keep my data model simple as to make it easy to traverse relationships.   &lt;/p&gt;\n\n&lt;p&gt;Appreciate any insight or help! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192s3r3", "is_robot_indexable": true, "report_reasons": null, "author": "PierreLemons", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192s3r3/big_data_graph_database_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192s3r3/big_data_graph_database_help/", "subreddit_subscribers": 151568, "created_utc": 1704840328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering! \n\nWe figured out how to scale GitHub repos to handle large files. One of the side effects of this is that you can use GitHub actions to run data ETL pipelines and store the results in the same repo as your ETL code. No need to manage S3 credentials or setup a bunch of extra tools (e.g. logging / observability).\n\nI wrote a short tutorial here: [https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions](https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions)\n\nThis is our starter repo if you want to try this yourself:\n\n[https://github.com/xetdata/easy-etl-template](https://github.com/xetdata/easy-etl-template)", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Run ETL Pipelines &amp; Store Data in your GitHub Repos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192mvcy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704827812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;We figured out how to scale GitHub repos to handle large files. One of the side effects of this is that you can use GitHub actions to run data ETL pipelines and store the results in the same repo as your ETL code. No need to manage S3 credentials or setup a bunch of extra tools (e.g. logging / observability).&lt;/p&gt;\n\n&lt;p&gt;I wrote a short tutorial here: &lt;a href=\"https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions\"&gt;https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is our starter repo if you want to try this yourself:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/xetdata/easy-etl-template\"&gt;https://github.com/xetdata/easy-etl-template&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?auto=webp&amp;s=ce2361d4419eab5a684702b36320b36c751b0021", "width": 999, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1938fefade0e4a0fc98d7373c5455998e6223552", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=647cf0aa213c535606e7a48d14a2fe5a7c0709fc", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21d2e3f28492f9d75297b14dd36d9ab3cc6f9c60", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2f4bd8dee603b2d9d3db569fff287d5846397e1", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/sdYV6ZkEi_gzGQ-MXB_vGnBrF8i34Rt_s57g3ZAxNbA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=787d94858e6e76f04ebb1524aa3562627ddb87f7", "width": 960, "height": 576}], "variants": {}, "id": "MZUbycikHgdIhqG0Jr7OWKJ4DUAVKQTfRwDuWOGvtow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "192mvcy", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192mvcy/run_etl_pipelines_store_data_in_your_github_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192mvcy/run_etl_pipelines_store_data_in_your_github_repos/", "subreddit_subscribers": 151568, "created_utc": 1704827812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?\n\nAlso, do you guys prefer multicloud solution or does your service provide a good enough SLA?\n\nMy case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.", "author_fullname": "t2_12ff33zm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys approach High Availability and DR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192c93r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704798302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?&lt;/p&gt;\n\n&lt;p&gt;Also, do you guys prefer multicloud solution or does your service provide a good enough SLA?&lt;/p&gt;\n\n&lt;p&gt;My case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192c93r", "is_robot_indexable": true, "report_reasons": null, "author": "RobvicRJ", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "subreddit_subscribers": 151568, "created_utc": 1704798302.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}