{"kind": "Listing", "data": {"after": "t3_199mah5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm coming up on 1.5 YoE at my job where my title is \"data analyst\". This is my first real job and I got it out of college. Up until today, I assumed that I was a data analyst doing data analysty things and building a career in data analytics. However, since finding out that data engineering is a separate thing, I've started to suspect that I may actually be working in an entry-level data engineering role.\n\nThe job description asked for mastery of Tableau and proficiency in Python. Since starting, I've used Python for scripting a fair amount, but have used Tableau EDA a grand total of zero times. They trained me up in Alteryx, an ETL tool, and now my work mainly consists of Alteryx, SQL, and Python.\n\n90% of my work is building automated data pipelines for other teams; they come to us with some process that they're doing manually in Excel and we make it automatic for them. We follow an Agile framework, gather requirements, build and test, deploy and support. Our typical end product is an app that another team uses, not a dashboard.\n\nAm I actually a trainee data engineer? ", "author_fullname": "t2_dkj7fy8k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did I get bamboozled into a data engineering job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199fg7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 111, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 111, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705545828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m coming up on 1.5 YoE at my job where my title is &amp;quot;data analyst&amp;quot;. This is my first real job and I got it out of college. Up until today, I assumed that I was a data analyst doing data analysty things and building a career in data analytics. However, since finding out that data engineering is a separate thing, I&amp;#39;ve started to suspect that I may actually be working in an entry-level data engineering role.&lt;/p&gt;\n\n&lt;p&gt;The job description asked for mastery of Tableau and proficiency in Python. Since starting, I&amp;#39;ve used Python for scripting a fair amount, but have used Tableau EDA a grand total of zero times. They trained me up in Alteryx, an ETL tool, and now my work mainly consists of Alteryx, SQL, and Python.&lt;/p&gt;\n\n&lt;p&gt;90% of my work is building automated data pipelines for other teams; they come to us with some process that they&amp;#39;re doing manually in Excel and we make it automatic for them. We follow an Agile framework, gather requirements, build and test, deploy and support. Our typical end product is an app that another team uses, not a dashboard.&lt;/p&gt;\n\n&lt;p&gt;Am I actually a trainee data engineer? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199fg7g", "is_robot_indexable": true, "report_reasons": null, "author": "WarCrimeWizard", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199fg7g/did_i_get_bamboozled_into_a_data_engineering_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199fg7g/did_i_get_bamboozled_into_a_data_engineering_job/", "subreddit_subscribers": 153770, "created_utc": 1705545828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y'all do while waiting for something to run?", "author_fullname": "t2_a2lh9x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you spend your time waiting for things to run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199lx3r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705568611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y&amp;#39;all do while waiting for something to run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199lx3r", "is_robot_indexable": true, "report_reasons": null, "author": "an27725", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199lx3r/how_do_you_spend_your_time_waiting_for_things_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199lx3r/how_do_you_spend_your_time_waiting_for_things_to/", "subreddit_subscribers": 153770, "created_utc": 1705568611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I\u2019m a mechanical engineer working currently as a mechanical designer. I do HVAC/plumbing system design and analysis. My company is a consulting firm and everyone here has architectural engineering degrees, so I feel out of place. It\u2019s also a really small office and I don\u2019t like it nor do I feel comfortable here. I\u2019ve been working there for 7 months. It is my first job out of college and I\u2019ve realized I want to do something more concentrated in data/analysis. My favorite classes as an engineering student were linear algebra, differential equations, statistics etc. Are there online certifications/programs out there that will help expand my skills and are worth the money and time so I can get a new job? I\u2019d like to avoid going back to school because I don\u2019t want more debt.", "author_fullname": "t2_rcat5shey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I get into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199dfy5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705540087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I\u2019m a mechanical engineer working currently as a mechanical designer. I do HVAC/plumbing system design and analysis. My company is a consulting firm and everyone here has architectural engineering degrees, so I feel out of place. It\u2019s also a really small office and I don\u2019t like it nor do I feel comfortable here. I\u2019ve been working there for 7 months. It is my first job out of college and I\u2019ve realized I want to do something more concentrated in data/analysis. My favorite classes as an engineering student were linear algebra, differential equations, statistics etc. Are there online certifications/programs out there that will help expand my skills and are worth the money and time so I can get a new job? I\u2019d like to avoid going back to school because I don\u2019t want more debt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199dfy5", "is_robot_indexable": true, "report_reasons": null, "author": "Serious-Look1597", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199dfy5/how_can_i_get_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199dfy5/how_can_i_get_into_data_engineering/", "subreddit_subscribers": 153770, "created_utc": 1705540087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 15+ year exp in DW engg and 8+ yrs in cloud aws with big data tools  including spark and data bricks.  I also have few AWS certifications and cloudera certifications but they are not current now.\n\nI recently got laid off in November and have been looking for opportunities.  seems like now a days all the buzz is ML and AI which i am lacking.  Question to fellow Engineers is it better to learn additional tools/platforms like Snowflake + aws professional level certs or jump into AI/ML learning which i got to say is not natural to me i am finding it difficult to learn.  Any advice\n\n&amp;#x200B;", "author_fullname": "t2_j52nsle3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "laid off and looking for next best option", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199n5mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705573849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 15+ year exp in DW engg and 8+ yrs in cloud aws with big data tools  including spark and data bricks.  I also have few AWS certifications and cloudera certifications but they are not current now.&lt;/p&gt;\n\n&lt;p&gt;I recently got laid off in November and have been looking for opportunities.  seems like now a days all the buzz is ML and AI which i am lacking.  Question to fellow Engineers is it better to learn additional tools/platforms like Snowflake + aws professional level certs or jump into AI/ML learning which i got to say is not natural to me i am finding it difficult to learn.  Any advice&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199n5mr", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Fee-4006", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199n5mr/laid_off_and_looking_for_next_best_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199n5mr/laid_off_and_looking_for_next_best_option/", "subreddit_subscribers": 153770, "created_utc": 1705573849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "   \nI've read a lot about data pipelines, I'd like to learn how to build them but I'm not sure what tools I should use. What do I need to know/learn in order to complete this task successfully?    \nAll I find is mostly generalized information nothing concrete and specific, could someone with experience help?   \nThanks.", "author_fullname": "t2_jrkpwx27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline for begunners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1994mwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705518174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read a lot about data pipelines, I&amp;#39;d like to learn how to build them but I&amp;#39;m not sure what tools I should use. What do I need to know/learn in order to complete this task successfully?&lt;br/&gt;\nAll I find is mostly generalized information nothing concrete and specific, could someone with experience help?&lt;br/&gt;\nThanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1994mwj", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane-Research9306", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1994mwj/data_pipeline_for_begunners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1994mwj/data_pipeline_for_begunners/", "subreddit_subscribers": 153770, "created_utc": 1705518174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have good strategies around keeping metabase organized?  \n\nWhat seems to happen is we get a lot of similar questions by different business units and then it\u2019s hard to find anything.\n\nOr the data changes- like google analytics not being supported has made a lot of charts go dead.\n\nHow do you all keep a handle on things?", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organising metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199e5tw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705542118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have good strategies around keeping metabase organized?  &lt;/p&gt;\n\n&lt;p&gt;What seems to happen is we get a lot of similar questions by different business units and then it\u2019s hard to find anything.&lt;/p&gt;\n\n&lt;p&gt;Or the data changes- like google analytics not being supported has made a lot of charts go dead.&lt;/p&gt;\n\n&lt;p&gt;How do you all keep a handle on things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199e5tw", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199e5tw/organising_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199e5tw/organising_metabase/", "subreddit_subscribers": 153770, "created_utc": 1705542118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I'm building a data-focused software app, so while I don't require the same scale that traditional data engineering requires (due to lack of actual users so far lol), I'd still like some DE input.\n\nThe app is surrounding League of Legends, and the part of the software I'm concerned with right now is getting the data from the Riot API into our database.\n\nI have to process one player at a time, because the API call is on a per-player/per-match basis\n\nThe steps are essentially:\n\n1. Get the match IDs that a player has played\n2. Loop through these match IDs to make an API call, that returns the actual in-game data for that particular match\n3. Append all this data into a pandas dataframe\n   1. I know pandas is looked down upon as a transformation tool, but the total matches for each player is (at max) 1000 rows and \\~200 columns, which always fits within memory\n4. CAVEAT - due to the game itself changing over time, they are continually adding new columns/stats that are tracked in matches, so there are a changing # of columns returned by the API\n5. Because of the caveat, I take the intersection of the columns we have in the database already, and those returned by the API. I \"manually\" add the columns we don't have in the database `ALTER TABLE ADD COLUMN`\n6. I then just run a `df.to_sql()` to get all the matches into the database\n\nI realize that this isn't the best approach, because to have a table that is continually changing can lead to troubles.\n\nI would ideally like to store ALL data, but I really only need a subset of the data for the actual product that I'm trying to create.\n\n# The main question I have is, how should I handle extra fields being added to the API response?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database design for my software app - how to handle changing fields from API response?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1995ho8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705520216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m building a data-focused software app, so while I don&amp;#39;t require the same scale that traditional data engineering requires (due to lack of actual users so far lol), I&amp;#39;d still like some DE input.&lt;/p&gt;\n\n&lt;p&gt;The app is surrounding League of Legends, and the part of the software I&amp;#39;m concerned with right now is getting the data from the Riot API into our database.&lt;/p&gt;\n\n&lt;p&gt;I have to process one player at a time, because the API call is on a per-player/per-match basis&lt;/p&gt;\n\n&lt;p&gt;The steps are essentially:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get the match IDs that a player has played&lt;/li&gt;\n&lt;li&gt;Loop through these match IDs to make an API call, that returns the actual in-game data for that particular match&lt;/li&gt;\n&lt;li&gt;Append all this data into a pandas dataframe\n\n&lt;ol&gt;\n&lt;li&gt;I know pandas is looked down upon as a transformation tool, but the total matches for each player is (at max) 1000 rows and ~200 columns, which always fits within memory&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;CAVEAT - due to the game itself changing over time, they are continually adding new columns/stats that are tracked in matches, so there are a changing # of columns returned by the API&lt;/li&gt;\n&lt;li&gt;Because of the caveat, I take the intersection of the columns we have in the database already, and those returned by the API. I &amp;quot;manually&amp;quot; add the columns we don&amp;#39;t have in the database &lt;code&gt;ALTER TABLE ADD COLUMN&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;I then just run a &lt;code&gt;df.to_sql()&lt;/code&gt; to get all the matches into the database&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I realize that this isn&amp;#39;t the best approach, because to have a table that is continually changing can lead to troubles.&lt;/p&gt;\n\n&lt;p&gt;I would ideally like to store ALL data, but I really only need a subset of the data for the actual product that I&amp;#39;m trying to create.&lt;/p&gt;\n\n&lt;h1&gt;The main question I have is, how should I handle extra fields being added to the API response?&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1995ho8", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1995ho8/database_design_for_my_software_app_how_to_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1995ho8/database_design_for_my_software_app_how_to_handle/", "subreddit_subscribers": 153770, "created_utc": 1705520216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software developer in a developing country. I've been working remotely for American companies for the past 8.5 years as a software engineer, but kind of in a niche industry that over the years I've grown to really dislike - so I've been thinking of switching industries for a while now. My main work line is in PHP/Javascript and I've been slowly but steadily moving further to the back-end, trying to get more work in the CI/CD pipelines, data modeling, database management and such.\n\nWithin the companies I've worked for, I've become known for being one of only people around who understands data/databases/search in a deeper way and I got a great deal of exposure to both MySQL, Elastic stack and Redis. I do have an Elastic Engineer certification and a fair amount of knowledge of Bash/Python, Linux, DevOps, CI, containers and such, but no real on-the-job experience with distributed computing/data tools.\n\nEarly last year I got laid off and started applying for DE jobs, but in the era of ATS I didn't land a single interview in this area and ended up taking another job in the same industry (a good one, at least).\n\nI've been around the block and I'm very confident in my ability to learn quickly on the job but I'm now 40 and I have two kids and a career, which means my time for on-the-side projects is really limited and I can't take a dip in salary to start over as a junior (though mostly because of location, my salary isn't really that high, I'm currently at 90k).\n\nIf you were in my shoes, how would you go about trying to land a DE job? Do you think routing through DevOps/SRE could be a good alternative?", "author_fullname": "t2_m3tn2vhhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career shift planning advice (Software Eng to DevOps/SRE to Data Eng?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199oy12", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705580531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software developer in a developing country. I&amp;#39;ve been working remotely for American companies for the past 8.5 years as a software engineer, but kind of in a niche industry that over the years I&amp;#39;ve grown to really dislike - so I&amp;#39;ve been thinking of switching industries for a while now. My main work line is in PHP/Javascript and I&amp;#39;ve been slowly but steadily moving further to the back-end, trying to get more work in the CI/CD pipelines, data modeling, database management and such.&lt;/p&gt;\n\n&lt;p&gt;Within the companies I&amp;#39;ve worked for, I&amp;#39;ve become known for being one of only people around who understands data/databases/search in a deeper way and I got a great deal of exposure to both MySQL, Elastic stack and Redis. I do have an Elastic Engineer certification and a fair amount of knowledge of Bash/Python, Linux, DevOps, CI, containers and such, but no real on-the-job experience with distributed computing/data tools.&lt;/p&gt;\n\n&lt;p&gt;Early last year I got laid off and started applying for DE jobs, but in the era of ATS I didn&amp;#39;t land a single interview in this area and ended up taking another job in the same industry (a good one, at least).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been around the block and I&amp;#39;m very confident in my ability to learn quickly on the job but I&amp;#39;m now 40 and I have two kids and a career, which means my time for on-the-side projects is really limited and I can&amp;#39;t take a dip in salary to start over as a junior (though mostly because of location, my salary isn&amp;#39;t really that high, I&amp;#39;m currently at 90k).&lt;/p&gt;\n\n&lt;p&gt;If you were in my shoes, how would you go about trying to land a DE job? Do you think routing through DevOps/SRE could be a good alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199oy12", "is_robot_indexable": true, "report_reasons": null, "author": "Time_Simple_3250", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199oy12/career_shift_planning_advice_software_eng_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199oy12/career_shift_planning_advice_software_eng_to/", "subreddit_subscribers": 153770, "created_utc": 1705580531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At [PeerDB](https://peerdb.io/), we recently added this new feature of SSH Tunneling to securely connect and replicate data from your Postgres Database to Data Warehouses. Sharing the blog that talks more about this feature - [https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication](https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication) We would love to hear your feedback!  \n\n\nhttps://i.redd.it/988cru7ik2dc1.gif", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSH Tunneling for Secure Postgres Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"988cru7ik2dc1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=8247debf48bd7b30dc2b632d6df292a3f242e6a4"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=b26a2dfc073a19318bce6c68203cae8a7b2ce71d"}, {"y": 212, "x": 320, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=2cf9ebcf84f11e7b7fffaba87c2fd7f00a7229d5"}, {"y": 425, "x": 640, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=9039157ad202ff353ef0d52e5985df4ba94d2205"}, {"y": 638, "x": 960, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=d503096068733ff38cb22debd3ead41083662dcc"}, {"y": 718, "x": 1080, "u": "https://preview.redd.it/988cru7ik2dc1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=edce4061c35e5baa5510ff80a57261aff55ded14"}], "s": {"y": 798, "gif": "https://i.redd.it/988cru7ik2dc1.gif", "mp4": "https://preview.redd.it/988cru7ik2dc1.gif?format=mp4&amp;s=8a19ea20ac78d0b5828f20c03272c1f0c63809c7", "x": 1200}, "id": "988cru7ik2dc1"}}, "name": "t3_1998e1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fyRlB0dqf4-D6iRV6cBPix78JLmW_cQB-0mLJhDnDIQ.jpg", "edited": 1705527371.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705527099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At &lt;a href=\"https://peerdb.io/\"&gt;PeerDB&lt;/a&gt;, we recently added this new feature of SSH Tunneling to securely connect and replicate data from your Postgres Database to Data Warehouses. Sharing the blog that talks more about this feature - &lt;a href=\"https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication\"&gt;https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication&lt;/a&gt; We would love to hear your feedback!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/988cru7ik2dc1.gif\"&gt;https://i.redd.it/988cru7ik2dc1.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1998e1x", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1998e1x/ssh_tunneling_for_secure_postgres_replication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1998e1x/ssh_tunneling_for_secure_postgres_replication/", "subreddit_subscribers": 153770, "created_utc": 1705527099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE position at a major financial services company, and after the initial interview, they chose another candidate. Two months later, the job was reposted, and somewhat impulsively, I reapplied. Now, I've been contacted for another interview. I'm uncertain about proceeding because I fear the initial panel's opinion might not have changed. Would it be advisable to call and cancel the interview?", "author_fullname": "t2_i4gjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reconsidering an Interview Opportunity After Initial Rejection: Seeking Advice on Whether to Proceed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1994v41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705518720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE position at a major financial services company, and after the initial interview, they chose another candidate. Two months later, the job was reposted, and somewhat impulsively, I reapplied. Now, I&amp;#39;ve been contacted for another interview. I&amp;#39;m uncertain about proceeding because I fear the initial panel&amp;#39;s opinion might not have changed. Would it be advisable to call and cancel the interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1994v41", "is_robot_indexable": true, "report_reasons": null, "author": "Greckol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1994v41/reconsidering_an_interview_opportunity_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1994v41/reconsidering_an_interview_opportunity_after/", "subreddit_subscribers": 153770, "created_utc": 1705518720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intro to SQL Indexes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_1991uni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2DRFBEjOvA1bk39vxVMwPZ3SCS2QfqVEESm-rljHqYQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705511648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/intro-to-sql-indexes", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?auto=webp&amp;s=db0dd46d348e43b93993efd89a3ab3188a2f750b", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb2ea7d2253e0b13f31c2b3a501bdb079c99f02a", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=774a9c7b5b1e9f2a6f0c3ad95da0fc6d8a27e60d", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59a28b06415dd585266957e277e86b821c702bb2", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe6f8284bd45732fa8e0bf3c2c0a5161ba9cfde4", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fadc5fdf695a74ba6f949375c70332ad149ab17d", "width": 960, "height": 562}], "variants": {}, "id": "Xv37p1IZSW1jS6YQK2dPjfcAGpVMaRZFFbZVqXsuRIE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1991uni", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1991uni/intro_to_sql_indexes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/intro-to-sql-indexes", "subreddit_subscribers": 153770, "created_utc": 1705511648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an upcoming interview where one of the steps is to create a mock data model what should I be reading up on in preparation. And what are the key things they will wlbe looking out for and be considering when doing such an exercise?\n\nFor context I have decent amount of data experience just lacking formal data Modeling experience any tips would be appreciated thanks in advance", "author_fullname": "t2_n2nlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling Interview scenario questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199nzng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705577110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an upcoming interview where one of the steps is to create a mock data model what should I be reading up on in preparation. And what are the key things they will wlbe looking out for and be considering when doing such an exercise?&lt;/p&gt;\n\n&lt;p&gt;For context I have decent amount of data experience just lacking formal data Modeling experience any tips would be appreciated thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "199nzng", "is_robot_indexable": true, "report_reasons": null, "author": "yanicklloyd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199nzng/data_modeling_interview_scenario_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199nzng/data_modeling_interview_scenario_questions/", "subreddit_subscribers": 153770, "created_utc": 1705577110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are doing another iteration on our dataplatform. This time we will overhaul our ingestion framework from full-loads only to incremental. The deltas are stored as received in a data lake (ADLS Gen2).\n\nWe are using dbt as transformation tool and Azure SQL database as compute / storage.\n\nMy question is how would you get the incrementals staged ready to be referenced by the dbt models?\nHow to set the desired column data types?\nWhere would you perform schema checking?\n\nAny advice is welcome \ud83e\udd17", "author_fullname": "t2_5zyyjiwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODS with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199n6gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705573940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are doing another iteration on our dataplatform. This time we will overhaul our ingestion framework from full-loads only to incremental. The deltas are stored as received in a data lake (ADLS Gen2).&lt;/p&gt;\n\n&lt;p&gt;We are using dbt as transformation tool and Azure SQL database as compute / storage.&lt;/p&gt;\n\n&lt;p&gt;My question is how would you get the incrementals staged ready to be referenced by the dbt models?\nHow to set the desired column data types?\nWhere would you perform schema checking?&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome \ud83e\udd17&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199n6gu", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed_Cantaloupe_9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199n6gu/ods_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199n6gu/ods_with_dbt/", "subreddit_subscribers": 153770, "created_utc": 1705573940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel in the next few years solutions will come out to do analysis on more unstructured data (no not just log files), I mean video, audio, text and doing sentiment analysis on it to extract data and do data quality check to train a LLM\n\nAnyone doing this in their current data engineering job? Any tech stacks to do this that you recommend?", "author_fullname": "t2_5q1b2lj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unstructured data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199hke6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705552263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel in the next few years solutions will come out to do analysis on more unstructured data (no not just log files), I mean video, audio, text and doing sentiment analysis on it to extract data and do data quality check to train a LLM&lt;/p&gt;\n\n&lt;p&gt;Anyone doing this in their current data engineering job? Any tech stacks to do this that you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199hke6", "is_robot_indexable": true, "report_reasons": null, "author": "Guilty-Commission435", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199hke6/unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199hke6/unstructured_data/", "subreddit_subscribers": 153770, "created_utc": 1705552263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you tried to programmatically export access details (name, role etc.) from SQL server (cloud or on-Prem) or oracle DBs? \n\nI am hoping to build a report on access and data rights across various data stores, primarily Azure SQL and others. \n\nAppreciate your perspective if you have tried something similar.", "author_fullname": "t2_dswp11x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question on the database access management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1993m10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705515737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you tried to programmatically export access details (name, role etc.) from SQL server (cloud or on-Prem) or oracle DBs? &lt;/p&gt;\n\n&lt;p&gt;I am hoping to build a report on access and data rights across various data stores, primarily Azure SQL and others. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your perspective if you have tried something similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1993m10", "is_robot_indexable": true, "report_reasons": null, "author": "abskiing403", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1993m10/quick_question_on_the_database_access_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1993m10/quick_question_on_the_database_access_management/", "subreddit_subscribers": 153770, "created_utc": 1705515737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was inspired by the [unofficial Python submission to the 1BRC](https://github.com/gunnarmorling/1brc/discussions/62) and wanted to share an implementation for Dask and PySpark: [https://github.com/gunnarmorling/1brc/discussions/450](https://github.com/gunnarmorling/1brc/discussions/450)  \nDask took \\~32 seconds, while Spark took \\~2 minutes.\n\nAmongst the other 1BRC Python submissions, Dask is pretty squarely in the middle. It\u2019s faster than Python\u2019s multiprocessing (except for the PyPy3 implementation) and slower than DuckDB and Polars. This is not too surprising given Polars and DuckDB tend to be faster than Dask on a smaller scale, especially on a single machine. I was actually pleasantly surprised to see this level of performance for Dask on a single machine for only a 13 GB dataset. This is largely due to a number of recent improvements in Dask like:  \n\\- Arrow strings  \n\\- New shuffling algorithms  \n\\- Query optimization  \n\n\nThough many of these improvements are still under active development in the dask-expr project, Dask users can expect to see these changes in core Dask DataFrame soon.  \n\n\nMore details in this blog post: [https://blog.coiled.io/blog/1brc.html](https://blog.coiled.io/blog/1brc.html)", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One billion row challenge - Dask vs. Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1992yx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705514242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was inspired by the &lt;a href=\"https://github.com/gunnarmorling/1brc/discussions/62\"&gt;unofficial Python submission to the 1BRC&lt;/a&gt; and wanted to share an implementation for Dask and PySpark: &lt;a href=\"https://github.com/gunnarmorling/1brc/discussions/450\"&gt;https://github.com/gunnarmorling/1brc/discussions/450&lt;/a&gt;&lt;br/&gt;\nDask took ~32 seconds, while Spark took ~2 minutes.&lt;/p&gt;\n\n&lt;p&gt;Amongst the other 1BRC Python submissions, Dask is pretty squarely in the middle. It\u2019s faster than Python\u2019s multiprocessing (except for the PyPy3 implementation) and slower than DuckDB and Polars. This is not too surprising given Polars and DuckDB tend to be faster than Dask on a smaller scale, especially on a single machine. I was actually pleasantly surprised to see this level of performance for Dask on a single machine for only a 13 GB dataset. This is largely due to a number of recent improvements in Dask like:&lt;br/&gt;\n- Arrow strings&lt;br/&gt;\n- New shuffling algorithms&lt;br/&gt;\n- Query optimization  &lt;/p&gt;\n\n&lt;p&gt;Though many of these improvements are still under active development in the dask-expr project, Dask users can expect to see these changes in core Dask DataFrame soon.  &lt;/p&gt;\n\n&lt;p&gt;More details in this blog post: &lt;a href=\"https://blog.coiled.io/blog/1brc.html\"&gt;https://blog.coiled.io/blog/1brc.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?auto=webp&amp;s=f9870422301fda8da802ca7aedf0b91b9a04a0c1", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e73b3946c993da59de05d21b3a12404282c3992", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5ed51e011fcf63e510de41119007081bb008f5f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=854c5c277985af2914f5dea4655f9fd1d43900f9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=976f2c6eac5ecd200bffac409312ac5d983f6a63", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37d855a33c5c112cadc15c254621dd8014f9abe5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8YLTJORZcvmRbhmo__AN3FponNxyUFe5iX_7CPWV-Ds.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fc3acfdcb972dc2115035367fa753bc6d9fb9b4", "width": 1080, "height": 540}], "variants": {}, "id": "FOD8kSXmvpIvMnYCzs25YbyWTjYZjt9ymZFa-nV8Zds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1992yx6", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1992yx6/one_billion_row_challenge_dask_vs_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1992yx6/one_billion_row_challenge_dask_vs_spark/", "subreddit_subscribers": 153770, "created_utc": 1705514242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello experienced devs,\nI am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. \n\nI am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I\u2019m going to ask for. Since I\u2019m a contractor, I cannot connect/ask for guidance in my workplace\n\nCouple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. \n\nOne day I found out that what I was doing was not very different from our in-house data analyst\u2019s role. \n\nThen, I learned Azure services and got a job as a contractor. Now I\u2019m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.\n\ntl;dr: a noob DE want a mentor to help navigate through the data carrier.", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got some time to mentor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1991m63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705511140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello experienced devs,\nI am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. &lt;/p&gt;\n\n&lt;p&gt;I am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I\u2019m going to ask for. Since I\u2019m a contractor, I cannot connect/ask for guidance in my workplace&lt;/p&gt;\n\n&lt;p&gt;Couple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. &lt;/p&gt;\n\n&lt;p&gt;One day I found out that what I was doing was not very different from our in-house data analyst\u2019s role. &lt;/p&gt;\n\n&lt;p&gt;Then, I learned Azure services and got a job as a contractor. Now I\u2019m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.&lt;/p&gt;\n\n&lt;p&gt;tl;dr: a noob DE want a mentor to help navigate through the data carrier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1991m63", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1991m63/got_some_time_to_mentor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1991m63/got_some_time_to_mentor/", "subreddit_subscribers": 153770, "created_utc": 1705511140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Reddit, i turn to you once again for a DE issue. TIA.\n\n&amp;#x200B;\n\n**Currently:**\n\n\"RDS MySQL &gt; DMS &gt; Redshift\"\n\nDMS is configured for a \\`full load, ongoing replication\\` (lags around 6-12 hours everyday ig)\n\n&amp;#x200B;\n\n**Problem:**\n\nExpense (unutilized instance after initial full load) and want to move away CDC to a different tool, as a POC\n\n&amp;#x200B;\n\n**POC** **TODO**: (*no choice here*)\n\nMySQL &gt; Kafka &gt; *Redshift sync connector (necessarily)*\n\n&amp;#x200B;\n\nAssuming the POC *has* to be done..\n\n*Questions:*\n\n1. **Duplicates**: There's mention of \"atleast once delivery\" into sync'd table, and a dummy topic set up on confluent cloud confirmed presence of \\~10k dupes per 1mil rows. Support could not answer pattern on when/why it occurs. How should one tackle this? Not sure if ksqdDB/single message transform can help here.\n2. **CDC:** There is a latest\\_updated\\_ts to help determine updates, how can the sync connector handle upserting? [docs](https://docs.confluent.io/kafka-connectors/aws-redshift/current/sink_config_options.html#redshift-sink-config-options) mention insert OR update (\"*this will only apply when modifying a record; you can\u2019t use this mode to insert a new record if the record doesn\u2019t already exist*\"). Solution to run two parallel ones?\n3. What other tests should one do? I've done basic data sanity checks so far, and plan to set up a kafka local to stream to redshift to test a dummy run.\n\nApologies if there's basic issues in my writings. TIA, once again!", "author_fullname": "t2_vvcegq1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift sync connector to replace DMS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199105x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705511098.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705509739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Reddit, i turn to you once again for a DE issue. TIA.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Currently:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;RDS MySQL &amp;gt; DMS &amp;gt; Redshift&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;DMS is configured for a `full load, ongoing replication` (lags around 6-12 hours everyday ig)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Expense (unutilized instance after initial full load) and want to move away CDC to a different tool, as a POC&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;POC&lt;/strong&gt; &lt;strong&gt;TODO&lt;/strong&gt;: (&lt;em&gt;no choice here&lt;/em&gt;)&lt;/p&gt;\n\n&lt;p&gt;MySQL &amp;gt; Kafka &amp;gt; &lt;em&gt;Redshift sync connector (necessarily)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Assuming the POC &lt;em&gt;has&lt;/em&gt; to be done..&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Questions:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Duplicates&lt;/strong&gt;: There&amp;#39;s mention of &amp;quot;atleast once delivery&amp;quot; into sync&amp;#39;d table, and a dummy topic set up on confluent cloud confirmed presence of ~10k dupes per 1mil rows. Support could not answer pattern on when/why it occurs. How should one tackle this? Not sure if ksqdDB/single message transform can help here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CDC:&lt;/strong&gt; There is a latest_updated_ts to help determine updates, how can the sync connector handle upserting? &lt;a href=\"https://docs.confluent.io/kafka-connectors/aws-redshift/current/sink_config_options.html#redshift-sink-config-options\"&gt;docs&lt;/a&gt; mention insert OR update (&amp;quot;&lt;em&gt;this will only apply when modifying a record; you can\u2019t use this mode to insert a new record if the record doesn\u2019t already exist&lt;/em&gt;&amp;quot;). Solution to run two parallel ones?&lt;/li&gt;\n&lt;li&gt;What other tests should one do? I&amp;#39;ve done basic data sanity checks so far, and plan to set up a kafka local to stream to redshift to test a dummy run.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Apologies if there&amp;#39;s basic issues in my writings. TIA, once again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199105x", "is_robot_indexable": true, "report_reasons": null, "author": "LocksmithConnect6201", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199105x/redshift_sync_connector_to_replace_dms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199105x/redshift_sync_connector_to_replace_dms/", "subreddit_subscribers": 153770, "created_utc": 1705509739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m introducing clickhouse to my company and we have a distributed clickhouse setup. I\u2019m facing challenges creating databases using the Replicated engine. Any help", "author_fullname": "t2_6y9busn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "introducing clickhouse to my company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_199rx1w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705589629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m introducing clickhouse to my company and we have a distributed clickhouse setup. I\u2019m facing challenges creating databases using the Replicated engine. Any help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199rx1w", "is_robot_indexable": true, "report_reasons": null, "author": "kwabsdev", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199rx1w/introducing_clickhouse_to_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199rx1w/introducing_clickhouse_to_my_company/", "subreddit_subscribers": 153770, "created_utc": 1705589629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was working on Informatica Cloud to move some blob storage of Azure on a S3 bucket. The problem is that when a create mapping, I don't know how to parameterized the fact that the blob storage of Azure create each day a new file with a new name. So in my mapping in Informatica the blob to get remain static. Anyone faced this problem and knows how to get the job dinamically? Thanks.", "author_fullname": "t2_27xod4p1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica cloud azure storage issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_199rrya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705589225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was working on Informatica Cloud to move some blob storage of Azure on a S3 bucket. The problem is that when a create mapping, I don&amp;#39;t know how to parameterized the fact that the blob storage of Azure create each day a new file with a new name. So in my mapping in Informatica the blob to get remain static. Anyone faced this problem and knows how to get the job dinamically? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199rrya", "is_robot_indexable": true, "report_reasons": null, "author": "DNSoundRM", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199rrya/informatica_cloud_azure_storage_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199rrya/informatica_cloud_azure_storage_issue/", "subreddit_subscribers": 153770, "created_utc": 1705589225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am trying to set up databricks Unity catalog and as I am fairly new to all of this I am struggling with the concept of UC. \n\nWhat I am interested the most is whether or not the default location for managed tables stated when establishing UC should be separate Storage accnt or? Also there are three separate Storage accnts for different enviroments (with bronze, silver, gold containers in it) and landing zone in separate accnt and different workspaces for different enviroments. \n\nWhat would be the best \"organization\" of Storage?\n\nThanks!", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Unity Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_199qz6u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705588308.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705586981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am trying to set up databricks Unity catalog and as I am fairly new to all of this I am struggling with the concept of UC. &lt;/p&gt;\n\n&lt;p&gt;What I am interested the most is whether or not the default location for managed tables stated when establishing UC should be separate Storage accnt or? Also there are three separate Storage accnts for different enviroments (with bronze, silver, gold containers in it) and landing zone in separate accnt and different workspaces for different enviroments. &lt;/p&gt;\n\n&lt;p&gt;What would be the best &amp;quot;organization&amp;quot; of Storage?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199qz6u", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199qz6u/databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199qz6u/databricks_unity_catalog/", "subreddit_subscribers": 153770, "created_utc": 1705586981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to run the new Prefect setup with a worker, a work pool, a flow, and prefect server. The flow has dependancies on specific versions of requests, pandas, and selenium. I backed up the flow in a remote gut repository and set it up so that workers shall pull it from this repository before every flow run. So, this way, flows stay up to date with the main branch of remote storage.\n\nRight now, it seems like the only way to get a worker to run this flow is to build the worker in an environment that contains all the flows library dependencies. Such as, creating a `venv`, `pip install -r requirements.txt`, `source ./venv/bin/activate`, then `prefect worker start` with environment variables set for the PREFECT_API_URL and PREFECT_API_KEY. Then the worker will connect to the server, see it\u2019s assigned work pool has work, pull the updated flow code from its repo, and execute the flow.\n\nAm I misunderstanding something though? This suggests that every worker must be built with foreknowledge of the flows it will run and the dependancies of that flow. Let\u2019s say you want to add a new flow with new dependencies, seems like you\u2019d need to rebuild the worker to contain those dependencies. If the two flows have conflicting dependencies, you\u2019d need an entire new worker and work pool. This means workers and work pools are essentially organized by clustering compatible tasks based on their dependancies. That specific architecture also seems to limit the usefulness of workers and work pools, as they now must follow this specific organization pattern.\n\nIs there any way at all to associate requirements with a flow, or a python `venv` with a flow, so that the worker just [builds and] activates the necessary `venv` prior to each flow run?\n\nIf not, what am I missing? How are we supposed to set up flows that have package dependancies without imposing additional complexity on how the platform much be used and planned?", "author_fullname": "t2_l924mfj1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect Users: Is it possible to have Flow dependent dynamic runtime environments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_199qn92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705586033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to run the new Prefect setup with a worker, a work pool, a flow, and prefect server. The flow has dependancies on specific versions of requests, pandas, and selenium. I backed up the flow in a remote gut repository and set it up so that workers shall pull it from this repository before every flow run. So, this way, flows stay up to date with the main branch of remote storage.&lt;/p&gt;\n\n&lt;p&gt;Right now, it seems like the only way to get a worker to run this flow is to build the worker in an environment that contains all the flows library dependencies. Such as, creating a &lt;code&gt;venv&lt;/code&gt;, &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;, &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;, then &lt;code&gt;prefect worker start&lt;/code&gt; with environment variables set for the PREFECT_API_URL and PREFECT_API_KEY. Then the worker will connect to the server, see it\u2019s assigned work pool has work, pull the updated flow code from its repo, and execute the flow.&lt;/p&gt;\n\n&lt;p&gt;Am I misunderstanding something though? This suggests that every worker must be built with foreknowledge of the flows it will run and the dependancies of that flow. Let\u2019s say you want to add a new flow with new dependencies, seems like you\u2019d need to rebuild the worker to contain those dependencies. If the two flows have conflicting dependencies, you\u2019d need an entire new worker and work pool. This means workers and work pools are essentially organized by clustering compatible tasks based on their dependancies. That specific architecture also seems to limit the usefulness of workers and work pools, as they now must follow this specific organization pattern.&lt;/p&gt;\n\n&lt;p&gt;Is there any way at all to associate requirements with a flow, or a python &lt;code&gt;venv&lt;/code&gt; with a flow, so that the worker just [builds and] activates the necessary &lt;code&gt;venv&lt;/code&gt; prior to each flow run?&lt;/p&gt;\n\n&lt;p&gt;If not, what am I missing? How are we supposed to set up flows that have package dependancies without imposing additional complexity on how the platform much be used and planned?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199qn92", "is_robot_indexable": true, "report_reasons": null, "author": "Duck-Delta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199qn92/prefect_users_is_it_possible_to_have_flow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199qn92/prefect_users_is_it_possible_to_have_flow/", "subreddit_subscribers": 153770, "created_utc": 1705586033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting Webhooks From Stripe \u2013 The Better Way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_199py4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RxmwO9TWmB9o8UrvueHi_STjUzydioKgXtL_lAdRPIo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705583887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memphis.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memphis.dev/blog/ingesting-webhooks-from-stripe-the-better-way/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?auto=webp&amp;s=857583b54d32d9869c42686585ed649aaedaf0b7", "width": 1600, "height": 901}, "resolutions": [{"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e37ac62b056da1a5a47aa92d0c7151b531de99f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a727bc5438a66e6251305ef2e265b29830c820e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=346a7a04965f8e2d584ba28d5bf61f1e05ed0c4f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=455d9d1a356873cd1a83a331d028132559fc620e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3209f71df27910f4969a4840e3aca1dd2fa3b4e", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f3bf526ac4844568667cc5d6b9b9cb33168d213", "width": 1080, "height": 608}], "variants": {}, "id": "gtv1gP4SY3ASsYpjxuOzkyxAxRv81vKbbUyaHtwsHRo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199py4s", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199py4s/ingesting_webhooks_from_stripe_the_better_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memphis.dev/blog/ingesting-webhooks-from-stripe-the-better-way/", "subreddit_subscribers": 153770, "created_utc": 1705583887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://i.redd.it/lnl14vvul6dc1.gif\n\n[Article is here](https://blog.devgenius.io/exposing-snowflake-data-as-a-rest-api-a-step-by-step-guide-746ceefecaa3)", "author_fullname": "t2_jjbftfcy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to expose the data resides in snowflake via REST API for downstream consumption?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lnl14vvul6dc1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=85cbdc411487f40e89e1a3d1549eb5aff5e6e269"}, {"y": 98, "x": 216, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=25ec0d74807989fa824334acc274caa243102b5a"}, {"y": 145, "x": 320, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f92287c14a6ae3fc24148ff18aa1cc2b9b4942da"}, {"y": 291, "x": 640, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=e3e296c59d3090a35ad4fda61b681cd64c598438"}, {"y": 437, "x": 960, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=af4aa443af0140fdd5b0ca33e38cd09f59edcfb3"}, {"y": 492, "x": 1080, "u": "https://preview.redd.it/lnl14vvul6dc1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=68c72bc27dfb75b2485521757a5562f60f0fb396"}], "s": {"y": 503, "gif": "https://i.redd.it/lnl14vvul6dc1.gif", "mp4": "https://preview.redd.it/lnl14vvul6dc1.gif?format=mp4&amp;s=57e8de237dc4c9db1ad91d38b58b2f7dcc8cd331", "x": 1104}, "id": "lnl14vvul6dc1"}}, "name": "t3_199nt3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/BtjkOji4csPvCafLHPhxIro2xi4nMtzTdAeVrXlLq34.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1705576405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/lnl14vvul6dc1.gif\"&gt;https://i.redd.it/lnl14vvul6dc1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.devgenius.io/exposing-snowflake-data-as-a-rest-api-a-step-by-step-guide-746ceefecaa3\"&gt;Article is here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JIhhgwDvtACWoiUvbIWFHS6V56MfU_tMj2xz0goRj2I.jpg?auto=webp&amp;s=72761ed7d1250f22002b0231b8ced2e6569574f9", "width": 800, "height": 487}, "resolutions": [{"url": "https://external-preview.redd.it/JIhhgwDvtACWoiUvbIWFHS6V56MfU_tMj2xz0goRj2I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a6d85adb5474f570a96bd3af14c75caae6812fb", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/JIhhgwDvtACWoiUvbIWFHS6V56MfU_tMj2xz0goRj2I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c64fd8a80c569a1fbe95a215f02b92a458da887", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/JIhhgwDvtACWoiUvbIWFHS6V56MfU_tMj2xz0goRj2I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d0cd0986c8058b4667cb5d32d552c9bb05ac68b", "width": 320, "height": 194}, {"url": "https://external-preview.redd.it/JIhhgwDvtACWoiUvbIWFHS6V56MfU_tMj2xz0goRj2I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9108a76580d1b3fee3f2b494f9e413b08feef1b", "width": 640, "height": 389}], "variants": {}, "id": "go2X7gEtBrGmdgQvl0M84bbCXbiOuCysCjzBLNvAMaY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199nt3j", "is_robot_indexable": true, "report_reasons": null, "author": "BigNo3623", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199nt3j/how_to_expose_the_data_resides_in_snowflake_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199nt3j/how_to_expose_the_data_resides_in_snowflake_via/", "subreddit_subscribers": 153770, "created_utc": 1705576405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vbapbjo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_199mah5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/MUnGNI0I2dg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/199mah5", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ixXoDNIuC3k5S2HZ0V2XNKha68B_BGU12enb9j7dZpE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705570252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/MUnGNI0I2dg?si=0sl1yfQ3LcAKpmOG", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?auto=webp&amp;s=469512af6df797365536f82946f5e0d106c506a3", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=935e4ed71c79c9444e2fed14d4a1be9548385b1f", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f56f6284da6a9826514326614264e122aa4dda7", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c6767a696cb8e6620741e42a92396aae68364bc", "width": 320, "height": 240}], "variants": {}, "id": "0eBu9j_LeMMEqxEAAh76GzxnlCUcl-M6kk6-9o8qUGg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199mah5", "is_robot_indexable": true, "report_reasons": null, "author": "dnulcon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199mah5/lstms_according_to_their_inventor_j\u00fcrgen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/MUnGNI0I2dg?si=0sl1yfQ3LcAKpmOG", "subreddit_subscribers": 153770, "created_utc": 1705570252.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/MUnGNI0I2dg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}], "before": null}}