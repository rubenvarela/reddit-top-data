{"kind": "Listing", "data": {"after": "t3_1afj9o0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.", "author_fullname": "t2_a0i580op", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern data pipeline for on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8ts7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706674252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af8ts7", "is_robot_indexable": true, "report_reasons": null, "author": "lezzgooooo", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "subreddit_subscribers": 157198, "created_utc": 1706674252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Comp Sci, Data Analytics, Information Systems, stats, math etc  \n\n\nYour thoughts?", "author_fullname": "t2_hiu1pyq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the ideal masters for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af15s3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706653128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Comp Sci, Data Analytics, Information Systems, stats, math etc  &lt;/p&gt;\n\n&lt;p&gt;Your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1af15s3", "is_robot_indexable": true, "report_reasons": null, "author": "PureLavishness8654", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af15s3/what_is_the_ideal_masters_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af15s3/what_is_the_ideal_masters_for_data_engineers/", "subreddit_subscribers": 157198, "created_utc": 1706653128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.\n\nIs there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.", "author_fullname": "t2_j8v7zu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Data Stack in Higher Education?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeulzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706637307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.&lt;/p&gt;\n\n&lt;p&gt;Is there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeulzm", "is_robot_indexable": true, "report_reasons": null, "author": "BIntelligent", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "subreddit_subscribers": 157198, "created_utc": 1706637307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit : Thank you guys, you're all awesome and helpful.\n\nMy manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.\n\nSorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.\n\n&amp;#x200B;\n\nThank you so much.", "author_fullname": "t2_43bjqfmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python and Spark Scala", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aff8gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706716375.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706698474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit : Thank you guys, you&amp;#39;re all awesome and helpful.&lt;/p&gt;\n\n&lt;p&gt;My manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.&lt;/p&gt;\n\n&lt;p&gt;Sorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aff8gu", "is_robot_indexable": true, "report_reasons": null, "author": "WadieXkiller", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "subreddit_subscribers": 157198, "created_utc": 1706698474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  \n\nWondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. \n\nThanks in advance!", "author_fullname": "t2_7qz1v7vg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best learning path and resources for Azure from a data engineering perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeu4ry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  &lt;/p&gt;\n\n&lt;p&gt;Wondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aeu4ry", "is_robot_indexable": true, "report_reasons": null, "author": "sajiDsarkaR12321", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "subreddit_subscribers": 157198, "created_utc": 1706636128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.\n\n[Link](https://rr43.net/posts/2024/1/Dremel/)", "author_fullname": "t2_dbozei2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My first blog about DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afakqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://rr43.net/posts/2024/1/Dremel/\"&gt;Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afakqy", "is_robot_indexable": true, "report_reasons": null, "author": "InstitutionalizedSon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "subreddit_subscribers": 157198, "created_utc": 1706679855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wrote a blog post which looks in depth at Snowflakes new feature for streaming ingestion. The analysis focuses on the Java SDK implementation.\n\nhttps://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/", "author_fullname": "t2_elarh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowpipe Streaming Deep Dive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aevqet", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706639988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wrote a blog post which looks in depth at Snowflakes new feature for streaming ingestion. The analysis focuses on the Java SDK implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/\"&gt;https://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?auto=webp&amp;s=835bf926ff136d72eca4f99033cf536e142609ce", "width": 6016, "height": 4016}, "resolutions": [{"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf704b3753dd8c194fa9453ffc4b2e338680d7f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f9fee6336318e224b5835f1f2780cc86d8a6ace", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a333dea35887c936e1a5ba9b6219ea21a839549b", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d4776d6ae3f8dea3cecfe13d180edee2e57cd92", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1113e1d5204fefa84386086a413a386c9e63b87", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=387d88e8557da6e22ceee2995cb5d5bc4e554b67", "width": 1080, "height": 720}], "variants": {}, "id": "cTIg0FsOyrYpaGVwlPjUJEFLNjW7rE9BO4u1GNe4fi0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aevqet", "is_robot_indexable": true, "report_reasons": null, "author": "yuvalos", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aevqet/snowpipe_streaming_deep_dive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aevqet/snowpipe_streaming_deep_dive/", "subreddit_subscribers": 157198, "created_utc": 1706639988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about how DuckDB WASM can be used to create data-driven applications in the browser, and how a browser-based SQL Playground can be used to quickly create and share SQL queries and Data Visualizations.\n\n[https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering](https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering) ", "author_fullname": "t2_cl6jnq23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using DuckDB WASM for in-browser Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af2fab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706656222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about how DuckDB WASM can be used to create data-driven applications in the browser, and how a browser-based SQL Playground can be used to quickly create and share SQL queries and Data Visualizations.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering\"&gt;https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?auto=webp&amp;s=9a55dca929112fb172874385c98d45ae7c113825", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=18da77dcba1e674544d372c7b2f193e89b795ebe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e63a7f2e770e42f6269f7f310e71b36fbaf5e39c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4391846b77a474ea272ef9be631adb2aa1b63c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aea9616ec6f138a1edb318393885db37b7e37436", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=069886e8690cd22fdd91e4a08a33c1ce4dd38a80", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b706f763ea19b6f93ba54b12e245f8a0be07430", "width": 1080, "height": 567}], "variants": {}, "id": "UL9SB4dYPLFNRVOq3radBxIFFdc9g8fh0FyEjIetobE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af2fab", "is_robot_indexable": true, "report_reasons": null, "author": "migh_t", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af2fab/using_duckdb_wasm_for_inbrowser_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af2fab/using_duckdb_wasm_for_inbrowser_data_engineering/", "subreddit_subscribers": 157198, "created_utc": 1706656222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_un9ertyb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 simple software engineering habits that transformed my productivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1afe5ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4kP-FUOwxhBUYC9OE08WdSJCvzdPdX92Eb4bizlVL74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706693946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "read.engineerscodex.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?auto=webp&amp;s=5871f0d7487438c46b01eebb7041b2f5a2ba7527", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdec1cf8f05f2821e71a81d57b8b1b702bc44ebf", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e5627d5bdcdcaa244366390da0a789cad4ee8c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ebe3ed6fb441eabc7f8c3f2481e9ead91151a20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a92ae3e201c4437dc18cbe46538d166170fc37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=371233137e9fb0406471aba4201351f9bf459b61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32f7bba1c8cf00ab2b15dfe0088d3570c3d8f56c", "width": 1080, "height": 540}], "variants": {}, "id": "jv2y0q2UCecenbfBaCYYmlJZojw4Z73RqFv9jW4FMmA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afe5ug", "is_robot_indexable": true, "report_reasons": null, "author": "boyrot37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afe5ug/4_simple_software_engineering_habits_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "subreddit_subscribers": 157198, "created_utc": 1706693946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have a &gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. ", "author_fullname": "t2_vos56utz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions for real-time data refreshes with large tables (&gt;1TB) in Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afacpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have a &amp;gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afacpu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Accountant9659", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "subreddit_subscribers": 157198, "created_utc": 1706679112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of **30 private VM servers and baremetal servers with GPU**. I\u2019m hoping to find a solution that can help me **spin up a dynamic instance like ECS**, **spin dev and prod Jupyter Lab notebooks**, have a MLflow and dagster setup on these instances and **manage an execution queue for GPU-based servers**.\n\nwe dont have a very large team to manage and support kubernetees installation, but I'm open to discuss if this is the only way. \n\nIf anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.\n\nThank you in advance!\n\nI hope this helps! Let me know if you have any other questions.", "author_fullname": "t2_4uziix4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions on a solution to orchestrate corporate infrastructure of 30 private VM servers and baremetal servers with GPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af6zgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of &lt;strong&gt;30 private VM servers and baremetal servers with GPU&lt;/strong&gt;. I\u2019m hoping to find a solution that can help me &lt;strong&gt;spin up a dynamic instance like ECS&lt;/strong&gt;, &lt;strong&gt;spin dev and prod Jupyter Lab notebooks&lt;/strong&gt;, have a MLflow and dagster setup on these instances and &lt;strong&gt;manage an execution queue for GPU-based servers&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;we dont have a very large team to manage and support kubernetees installation, but I&amp;#39;m open to discuss if this is the only way. &lt;/p&gt;\n\n&lt;p&gt;If anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;I hope this helps! Let me know if you have any other questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1af6zgk", "is_robot_indexable": true, "report_reasons": null, "author": "mvishruth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "subreddit_subscribers": 157198, "created_utc": 1706668736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Redshift Enthusiasts,\n\nI'm encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.\n\nHere's the scenario: I'm in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it's taking quite a bit of time to complete.\n\nIn an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.\n\nHowever, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.\n\nI've pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?\n\nI'd greatly appreciate any insights, tips, or experiences you've had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?\n\nLooking forward to your thoughts and expertise!\n\nCheers!", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Troubleshooting Redshift COPY Command Performance: Need Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8onh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706673814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redshift Enthusiasts,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the scenario: I&amp;#39;m in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it&amp;#39;s taking quite a bit of time to complete.&lt;/p&gt;\n\n&lt;p&gt;In an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.&lt;/p&gt;\n\n&lt;p&gt;However, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d greatly appreciate any insights, tips, or experiences you&amp;#39;ve had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your thoughts and expertise!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af8onh", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "subreddit_subscribers": 157198, "created_utc": 1706673814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\njust looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have 'dimCustomerId', 'CustomerId', 'CustomerName' as fields. The 'dimCustomerId' is an auto incrementing ID. The dimCustomerId is what is used in my fact table. \n\n  \nWhat is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use 'CustomerId'. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? \n\nWould it be suitable to have a dimCustomer table with 'CustomerId' as the PK and 'CustomerName' as the field, removing the dimCustomerId altogether? I'm not looking to actually do this, just for my own learning. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dimensional table ids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af7s1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706671079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;just looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have &amp;#39;dimCustomerId&amp;#39;, &amp;#39;CustomerId&amp;#39;, &amp;#39;CustomerName&amp;#39; as fields. The &amp;#39;dimCustomerId&amp;#39; is an auto incrementing ID. The dimCustomerId is what is used in my fact table. &lt;/p&gt;\n\n&lt;p&gt;What is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use &amp;#39;CustomerId&amp;#39;. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? &lt;/p&gt;\n\n&lt;p&gt;Would it be suitable to have a dimCustomer table with &amp;#39;CustomerId&amp;#39; as the PK and &amp;#39;CustomerName&amp;#39; as the field, removing the dimCustomerId altogether? I&amp;#39;m not looking to actually do this, just for my own learning. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af7s1s", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "subreddit_subscribers": 157198, "created_utc": 1706671079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI am a newbie in DE and want to tryout what I've learned with Astronomer Airflow 1:1 course. My ambition is to build another pet project for portfolio.\n\nThe idea is to build a simple pipeline via Airflow (deployed in Managed Workers) that pulls the data from Spotify API, loads it into S3, then performs basic wrangling and loads it into reporting view in Snowflake for further visualization.\n\nKey phases:\n\n1. Create VENV and test/deploy all connection\n2. Containerize the solution with Docker\n3. Deploy the solution in the cloud\n\nDo you mind put some of your thoughts/critique/advice?\n\nThank you!", "author_fullname": "t2_16bfjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please provide suggestions to my pet project on building Airflow pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af3uuo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706659940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I am a newbie in DE and want to tryout what I&amp;#39;ve learned with Astronomer Airflow 1:1 course. My ambition is to build another pet project for portfolio.&lt;/p&gt;\n\n&lt;p&gt;The idea is to build a simple pipeline via Airflow (deployed in Managed Workers) that pulls the data from Spotify API, loads it into S3, then performs basic wrangling and loads it into reporting view in Snowflake for further visualization.&lt;/p&gt;\n\n&lt;p&gt;Key phases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create VENV and test/deploy all connection&lt;/li&gt;\n&lt;li&gt;Containerize the solution with Docker&lt;/li&gt;\n&lt;li&gt;Deploy the solution in the cloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do you mind put some of your thoughts/critique/advice?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af3uuo", "is_robot_indexable": true, "report_reasons": null, "author": "gikis1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af3uuo/please_provide_suggestions_to_my_pet_project_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af3uuo/please_provide_suggestions_to_my_pet_project_on/", "subreddit_subscribers": 157198, "created_utc": 1706659940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI: An introduction to prompt engineering and LangChain for data practitioners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1af0utx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1LlBRyGH7Ruu7iq-whgK92xtlGcmsPFF1fpc6RNrraw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706652381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/teradata/generative-ai-part-1-an-introduction-to-prompt-engineering-and-langchain-742987f2d9c1?source=friends_link&amp;sk=e62cbac26db2d6ed662b6d08c2469504", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?auto=webp&amp;s=792aaa2459236bcca3a62b5897ccb2fa52cda2dc", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f39363f82738e33360409f712d703329008b84eb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af1879dd02eff4b1f44467912c11dd84b6f9fb5e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=391f964d027ef7a94b467cc31c39d321c8f5586d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7c481fe23e7de5dfc232c1a7c9405821d73033a2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e82e37157f489c7f5aa8648ee5b996f084b7b5ae", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e77d5093f573cc3157b395821860e00960f2ee41", "width": 1080, "height": 567}], "variants": {}, "id": "J1m4rmgFumu25yh6TpxE1seywkLq5EsdLh_mIIK-4gg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af0utx", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af0utx/generative_ai_an_introduction_to_prompt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/teradata/generative-ai-part-1-an-introduction-to-prompt-engineering-and-langchain-742987f2d9c1?source=friends_link&amp;sk=e62cbac26db2d6ed662b6d08c2469504", "subreddit_subscribers": 157198, "created_utc": 1706652381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Disney Hotstar Captures One Billion Emojis!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1aev0l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How Disney Hotstar Captures One Billion Emojis!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UN1kW5AHid4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1aev0l4", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fgu0-vc156RFpuWdsuUURMAzfYG3czuh6pmQzZ_QMuo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706638288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=UN1kW5AHid4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?auto=webp&amp;s=9e1823e21dd040c84a0d92a629583897608c8b3a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=213ad23bed5dceed758188cb1a390f51614166a1", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b64681ca8e03c58910d4959185d2dbbd335eb12", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d033c897cf066d4f2d1e9aa4a7225b7b2291c5d5", "width": 320, "height": 240}], "variants": {}, "id": "GMU0CgoG92Xc4dyKC7706W9SAKFbpBUPbuiqgBfhfQY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aev0l4", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aev0l4/how_disney_hotstar_captures_one_billion_emojis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=UN1kW5AHid4", "subreddit_subscribers": 157198, "created_utc": 1706638288.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How Disney Hotstar Captures One Billion Emojis!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UN1kW5AHid4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I've managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?", "author_fullname": "t2_hae00nzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift performance optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afcicb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706686938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I&amp;#39;ve managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afcicb", "is_robot_indexable": true, "report_reasons": null, "author": "New-Statistician-155", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "subreddit_subscribers": 157198, "created_utc": 1706686938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Folks aren't too happy with Synapse for multiple reasons; one is that we can't get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.\n\nThis is our data flow  Dataverse --&gt; Synapse Link --- &gt; Datalake Storage Gen 2 ---&gt; Synapse Analytics serverless SQL Endpoint ----&gt; Synapse Pipelines -----&gt; Upsert data and schema evolution to Azure SQL Server -----&gt; Snaplogic ------&gt; AWS Redshift.\n\nI am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I've tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it's not supported.\n\nAny suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it's worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.", "author_fullname": "t2_5yj82gi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Platform suggestions to migrate off of Azure Synapse Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af70gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks aren&amp;#39;t too happy with Synapse for multiple reasons; one is that we can&amp;#39;t get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.&lt;/p&gt;\n\n&lt;p&gt;This is our data flow  Dataverse --&amp;gt; Synapse Link --- &amp;gt; Datalake Storage Gen 2 ---&amp;gt; Synapse Analytics serverless SQL Endpoint ----&amp;gt; Synapse Pipelines -----&amp;gt; Upsert data and schema evolution to Azure SQL Server -----&amp;gt; Snaplogic ------&amp;gt; AWS Redshift.&lt;/p&gt;\n\n&lt;p&gt;I am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I&amp;#39;ve tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it&amp;#39;s not supported.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it&amp;#39;s worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af70gd", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Cry_6841", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "subreddit_subscribers": 157198, "created_utc": 1706668813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here goes our latest blog on Data Types and their impact on Database Replication - [https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication](https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication)\n\nDid you know that carefully designing data type mapping during database replication can?  \n\n\n1. **Save Costs:** Replicating to native data types in the Warehouse avoids additional transformations and typecasting from apps such as [dbt Labs](https://www.linkedin.com/company/dbtlabs/), thus saving on compute costs.\n2. **Enable Advanced Analytics:** Replicating to native data types in the Warehouse enables consumer apps to use advanced functions and operators, facilitating advanced querying and analytics.\n3. **Reduce Tech Debt:** Your data engineers don't need to write additional code to convert strings to native data types, thereby reducing technical debt.\n\nThe blog also talks about how [PeerDB](https://www.peerdb.io/)\u00a0handles data type mapping, with practical examples.", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Types and their impact on Database Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af0woe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706652508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here goes our latest blog on Data Types and their impact on Database Replication - &lt;a href=\"https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication\"&gt;https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Did you know that carefully designing data type mapping during database replication can?  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Save Costs:&lt;/strong&gt; Replicating to native data types in the Warehouse avoids additional transformations and typecasting from apps such as &lt;a href=\"https://www.linkedin.com/company/dbtlabs/\"&gt;dbt Labs&lt;/a&gt;, thus saving on compute costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enable Advanced Analytics:&lt;/strong&gt; Replicating to native data types in the Warehouse enables consumer apps to use advanced functions and operators, facilitating advanced querying and analytics.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reduce Tech Debt:&lt;/strong&gt; Your data engineers don&amp;#39;t need to write additional code to convert strings to native data types, thereby reducing technical debt.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The blog also talks about how &lt;a href=\"https://www.peerdb.io/\"&gt;PeerDB&lt;/a&gt;\u00a0handles data type mapping, with practical examples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?auto=webp&amp;s=3d7d36799bd51f98de4b9353f2e78017c7679558", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb37b2ff5f6bc1dbedb12c7d71a2aafd8716f0be", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a6c011458b3e6fd2ea7c273a9218fbcaa1d8cd6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc82869bd56aa6f98d5a6f517977a41e3c679a3b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdcf68b083a1ec1a62991f6114a742e6a31dfa13", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0c8c5bbe8d974c8a8a3cfecf2bf494deeb905833", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14f0975bd04e16ab1af2688fca84519217c0372f", "width": 1080, "height": 567}], "variants": {}, "id": "L3mxAd9-4u7t5R_5a5u44x49ddQcP6mlPm3L75SeZsQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af0woe", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af0woe/data_types_and_their_impact_on_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af0woe/data_types_and_their_impact_on_database/", "subreddit_subscribers": 157198, "created_utc": 1706652508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like I need to learn cloud infrastructure. I have about 8 years of experience as a data analyst. Here is my program/language stack:\n\n* Python\n* VBA (lol)\n* Power BI/Power Query\n* SQL\n* Alteryx\n\nI'm pretty comfortable/experienced with general programming/automation/ETL/visualization but I've never worked with big data/cloud stuff. Just wondering what a data engineering hiring manager would want to see.", "author_fullname": "t2_1ad62ux7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I need to add to my stack to break into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af3rg5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706659697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I need to learn cloud infrastructure. I have about 8 years of experience as a data analyst. Here is my program/language stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;li&gt;VBA (lol)&lt;/li&gt;\n&lt;li&gt;Power BI/Power Query&lt;/li&gt;\n&lt;li&gt;SQL&lt;/li&gt;\n&lt;li&gt;Alteryx&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m pretty comfortable/experienced with general programming/automation/ETL/visualization but I&amp;#39;ve never worked with big data/cloud stuff. Just wondering what a data engineering hiring manager would want to see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1af3rg5", "is_robot_indexable": true, "report_reasons": null, "author": "VegaGT-VZ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af3rg5/what_do_i_need_to_add_to_my_stack_to_break_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af3rg5/what_do_i_need_to_add_to_my_stack_to_break_into/", "subreddit_subscribers": 157198, "created_utc": 1706659697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!  \nI was about to start reading 'The Data Warehouse Toolkit' by Ralph Kimball and Margy Ross. However, I understand that it has become somewhat outdated.\n\nI would like to know what resources I could use to learn about modern architectures and dimensional modeling.\n\nThank you very much!", "author_fullname": "t2_6kkepa9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Books and Resources for Analytics Engineering and Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aex15l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706643063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI was about to start reading &amp;#39;The Data Warehouse Toolkit&amp;#39; by Ralph Kimball and Margy Ross. However, I understand that it has become somewhat outdated.&lt;/p&gt;\n\n&lt;p&gt;I would like to know what resources I could use to learn about modern architectures and dimensional modeling.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aex15l", "is_robot_indexable": true, "report_reasons": null, "author": "hypnotize9", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aex15l/books_and_resources_for_analytics_engineering_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aex15l/books_and_resources_for_analytics_engineering_and/", "subreddit_subscribers": 157198, "created_utc": 1706643063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Okay so, here goes. I'm not a Data Engineer so please forgive me if I get a few things wrong. \n\nWe use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I'm lost as to what tools or \"pipeline\" (for lack of a better word) to suggest. \n\nIn my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? \n\nI'm open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don't mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I'm off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. \n\nTLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.", "author_fullname": "t2_tcnjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with an ETL process and MongoDB (Am not a Data Engineer)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeufvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay so, here goes. I&amp;#39;m not a Data Engineer so please forgive me if I get a few things wrong. &lt;/p&gt;\n\n&lt;p&gt;We use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I&amp;#39;m lost as to what tools or &amp;quot;pipeline&amp;quot; (for lack of a better word) to suggest. &lt;/p&gt;\n\n&lt;p&gt;In my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don&amp;#39;t mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I&amp;#39;m off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. &lt;/p&gt;\n\n&lt;p&gt;TLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeufvx", "is_robot_indexable": true, "report_reasons": null, "author": "Zulowie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "subreddit_subscribers": 157198, "created_utc": 1706636897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, everyone. I'm doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,\n\nRight now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I'm appending the new ones (using polars write\\_database() and setting the if\\_exists parameter to \"append\"), without any double check, but I'm not sure how to do the update part. I mean, I know that I have to do something like:\n\nUPDATE schema.table\\_name SET col1=val1, col2 = val2 WHERE id = id\\_val;\n\nBut how to do that for tables with different schemas?\n\nThanks btw", "author_fullname": "t2_5i1nco5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generic Update Statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aesa2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706631697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, everyone. I&amp;#39;m doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,&lt;/p&gt;\n\n&lt;p&gt;Right now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I&amp;#39;m appending the new ones (using polars write_database() and setting the if_exists parameter to &amp;quot;append&amp;quot;), without any double check, but I&amp;#39;m not sure how to do the update part. I mean, I know that I have to do something like:&lt;/p&gt;\n\n&lt;p&gt;UPDATE schema.table_name SET col1=val1, col2 = val2 WHERE id = id_val;&lt;/p&gt;\n\n&lt;p&gt;But how to do that for tables with different schemas?&lt;/p&gt;\n\n&lt;p&gt;Thanks btw&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aesa2v", "is_robot_indexable": true, "report_reasons": null, "author": "gera0220", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aesa2v/generic_update_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aesa2v/generic_update_statement/", "subreddit_subscribers": 157198, "created_utc": 1706631697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a Power BI premium capacity, and some developers sometimes overload the capacity. I need to monitor the cpu, mem usage in realtime. The Power BI capacity metrics app only shows data a day old. Is there some way to access this data in realtime? It's insane if a service which is essentially a 10k a month VM doesn't offer resource monitoring.", "author_fullname": "t2_6h22qz3w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI premium capacity realtime usage metrics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afk4cp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706714127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a Power BI premium capacity, and some developers sometimes overload the capacity. I need to monitor the cpu, mem usage in realtime. The Power BI capacity metrics app only shows data a day old. Is there some way to access this data in realtime? It&amp;#39;s insane if a service which is essentially a 10k a month VM doesn&amp;#39;t offer resource monitoring.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afk4cp", "is_robot_indexable": true, "report_reasons": null, "author": "Rough_Pass_4016", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afk4cp/power_bi_premium_capacity_realtime_usage_metrics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afk4cp/power_bi_premium_capacity_realtime_usage_metrics/", "subreddit_subscribers": 157198, "created_utc": 1706714127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are converting a database load based to DBT. The old value chain was often like this:\n\n1. Insert new base data into table X \n2. Run multiple update statement or insert statement on table X\n\nThis was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). \n\nI feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.\n\nDo you try to organize models in folders in such a way that make that closely related models are kept together? \n\nIs a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7hbs1ihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organize code in DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afj9o0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706711826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are converting a database load based to DBT. The old value chain was often like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert new base data into table X &lt;/li&gt;\n&lt;li&gt;Run multiple update statement or insert statement on table X&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). &lt;/p&gt;\n\n&lt;p&gt;I feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.&lt;/p&gt;\n\n&lt;p&gt;Do you try to organize models in folders in such a way that make that closely related models are kept together? &lt;/p&gt;\n\n&lt;p&gt;Is a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afj9o0", "is_robot_indexable": true, "report_reasons": null, "author": "Wise-Ad-7492", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "subreddit_subscribers": 157198, "created_utc": 1706711826.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}