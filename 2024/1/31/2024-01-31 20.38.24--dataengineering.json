{"kind": "Listing", "data": {"after": "t3_1afqsf5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.", "author_fullname": "t2_a0i580op", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern data pipeline for on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8ts7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706674252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af8ts7", "is_robot_indexable": true, "report_reasons": null, "author": "lezzgooooo", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "subreddit_subscribers": 157241, "created_utc": 1706674252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Comp Sci, Data Analytics, Information Systems, stats, math etc  \n\n\nYour thoughts?", "author_fullname": "t2_hiu1pyq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the ideal masters for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af15s3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706653128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Comp Sci, Data Analytics, Information Systems, stats, math etc  &lt;/p&gt;\n\n&lt;p&gt;Your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1af15s3", "is_robot_indexable": true, "report_reasons": null, "author": "PureLavishness8654", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af15s3/what_is_the_ideal_masters_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af15s3/what_is_the_ideal_masters_for_data_engineers/", "subreddit_subscribers": 157241, "created_utc": 1706653128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Dagster Believes About Data Platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1afnd1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1HfcUuzrMpvBBY94JElFNYso1NQQ4RrLwGz_rUBvKb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706722293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?auto=webp&amp;s=94dcf8ce7019ce3e221a84088531c82d39c59478", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eea7cd5c0c3002c13b2398269a37e06cf911e3ae", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6982f3cf54d8a8754e64ee7e2c4bc544d1a5422c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74ed1228f650dddfb6ed29eb136a6b8f73ada0ac", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=80a10915705eaf3527c7d89450431f653a590f8d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fbe788ae92611b87791c735d2d71eb2e00e28b5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=006b1460ef750708d063f2e3c7670fde92270524", "width": 1080, "height": 567}], "variants": {}, "id": "9BPh1HBe8dpb9hwolf4ivC5UxBl9ds4I31wiMA6yM0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afnd1d", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afnd1d/what_dagster_believes_about_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "subreddit_subscribers": 157241, "created_utc": 1706722293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit : Thank you guys, you're all awesome and helpful.\n\nMy manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.\n\nSorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.\n\n&amp;#x200B;\n\nThank you so much.", "author_fullname": "t2_43bjqfmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python and Spark Scala", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aff8gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706716375.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706698474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit : Thank you guys, you&amp;#39;re all awesome and helpful.&lt;/p&gt;\n\n&lt;p&gt;My manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.&lt;/p&gt;\n\n&lt;p&gt;Sorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aff8gu", "is_robot_indexable": true, "report_reasons": null, "author": "WadieXkiller", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "subreddit_subscribers": 157241, "created_utc": 1706698474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.\n\n[Link](https://rr43.net/posts/2024/1/Dremel/)", "author_fullname": "t2_dbozei2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My first blog about DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afakqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://rr43.net/posts/2024/1/Dremel/\"&gt;Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afakqy", "is_robot_indexable": true, "report_reasons": null, "author": "InstitutionalizedSon", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "subreddit_subscribers": 157241, "created_utc": 1706679855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, there's been a lot of buzz about layoffs affecting Software Engineers (SWEs) across various sectors. It got me wondering about how these trends are impacting Data Engineers. Are you, as Data Engineers, experiencing similar challenges in terms of job security, finding new roles, or facing layoffs? How do you feel the market is right now for Data Engineering?\n\nAdditionally, with the rapid advancements and increasing reliance on AI and Data Science, I'm curious about your thoughts on the future of Data Engineering as a career choice. Do you believe Data Engineering will continue to be a safe and stable career path, or do you think pivoting towards AI and Data Science might offer more security and opportunities in the long run?", "author_fullname": "t2_3tzpeuhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the Current Job Market for Data Engineers Compared to SWEs, and What's the Future Outlook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afo6as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706724240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, there&amp;#39;s been a lot of buzz about layoffs affecting Software Engineers (SWEs) across various sectors. It got me wondering about how these trends are impacting Data Engineers. Are you, as Data Engineers, experiencing similar challenges in terms of job security, finding new roles, or facing layoffs? How do you feel the market is right now for Data Engineering?&lt;/p&gt;\n\n&lt;p&gt;Additionally, with the rapid advancements and increasing reliance on AI and Data Science, I&amp;#39;m curious about your thoughts on the future of Data Engineering as a career choice. Do you believe Data Engineering will continue to be a safe and stable career path, or do you think pivoting towards AI and Data Science might offer more security and opportunities in the long run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afo6as", "is_robot_indexable": true, "report_reasons": null, "author": "randomusicjunkie", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afo6as/hows_the_current_job_market_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afo6as/hows_the_current_job_market_for_data_engineers/", "subreddit_subscribers": 157241, "created_utc": 1706724240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite Python Data Processing Libraries? (PyArrow, Pandas, Polars, DuckDB, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflut4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706718628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aflut4", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "subreddit_subscribers": 157241, "created_utc": 1706718628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_un9ertyb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 simple software engineering habits that transformed my productivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1afe5ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4kP-FUOwxhBUYC9OE08WdSJCvzdPdX92Eb4bizlVL74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706693946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "read.engineerscodex.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?auto=webp&amp;s=5871f0d7487438c46b01eebb7041b2f5a2ba7527", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdec1cf8f05f2821e71a81d57b8b1b702bc44ebf", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e5627d5bdcdcaa244366390da0a789cad4ee8c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ebe3ed6fb441eabc7f8c3f2481e9ead91151a20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a92ae3e201c4437dc18cbe46538d166170fc37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=371233137e9fb0406471aba4201351f9bf459b61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32f7bba1c8cf00ab2b15dfe0088d3570c3d8f56c", "width": 1080, "height": 540}], "variants": {}, "id": "jv2y0q2UCecenbfBaCYYmlJZojw4Z73RqFv9jW4FMmA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afe5ug", "is_robot_indexable": true, "report_reasons": null, "author": "boyrot37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afe5ug/4_simple_software_engineering_habits_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "subreddit_subscribers": 157241, "created_utc": 1706693946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have a &gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. ", "author_fullname": "t2_vos56utz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions for real-time data refreshes with large tables (&gt;1TB) in Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afacpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have a &amp;gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afacpu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Accountant9659", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "subreddit_subscribers": 157241, "created_utc": 1706679112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about how DuckDB WASM can be used to create data-driven applications in the browser, and how a browser-based SQL Playground can be used to quickly create and share SQL queries and Data Visualizations.\n\n[https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering](https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering) ", "author_fullname": "t2_cl6jnq23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using DuckDB WASM for in-browser Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af2fab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706656222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about how DuckDB WASM can be used to create data-driven applications in the browser, and how a browser-based SQL Playground can be used to quickly create and share SQL queries and Data Visualizations.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering\"&gt;https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?auto=webp&amp;s=9a55dca929112fb172874385c98d45ae7c113825", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=18da77dcba1e674544d372c7b2f193e89b795ebe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e63a7f2e770e42f6269f7f310e71b36fbaf5e39c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4391846b77a474ea272ef9be631adb2aa1b63c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aea9616ec6f138a1edb318393885db37b7e37436", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=069886e8690cd22fdd91e4a08a33c1ce4dd38a80", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EzFH0p7EBUBzumH4rR0JEatagGuFhXEHmGPJpQGZxGQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b706f763ea19b6f93ba54b12e245f8a0be07430", "width": 1080, "height": 567}], "variants": {}, "id": "UL9SB4dYPLFNRVOq3radBxIFFdc9g8fh0FyEjIetobE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af2fab", "is_robot_indexable": true, "report_reasons": null, "author": "migh_t", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af2fab/using_duckdb_wasm_for_inbrowser_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af2fab/using_duckdb_wasm_for_inbrowser_data_engineering/", "subreddit_subscribers": 157241, "created_utc": 1706656222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of **30 private VM servers and baremetal servers with GPU**. I\u2019m hoping to find a solution that can help me **spin up a dynamic instance like ECS**, **spin dev and prod Jupyter Lab notebooks**, have a MLflow and dagster setup on these instances and **manage an execution queue for GPU-based servers**.\n\nwe dont have a very large team to manage and support kubernetees installation, but I'm open to discuss if this is the only way. \n\nIf anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.\n\nThank you in advance!\n\nI hope this helps! Let me know if you have any other questions.", "author_fullname": "t2_4uziix4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions on a solution to orchestrate corporate infrastructure of 30 private VM servers and baremetal servers with GPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af6zgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of &lt;strong&gt;30 private VM servers and baremetal servers with GPU&lt;/strong&gt;. I\u2019m hoping to find a solution that can help me &lt;strong&gt;spin up a dynamic instance like ECS&lt;/strong&gt;, &lt;strong&gt;spin dev and prod Jupyter Lab notebooks&lt;/strong&gt;, have a MLflow and dagster setup on these instances and &lt;strong&gt;manage an execution queue for GPU-based servers&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;we dont have a very large team to manage and support kubernetees installation, but I&amp;#39;m open to discuss if this is the only way. &lt;/p&gt;\n\n&lt;p&gt;If anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;I hope this helps! Let me know if you have any other questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1af6zgk", "is_robot_indexable": true, "report_reasons": null, "author": "mvishruth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "subreddit_subscribers": 157241, "created_utc": 1706668736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me this month, 12 initial interviews, 5 technical, 1 final round, no offers\n\n3.5 years exp. Wanna leave my current job asap.", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the job search going for you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflkig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706717890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me this month, 12 initial interviews, 5 technical, 1 final round, no offers&lt;/p&gt;\n\n&lt;p&gt;3.5 years exp. Wanna leave my current job asap.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aflkig", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflkig/hows_the_job_search_going_for_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflkig/hows_the_job_search_going_for_you/", "subreddit_subscribers": 157241, "created_utc": 1706717890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are converting a database load based to DBT. The old value chain was often like this:\n\n1. Insert new base data into table X \n2. Run multiple update statement or insert statement on table X\n\nThis was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). \n\nI feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.\n\nDo you try to organize models in folders in such a way that make that closely related models are kept together? \n\nIs a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7hbs1ihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organize code in DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afj9o0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706711826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are converting a database load based to DBT. The old value chain was often like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert new base data into table X &lt;/li&gt;\n&lt;li&gt;Run multiple update statement or insert statement on table X&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). &lt;/p&gt;\n\n&lt;p&gt;I feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.&lt;/p&gt;\n\n&lt;p&gt;Do you try to organize models in folders in such a way that make that closely related models are kept together? &lt;/p&gt;\n\n&lt;p&gt;Is a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afj9o0", "is_robot_indexable": true, "report_reasons": null, "author": "Wise-Ad-7492", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "subreddit_subscribers": 157241, "created_utc": 1706711826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I've managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?", "author_fullname": "t2_hae00nzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift performance optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afcicb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706686938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I&amp;#39;ve managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afcicb", "is_robot_indexable": true, "report_reasons": null, "author": "New-Statistician-155", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "subreddit_subscribers": 157241, "created_utc": 1706686938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Redshift Enthusiasts,\n\nI'm encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.\n\nHere's the scenario: I'm in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it's taking quite a bit of time to complete.\n\nIn an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.\n\nHowever, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.\n\nI've pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?\n\nI'd greatly appreciate any insights, tips, or experiences you've had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?\n\nLooking forward to your thoughts and expertise!\n\nCheers!", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Troubleshooting Redshift COPY Command Performance: Need Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8onh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706673814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redshift Enthusiasts,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the scenario: I&amp;#39;m in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it&amp;#39;s taking quite a bit of time to complete.&lt;/p&gt;\n\n&lt;p&gt;In an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.&lt;/p&gt;\n\n&lt;p&gt;However, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d greatly appreciate any insights, tips, or experiences you&amp;#39;ve had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your thoughts and expertise!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af8onh", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "subreddit_subscribers": 157241, "created_utc": 1706673814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\njust looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have 'dimCustomerId', 'CustomerId', 'CustomerName' as fields. The 'dimCustomerId' is an auto incrementing ID. The dimCustomerId is what is used in my fact table. \n\n  \nWhat is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use 'CustomerId'. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? \n\nWould it be suitable to have a dimCustomer table with 'CustomerId' as the PK and 'CustomerName' as the field, removing the dimCustomerId altogether? I'm not looking to actually do this, just for my own learning. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dimensional table ids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af7s1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706671079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;just looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have &amp;#39;dimCustomerId&amp;#39;, &amp;#39;CustomerId&amp;#39;, &amp;#39;CustomerName&amp;#39; as fields. The &amp;#39;dimCustomerId&amp;#39; is an auto incrementing ID. The dimCustomerId is what is used in my fact table. &lt;/p&gt;\n\n&lt;p&gt;What is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use &amp;#39;CustomerId&amp;#39;. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? &lt;/p&gt;\n\n&lt;p&gt;Would it be suitable to have a dimCustomer table with &amp;#39;CustomerId&amp;#39; as the PK and &amp;#39;CustomerName&amp;#39; as the field, removing the dimCustomerId altogether? I&amp;#39;m not looking to actually do this, just for my own learning. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af7s1s", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "subreddit_subscribers": 157241, "created_utc": 1706671079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI am a newbie in DE and want to tryout what I've learned with Astronomer Airflow 1:1 course. My ambition is to build another pet project for portfolio.\n\nThe idea is to build a simple pipeline via Airflow (deployed in Managed Workers) that pulls the data from Spotify API, loads it into S3, then performs basic wrangling and loads it into reporting view in Snowflake for further visualization.\n\nKey phases:\n\n1. Create VENV and test/deploy all connection\n2. Containerize the solution with Docker\n3. Deploy the solution in the cloud\n\nDo you mind put some of your thoughts/critique/advice?\n\nThank you!", "author_fullname": "t2_16bfjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please provide suggestions to my pet project on building Airflow pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af3uuo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706659940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I am a newbie in DE and want to tryout what I&amp;#39;ve learned with Astronomer Airflow 1:1 course. My ambition is to build another pet project for portfolio.&lt;/p&gt;\n\n&lt;p&gt;The idea is to build a simple pipeline via Airflow (deployed in Managed Workers) that pulls the data from Spotify API, loads it into S3, then performs basic wrangling and loads it into reporting view in Snowflake for further visualization.&lt;/p&gt;\n\n&lt;p&gt;Key phases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create VENV and test/deploy all connection&lt;/li&gt;\n&lt;li&gt;Containerize the solution with Docker&lt;/li&gt;\n&lt;li&gt;Deploy the solution in the cloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do you mind put some of your thoughts/critique/advice?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af3uuo", "is_robot_indexable": true, "report_reasons": null, "author": "gikis1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af3uuo/please_provide_suggestions_to_my_pet_project_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af3uuo/please_provide_suggestions_to_my_pet_project_on/", "subreddit_subscribers": 157241, "created_utc": 1706659940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI: An introduction to prompt engineering and LangChain for data practitioners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1af0utx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1LlBRyGH7Ruu7iq-whgK92xtlGcmsPFF1fpc6RNrraw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706652381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/teradata/generative-ai-part-1-an-introduction-to-prompt-engineering-and-langchain-742987f2d9c1?source=friends_link&amp;sk=e62cbac26db2d6ed662b6d08c2469504", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?auto=webp&amp;s=792aaa2459236bcca3a62b5897ccb2fa52cda2dc", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f39363f82738e33360409f712d703329008b84eb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af1879dd02eff4b1f44467912c11dd84b6f9fb5e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=391f964d027ef7a94b467cc31c39d321c8f5586d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7c481fe23e7de5dfc232c1a7c9405821d73033a2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e82e37157f489c7f5aa8648ee5b996f084b7b5ae", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sNZLb38iNgJDN3K4aSZBNRRDgCuA9cxUZmu00W-YOM4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e77d5093f573cc3157b395821860e00960f2ee41", "width": 1080, "height": 567}], "variants": {}, "id": "J1m4rmgFumu25yh6TpxE1seywkLq5EsdLh_mIIK-4gg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af0utx", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af0utx/generative_ai_an_introduction_to_prompt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/teradata/generative-ai-part-1-an-introduction-to-prompt-engineering-and-langchain-742987f2d9c1?source=friends_link&amp;sk=e62cbac26db2d6ed662b6d08c2469504", "subreddit_subscribers": 157241, "created_utc": 1706652381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. \n\nSo when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.\n\nFor running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.\n\nSo is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?", "author_fullname": "t2_txl4izdo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aforig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706725682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. &lt;/p&gt;\n\n&lt;p&gt;So when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.&lt;/p&gt;\n\n&lt;p&gt;For running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.&lt;/p&gt;\n\n&lt;p&gt;So is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aforig", "is_robot_indexable": true, "report_reasons": null, "author": "__1l0__", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "subreddit_subscribers": 157241, "created_utc": 1706725682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Folks aren't too happy with Synapse for multiple reasons; one is that we can't get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.\n\nThis is our data flow  Dataverse --&gt; Synapse Link --- &gt; Datalake Storage Gen 2 ---&gt; Synapse Analytics serverless SQL Endpoint ----&gt; Synapse Pipelines -----&gt; Upsert data and schema evolution to Azure SQL Server -----&gt; Snaplogic ------&gt; AWS Redshift.\n\nI am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I've tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it's not supported.\n\nAny suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it's worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.", "author_fullname": "t2_5yj82gi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Platform suggestions to migrate off of Azure Synapse Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af70gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks aren&amp;#39;t too happy with Synapse for multiple reasons; one is that we can&amp;#39;t get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.&lt;/p&gt;\n\n&lt;p&gt;This is our data flow  Dataverse --&amp;gt; Synapse Link --- &amp;gt; Datalake Storage Gen 2 ---&amp;gt; Synapse Analytics serverless SQL Endpoint ----&amp;gt; Synapse Pipelines -----&amp;gt; Upsert data and schema evolution to Azure SQL Server -----&amp;gt; Snaplogic ------&amp;gt; AWS Redshift.&lt;/p&gt;\n\n&lt;p&gt;I am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I&amp;#39;ve tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it&amp;#39;s not supported.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it&amp;#39;s worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af70gd", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Cry_6841", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "subreddit_subscribers": 157241, "created_utc": 1706668813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like I need to learn cloud infrastructure. I have about 8 years of experience as a data analyst. Here is my program/language stack:\n\n* Python\n* VBA (lol)\n* Power BI/Power Query\n* SQL\n* Alteryx\n\nI'm pretty comfortable/experienced with general programming/automation/ETL/visualization but I've never worked with big data/cloud stuff. Just wondering what a data engineering hiring manager would want to see.", "author_fullname": "t2_1ad62ux7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I need to add to my stack to break into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af3rg5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706659697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I need to learn cloud infrastructure. I have about 8 years of experience as a data analyst. Here is my program/language stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;li&gt;VBA (lol)&lt;/li&gt;\n&lt;li&gt;Power BI/Power Query&lt;/li&gt;\n&lt;li&gt;SQL&lt;/li&gt;\n&lt;li&gt;Alteryx&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m pretty comfortable/experienced with general programming/automation/ETL/visualization but I&amp;#39;ve never worked with big data/cloud stuff. Just wondering what a data engineering hiring manager would want to see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1af3rg5", "is_robot_indexable": true, "report_reasons": null, "author": "VegaGT-VZ", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af3rg5/what_do_i_need_to_add_to_my_stack_to_break_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af3rg5/what_do_i_need_to_add_to_my_stack_to_break_into/", "subreddit_subscribers": 157241, "created_utc": 1706659697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here goes our latest blog on Data Types and their impact on Database Replication - [https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication](https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication)\n\nDid you know that carefully designing data type mapping during database replication can?  \n\n\n1. **Save Costs:** Replicating to native data types in the Warehouse avoids additional transformations and typecasting from apps such as [dbt Labs](https://www.linkedin.com/company/dbtlabs/), thus saving on compute costs.\n2. **Enable Advanced Analytics:** Replicating to native data types in the Warehouse enables consumer apps to use advanced functions and operators, facilitating advanced querying and analytics.\n3. **Reduce Tech Debt:** Your data engineers don't need to write additional code to convert strings to native data types, thereby reducing technical debt.\n\nThe blog also talks about how [PeerDB](https://www.peerdb.io/)\u00a0handles data type mapping, with practical examples.", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Types and their impact on Database Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af0woe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706652508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here goes our latest blog on Data Types and their impact on Database Replication - &lt;a href=\"https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication\"&gt;https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Did you know that carefully designing data type mapping during database replication can?  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Save Costs:&lt;/strong&gt; Replicating to native data types in the Warehouse avoids additional transformations and typecasting from apps such as &lt;a href=\"https://www.linkedin.com/company/dbtlabs/\"&gt;dbt Labs&lt;/a&gt;, thus saving on compute costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enable Advanced Analytics:&lt;/strong&gt; Replicating to native data types in the Warehouse enables consumer apps to use advanced functions and operators, facilitating advanced querying and analytics.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reduce Tech Debt:&lt;/strong&gt; Your data engineers don&amp;#39;t need to write additional code to convert strings to native data types, thereby reducing technical debt.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The blog also talks about how &lt;a href=\"https://www.peerdb.io/\"&gt;PeerDB&lt;/a&gt;\u00a0handles data type mapping, with practical examples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?auto=webp&amp;s=3d7d36799bd51f98de4b9353f2e78017c7679558", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb37b2ff5f6bc1dbedb12c7d71a2aafd8716f0be", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a6c011458b3e6fd2ea7c273a9218fbcaa1d8cd6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc82869bd56aa6f98d5a6f517977a41e3c679a3b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdcf68b083a1ec1a62991f6114a742e6a31dfa13", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0c8c5bbe8d974c8a8a3cfecf2bf494deeb905833", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fjDeiWYd7B8nKeVueq6M5VvWSFHJik8v4izOH1FOmLM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14f0975bd04e16ab1af2688fca84519217c0372f", "width": 1080, "height": 567}], "variants": {}, "id": "L3mxAd9-4u7t5R_5a5u44x49ddQcP6mlPm3L75SeZsQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1af0woe", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af0woe/data_types_and_their_impact_on_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af0woe/data_types_and_their_impact_on_database/", "subreddit_subscribers": 157241, "created_utc": 1706652508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don't really see the point of using this extensive paid tool for basically doing just simple API and database calls. \n\nIs there a better tool/method for automating and managing a copy activity easily? (in Azure)", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Near realtime copy ELT with other tool than ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflaae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706717177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don&amp;#39;t really see the point of using this extensive paid tool for basically doing just simple API and database calls. &lt;/p&gt;\n\n&lt;p&gt;Is there a better tool/method for automating and managing a copy activity easily? (in Azure)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aflaae", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "subreddit_subscribers": 157241, "created_utc": 1706717177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI am exploring options to build data mart to support some financial dashboards and I would love to know your opinions on this. Sorry for the lengthy post\n\nWe receive data from 2 ERP systems and a custom web and desktop applications. the current state of analytics platform is there are ADF pipelines which dump data into Azure SQL so basically it is replica of the different sources. We are BI team of 3 people and every one builds their own reporting views with lot of joins, when creating power bi reports so no body owns anything in the Azure SQL database and there is lot of rework being done for each dashboard due to the lack of common model . \n\n&amp;#x200B;\n\nAt least our finance team is now coming up with some well defined requirements for couple of new dashboards. So we are thinking of creating a data mart for finance team and thinking of choosing the right architecture. Please correct me if any of my points are dumb, I am in early stage of my BI career and don't have much experience in choosing the architecture. below are the things that I am considering\n\n**1**. I am planning to dump source data  to ADLS(there are some large xml files that we need to parse so I am thinking using ADLS might reduce cots) and perform all the transformations using spark notebooks in synapse and write as delta tables(synapse server less sql pool) and have these delta tables serve as data mart with dimensions and fact tables ready to be imported to power bi.\n\n**issues that I am seeing :** \n\n1.can I use the delta tables to build dimensionally modeled data mart. I think we can not create relationships between tables ,implement incremental surrogate keys also how difficult is it to implement SCDs?\n\n2.If we want any adhoc reports, I dont see any easy way to just do some joins. To over come this do i need to create another serverless sql pool between adls and my data mart which serves as replica of the source tables? if that is the case am i over complicating things by having too many layers?\n\n3. Also using serverless sql pool we can  not create stored procs and do some traditional tsql things\n\n&amp;#x200B;\n\n**2.**  how about dedicated sql pools? but I am not sure if i should really use it because of the costs. my entire data would be around 5 tb with daily data about 2-3 gb. \n\n**3.** I want to leverage adls to minimize storage costs but also want my data mart to have traditional t sql features to perform dimensional modelling. so instead of using Azure sql as staging layer can i use synapse server less sql pool and the have Azure SQL Db as data mart. if so is there any way to write from the delta tables to Azure SQL?\n\n**4**. should I even consider using ADLS? or should I do the traditional way of having staging area and a data mart both in a Azure SQL db?\n\n ", "author_fullname": "t2_7s9ezasyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help in choosing Synapse server less sql pool as data mart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af6al2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706666697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am exploring options to build data mart to support some financial dashboards and I would love to know your opinions on this. Sorry for the lengthy post&lt;/p&gt;\n\n&lt;p&gt;We receive data from 2 ERP systems and a custom web and desktop applications. the current state of analytics platform is there are ADF pipelines which dump data into Azure SQL so basically it is replica of the different sources. We are BI team of 3 people and every one builds their own reporting views with lot of joins, when creating power bi reports so no body owns anything in the Azure SQL database and there is lot of rework being done for each dashboard due to the lack of common model . &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;At least our finance team is now coming up with some well defined requirements for couple of new dashboards. So we are thinking of creating a data mart for finance team and thinking of choosing the right architecture. Please correct me if any of my points are dumb, I am in early stage of my BI career and don&amp;#39;t have much experience in choosing the architecture. below are the things that I am considering&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. I am planning to dump source data  to ADLS(there are some large xml files that we need to parse so I am thinking using ADLS might reduce cots) and perform all the transformations using spark notebooks in synapse and write as delta tables(synapse server less sql pool) and have these delta tables serve as data mart with dimensions and fact tables ready to be imported to power bi.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;issues that I am seeing :&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;1.can I use the delta tables to build dimensionally modeled data mart. I think we can not create relationships between tables ,implement incremental surrogate keys also how difficult is it to implement SCDs?&lt;/p&gt;\n\n&lt;p&gt;2.If we want any adhoc reports, I dont see any easy way to just do some joins. To over come this do i need to create another serverless sql pool between adls and my data mart which serves as replica of the source tables? if that is the case am i over complicating things by having too many layers?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Also using serverless sql pool we can  not create stored procs and do some traditional tsql things&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;  how about dedicated sql pools? but I am not sure if i should really use it because of the costs. my entire data would be around 5 tb with daily data about 2-3 gb. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; I want to leverage adls to minimize storage costs but also want my data mart to have traditional t sql features to perform dimensional modelling. so instead of using Azure sql as staging layer can i use synapse server less sql pool and the have Azure SQL Db as data mart. if so is there any way to write from the delta tables to Azure SQL?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. should I even consider using ADLS? or should I do the traditional way of having staging area and a data mart both in a Azure SQL db?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af6al2", "is_robot_indexable": true, "report_reasons": null, "author": "Soft_Manufacturer314", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af6al2/need_help_in_choosing_synapse_server_less_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af6al2/need_help_in_choosing_synapse_server_less_sql/", "subreddit_subscribers": 157241, "created_utc": 1706666697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know good ressources to get insights into famous products t architectures ?", "author_fullname": "t2_adlmr9gsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insight on famous techs tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afqsf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706730576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know good ressources to get insights into famous products t architectures ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afqsf5", "is_robot_indexable": true, "report_reasons": null, "author": "tech_curious27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "subreddit_subscribers": 157241, "created_utc": 1706730576.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}