{"kind": "Listing", "data": {"after": "t3_1adqpth", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do amateurs usually not do well?", "author_fullname": "t2_e7studq8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What distinguishes production-grade data pipelines from amateur setups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1addsj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 79, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 79, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706477671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do amateurs usually not do well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1addsj4", "is_robot_indexable": true, "report_reasons": null, "author": "CupcakeFun4746", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1addsj4/what_distinguishes_productiongrade_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1addsj4/what_distinguishes_productiongrade_data_pipelines/", "subreddit_subscribers": 156641, "created_utc": 1706477671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some context : I'm working on a predictive maintenance prototype on Azure. Essentially, the sensors send in readings periodically every 30s (Temperature, Vibrations, Pressure, Noice, etc.). The data is added into an event hub. The data is then processed and dumped into ADLS V2. The readings are passed into an ML model and run against some basic checks(If temp exceeds, send an email notification to the asset owner, etc...) The notifications (for now) are processed via logic apps(When a blob is created within the datalake)\n\nCan these events directly be processed via an event driven architecture instead of using Kafka? Or processing the data through serverless functions? \n\nAlso, what are some good visualization tools that can let me monotor this data in near real time?\n\nI've just started learning to use Kafka, and would appreciate any answers. ", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the purpose of using Kafka, when the same can be processed though an Event Driven Architecuture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad1xqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706446067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some context : I&amp;#39;m working on a predictive maintenance prototype on Azure. Essentially, the sensors send in readings periodically every 30s (Temperature, Vibrations, Pressure, Noice, etc.). The data is added into an event hub. The data is then processed and dumped into ADLS V2. The readings are passed into an ML model and run against some basic checks(If temp exceeds, send an email notification to the asset owner, etc...) The notifications (for now) are processed via logic apps(When a blob is created within the datalake)&lt;/p&gt;\n\n&lt;p&gt;Can these events directly be processed via an event driven architecture instead of using Kafka? Or processing the data through serverless functions? &lt;/p&gt;\n\n&lt;p&gt;Also, what are some good visualization tools that can let me monotor this data in near real time?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just started learning to use Kafka, and would appreciate any answers. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ad1xqt", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad1xqt/whats_the_purpose_of_using_kafka_when_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad1xqt/whats_the_purpose_of_using_kafka_when_the_same/", "subreddit_subscribers": 156641, "created_utc": 1706446067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is by far the hardest course I ever took.\n\nMost of the things don't work I am constantly on Slack and running around on the net and harrasing gpt.\n\nI can't even figure out how to do the homework.\n\nSome questions are trivial, but there are others that require knowledge of data ingestion and docker compose.\n\nI have such a headache these last few days as I haven't moved an inch. The project for certification is copy paste, but the homework is basically knowing advanced knowledge for the thing you are supposed to be learning.  \nEDIT: I was wrong, I must have been somewhere else.  \nFortunately there exists a template for creating personal projects.  \nSo if I manage to get there that is a big plus.\n\nIt can't just be me who has the issue with the structure?\n\n&amp;#x200B;", "author_fullname": "t2_yn7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else struggle in DE zoomcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad7pcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706471070.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706462510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is by far the hardest course I ever took.&lt;/p&gt;\n\n&lt;p&gt;Most of the things don&amp;#39;t work I am constantly on Slack and running around on the net and harrasing gpt.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t even figure out how to do the homework.&lt;/p&gt;\n\n&lt;p&gt;Some questions are trivial, but there are others that require knowledge of data ingestion and docker compose.&lt;/p&gt;\n\n&lt;p&gt;I have such a headache these last few days as I haven&amp;#39;t moved an inch. The project for certification is copy paste, but the homework is basically knowing advanced knowledge for the thing you are supposed to be learning.&lt;br/&gt;\nEDIT: I was wrong, I must have been somewhere else.&lt;br/&gt;\nFortunately there exists a template for creating personal projects.&lt;br/&gt;\nSo if I manage to get there that is a big plus.&lt;/p&gt;\n\n&lt;p&gt;It can&amp;#39;t just be me who has the issue with the structure?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ad7pcc", "is_robot_indexable": true, "report_reasons": null, "author": "SemperPistos", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad7pcc/does_anyone_else_struggle_in_de_zoomcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad7pcc/does_anyone_else_struggle_in_de_zoomcamp/", "subreddit_subscribers": 156641, "created_utc": 1706462510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn even more about data engineering and would really like to connect to enthusiastic data folks. Does anybody has recommendations for good data related conference across europe in 2024?", "author_fullname": "t2_lj5pjrd6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best data conferences in Europe - 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad0vkk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706442110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn even more about data engineering and would really like to connect to enthusiastic data folks. Does anybody has recommendations for good data related conference across europe in 2024?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ad0vkk", "is_robot_indexable": true, "report_reasons": null, "author": "HolidayCritical3665", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad0vkk/best_data_conferences_in_europe_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad0vkk/best_data_conferences_in_europe_2024/", "subreddit_subscribers": 156641, "created_utc": 1706442110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, fellow data enthusiasts!\n\nI'm back with my second post, and I want to thank you for the incredible insights and support you all provided on my first one. This community is truly a goldmine of collective wisdom!\n\nNow, onto my current conundrum. The use cases at my day job are less than inspiring. I've tried to shake things up, suggesting new ideas and approaches I've discovered online, but the company is sticking to the old ways due to budget constraints.\n\nI firmly believe in the power of side projects to keep skills sharp and stay competitive in the tech market. I'm hitting a roadblock when finding side projects that could add some weight to my skills.\n\nSo, I'm reaching out to you, my fellow data engineers, for some advice on how you go about finding side projects that are not only meaningful but also enhance your skills\n\nAny advice, resources, or personal experiences you can share would be greatly appreciated", "author_fullname": "t2_vncr806l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Navigating Uninspiring Work Use Cases &amp; Finding Valuable Side Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad7y3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706463116.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, fellow data enthusiasts!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m back with my second post, and I want to thank you for the incredible insights and support you all provided on my first one. This community is truly a goldmine of collective wisdom!&lt;/p&gt;\n\n&lt;p&gt;Now, onto my current conundrum. The use cases at my day job are less than inspiring. I&amp;#39;ve tried to shake things up, suggesting new ideas and approaches I&amp;#39;ve discovered online, but the company is sticking to the old ways due to budget constraints.&lt;/p&gt;\n\n&lt;p&gt;I firmly believe in the power of side projects to keep skills sharp and stay competitive in the tech market. I&amp;#39;m hitting a roadblock when finding side projects that could add some weight to my skills.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m reaching out to you, my fellow data engineers, for some advice on how you go about finding side projects that are not only meaningful but also enhance your skills&lt;/p&gt;\n\n&lt;p&gt;Any advice, resources, or personal experiences you can share would be greatly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ad7y3l", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedSparrow8314", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad7y3l/navigating_uninspiring_work_use_cases_finding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad7y3l/navigating_uninspiring_work_use_cases_finding/", "subreddit_subscribers": 156641, "created_utc": 1706463116.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wrapping up an MVP for a service that transfers user data from one popular service's REST API to another. I initially based my cost calculations on execution time and memory allocation. However, I've started to incur charges for Data Transfer Out, specifically for the bandwidth used by these Lambdas. \n\nIs anyone here applying this in Data Engineering? I'm curious about the impact of this metric on your project or company's expenses.", "author_fullname": "t2_c8p730vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here using AWS Lambda Functions for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adk7dr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706495234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wrapping up an MVP for a service that transfers user data from one popular service&amp;#39;s REST API to another. I initially based my cost calculations on execution time and memory allocation. However, I&amp;#39;ve started to incur charges for Data Transfer Out, specifically for the bandwidth used by these Lambdas. &lt;/p&gt;\n\n&lt;p&gt;Is anyone here applying this in Data Engineering? I&amp;#39;m curious about the impact of this metric on your project or company&amp;#39;s expenses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adk7dr", "is_robot_indexable": true, "report_reasons": null, "author": "Dry_Big_4955", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adk7dr/anyone_here_using_aws_lambda_functions_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adk7dr/anyone_here_using_aws_lambda_functions_for_de/", "subreddit_subscribers": 156641, "created_utc": 1706495234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started at a small company (the whole data team is myself and three other self-taught guys doing SQL development, dashboarding, and basic DBA work) whose whole stack is in MSSQL, and they\u2019ve got no dev/test environment. All of their queries are tested right there in their production database that feeds all of their reporting solutions, which is really concerning me.\n\nI\u2019m coming from a major firm where they had a full sandbox with regularly snapshotted copies of the production DBs that you could develop and test on, but that\u2019s waaaaayyy too big for this company, so I\u2019m trying to figure out what the best practice for creating at least a development environment for them would be. Something that also has their linked server connections that the main DB has, ideally.\n\nIs the best option a replicated copy of the prod DB, taken every morning after the overnight ETL run, that we all just develop on? They\u2019re still all on SQL Agent and SSIS (old school, but it works for now), so can we migrate any jobs/packages we develop on the dev environment over to prod?\n\nWould it be best to entirely separate prod and dev, running on separate VMs, rather than having multiple DBs on the same server? We\u2019ll soon have a two more SQL devs joining us, so we could get pretty full up some people are in prod and others are in dev, all on the same server simultaneously.", "author_fullname": "t2_bnb6qfsd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a prod/dev(/test?) SQL environment split for a small company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adix2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706491392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started at a small company (the whole data team is myself and three other self-taught guys doing SQL development, dashboarding, and basic DBA work) whose whole stack is in MSSQL, and they\u2019ve got no dev/test environment. All of their queries are tested right there in their production database that feeds all of their reporting solutions, which is really concerning me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m coming from a major firm where they had a full sandbox with regularly snapshotted copies of the production DBs that you could develop and test on, but that\u2019s waaaaayyy too big for this company, so I\u2019m trying to figure out what the best practice for creating at least a development environment for them would be. Something that also has their linked server connections that the main DB has, ideally.&lt;/p&gt;\n\n&lt;p&gt;Is the best option a replicated copy of the prod DB, taken every morning after the overnight ETL run, that we all just develop on? They\u2019re still all on SQL Agent and SSIS (old school, but it works for now), so can we migrate any jobs/packages we develop on the dev environment over to prod?&lt;/p&gt;\n\n&lt;p&gt;Would it be best to entirely separate prod and dev, running on separate VMs, rather than having multiple DBs on the same server? We\u2019ll soon have a two more SQL devs joining us, so we could get pretty full up some people are in prod and others are in dev, all on the same server simultaneously.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adix2j", "is_robot_indexable": true, "report_reasons": null, "author": "Seth_Littrells_alt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adix2j/creating_a_proddevtest_sql_environment_split_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adix2j/creating_a_proddevtest_sql_environment_split_for/", "subreddit_subscribers": 156641, "created_utc": 1706491392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What you think the future of Analytics Engineering will be especially compared to data engineering.", "author_fullname": "t2_7tah61j5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the future of Analytics Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adl2sj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706497823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What you think the future of Analytics Engineering will be especially compared to data engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adl2sj", "is_robot_indexable": true, "report_reasons": null, "author": "Test_Known", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adl2sj/what_is_the_future_of_analytics_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adl2sj/what_is_the_future_of_analytics_engineering/", "subreddit_subscribers": 156641, "created_utc": 1706497823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was discussing professional certifications with my sister, she just completed 3 years of experience. \nThe place where she works at, just got established and the business is in its early stages.. however, she wants to start building the business intelligence centre with them and hopefully can lead the team in a year or two. \nWhat professional certifications will help her? and what skills she should develop? \n\nPersonally I think PMP and Data+ can help but she kinda hesitant.", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What professional certifications will help to lead BI/Analytics team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ada7r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706468743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was discussing professional certifications with my sister, she just completed 3 years of experience. \nThe place where she works at, just got established and the business is in its early stages.. however, she wants to start building the business intelligence centre with them and hopefully can lead the team in a year or two. \nWhat professional certifications will help her? and what skills she should develop? &lt;/p&gt;\n\n&lt;p&gt;Personally I think PMP and Data+ can help but she kinda hesitant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ada7r0", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ada7r0/what_professional_certifications_will_help_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ada7r0/what_professional_certifications_will_help_to/", "subreddit_subscribers": 156641, "created_utc": 1706468743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For our dimensional tables we use SCD 2 and cut a new record every time there is a change in any one of the fields. We use SSIS's SCD component which does it nicely.\n\nWe will be now moving the DWH to snowflake. Not sure if we want to keep SSIS as the tool for it.\n\nWhat do you use with snowflake for SCD?", "author_fullname": "t2_5epqry7a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly changing dimension in DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad986k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706466292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For our dimensional tables we use SCD 2 and cut a new record every time there is a change in any one of the fields. We use SSIS&amp;#39;s SCD component which does it nicely.&lt;/p&gt;\n\n&lt;p&gt;We will be now moving the DWH to snowflake. Not sure if we want to keep SSIS as the tool for it.&lt;/p&gt;\n\n&lt;p&gt;What do you use with snowflake for SCD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ad986k", "is_robot_indexable": true, "report_reasons": null, "author": "liskeeksil", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad986k/slowly_changing_dimension_in_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad986k/slowly_changing_dimension_in_dwh/", "subreddit_subscribers": 156641, "created_utc": 1706466292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I came across freecodecamp\u2019s Data Engineering course on YouTube. Has anyone gone through this course and is it worth doing it as a beginner?\n\nhttps://youtu.be/PHsC_t0j1dU?si=-7RAbaZu2Au9ciz2", "author_fullname": "t2_yoget", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FreeCodeCamp Data Engineering video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1admw7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706503536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across freecodecamp\u2019s Data Engineering course on YouTube. Has anyone gone through this course and is it worth doing it as a beginner?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/PHsC_t0j1dU?si=-7RAbaZu2Au9ciz2\"&gt;https://youtu.be/PHsC_t0j1dU?si=-7RAbaZu2Au9ciz2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mGsYg_ylg57VHgJAIM96SMePGnSlyMAcPguHqtECRh4.jpg?auto=webp&amp;s=ae21261c6da8f1a7471bb509cb387ee2ebe68908", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/mGsYg_ylg57VHgJAIM96SMePGnSlyMAcPguHqtECRh4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bc643bc5f7f31a4e7c8094b9cb9a323834ac05a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/mGsYg_ylg57VHgJAIM96SMePGnSlyMAcPguHqtECRh4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86a5720cb1c125c5b565b252a4a3d29effa47389", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/mGsYg_ylg57VHgJAIM96SMePGnSlyMAcPguHqtECRh4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b157ec43c10c7c49883384a830f079e4ba256053", "width": 320, "height": 240}], "variants": {}, "id": "K2rE4jZIYuh2uorc6Xa2478uROUgs6cHbxzuj_WWC6Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1admw7y", "is_robot_indexable": true, "report_reasons": null, "author": "Drrazor", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1admw7y/freecodecamp_data_engineering_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1admw7y/freecodecamp_data_engineering_video/", "subreddit_subscribers": 156641, "created_utc": 1706503536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I read the Designing Data-Intensive Applications book by Martin Kleppmann a few years ago. It is an accessible encyclopedia-like book about databases and distributed systems. The big plus of this book is that it has a lot of references for further reading.\n\nNow I'm looking for something similar in terms of content, but with a focus on the modern data engineering stack that covers technologies like Flink, Kafka, Pulsar, Pinot, Spark, Iceberg, Clickhouse, and others. The use cases behind using them, their advantages and disadvantages, and how they all integrate.\n\nIn other words, something that covers the whole modern landscape of data engineering tools.\n\nIt would be good if it wouldn't be an in-depth 1000+ pages read. Something that an experienced backend engineer (not a data engineer) could grasp in a few weekends. It may be a book, a set of articles, or a series of videos. The form is not important.\n\nWhat would you recommend?", "author_fullname": "t2_15hg9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive recommendations on the modern data engineering tools ecosystem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adckj2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706474655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read the Designing Data-Intensive Applications book by Martin Kleppmann a few years ago. It is an accessible encyclopedia-like book about databases and distributed systems. The big plus of this book is that it has a lot of references for further reading.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m looking for something similar in terms of content, but with a focus on the modern data engineering stack that covers technologies like Flink, Kafka, Pulsar, Pinot, Spark, Iceberg, Clickhouse, and others. The use cases behind using them, their advantages and disadvantages, and how they all integrate.&lt;/p&gt;\n\n&lt;p&gt;In other words, something that covers the whole modern landscape of data engineering tools.&lt;/p&gt;\n\n&lt;p&gt;It would be good if it wouldn&amp;#39;t be an in-depth 1000+ pages read. Something that an experienced backend engineer (not a data engineer) could grasp in a few weekends. It may be a book, a set of articles, or a series of videos. The form is not important.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adckj2", "is_robot_indexable": true, "report_reasons": null, "author": "visortelle", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adckj2/comprehensive_recommendations_on_the_modern_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adckj2/comprehensive_recommendations_on_the_modern_data/", "subreddit_subscribers": 156641, "created_utc": 1706474655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have the first round of interviews coming up in a week. The interviewer asked to solve HackerRank hard SQL and Medium Python. Have gone through the SQL questions but I would like to have some more practice. Any tips on where to find more relevant SQL exercises for this role? \n\nAlso, would highly appreciate if anyone with experience of going through this interview or with experience of working in this role leave any comments/tips for the interview. Thanks in advance!", "author_fullname": "t2_9axqyq8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips for the technical round prep (SQL, Python) for Amazon Sr. BIE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad3mxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706451522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have the first round of interviews coming up in a week. The interviewer asked to solve HackerRank hard SQL and Medium Python. Have gone through the SQL questions but I would like to have some more practice. Any tips on where to find more relevant SQL exercises for this role? &lt;/p&gt;\n\n&lt;p&gt;Also, would highly appreciate if anyone with experience of going through this interview or with experience of working in this role leave any comments/tips for the interview. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ad3mxb", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult-Big-3890", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad3mxb/any_tips_for_the_technical_round_prep_sql_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad3mxb/any_tips_for_the_technical_round_prep_sql_python/", "subreddit_subscribers": 156641, "created_utc": 1706451522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.\n\nDo you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?", "author_fullname": "t2_hnhowjrxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happened recently with Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1adsbnd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706524483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.&lt;/p&gt;\n\n&lt;p&gt;Do you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adsbnd", "is_robot_indexable": true, "report_reasons": null, "author": "BubblyImpress7078", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsbnd/what_happened_recently_with_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsbnd/what_happened_recently_with_snowflake/", "subreddit_subscribers": 156641, "created_utc": 1706524483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically, I have a macro that dynamically parses a json. This is advantageous bc upstream we find that databases change occasionally. However, I\u2019m having a hell of time figuring out how to type floats, dates, integers, etc in the downstream model without explicitly casting each column. Many tables per database are 150+ columns. \n\nAnd while Snowflake will interpret fairly well as consumers query either the string or variant it gets messy when data consumers are using Jupyter or SAS (can\u2019t change it, nothing to do there). \n\nSo, how do I cast these data types cleanly through a macro that also parses a json field. \n\nAny help or insight is greatly appreciated.\n\nEdit: more info \u2014 Kafka as upstream, DBT for transformation, Snowflake as db. ", "author_fullname": "t2_j2qne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Frustrated with DBT and Kafka, how are you supposed to type data as opposed to parsing all of it dynamically into a string or a variant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adjbud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706492597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, I have a macro that dynamically parses a json. This is advantageous bc upstream we find that databases change occasionally. However, I\u2019m having a hell of time figuring out how to type floats, dates, integers, etc in the downstream model without explicitly casting each column. Many tables per database are 150+ columns. &lt;/p&gt;\n\n&lt;p&gt;And while Snowflake will interpret fairly well as consumers query either the string or variant it gets messy when data consumers are using Jupyter or SAS (can\u2019t change it, nothing to do there). &lt;/p&gt;\n\n&lt;p&gt;So, how do I cast these data types cleanly through a macro that also parses a json field. &lt;/p&gt;\n\n&lt;p&gt;Any help or insight is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Edit: more info \u2014 Kafka as upstream, DBT for transformation, Snowflake as db. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adjbud", "is_robot_indexable": true, "report_reasons": null, "author": "r0ck13r4c00n", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adjbud/frustrated_with_dbt_and_kafka_how_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adjbud/frustrated_with_dbt_and_kafka_how_are_you/", "subreddit_subscribers": 156641, "created_utc": 1706492597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reviewing new systems and one key component that they reference is the usage of complex transformations. \n\nI know that generally, people will say \"Lots of joins, aggregations etc\". But that is a little too general for my level of experience (which is low). \n\nCan you please give me some sort of guideline, heuristic, rule of thumb, or scoring guide that will allow me to more objectively determine if I have complex transformations in my data environment? \n\nAny and all tips and greatly appreciated. ", "author_fullname": "t2_y8ebf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you determine if you have 'Complex Transformations'?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adiie2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706490177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reviewing new systems and one key component that they reference is the usage of complex transformations. &lt;/p&gt;\n\n&lt;p&gt;I know that generally, people will say &amp;quot;Lots of joins, aggregations etc&amp;quot;. But that is a little too general for my level of experience (which is low). &lt;/p&gt;\n\n&lt;p&gt;Can you please give me some sort of guideline, heuristic, rule of thumb, or scoring guide that will allow me to more objectively determine if I have complex transformations in my data environment? &lt;/p&gt;\n\n&lt;p&gt;Any and all tips and greatly appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adiie2", "is_robot_indexable": true, "report_reasons": null, "author": "Raging-Loner", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adiie2/how_do_you_determine_if_you_have_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adiie2/how_do_you_determine_if_you_have_complex/", "subreddit_subscribers": 156641, "created_utc": 1706490177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that we need to learn all these different tools to build a stack as a data engineer but i was wondering about whether there is a CI/CD process involved in a professional project. if so what additional tools are used for this.", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer as a Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1adsmmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706525722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that we need to learn all these different tools to build a stack as a data engineer but i was wondering about whether there is a CI/CD process involved in a professional project. if so what additional tools are used for this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1adsmmu", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsmmu/data_engineer_as_a_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsmmu/data_engineer_as_a_professional/", "subreddit_subscribers": 156641, "created_utc": 1706525722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The challenge in optimizing the data lake for PowerBI analytics arises from distinct product dimensions with specific features in each department, causing complications in cross-department reporting due to inconsistent SKs assigned to the same product. \n\nTo address this issue, [a proposed solution involves creating a master product dimension for a unified source of truth](https://www.dataversity.net/what-is-master-data-management-and-why-is-it-important/#:%7E:text=Improved%20Data%20Quality%3A%20Master%20Data,businesses%20to%20make%20better%20decisions.).  Concerns have been raised about complexity, especially regarding varying product details in sales and marketing, and the inclusion of unique finance products. \n\nI undertook the following exercise to gain insight into the potential behavior of this master data. As you can observe below, the number of lines is anticipated to be substantial. This is due to the fact that, for every combination of product and features, an individual line would be generated, distinguished by a unique SKU identifier.\n\nhttps://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3\n\nSo here the questions arise:\n\n1. Are there any technical drawbacks or best practice violations associated with maintaining a global product table like that, considering the potential table size and repetition information, like the finance one, for example, across multiple lines?\n2. How does the proposed solution affect the ease of maintenance and updates to the global product table? Are there complexities in managing changes, additions, or deletions with different departments having distinct products or features?\n3. How would maintaining a large master product dimension impact system performance and scalability?", "author_fullname": "t2_kxhgl4or", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Dimension Models in Data Lakes: Master Product Dimension vs. Tailored Department Dimensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": true, "media_metadata": {"4royh9jaycfc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=198095c418c4de88cd6631fb31c4a194466ea26b"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a5872b58420ef2f6b3897ca956c274ec0f28527"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=557528dc4bce94b151f9e3ce010fc6e99c5ff7fd"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d69dea6f8fe9985a005893743be2e8246aa1c2bb"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2277d703a368096561e52b557c0c2a6678c184d4"}, {"y": 473, "x": 1080, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a5b6bc31d3d09dbf4d571b7e807bef837721035"}], "s": {"y": 480, "x": 1095, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3"}, "id": "4royh9jaycfc1"}}, "name": "t3_1adsfka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DlYDL1FDMhWqArc7ApoIzBWHhCEojYIAYqLv5D7xgxc.jpg", "edited": 1706525156.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1706524927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The challenge in optimizing the data lake for PowerBI analytics arises from distinct product dimensions with specific features in each department, causing complications in cross-department reporting due to inconsistent SKs assigned to the same product. &lt;/p&gt;\n\n&lt;p&gt;To address this issue, &lt;a href=\"https://www.dataversity.net/what-is-master-data-management-and-why-is-it-important/#:%7E:text=Improved%20Data%20Quality%3A%20Master%20Data,businesses%20to%20make%20better%20decisions.\"&gt;a proposed solution involves creating a master product dimension for a unified source of truth&lt;/a&gt;.  Concerns have been raised about complexity, especially regarding varying product details in sales and marketing, and the inclusion of unique finance products. &lt;/p&gt;\n\n&lt;p&gt;I undertook the following exercise to gain insight into the potential behavior of this master data. As you can observe below, the number of lines is anticipated to be substantial. This is due to the fact that, for every combination of product and features, an individual line would be generated, distinguished by a unique SKU identifier.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3\"&gt;https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So here the questions arise:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there any technical drawbacks or best practice violations associated with maintaining a global product table like that, considering the potential table size and repetition information, like the finance one, for example, across multiple lines?&lt;/li&gt;\n&lt;li&gt;How does the proposed solution affect the ease of maintenance and updates to the global product table? Are there complexities in managing changes, additions, or deletions with different departments having distinct products or features?&lt;/li&gt;\n&lt;li&gt;How would maintaining a large master product dimension impact system performance and scalability?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?auto=webp&amp;s=438a6872ab09140ec6dc9fb7b68ccc6c20d6259f", "width": 600, "height": 448}, "resolutions": [{"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea7fa6486e997424307e15991720d1f145d00397", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e59110c0f252a0f8bcb310a31575865828db2f37", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddaa41a3c5878fd9d0da4c1e932964d019e459cd", "width": 320, "height": 238}], "variants": {}, "id": "8Hfj_A51oBRRXTV3N6uczaapOWcexuKas5PYBkTU0uw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adsfka", "is_robot_indexable": true, "report_reasons": null, "author": "tazz_bh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsfka/optimizing_dimension_models_in_data_lakes_master/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsfka/optimizing_dimension_models_in_data_lakes_master/", "subreddit_subscribers": 156641, "created_utc": 1706524927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've an interview tomorrow and in JO they have specified a line about data integrity and accuracy.\nI expect that a question on data integrity and accuracy will be asked and I'm wondering which real practice could be done for data integrity and accuracy.\n\nHow do you manage Data Integrity and Accuracy in your projects ?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you implement data integrity and Accuracy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1adse74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706524777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve an interview tomorrow and in JO they have specified a line about data integrity and accuracy.\nI expect that a question on data integrity and accuracy will be asked and I&amp;#39;m wondering which real practice could be done for data integrity and accuracy.&lt;/p&gt;\n\n&lt;p&gt;How do you manage Data Integrity and Accuracy in your projects ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1adse74", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adse74/how_do_you_implement_data_integrity_and_accuracy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adse74/how_do_you_implement_data_integrity_and_accuracy/", "subreddit_subscribers": 156641, "created_utc": 1706524777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! There is a role I have got a referral for that pretty much aligns with what I intend to learn for my own upskilling. Pretty new to data engineering but I really want to go down this rabbit hole.  \n\n\n**Technical skills:**\n\n* Has experience in creating reports, visualizations and analysis from large data sets\n* Has written scripting to extract or to analyze data sets\n* Understands the role of a data engineer, software engineer, data analyst, data scientist, or related roles\n* Has some experience and has worked with the following technologies\n   * Essential\n      * Microsoft Azure Data Integration Stack (Azure Data Lake, Azure Data Factory, Delta Lake, Databricks), or equivalent\n      * Data Visualization tools, for example, Power BI, Tableau, Sigma, Qlik, or equivalent\n   * Desirable\n      * Programming language for analyzing data, for example, Python, R, or equivalent\n      * Generative/Open AI\n      * Other RPA tools\n      * Azure GIT or Jira or Confluence\n* Technical documentation and writing skills, and have published API documentation or similar\n* Experience or familiar with Agile ways of working  \n\n\nHow would you best advice me on preparing for the interview or any other DE internship to be honest, assuming I have almost 0 experience working with azure products or any other cloud-related products. I only have a small project using tkinter and matplot for data visualisation which is not common industry practice and a small webscraping script I made. I have about a years worth of experience with Python and C.  \n\n\nDo let me know what other information I could give about myself that I left out that would be important to the advice you want to give me or others in the same position as me. Cheers!  \n\n\n&amp;#x200B;", "author_fullname": "t2_bcg3mjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview help for Data Engineering Internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ads8ru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706524153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! There is a role I have got a referral for that pretty much aligns with what I intend to learn for my own upskilling. Pretty new to data engineering but I really want to go down this rabbit hole.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical skills:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has experience in creating reports, visualizations and analysis from large data sets&lt;/li&gt;\n&lt;li&gt;Has written scripting to extract or to analyze data sets&lt;/li&gt;\n&lt;li&gt;Understands the role of a data engineer, software engineer, data analyst, data scientist, or related roles&lt;/li&gt;\n&lt;li&gt;Has some experience and has worked with the following technologies\n\n&lt;ul&gt;\n&lt;li&gt;Essential\n\n&lt;ul&gt;\n&lt;li&gt;Microsoft Azure Data Integration Stack (Azure Data Lake, Azure Data Factory, Delta Lake, Databricks), or equivalent&lt;/li&gt;\n&lt;li&gt;Data Visualization tools, for example, Power BI, Tableau, Sigma, Qlik, or equivalent&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Desirable\n\n&lt;ul&gt;\n&lt;li&gt;Programming language for analyzing data, for example, Python, R, or equivalent&lt;/li&gt;\n&lt;li&gt;Generative/Open AI&lt;/li&gt;\n&lt;li&gt;Other RPA tools&lt;/li&gt;\n&lt;li&gt;Azure GIT or Jira or Confluence&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Technical documentation and writing skills, and have published API documentation or similar&lt;/li&gt;\n&lt;li&gt;Experience or familiar with Agile ways of working&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How would you best advice me on preparing for the interview or any other DE internship to be honest, assuming I have almost 0 experience working with azure products or any other cloud-related products. I only have a small project using tkinter and matplot for data visualisation which is not common industry practice and a small webscraping script I made. I have about a years worth of experience with Python and C.  &lt;/p&gt;\n\n&lt;p&gt;Do let me know what other information I could give about myself that I left out that would be important to the advice you want to give me or others in the same position as me. Cheers!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ads8ru", "is_robot_indexable": true, "report_reasons": null, "author": "PatientClothes9394", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ads8ru/interview_help_for_data_engineering_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ads8ru/interview_help_for_data_engineering_internship/", "subreddit_subscribers": 156641, "created_utc": 1706524153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "mod pls delete if not relevant.\n\ni was a AI Engineer trainee for about a year, then a Data Analyst for a year before that. my company is laying off people so im urgently looking for another job and surprisingly a mid-level DE position in a government-linked company invited me for an interview though i applied for a DA position. They told me my background was \"too technical\" for a DA role and now i guess i have a DE interview tomorrow?? \n\nI suspect the HR saw i used azure cloud and databricks and automatically assumed theyre the same as a DE skillset..i dont know man.\n\nAre there any transferable skills to be a DE from a DA/AI type role and any tips anyone here can give? im interested in Data Engineering but worried i will bomb the interview and not be up for the job. i have no clue how to design databases or tables or pipelines other than the usual star/snowflake schema basic stuff u learn in powerBI class lmao.", "author_fullname": "t2_8xqdvapu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for DE interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ads4nq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706523684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;mod pls delete if not relevant.&lt;/p&gt;\n\n&lt;p&gt;i was a AI Engineer trainee for about a year, then a Data Analyst for a year before that. my company is laying off people so im urgently looking for another job and surprisingly a mid-level DE position in a government-linked company invited me for an interview though i applied for a DA position. They told me my background was &amp;quot;too technical&amp;quot; for a DA role and now i guess i have a DE interview tomorrow?? &lt;/p&gt;\n\n&lt;p&gt;I suspect the HR saw i used azure cloud and databricks and automatically assumed theyre the same as a DE skillset..i dont know man.&lt;/p&gt;\n\n&lt;p&gt;Are there any transferable skills to be a DE from a DA/AI type role and any tips anyone here can give? im interested in Data Engineering but worried i will bomb the interview and not be up for the job. i have no clue how to design databases or tables or pipelines other than the usual star/snowflake schema basic stuff u learn in powerBI class lmao.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ads4nq", "is_robot_indexable": true, "report_reasons": null, "author": "raspberry6754", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ads4nq/advice_for_de_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ads4nq/advice_for_de_interview/", "subreddit_subscribers": 156641, "created_utc": 1706523684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nthe company I am working for using databricks (standard) for the transformations on the datalake that is built on Azure.\n\n&amp;#x200B;\n\nNow I need to give access to an external developer for some developments but he is only allowed access to a certain specific tables. I know it's not possible to do this without the premium tier, but the company also doesn't want to upgrade databricks to premium tier.\n\n&amp;#x200B;\n\nWhat is the best way to go around this? Should I create a temporary new databricks environment and deliver the tables in CSV/Parquet files and let the developer read those tables in the new databricks environment and make the developments? And once those developments are made, should I just copy those development to our real databricks environment?\n\n&amp;#x200B;\n\nOr do you guys see another way to manage this\n\n&amp;#x200B;", "author_fullname": "t2_2cd0q7u5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on how to give access to specific tables in databricks (standard tier)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adrfku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706520846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;the company I am working for using databricks (standard) for the transformations on the datalake that is built on Azure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now I need to give access to an external developer for some developments but he is only allowed access to a certain specific tables. I know it&amp;#39;s not possible to do this without the premium tier, but the company also doesn&amp;#39;t want to upgrade databricks to premium tier.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the best way to go around this? Should I create a temporary new databricks environment and deliver the tables in CSV/Parquet files and let the developer read those tables in the new databricks environment and make the developments? And once those developments are made, should I just copy those development to our real databricks environment?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or do you guys see another way to manage this&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adrfku", "is_robot_indexable": true, "report_reasons": null, "author": "kbic93", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adrfku/advice_on_how_to_give_access_to_specific_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adrfku/advice_on_how_to_give_access_to_specific_tables/", "subreddit_subscribers": 156641, "created_utc": 1706520846.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_46tlcjz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notes on Postgres user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1adrdbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ItXhhbPlVmWQpOS6kemUmpUWhRzTR3sZQvF1DJkso2A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706520564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "telablog.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://telablog.com/notes-on-postgres-user-management", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?auto=webp&amp;s=ff5fb6ff9a2f3334a2cd017ae15057f8307ebd39", "width": 800, "height": 533}, "resolutions": [{"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5af5b2f61917e95b77c9f334bc195fb37d0d884", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9dd1168e671cf200608d409f57acc42084cc3a7", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=32aecf9bd4dc2ea8d3654641774d4c7adf9b50f7", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41f6721dbb4c6b68e66b75dc35fbf1fad48b6c76", "width": 640, "height": 426}], "variants": {}, "id": "zN5Pqvs0hqwGFjxK4E-GpZ9fGcOAAObX-PeopO8JVGo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1adrdbg", "is_robot_indexable": true, "report_reasons": null, "author": "stjohn_piano", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adrdbg/notes_on_postgres_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://telablog.com/notes-on-postgres-user-management", "subreddit_subscribers": 156641, "created_utc": 1706520564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I need some help with the following.\nMy organisation has setup Google Ads to Bigquery pipelines via Bigquery Data Transfer Service. However the data being ingested is t-2 fashion and the same is being reflected on dashboards. \nSchedule Backfill is an option but does running it as a one time transfer fetch historical data as well? Since we don\u2019t want any duplicates. What would be the best practice in the above scenario?\nKindly help and let me know accordingly", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google ads to Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adr96e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706520072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I need some help with the following.\nMy organisation has setup Google Ads to Bigquery pipelines via Bigquery Data Transfer Service. However the data being ingested is t-2 fashion and the same is being reflected on dashboards. \nSchedule Backfill is an option but does running it as a one time transfer fetch historical data as well? Since we don\u2019t want any duplicates. What would be the best practice in the above scenario?\nKindly help and let me know accordingly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adr96e", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adr96e/google_ads_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adr96e/google_ads_to_bigquery/", "subreddit_subscribers": 156641, "created_utc": 1706520072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I need your insights about my problem in my current project.\n\nSo, we need to ingest data from a DB to S3 using AWS Glue. We're planning to use incremental reload after the first ingestion since our tables are large ( 200M rows).\n\nOur problem is that are tables do not have primary keys and a date column that will indicate when was the record is inserted in the DB.\n\nOne workaround is we do full reload, but this will be costly since we're planning to do our data refresh daily.\n\nDo you have any idea for a workaround?", "author_fullname": "t2_num46cmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is incremental reload possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adqpth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706517793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I need your insights about my problem in my current project.&lt;/p&gt;\n\n&lt;p&gt;So, we need to ingest data from a DB to S3 using AWS Glue. We&amp;#39;re planning to use incremental reload after the first ingestion since our tables are large ( 200M rows).&lt;/p&gt;\n\n&lt;p&gt;Our problem is that are tables do not have primary keys and a date column that will indicate when was the record is inserted in the DB.&lt;/p&gt;\n\n&lt;p&gt;One workaround is we do full reload, but this will be costly since we&amp;#39;re planning to do our data refresh daily.&lt;/p&gt;\n\n&lt;p&gt;Do you have any idea for a workaround?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adqpth", "is_robot_indexable": true, "report_reasons": null, "author": "matchaaa_latte", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adqpth/is_incremental_reload_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adqpth/is_incremental_reload_possible/", "subreddit_subscribers": 156641, "created_utc": 1706517793.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}