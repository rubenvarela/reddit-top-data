{"kind": "Listing", "data": {"after": "t3_1aenokc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I\u2019m a Jr data engineer which is facing a challenge and I would love to know your opinions and expertise in this topic:\n\nI\u2019m currently handling allots of data in SQL, we receive at a high frequency JSONs with raw data in it (in a single json there could be more than 10k raws) \n\nThe thing is that we need to make some statistics with this JSONS \nWe need to concatenate several Jsons and then apply the statistics (calculate outliers, calculate avgs, calculate percentages, stds, frequency, etc\u2026) \n\nAnd after calculating it we need to insert it in a new table which handles summarizes data. \nAll of this in a SQL stored procedure, the hole process lasts more than 3hours to complete, is there any advice for this kind of stuff, some literature I can read, videos or something to optimize the solution? \nI\u2019m also open to other robust pipelines besides only using SQL!", "author_fullname": "t2_gcej0pe8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing high amount of data with SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aepsld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706625257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I\u2019m a Jr data engineer which is facing a challenge and I would love to know your opinions and expertise in this topic:&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently handling allots of data in SQL, we receive at a high frequency JSONs with raw data in it (in a single json there could be more than 10k raws) &lt;/p&gt;\n\n&lt;p&gt;The thing is that we need to make some statistics with this JSONS \nWe need to concatenate several Jsons and then apply the statistics (calculate outliers, calculate avgs, calculate percentages, stds, frequency, etc\u2026) &lt;/p&gt;\n\n&lt;p&gt;And after calculating it we need to insert it in a new table which handles summarizes data. \nAll of this in a SQL stored procedure, the hole process lasts more than 3hours to complete, is there any advice for this kind of stuff, some literature I can read, videos or something to optimize the solution? \nI\u2019m also open to other robust pipelines besides only using SQL!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aepsld", "is_robot_indexable": true, "report_reasons": null, "author": "Alex_Alca_", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aepsld/processing_high_amount_of_data_with_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aepsld/processing_high_amount_of_data_with_sql/", "subreddit_subscribers": 157048, "created_utc": 1706625257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There's been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there's an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. \n\n&amp;#x200B;\n\nMost of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I'm trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn't have a separate develop environment to develop models, like I'll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don't want to break anything.\n\n&amp;#x200B;\n\nBasically, I'm getting anxious about not knowing what I don't know regarding most things DE and I want to sack up and try to get better at it but I'm not quite sure where to start.", "author_fullname": "t2_4xc90hk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do to elevate your skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeefxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706584492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There&amp;#39;s been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there&amp;#39;s an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Most of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I&amp;#39;m trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn&amp;#39;t have a separate develop environment to develop models, like I&amp;#39;ll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don&amp;#39;t want to break anything.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, I&amp;#39;m getting anxious about not knowing what I don&amp;#39;t know regarding most things DE and I want to sack up and try to get better at it but I&amp;#39;m not quite sure where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeefxr", "is_robot_indexable": true, "report_reasons": null, "author": "thisisformeworking", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "subreddit_subscribers": 157048, "created_utc": 1706584492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.\n\nIs there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.", "author_fullname": "t2_j8v7zu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Data Stack in Higher Education?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeulzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706637307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.&lt;/p&gt;\n\n&lt;p&gt;Is there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeulzm", "is_robot_indexable": true, "report_reasons": null, "author": "BIntelligent", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "subreddit_subscribers": 157048, "created_utc": 1706637307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Shouldn't be a huge amount of data, &lt;200 GB currently in the DB.  \n\nI titled this as 'replicate', but it's really just ELT without the T.  This replication wouldn't have to be realtime, by the way.\n\nWe're hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?\n\nEdit: edited for clarity.", "author_fullname": "t2_j15uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to replicate from AWS Sql Server db to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae7u5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706566863.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706566365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Shouldn&amp;#39;t be a huge amount of data, &amp;lt;200 GB currently in the DB.  &lt;/p&gt;\n\n&lt;p&gt;I titled this as &amp;#39;replicate&amp;#39;, but it&amp;#39;s really just ELT without the T.  This replication wouldn&amp;#39;t have to be realtime, by the way.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?&lt;/p&gt;\n\n&lt;p&gt;Edit: edited for clarity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae7u5p", "is_robot_indexable": true, "report_reasons": null, "author": "jbrune", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "subreddit_subscribers": 157048, "created_utc": 1706566365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe've been using ADF in our team, purely ADF. With AI especially my development is so much faster with SP's and scripts, but theres a push to do everything in a dataflow. Obviously its worse off for us if something takes 5-10x longer to develop when I can't use a SP for everything, happy to orchestrate runs in ADF but the actual ETL logic I feel should be done using SQL or python databricks.\n\nHe tried to solely use an ADF to run an API, and when he couldn't he said we can't use the API anymore, insanity. He then blamed the company itself when it wouldn't work.\n\nI'm going to see if I can do a cost comparison in my own time and show everyone how wrong he is, but does anyone else have any information or knowledge on this? I'm actually new to azure and adf so I can't say.", "author_fullname": "t2_4nomabkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost of ADF using Stored Procedures vs pure ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aephtl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706624397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been using ADF in our team, purely ADF. With AI especially my development is so much faster with SP&amp;#39;s and scripts, but theres a push to do everything in a dataflow. Obviously its worse off for us if something takes 5-10x longer to develop when I can&amp;#39;t use a SP for everything, happy to orchestrate runs in ADF but the actual ETL logic I feel should be done using SQL or python databricks.&lt;/p&gt;\n\n&lt;p&gt;He tried to solely use an ADF to run an API, and when he couldn&amp;#39;t he said we can&amp;#39;t use the API anymore, insanity. He then blamed the company itself when it wouldn&amp;#39;t work.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to see if I can do a cost comparison in my own time and show everyone how wrong he is, but does anyone else have any information or knowledge on this? I&amp;#39;m actually new to azure and adf so I can&amp;#39;t say.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aephtl", "is_robot_indexable": true, "report_reasons": null, "author": "cantseemelol", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aephtl/cost_of_adf_using_stored_procedures_vs_pure_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aephtl/cost_of_adf_using_stored_procedures_vs_pure_adf/", "subreddit_subscribers": 157048, "created_utc": 1706624397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lw1vuj1gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Things Data Consulting Clients Absolutely Hate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_1aen5t7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/O0jx9r0F8PJm7ckhQLiBQqDNcA8zRlzNAsz1k3-Ivo8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706617296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/6-things-data-consulting-clients-absolutely-hate/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?auto=webp&amp;s=8b86994c2f69b95b57e1c4dac243605c5ee3e5a5", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b230520895ff2aed8deb78df1a1e8cce655a7bb", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8094d9413e68a56bfb087cffcb372bab7299e5f8", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb00dbece4019b1772068b1854ed84fdea198835", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=274870ffbf4102b8f2c3313354f6a94e232a4dbf", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1332e876938e87b03510feed48a4e0748858140", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=194f45e91b4884dcbb348d587cc7bfa5d690697f", "width": 1080, "height": 617}], "variants": {}, "id": "1jE9dc2gAnTAQGWvyOZUUSlh1-b8h_HZjfgyLv41uUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aen5t7", "is_robot_indexable": true, "report_reasons": null, "author": "Distinct-Economics24", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aen5t7/6_things_data_consulting_clients_absolutely_hate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/6-things-data-consulting-clients-absolutely-hate/", "subreddit_subscribers": 157048, "created_utc": 1706617296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a computer science student about to graduate and I'm trying to find projects to help build my skills in data engineering. I'm having trouble finding API's with good free tiers that I could build a project around. Does anyone have any suggestions? I've tried looking into sports data ones specifically the NBA but the ones I've found aren't cheap", "author_fullname": "t2_9x6d0tf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free API's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aee0tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706583283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a computer science student about to graduate and I&amp;#39;m trying to find projects to help build my skills in data engineering. I&amp;#39;m having trouble finding API&amp;#39;s with good free tiers that I could build a project around. Does anyone have any suggestions? I&amp;#39;ve tried looking into sports data ones specifically the NBA but the ones I&amp;#39;ve found aren&amp;#39;t cheap&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aee0tx", "is_robot_indexable": true, "report_reasons": null, "author": "TheDrunkInTheNorth", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aee0tx/free_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aee0tx/free_apis/", "subreddit_subscribers": 157048, "created_utc": 1706583283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. What do you think of the data mesh concept?\n2. Have you implemented or used a data mesh?\n3. If so, what was the experience? What did you learn?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you implemented or used a data mesh?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeqowi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706627670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;What do you think of the data mesh concept?&lt;/li&gt;\n&lt;li&gt;Have you implemented or used a data mesh?&lt;/li&gt;\n&lt;li&gt;If so, what was the experience? What did you learn?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeqowi", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeqowi/have_you_implemented_or_used_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeqowi/have_you_implemented_or_used_a_data_mesh/", "subreddit_subscribers": 157048, "created_utc": 1706627670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,   \n\n\nAt the moment, I am in the process of setting up a new data architecture with a focus on making it more scalable. My data is mostly text coming in from different sources (mostly internal, some external). Each piece of text has associated metadata fields directly stored with the text in a field and some metadata in additional tables (e.g. topics). There is quite a lot data coming in each week and the most recent data is always the most relevant.  \n\n\nFor each text, we also want to compute the embeddings to filter and find the most relevant texts within a topic (mostly a focus area within our enterprise) and make it available to the user. The relevant texts are then summarized and tagged with additional keywords to make the search easier. I could get around storing only the most recent relevant data (e.g. past month or quarter).   \n\n\nI want to build it scalable and have the time to test a few different architectures and systems, but I would love to hear what other people think first.   \n\n\nAt the moment, I am thinking of something like this:  \n1. NoSQL, where I parse in the entire data and all new incoming data (something like MongoDB).   \n2. Something faster and more structured to store the \"relevant texts\" (e.g. Cassandra, Postgres, Elastic). This should also be able to capture feedback data, so I can connect it to the specific texts directly.    \n3. Purge the structured storage after 1 month or so and put it into a S3 for future reference (we collect feedback data on which things were clicked to improve the system over time).  \n\n\nThe amount of daily data varies but its at least 50MB can go up to 500MB.   \n\n\nWhat are your thoughts on how you would set it up and what different components would you use? If you need any additional information feel free to reach out. ", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding the right data architecture for mostly text-based data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeku01", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706608316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,   &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am in the process of setting up a new data architecture with a focus on making it more scalable. My data is mostly text coming in from different sources (mostly internal, some external). Each piece of text has associated metadata fields directly stored with the text in a field and some metadata in additional tables (e.g. topics). There is quite a lot data coming in each week and the most recent data is always the most relevant.  &lt;/p&gt;\n\n&lt;p&gt;For each text, we also want to compute the embeddings to filter and find the most relevant texts within a topic (mostly a focus area within our enterprise) and make it available to the user. The relevant texts are then summarized and tagged with additional keywords to make the search easier. I could get around storing only the most recent relevant data (e.g. past month or quarter).   &lt;/p&gt;\n\n&lt;p&gt;I want to build it scalable and have the time to test a few different architectures and systems, but I would love to hear what other people think first.   &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am thinking of something like this:&lt;br/&gt;\n1. NoSQL, where I parse in the entire data and all new incoming data (something like MongoDB).&lt;br/&gt;\n2. Something faster and more structured to store the &amp;quot;relevant texts&amp;quot; (e.g. Cassandra, Postgres, Elastic). This should also be able to capture feedback data, so I can connect it to the specific texts directly.&lt;br/&gt;\n3. Purge the structured storage after 1 month or so and put it into a S3 for future reference (we collect feedback data on which things were clicked to improve the system over time).  &lt;/p&gt;\n\n&lt;p&gt;The amount of daily data varies but its at least 50MB can go up to 500MB.   &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on how you would set it up and what different components would you use? If you need any additional information feel free to reach out. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeku01", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeku01/finding_the_right_data_architecture_for_mostly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeku01/finding_the_right_data_architecture_for_mostly/", "subreddit_subscribers": 157048, "created_utc": 1706608316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aegsqh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706592086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aegsqh", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aegsqh/solutions_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aegsqh/solutions_architect/", "subreddit_subscribers": 157048, "created_utc": 1706592086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to work on a table with 50mil rows, but I'm not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with a sql table of 50mil records in pandas? Will sagemaker work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeawka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706574401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to work on a table with 50mil rows, but I&amp;#39;m not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeawka", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "subreddit_subscribers": 157048, "created_utc": 1706574401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  \n\nWondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. \n\nThanks in advance!", "author_fullname": "t2_7qz1v7vg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best learning path and resources for Azure from a data engineering perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeu4ry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  &lt;/p&gt;\n\n&lt;p&gt;Wondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aeu4ry", "is_robot_indexable": true, "report_reasons": null, "author": "sajiDsarkaR12321", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "subreddit_subscribers": 157048, "created_utc": 1706636128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Guys, does somebody know about any Databricks vouchers in 2024.\nI don\u2019t see so far that Databricks events on databricks.com/events offer it this year. Maybe there are other resources where you can get it?!", "author_fullname": "t2_p4w5948ch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks voucher certificate in 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ael37k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706609390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Guys, does somebody know about any Databricks vouchers in 2024.\nI don\u2019t see so far that Databricks events on databricks.com/events offer it this year. Maybe there are other resources where you can get it?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ael37k", "is_robot_indexable": true, "report_reasons": null, "author": "fearlessevy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ael37k/databricks_voucher_certificate_in_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ael37k/databricks_voucher_certificate_in_2024/", "subreddit_subscribers": 157048, "created_utc": 1706609390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wrote a blog post which looks in depth at Snowflakes new feature for streaming ingestion. The analysis focuses on the Java SDK implementation.\n\nhttps://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/", "author_fullname": "t2_elarh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowpipe Streaming Deep Dive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aevqet", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706639988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wrote a blog post which looks in depth at Snowflakes new feature for streaming ingestion. The analysis focuses on the Java SDK implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/\"&gt;https://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?auto=webp&amp;s=835bf926ff136d72eca4f99033cf536e142609ce", "width": 6016, "height": 4016}, "resolutions": [{"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf704b3753dd8c194fa9453ffc4b2e338680d7f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f9fee6336318e224b5835f1f2780cc86d8a6ace", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a333dea35887c936e1a5ba9b6219ea21a839549b", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d4776d6ae3f8dea3cecfe13d180edee2e57cd92", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1113e1d5204fefa84386086a413a386c9e63b87", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/BQMlr5BJcPBzGFInNQyS7tVGM5l1Jje1QykaIHqVfOw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=387d88e8557da6e22ceee2995cb5d5bc4e554b67", "width": 1080, "height": 720}], "variants": {}, "id": "cTIg0FsOyrYpaGVwlPjUJEFLNjW7rE9BO4u1GNe4fi0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aevqet", "is_robot_indexable": true, "report_reasons": null, "author": "yuvalos", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aevqet/snowpipe_streaming_deep_dive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aevqet/snowpipe_streaming_deep_dive/", "subreddit_subscribers": 157048, "created_utc": 1706639988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Disney Hotstar Captures One Billion Emojis!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1aev0l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How Disney Hotstar Captures One Billion Emojis!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UN1kW5AHid4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1aev0l4", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fgu0-vc156RFpuWdsuUURMAzfYG3czuh6pmQzZ_QMuo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706638288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=UN1kW5AHid4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?auto=webp&amp;s=9e1823e21dd040c84a0d92a629583897608c8b3a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=213ad23bed5dceed758188cb1a390f51614166a1", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b64681ca8e03c58910d4959185d2dbbd335eb12", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/mIHuYvDYQbfQRLOny2EI4sH32JUAWGpMGlwwr3ZqjOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d033c897cf066d4f2d1e9aa4a7225b7b2291c5d5", "width": 320, "height": 240}], "variants": {}, "id": "GMU0CgoG92Xc4dyKC7706W9SAKFbpBUPbuiqgBfhfQY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aev0l4", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aev0l4/how_disney_hotstar_captures_one_billion_emojis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=UN1kW5AHid4", "subreddit_subscribers": 157048, "created_utc": 1706638288.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How Disney Hotstar Captures One Billion Emojis!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/UN1kW5AHid4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How Disney Hotstar Captures One Billion Emojis!\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UN1kW5AHid4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Maps Web Scraping: 3 Practical Use Cases and How to Make the Most of Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1aedw84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/f8g_U5jBsJM2NEpGIUDWiYZcTFPXGTuOLB2SlNkfK3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706582904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?auto=webp&amp;s=46d33550cc81f9decd30fc91d60f6d2eeb67d612", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cac97fdd038cc2a317548d28ca7e8c67e25d359", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17bd1c7742d995fe6de3bdf8b35eeaf0fb7bbd5d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67aace18bb950eb12834c9d34e6c62d5ea85fec2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bcb8328a6c4d241ae19d9f1c42540e44059486e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=95819aa2b0595c6080f137c4b7c50287242c2787", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ec8499ad5ccce921e23e6417b7f23f5ba77d7c4", "width": 1080, "height": 567}], "variants": {}, "id": "XyWQAoZBD9EGqALl6yef2IcN-kJAsS4GMso9KUxnV-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aedw84", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aedw84/google_maps_web_scraping_3_practical_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "subreddit_subscribers": 157048, "created_utc": 1706582904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, everyone. I'm doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,\n\nRight now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I'm appending the new ones (using polars write\\_database() and setting the if\\_exists parameter to \"append\"), without any double check, but I'm not sure how to do the update part. I mean, I know that I have to do something like:\n\nUPDATE schema.table\\_name SET col1=val1, col2 = val2 WHERE id = id\\_val;\n\nBut how to do that for tables with different schemas?\n\nThanks btw", "author_fullname": "t2_5i1nco5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generic Update Statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aesa2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706631697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, everyone. I&amp;#39;m doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,&lt;/p&gt;\n\n&lt;p&gt;Right now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I&amp;#39;m appending the new ones (using polars write_database() and setting the if_exists parameter to &amp;quot;append&amp;quot;), without any double check, but I&amp;#39;m not sure how to do the update part. I mean, I know that I have to do something like:&lt;/p&gt;\n\n&lt;p&gt;UPDATE schema.table_name SET col1=val1, col2 = val2 WHERE id = id_val;&lt;/p&gt;\n\n&lt;p&gt;But how to do that for tables with different schemas?&lt;/p&gt;\n\n&lt;p&gt;Thanks btw&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aesa2v", "is_robot_indexable": true, "report_reasons": null, "author": "gera0220", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aesa2v/generic_update_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aesa2v/generic_update_statement/", "subreddit_subscribers": 157048, "created_utc": 1706631697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, we\u2019re relatively a young team. So we have loads of json data, csv data related to vehicles how all devices work in the vehicle. The manufacturer of the devices, their power rating etc. \n\nWe have information from the basic till the vehicle is fully assembled and given to a customer. The variants of the vehicle etc. So basically we have loads of data spread across json, csv and confluence pages. \n\nData tracking is getting hard and when customers come to us with some problems, it\u2019s hard to track which vehicle has what stuff in it to get to the root cause of the problem. \n\nNow we are planning to move towards a cloud solution? Maybe have a database and then a dashboard of how many vehicles are being assembled, already assembled, how many have what devices? Something like that, basically so we know that everything is documented and can be tracked. Also need to link Jira tickets to this system.\n\nWhat solution would you guys suggest for this? I\u2019m thinking of services from AWS/ Azure? Sorry I am new to this and not sure how to find solutions for this. \n\nThanks in advance for the help!", "author_fullname": "t2_gdjxkf2v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions for database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeknta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706607564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, we\u2019re relatively a young team. So we have loads of json data, csv data related to vehicles how all devices work in the vehicle. The manufacturer of the devices, their power rating etc. &lt;/p&gt;\n\n&lt;p&gt;We have information from the basic till the vehicle is fully assembled and given to a customer. The variants of the vehicle etc. So basically we have loads of data spread across json, csv and confluence pages. &lt;/p&gt;\n\n&lt;p&gt;Data tracking is getting hard and when customers come to us with some problems, it\u2019s hard to track which vehicle has what stuff in it to get to the root cause of the problem. &lt;/p&gt;\n\n&lt;p&gt;Now we are planning to move towards a cloud solution? Maybe have a database and then a dashboard of how many vehicles are being assembled, already assembled, how many have what devices? Something like that, basically so we know that everything is documented and can be tracked. Also need to link Jira tickets to this system.&lt;/p&gt;\n\n&lt;p&gt;What solution would you guys suggest for this? I\u2019m thinking of services from AWS/ Azure? Sorry I am new to this and not sure how to find solutions for this. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeknta", "is_robot_indexable": true, "report_reasons": null, "author": "Informal_Poem_4394", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeknta/solutions_for_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeknta/solutions_for_database/", "subreddit_subscribers": 157048, "created_utc": 1706607564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently moved laterally within my company to a new role in data engineering from a traditional manufacturing engineering role. I don\u2019t have experience with data engineering but have been doing my best to keep up through online learning. My new responsibilities require me to spearhead the introduction of a data warehouse and its surrounding ecosystem (mostly create ETL pipelines and occasionally program in Python to create custom visualization tools). It is also very likely that the warehouse will be an on-prem postgreSQL server.\n\nTo provide background, we are a mid-size semiconductor manufacturing company with only a couple people with expertise in data engineering. It is my first task to review and provide feedback on a report that a consultant produced. The report went in depth on evaluating 3 main different analytics platforms based off the requirements we provided. They suggested the following:\n\n1) Microsoft specific tools (Synapse Analytics, Power BI)\n2) Microsoft Data Fabric\n3) Alternative BI platform like Qlik and Domo\n\nThe executive team is leaning towards option 3 because the consultant said it\u2019ll be faster to implement and it is cheaper for both operational and subscription costs (and more)\n\nI am honestly a bit disappointed because I wanted to get my hands-on with cloud tools like the Azure stacks that will help drive my career as a data/software engineer. \n\nHonestly I think it would be too much to ask Reddit if this is the correct business decision since I am not providing all the details of our data and current infrastructure. So instead I wanted to ask if this will hinder my professional development. Are these end-to-end analytic platforms used to develop all the pipelines from data source up till report generation? Am I going to get stuck using Domo to create all the pipelines since we paid for their subscription? I\u2019m scared that I will also get sucked into just creating a bunch of pipelines using no/low-code solutions as well. Or does it make sense for me to introduce things like Python and Apache Spark as our pipelines?\n\nI\u2019ve just noticed a lot of job postings requirements for software/data engineers asking for exposure to cloud services and big data tools like Apache Spark. Really wanting to get the most out of this position to set a strong foundation. And pardon my ignorance as I\u2019m new in this field, and I appreciate any help!", "author_fullname": "t2_es1pfwhmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing a Data Analytics Platform as a Noob", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1af06f0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706650758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently moved laterally within my company to a new role in data engineering from a traditional manufacturing engineering role. I don\u2019t have experience with data engineering but have been doing my best to keep up through online learning. My new responsibilities require me to spearhead the introduction of a data warehouse and its surrounding ecosystem (mostly create ETL pipelines and occasionally program in Python to create custom visualization tools). It is also very likely that the warehouse will be an on-prem postgreSQL server.&lt;/p&gt;\n\n&lt;p&gt;To provide background, we are a mid-size semiconductor manufacturing company with only a couple people with expertise in data engineering. It is my first task to review and provide feedback on a report that a consultant produced. The report went in depth on evaluating 3 main different analytics platforms based off the requirements we provided. They suggested the following:&lt;/p&gt;\n\n&lt;p&gt;1) Microsoft specific tools (Synapse Analytics, Power BI)\n2) Microsoft Data Fabric\n3) Alternative BI platform like Qlik and Domo&lt;/p&gt;\n\n&lt;p&gt;The executive team is leaning towards option 3 because the consultant said it\u2019ll be faster to implement and it is cheaper for both operational and subscription costs (and more)&lt;/p&gt;\n\n&lt;p&gt;I am honestly a bit disappointed because I wanted to get my hands-on with cloud tools like the Azure stacks that will help drive my career as a data/software engineer. &lt;/p&gt;\n\n&lt;p&gt;Honestly I think it would be too much to ask Reddit if this is the correct business decision since I am not providing all the details of our data and current infrastructure. So instead I wanted to ask if this will hinder my professional development. Are these end-to-end analytic platforms used to develop all the pipelines from data source up till report generation? Am I going to get stuck using Domo to create all the pipelines since we paid for their subscription? I\u2019m scared that I will also get sucked into just creating a bunch of pipelines using no/low-code solutions as well. Or does it make sense for me to introduce things like Python and Apache Spark as our pipelines?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve just noticed a lot of job postings requirements for software/data engineers asking for exposure to cloud services and big data tools like Apache Spark. Really wanting to get the most out of this position to set a strong foundation. And pardon my ignorance as I\u2019m new in this field, and I appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af06f0", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Series-2277", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af06f0/introducing_a_data_analytics_platform_as_a_noob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af06f0/introducing_a_data_analytics_platform_as_a_noob/", "subreddit_subscribers": 157048, "created_utc": 1706650758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use a rest API to change the tier of files which are older then 10 days to archive in azure adf web activity passing the path dynamically which are stored in control table in an azure database, but fail to achieve so \nI don\u2019t want to use the azure life cycle management policies as I have define policy for each folder with the name.\nDoes anyone have any experience doing it or can just guide me in the right direction \nThanks in advance ", "author_fullname": "t2_fr0elqpw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Changing access tier using API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aezyhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706650234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use a rest API to change the tier of files which are older then 10 days to archive in azure adf web activity passing the path dynamically which are stored in control table in an azure database, but fail to achieve so \nI don\u2019t want to use the azure life cycle management policies as I have define policy for each folder with the name.\nDoes anyone have any experience doing it or can just guide me in the right direction \nThanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aezyhq", "is_robot_indexable": true, "report_reasons": null, "author": "Due_Swordfish_7979", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aezyhq/changing_access_tier_using_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aezyhq/changing_access_tier_using_api/", "subreddit_subscribers": 157048, "created_utc": 1706650234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!  \nI was about to start reading 'The Data Warehouse Toolkit' by Ralph Kimball and Margy Ross. However, I understand that it has become somewhat outdated.\n\nI would like to know what resources I could use to learn about modern architectures and dimensional modeling.\n\nThank you very much!", "author_fullname": "t2_6kkepa9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Books and Resources for Analytics Engineering and Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aex15l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706643063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI was about to start reading &amp;#39;The Data Warehouse Toolkit&amp;#39; by Ralph Kimball and Margy Ross. However, I understand that it has become somewhat outdated.&lt;/p&gt;\n\n&lt;p&gt;I would like to know what resources I could use to learn about modern architectures and dimensional modeling.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aex15l", "is_robot_indexable": true, "report_reasons": null, "author": "hypnotize9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aex15l/books_and_resources_for_analytics_engineering_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aex15l/books_and_resources_for_analytics_engineering_and/", "subreddit_subscribers": 157048, "created_utc": 1706643063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on the docs it looks like dynamoDB is a great choice since it's used by a lot of businesses and can handle a lot of requests simultaneously. Not to say cosmosDBs can't.\n\nBut what is preferred in the market out there? I am looking for speed and reliability to handle simultaneous writes. ", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dynamo DB vs Cosmos DB for mongoDB instance - What should I choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aewe97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706641546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the docs it looks like dynamoDB is a great choice since it&amp;#39;s used by a lot of businesses and can handle a lot of requests simultaneously. Not to say cosmosDBs can&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;But what is preferred in the market out there? I am looking for speed and reliability to handle simultaneous writes. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aewe97", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aewe97/dynamo_db_vs_cosmos_db_for_mongodb_instance_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aewe97/dynamo_db_vs_cosmos_db_for_mongodb_instance_what/", "subreddit_subscribers": 157048, "created_utc": 1706641546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Okay so, here goes. I'm not a Data Engineer so please forgive me if I get a few things wrong. \n\nWe use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I'm lost as to what tools or \"pipeline\" (for lack of a better word) to suggest. \n\nIn my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? \n\nI'm open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don't mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I'm off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. \n\nTLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.", "author_fullname": "t2_tcnjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with an ETL process and MongoDB (Am not a Data Engineer)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeufvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay so, here goes. I&amp;#39;m not a Data Engineer so please forgive me if I get a few things wrong. &lt;/p&gt;\n\n&lt;p&gt;We use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I&amp;#39;m lost as to what tools or &amp;quot;pipeline&amp;quot; (for lack of a better word) to suggest. &lt;/p&gt;\n\n&lt;p&gt;In my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don&amp;#39;t mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I&amp;#39;m off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. &lt;/p&gt;\n\n&lt;p&gt;TLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeufvx", "is_robot_indexable": true, "report_reasons": null, "author": "Zulowie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "subreddit_subscribers": 157048, "created_utc": 1706636897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you're navigating the waters of data migration from Hive to BigQuery, you'll appreciate the precision and scalability challenges we face. My latest blog post dives into the art of transforming Hive UDAFs to BigQuery's SQL UDFs, with a focus on calculating the median. Discover how SQL UDFs can enhance your data processing practices and make your transition smoother.   \n[Post: Perfecting the Mediam](https://www.aliz.ai/en/blog/perfecting-the-median-transforming-hive-udafs-to-bigquerys-advanced-sql?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=personal_boost&amp;utm_id=personal_boost&amp;utm_content=r2)", "author_fullname": "t2_i8mbe4p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hive to UDAF to Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aep1pr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706623166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re navigating the waters of data migration from Hive to BigQuery, you&amp;#39;ll appreciate the precision and scalability challenges we face. My latest blog post dives into the art of transforming Hive UDAFs to BigQuery&amp;#39;s SQL UDFs, with a focus on calculating the median. Discover how SQL UDFs can enhance your data processing practices and make your transition smoother.&lt;br/&gt;\n&lt;a href=\"https://www.aliz.ai/en/blog/perfecting-the-median-transforming-hive-udafs-to-bigquerys-advanced-sql?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=personal_boost&amp;amp;utm_id=personal_boost&amp;amp;utm_content=r2\"&gt;Post: Perfecting the Mediam&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?auto=webp&amp;s=bdc91a5bd99f63eb819295075cce1cc61ce3a398", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f4dfe6e48f3d7d820b6c5488fcb4197c3b0177", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc3c336087ccc453b5ab6bc72e0013455ef0e221", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b55056b9a3e3e3f959e75979d90c7dace2b484a", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4705a22a101a3e0b0743aa5ed4fd5e2343501be5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2004dc275fb5ff18edb0959547c42d1e985a2132", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/VHg_G3trnyYgoflb6USD5QRLthXlvkSC9l4E5MaLewE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9fe6608803e73129fd6fd5c2d65b85f69d6bde3", "width": 1080, "height": 607}], "variants": {}, "id": "RLhoGS0_Fj_NS4GllG7yOdEmqfveFzlj5BOmzjo0rT0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aep1pr", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Collar9129", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aep1pr/hive_to_udaf_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aep1pr/hive_to_udaf_to_bigquery/", "subreddit_subscribers": 157048, "created_utc": 1706623166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!  \n\n\nWhat recommendations do you have for automatic exposures &amp; lineage for dbt &gt; bigquery &gt; looker studio (not looker).\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_gdhnbh58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lineage Options/Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aenokc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706619057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!  &lt;/p&gt;\n\n&lt;p&gt;What recommendations do you have for automatic exposures &amp;amp; lineage for dbt &amp;gt; bigquery &amp;gt; looker studio (not looker).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aenokc", "is_robot_indexable": true, "report_reasons": null, "author": "Major-Car342", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aenokc/data_lineage_optionstools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aenokc/data_lineage_optionstools/", "subreddit_subscribers": 157048, "created_utc": 1706619057.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}