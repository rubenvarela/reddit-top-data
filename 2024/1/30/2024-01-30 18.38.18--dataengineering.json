{"kind": "Listing", "data": {"after": "t3_1aesa2v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked as a BI Developer/SQL DBA/ SQL Developer for like 5 years.For the last 2 years I have been working as a Big data analyst working on Databricks, Python,Pyspark and some AWS services.\nAlso been working on Looker.\n\nI have been trying to switch to a pure DE role where I get to create pipelines and work on more advanced AWS services etc.But most do the DE roles out there requires lots of skills set and I don\u2019t even see many entry level DE roles.\n\nAny one in similar situation?", "author_fullname": "t2_slia7yw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is it so hard to land a DE role.?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae4wnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706559133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked as a BI Developer/SQL DBA/ SQL Developer for like 5 years.For the last 2 years I have been working as a Big data analyst working on Databricks, Python,Pyspark and some AWS services.\nAlso been working on Looker.&lt;/p&gt;\n\n&lt;p&gt;I have been trying to switch to a pure DE role where I get to create pipelines and work on more advanced AWS services etc.But most do the DE roles out there requires lots of skills set and I don\u2019t even see many entry level DE roles.&lt;/p&gt;\n\n&lt;p&gt;Any one in similar situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ae4wnv", "is_robot_indexable": true, "report_reasons": null, "author": "cruze_8907", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae4wnv/why_is_it_so_hard_to_land_a_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae4wnv/why_is_it_so_hard_to_land_a_de_role/", "subreddit_subscribers": 156995, "created_utc": 1706559133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys,\n\nWhich one is better to choose in your opinion? I am a self-taught data engineer, but I would like to get certified so that future employers stop questioning my Bachelor's degree in Economics.\n\ni know that the main diffrence is in the cloud platforms but i would like to know which one offers a greater and better depth into the field of data engineer? \n\nthank you in advance for ur help :D\n\n[https://aws.amazon.com/certification/certified-data-engineer-associate/](https://aws.amazon.com/certification/certified-data-engineer-associate/)\n\n[https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/)", "author_fullname": "t2_qmhxkds80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Certified Data Engineer - Associate VS Azure Data Engineer Associate (DP 203 exam)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae6h40", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706563015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;Which one is better to choose in your opinion? I am a self-taught data engineer, but I would like to get certified so that future employers stop questioning my Bachelor&amp;#39;s degree in Economics.&lt;/p&gt;\n\n&lt;p&gt;i know that the main diffrence is in the cloud platforms but i would like to know which one offers a greater and better depth into the field of data engineer? &lt;/p&gt;\n\n&lt;p&gt;thank you in advance for ur help :D&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/certification/certified-data-engineer-associate/\"&gt;https://aws.amazon.com/certification/certified-data-engineer-associate/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/\"&gt;https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?auto=webp&amp;s=8afacfc14dfed09cec0415cac7d36db9c3374c61", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a1b1322ed94c559cb213e6a08f3eb426a3fb0b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9edddfdb28bb0e92ceb041859aacef81ab9ed42e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de73cdc9da2d0b04938bb7d051ab1a3ceb783323", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60037829d2ce04de0705a2b45123d8ab7c12d41c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5a8f0da08b9281c578a8ab6f49a5b3f577ec9b8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9a9c1c38bd543a7ea6b718e139a9c1e6b62d18", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae6h40", "is_robot_indexable": true, "report_reasons": null, "author": "Comment_Error", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae6h40/aws_certified_data_engineer_associate_vs_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae6h40/aws_certified_data_engineer_associate_vs_azure/", "subreddit_subscribers": 156995, "created_utc": 1706563015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I\u2019m a Jr data engineer which is facing a challenge and I would love to know your opinions and expertise in this topic:\n\nI\u2019m currently handling allots of data in SQL, we receive at a high frequency JSONs with raw data in it (in a single json there could be more than 10k raws) \n\nThe thing is that we need to make some statistics with this JSONS \nWe need to concatenate several Jsons and then apply the statistics (calculate outliers, calculate avgs, calculate percentages, stds, frequency, etc\u2026) \n\nAnd after calculating it we need to insert it in a new table which handles summarizes data. \nAll of this in a SQL stored procedure, the hole process lasts more than 3hours to complete, is there any advice for this kind of stuff, some literature I can read, videos or something to optimize the solution? \nI\u2019m also open to other robust pipelines besides only using SQL!", "author_fullname": "t2_gcej0pe8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing high amount of data with SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aepsld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706625257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I\u2019m a Jr data engineer which is facing a challenge and I would love to know your opinions and expertise in this topic:&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently handling allots of data in SQL, we receive at a high frequency JSONs with raw data in it (in a single json there could be more than 10k raws) &lt;/p&gt;\n\n&lt;p&gt;The thing is that we need to make some statistics with this JSONS \nWe need to concatenate several Jsons and then apply the statistics (calculate outliers, calculate avgs, calculate percentages, stds, frequency, etc\u2026) &lt;/p&gt;\n\n&lt;p&gt;And after calculating it we need to insert it in a new table which handles summarizes data. \nAll of this in a SQL stored procedure, the hole process lasts more than 3hours to complete, is there any advice for this kind of stuff, some literature I can read, videos or something to optimize the solution? \nI\u2019m also open to other robust pipelines besides only using SQL!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aepsld", "is_robot_indexable": true, "report_reasons": null, "author": "Alex_Alca_", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aepsld/processing_high_amount_of_data_with_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aepsld/processing_high_amount_of_data_with_sql/", "subreddit_subscribers": 156995, "created_utc": 1706625257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There's been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there's an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. \n\n&amp;#x200B;\n\nMost of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I'm trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn't have a separate develop environment to develop models, like I'll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don't want to break anything.\n\n&amp;#x200B;\n\nBasically, I'm getting anxious about not knowing what I don't know regarding most things DE and I want to sack up and try to get better at it but I'm not quite sure where to start.", "author_fullname": "t2_4xc90hk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do to elevate your skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeefxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706584492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There&amp;#39;s been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there&amp;#39;s an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Most of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I&amp;#39;m trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn&amp;#39;t have a separate develop environment to develop models, like I&amp;#39;ll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don&amp;#39;t want to break anything.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, I&amp;#39;m getting anxious about not knowing what I don&amp;#39;t know regarding most things DE and I want to sack up and try to get better at it but I&amp;#39;m not quite sure where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeefxr", "is_robot_indexable": true, "report_reasons": null, "author": "thisisformeworking", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "subreddit_subscribers": 156995, "created_utc": 1706584492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Shouldn't be a huge amount of data, &lt;200 GB currently in the DB.  \n\nI titled this as 'replicate', but it's really just ELT without the T.  This replication wouldn't have to be realtime, by the way.\n\nWe're hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?\n\nEdit: edited for clarity.", "author_fullname": "t2_j15uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to replicate from AWS Sql Server db to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae7u5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706566863.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706566365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Shouldn&amp;#39;t be a huge amount of data, &amp;lt;200 GB currently in the DB.  &lt;/p&gt;\n\n&lt;p&gt;I titled this as &amp;#39;replicate&amp;#39;, but it&amp;#39;s really just ELT without the T.  This replication wouldn&amp;#39;t have to be realtime, by the way.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?&lt;/p&gt;\n\n&lt;p&gt;Edit: edited for clarity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae7u5p", "is_robot_indexable": true, "report_reasons": null, "author": "jbrune", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "subreddit_subscribers": 156995, "created_utc": 1706566365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a computer science student about to graduate and I'm trying to find projects to help build my skills in data engineering. I'm having trouble finding API's with good free tiers that I could build a project around. Does anyone have any suggestions? I've tried looking into sports data ones specifically the NBA but the ones I've found aren't cheap", "author_fullname": "t2_9x6d0tf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free API's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aee0tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706583283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a computer science student about to graduate and I&amp;#39;m trying to find projects to help build my skills in data engineering. I&amp;#39;m having trouble finding API&amp;#39;s with good free tiers that I could build a project around. Does anyone have any suggestions? I&amp;#39;ve tried looking into sports data ones specifically the NBA but the ones I&amp;#39;ve found aren&amp;#39;t cheap&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aee0tx", "is_robot_indexable": true, "report_reasons": null, "author": "TheDrunkInTheNorth", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aee0tx/free_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aee0tx/free_apis/", "subreddit_subscribers": 156995, "created_utc": 1706583283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,   \n\n\nAt the moment, I am in the process of setting up a new data architecture with a focus on making it more scalable. My data is mostly text coming in from different sources (mostly internal, some external). Each piece of text has associated metadata fields directly stored with the text in a field and some metadata in additional tables (e.g. topics). There is quite a lot data coming in each week and the most recent data is always the most relevant.  \n\n\nFor each text, we also want to compute the embeddings to filter and find the most relevant texts within a topic (mostly a focus area within our enterprise) and make it available to the user. The relevant texts are then summarized and tagged with additional keywords to make the search easier. I could get around storing only the most recent relevant data (e.g. past month or quarter).   \n\n\nI want to build it scalable and have the time to test a few different architectures and systems, but I would love to hear what other people think first.   \n\n\nAt the moment, I am thinking of something like this:  \n1. NoSQL, where I parse in the entire data and all new incoming data (something like MongoDB).   \n2. Something faster and more structured to store the \"relevant texts\" (e.g. Cassandra, Postgres, Elastic). This should also be able to capture feedback data, so I can connect it to the specific texts directly.    \n3. Purge the structured storage after 1 month or so and put it into a S3 for future reference (we collect feedback data on which things were clicked to improve the system over time).  \n\n\nThe amount of daily data varies but its at least 50MB can go up to 500MB.   \n\n\nWhat are your thoughts on how you would set it up and what different components would you use? If you need any additional information feel free to reach out. ", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding the right data architecture for mostly text-based data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeku01", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706608316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,   &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am in the process of setting up a new data architecture with a focus on making it more scalable. My data is mostly text coming in from different sources (mostly internal, some external). Each piece of text has associated metadata fields directly stored with the text in a field and some metadata in additional tables (e.g. topics). There is quite a lot data coming in each week and the most recent data is always the most relevant.  &lt;/p&gt;\n\n&lt;p&gt;For each text, we also want to compute the embeddings to filter and find the most relevant texts within a topic (mostly a focus area within our enterprise) and make it available to the user. The relevant texts are then summarized and tagged with additional keywords to make the search easier. I could get around storing only the most recent relevant data (e.g. past month or quarter).   &lt;/p&gt;\n\n&lt;p&gt;I want to build it scalable and have the time to test a few different architectures and systems, but I would love to hear what other people think first.   &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am thinking of something like this:&lt;br/&gt;\n1. NoSQL, where I parse in the entire data and all new incoming data (something like MongoDB).&lt;br/&gt;\n2. Something faster and more structured to store the &amp;quot;relevant texts&amp;quot; (e.g. Cassandra, Postgres, Elastic). This should also be able to capture feedback data, so I can connect it to the specific texts directly.&lt;br/&gt;\n3. Purge the structured storage after 1 month or so and put it into a S3 for future reference (we collect feedback data on which things were clicked to improve the system over time).  &lt;/p&gt;\n\n&lt;p&gt;The amount of daily data varies but its at least 50MB can go up to 500MB.   &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on how you would set it up and what different components would you use? If you need any additional information feel free to reach out. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeku01", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeku01/finding_the_right_data_architecture_for_mostly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeku01/finding_the_right_data_architecture_for_mostly/", "subreddit_subscribers": 156995, "created_utc": 1706608316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aegsqh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706592086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aegsqh", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aegsqh/solutions_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aegsqh/solutions_architect/", "subreddit_subscribers": 156995, "created_utc": 1706592086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to work on a table with 50mil rows, but I'm not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with a sql table of 50mil records in pandas? Will sagemaker work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeawka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706574401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to work on a table with 50mil rows, but I&amp;#39;m not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeawka", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "subreddit_subscribers": 156995, "created_utc": 1706574401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lw1vuj1gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Things Data Consulting Clients Absolutely Hate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_1aen5t7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.59, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/O0jx9r0F8PJm7ckhQLiBQqDNcA8zRlzNAsz1k3-Ivo8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706617296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/6-things-data-consulting-clients-absolutely-hate/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?auto=webp&amp;s=8b86994c2f69b95b57e1c4dac243605c5ee3e5a5", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b230520895ff2aed8deb78df1a1e8cce655a7bb", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8094d9413e68a56bfb087cffcb372bab7299e5f8", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb00dbece4019b1772068b1854ed84fdea198835", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=274870ffbf4102b8f2c3313354f6a94e232a4dbf", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1332e876938e87b03510feed48a4e0748858140", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/bnf25_oUIPNiLb-crkVTAKO3aqLVOhTrdQVSVTQQTjc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=194f45e91b4884dcbb348d587cc7bfa5d690697f", "width": 1080, "height": 617}], "variants": {}, "id": "1jE9dc2gAnTAQGWvyOZUUSlh1-b8h_HZjfgyLv41uUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aen5t7", "is_robot_indexable": true, "report_reasons": null, "author": "Distinct-Economics24", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aen5t7/6_things_data_consulting_clients_absolutely_hate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/6-things-data-consulting-clients-absolutely-hate/", "subreddit_subscribers": 156995, "created_utc": 1706617296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I am curios to know that what ideas and innovations you introduced in your data engineering project?", "author_fullname": "t2_uok38vei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas and innovations in data engineering project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae3glr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706555617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curios to know that what ideas and innovations you introduced in your data engineering project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae3glr", "is_robot_indexable": true, "report_reasons": null, "author": "loosernew7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae3glr/ideas_and_innovations_in_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae3glr/ideas_and_innovations_in_data_engineering_project/", "subreddit_subscribers": 156995, "created_utc": 1706555617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nCompany is currently in the process of designing a custom architecture for a data warehouse. Due to data residency requirements, our data is split across 7 different azure regions in seven different databases. In addition, the databases are semi multi-tenant \u2014 some clients have their own specialized schema, though the tables are constant in each schema. In short, there are \"50\" targets where data could be stored.\n\nAnyways, we rely on Presto to query this data, which is inefficient and expensive. We're looking into centralizing everything, which will be done in Databricks for the creation of the Silver/Bronze layers and Unity for ACL. However, to prevent vendor lock-in, we're looking to engineer a custom solution to get all the data from our databases, incrementally update via CDC it in a bronze layer stored in Azure S3, so we could move vendors and not re-invent the wheel if business priorities change.\n\nHere's the architecture I'm currently considering, based on a couple days of research.\n\nMariaDB Databases -&gt; CDC read through Debezium Server -&gt; Dumped to common FastAPI/Axum HTTP Server via Json/Avro (between all 7 databases) -&gt; Instructions for recreating data inserted into Delta Table -&gt; Reconstruct entire dataset based on Instructions (Delta or Iceberg, open question) -&gt; Create Silver/Gold layers through Databricks ([process here](https://docs.databricks.com/en/delta/clone-parquet.html))\n\nSome open questions I have\n\n\\- Is Iceberg or Delta the preferred solution here? I was mocking the HTTP server with Iceberg because it supports schema evolution. I was looking at delta-rs for this, but doesn't support schema evolution yet, which I would like to be intention since Iceberg supports this natively. I would like to avoid using Spark if possible.\n\n\\- I would be processing the CDC as they come in, which means I would constantly be inserting one row. I'd imagine these parquet-based solutions are not particularly good at inserts of one row. I could theoretically implement native batching. Is there any alternate solution, that is ideally file-based like Delta/Iceberg and open-source?\n\n\\- Am I overthinking it by storing the instructions for how to recreate the data rather than just translating the Debezium instructions directly into the relevant data?\n\nAny thoughts would be appreciated here. Thank you!", "author_fullname": "t2_v12atn7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Debezium-based Multimodal Datastore - critiques needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae36c3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706554911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;Company is currently in the process of designing a custom architecture for a data warehouse. Due to data residency requirements, our data is split across 7 different azure regions in seven different databases. In addition, the databases are semi multi-tenant \u2014 some clients have their own specialized schema, though the tables are constant in each schema. In short, there are &amp;quot;50&amp;quot; targets where data could be stored.&lt;/p&gt;\n\n&lt;p&gt;Anyways, we rely on Presto to query this data, which is inefficient and expensive. We&amp;#39;re looking into centralizing everything, which will be done in Databricks for the creation of the Silver/Bronze layers and Unity for ACL. However, to prevent vendor lock-in, we&amp;#39;re looking to engineer a custom solution to get all the data from our databases, incrementally update via CDC it in a bronze layer stored in Azure S3, so we could move vendors and not re-invent the wheel if business priorities change.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the architecture I&amp;#39;m currently considering, based on a couple days of research.&lt;/p&gt;\n\n&lt;p&gt;MariaDB Databases -&amp;gt; CDC read through Debezium Server -&amp;gt; Dumped to common FastAPI/Axum HTTP Server via Json/Avro (between all 7 databases) -&amp;gt; Instructions for recreating data inserted into Delta Table -&amp;gt; Reconstruct entire dataset based on Instructions (Delta or Iceberg, open question) -&amp;gt; Create Silver/Gold layers through Databricks (&lt;a href=\"https://docs.databricks.com/en/delta/clone-parquet.html\"&gt;process here&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Some open questions I have&lt;/p&gt;\n\n&lt;p&gt;- Is Iceberg or Delta the preferred solution here? I was mocking the HTTP server with Iceberg because it supports schema evolution. I was looking at delta-rs for this, but doesn&amp;#39;t support schema evolution yet, which I would like to be intention since Iceberg supports this natively. I would like to avoid using Spark if possible.&lt;/p&gt;\n\n&lt;p&gt;- I would be processing the CDC as they come in, which means I would constantly be inserting one row. I&amp;#39;d imagine these parquet-based solutions are not particularly good at inserts of one row. I could theoretically implement native batching. Is there any alternate solution, that is ideally file-based like Delta/Iceberg and open-source?&lt;/p&gt;\n\n&lt;p&gt;- Am I overthinking it by storing the instructions for how to recreate the data rather than just translating the Debezium instructions directly into the relevant data?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be appreciated here. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data/Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae36c3", "is_robot_indexable": true, "report_reasons": null, "author": "Dazzling-Reason-5140", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ae36c3/building_a_debeziumbased_multimodal_datastore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae36c3/building_a_debeziumbased_multimodal_datastore/", "subreddit_subscribers": 156995, "created_utc": 1706554911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe've been using ADF in our team, purely ADF. With AI especially my development is so much faster with SP's and scripts, but theres a push to do everything in a dataflow. Obviously its worse off for us if something takes 5-10x longer to develop when I can't use a SP for everything, happy to orchestrate runs in ADF but the actual ETL logic I feel should be done using SQL or python databricks.\n\nHe tried to solely use an ADF to run an API, and when he couldn't he said we can't use the API anymore, insanity. He then blamed the company itself when it wouldn't work.\n\nI'm going to see if I can do a cost comparison in my own time and show everyone how wrong he is, but does anyone else have any information or knowledge on this? I'm actually new to azure and adf so I can't say.", "author_fullname": "t2_4nomabkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost of ADF using Stored Procedures vs pure ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aephtl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706624397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been using ADF in our team, purely ADF. With AI especially my development is so much faster with SP&amp;#39;s and scripts, but theres a push to do everything in a dataflow. Obviously its worse off for us if something takes 5-10x longer to develop when I can&amp;#39;t use a SP for everything, happy to orchestrate runs in ADF but the actual ETL logic I feel should be done using SQL or python databricks.&lt;/p&gt;\n\n&lt;p&gt;He tried to solely use an ADF to run an API, and when he couldn&amp;#39;t he said we can&amp;#39;t use the API anymore, insanity. He then blamed the company itself when it wouldn&amp;#39;t work.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to see if I can do a cost comparison in my own time and show everyone how wrong he is, but does anyone else have any information or knowledge on this? I&amp;#39;m actually new to azure and adf so I can&amp;#39;t say.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aephtl", "is_robot_indexable": true, "report_reasons": null, "author": "cantseemelol", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aephtl/cost_of_adf_using_stored_procedures_vs_pure_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aephtl/cost_of_adf_using_stored_procedures_vs_pure_adf/", "subreddit_subscribers": 156995, "created_utc": 1706624397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Maps Web Scraping: 3 Practical Use Cases and How to Make the Most of Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1aedw84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/f8g_U5jBsJM2NEpGIUDWiYZcTFPXGTuOLB2SlNkfK3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706582904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?auto=webp&amp;s=46d33550cc81f9decd30fc91d60f6d2eeb67d612", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cac97fdd038cc2a317548d28ca7e8c67e25d359", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17bd1c7742d995fe6de3bdf8b35eeaf0fb7bbd5d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67aace18bb950eb12834c9d34e6c62d5ea85fec2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bcb8328a6c4d241ae19d9f1c42540e44059486e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=95819aa2b0595c6080f137c4b7c50287242c2787", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ec8499ad5ccce921e23e6417b7f23f5ba77d7c4", "width": 1080, "height": 567}], "variants": {}, "id": "XyWQAoZBD9EGqALl6yef2IcN-kJAsS4GMso9KUxnV-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aedw84", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aedw84/google_maps_web_scraping_3_practical_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "subreddit_subscribers": 156995, "created_utc": 1706582904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nSorry for the long post, but I am not sure how to sum this up into a tl;dr.\n\nI am fairly early in my career, and I have been tasked with setting up a monitoring system that is able to provide visibility at the row level.\n\nFor clarification, the lead of my team wants to be able to pick an arbitrary row in a table and know exactly how long it took to get from the source to destination and the details at each step in between.\n\nThe concept is simple at a high level. It's just subtracting timestamps. However, we use a lot of no-code solutions that don't provide APIs or access to what's actually going on behind the scenes, and the source system metrics are closed to external use. Getting a creation time and visibility into each step is extremely difficult as I see it.\n\nI feel like time would be better spent fixing the architecture that is unstable instead of chasing a level of monitoring that I am not sure is even possible to create reliably due to manual exports of data and hacky code due to closed systems.\n\nHowever, maybe I am missing something obvious due to a lack of knowledge. I am the only data engineer and my boss isn't technical, so I am mostly on my own here.\n\nAny advice?\n\nThe current pipeline is SAP -&gt; Qlik Replicate -&gt; Databricks DLT -&gt; Synapse serverless.", "author_fullname": "t2_96mteoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wasting Resources Chasing Dreams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae2x5u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706554292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post, but I am not sure how to sum this up into a tl;dr.&lt;/p&gt;\n\n&lt;p&gt;I am fairly early in my career, and I have been tasked with setting up a monitoring system that is able to provide visibility at the row level.&lt;/p&gt;\n\n&lt;p&gt;For clarification, the lead of my team wants to be able to pick an arbitrary row in a table and know exactly how long it took to get from the source to destination and the details at each step in between.&lt;/p&gt;\n\n&lt;p&gt;The concept is simple at a high level. It&amp;#39;s just subtracting timestamps. However, we use a lot of no-code solutions that don&amp;#39;t provide APIs or access to what&amp;#39;s actually going on behind the scenes, and the source system metrics are closed to external use. Getting a creation time and visibility into each step is extremely difficult as I see it.&lt;/p&gt;\n\n&lt;p&gt;I feel like time would be better spent fixing the architecture that is unstable instead of chasing a level of monitoring that I am not sure is even possible to create reliably due to manual exports of data and hacky code due to closed systems.&lt;/p&gt;\n\n&lt;p&gt;However, maybe I am missing something obvious due to a lack of knowledge. I am the only data engineer and my boss isn&amp;#39;t technical, so I am mostly on my own here.&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n\n&lt;p&gt;The current pipeline is SAP -&amp;gt; Qlik Replicate -&amp;gt; Databricks DLT -&amp;gt; Synapse serverless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae2x5u", "is_robot_indexable": true, "report_reasons": null, "author": "kevrinth", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae2x5u/wasting_resources_chasing_dreams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae2x5u/wasting_resources_chasing_dreams/", "subreddit_subscribers": 156995, "created_utc": 1706554292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've been in DE/DA field for 2 years now, and I'd like to start working with AWS. I'd like to get the \"AWS Certified Data Engineer\" certification to improve my resume and to find a good job easily. I've done few simple pipelines at my previous job, but nothing big, mainly using Appflow, S3, Athena, Redshift and Quicksight. So no Spark, no Kafka, no Glue. \n\nMy maine issue actually is : what path should I follow and what ressources should I use to prepare this exam ?\n\nThe AWS SkillBuilder recommands to get 2 certifications before this one : Foundational Cloud Practitioner and Associate Solutions Architect. Do I really have to get these 2 certifications first (especially the Associate one which is probably harder and more expensive) ? Or it won't bring me any value for the DE one and I can skip them, saving time and money ?\n\nFor these 3 certifications, there are a bunch of courses provided by AWS SkillBuilder for 29$/mo, is it good and worth buying/following ? And is it enough ?", "author_fullname": "t2_1cdu2cz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare \"AWS Certified Data Engineer - Associate\" ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae2r8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706553894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been in DE/DA field for 2 years now, and I&amp;#39;d like to start working with AWS. I&amp;#39;d like to get the &amp;quot;AWS Certified Data Engineer&amp;quot; certification to improve my resume and to find a good job easily. I&amp;#39;ve done few simple pipelines at my previous job, but nothing big, mainly using Appflow, S3, Athena, Redshift and Quicksight. So no Spark, no Kafka, no Glue. &lt;/p&gt;\n\n&lt;p&gt;My maine issue actually is : what path should I follow and what ressources should I use to prepare this exam ?&lt;/p&gt;\n\n&lt;p&gt;The AWS SkillBuilder recommands to get 2 certifications before this one : Foundational Cloud Practitioner and Associate Solutions Architect. Do I really have to get these 2 certifications first (especially the Associate one which is probably harder and more expensive) ? Or it won&amp;#39;t bring me any value for the DE one and I can skip them, saving time and money ?&lt;/p&gt;\n\n&lt;p&gt;For these 3 certifications, there are a bunch of courses provided by AWS SkillBuilder for 29$/mo, is it good and worth buying/following ? And is it enough ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae2r8k", "is_robot_indexable": true, "report_reasons": null, "author": "imKrypex", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae2r8k/how_to_prepare_aws_certified_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae2r8k/how_to_prepare_aws_certified_data_engineer/", "subreddit_subscribers": 156995, "created_utc": 1706553894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.\n\nIs there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.", "author_fullname": "t2_j8v7zu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Data Stack in Higher Education?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aeulzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706637307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working with a handful of higher ed institutions on data modernization. Cloud warehouses (Snowflake, Redshift), moving from ETL to ELT, dbt for the \u201cT\u201d, and dimensional modeling. Most are just moving from on-premises to cloud, have never heard of dbt, and don\u2019t do much in the way of dimensional modelling. The term \u201cData Engineer\u201d is almost never used.&lt;/p&gt;\n\n&lt;p&gt;Is there anybody out there? I\u2019d love to hear where people in this community are at, perhaps collaborate or share some stories of what\u2019s working, not working, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeulzm", "is_robot_indexable": true, "report_reasons": null, "author": "BIntelligent", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeulzm/modern_data_stack_in_higher_education/", "subreddit_subscribers": 156995, "created_utc": 1706637307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  \n\nWondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. \n\nThanks in advance!", "author_fullname": "t2_7qz1v7vg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best learning path and resources for Azure from a data engineering perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aeu4ry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 certifications (Snowpro Core, AWS SAA, and Databricks DEA). I am aiming to specialize in Azure and Databricks, and currently working as a data engineering consultant. I am still new to data engineering and have a ton of things to improve and learn.  &lt;/p&gt;\n\n&lt;p&gt;Wondering if the community can advise me 1 or 2 solid resource(s) for learning Azure, starting from fundamental concepts in Azure. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aeu4ry", "is_robot_indexable": true, "report_reasons": null, "author": "sajiDsarkaR12321", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeu4ry/best_learning_path_and_resources_for_azure_from_a/", "subreddit_subscribers": 156995, "created_utc": 1706636128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. What do you think of the data mesh concept?\n2. Have you implemented or used a data mesh?\n3. If so, what was the experience? What did you learn?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you implemented or used a data mesh?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeqowi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706627670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;What do you think of the data mesh concept?&lt;/li&gt;\n&lt;li&gt;Have you implemented or used a data mesh?&lt;/li&gt;\n&lt;li&gt;If so, what was the experience? What did you learn?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aeqowi", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeqowi/have_you_implemented_or_used_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeqowi/have_you_implemented_or_used_a_data_mesh/", "subreddit_subscribers": 156995, "created_utc": 1706627670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for Software / Web App\n\nI am gathering data from multiple sources say three sources (s1, s2, s3). Data is being gathered for different attributes say name, date, description, city. In my final dataset I want to have each attribute filled with data from any of the one source. E.g I click to select name and date attributes from s2 while description from s1 and city from s3. \n\nI am looking for a Web app or anything that can help me do that with a simple interface.", "author_fullname": "t2_3avqegqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for tool recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeqaav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706626628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for Software / Web App&lt;/p&gt;\n\n&lt;p&gt;I am gathering data from multiple sources say three sources (s1, s2, s3). Data is being gathered for different attributes say name, date, description, city. In my final dataset I want to have each attribute filled with data from any of the one source. E.g I click to select name and date attributes from s2 while description from s1 and city from s3. &lt;/p&gt;\n\n&lt;p&gt;I am looking for a Web app or anything that can help me do that with a simple interface.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeqaav", "is_robot_indexable": true, "report_reasons": null, "author": "saadcarnot", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeqaav/looking_for_tool_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeqaav/looking_for_tool_recommendations/", "subreddit_subscribers": 156995, "created_utc": 1706626628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Guys, does somebody know about any Databricks vouchers in 2024.\nI don\u2019t see so far that Databricks events on databricks.com/events offer it this year. Maybe there are other resources where you can get it?!", "author_fullname": "t2_p4w5948ch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks voucher certificate in 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ael37k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706609390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Guys, does somebody know about any Databricks vouchers in 2024.\nI don\u2019t see so far that Databricks events on databricks.com/events offer it this year. Maybe there are other resources where you can get it?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ael37k", "is_robot_indexable": true, "report_reasons": null, "author": "fearlessevy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ael37k/databricks_voucher_certificate_in_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ael37k/databricks_voucher_certificate_in_2024/", "subreddit_subscribers": 156995, "created_utc": 1706609390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure what to expect, if any of you have gone through mid/senior interviews, what do you suggest I prepare for?  (e.g. DS&amp;Algo Leetcode style then Medium/Hard questions? etc)", "author_fullname": "t2_3tzpeuhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do data engineer interviews (mid/senior) look like at hedge funds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae5l9n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706560835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure what to expect, if any of you have gone through mid/senior interviews, what do you suggest I prepare for?  (e.g. DS&amp;amp;Algo Leetcode style then Medium/Hard questions? etc)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae5l9n", "is_robot_indexable": true, "report_reasons": null, "author": "randomusicjunkie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae5l9n/what_do_data_engineer_interviews_midsenior_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae5l9n/what_do_data_engineer_interviews_midsenior_look/", "subreddit_subscribers": 156995, "created_utc": 1706560835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For over 8 years, I've been using Laravel daily. One of the gaps I currently feel in the Geoglify stack is the ease of organizing initial data and automatically populating the database, as I do with Laravel Seed. Therefore, I created a small example for MongoDB. This example allows for a straightforward addition of static data related to ships, ports, zones, and more to MongoDB.", "author_fullname": "t2_7svy5qp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Initializing MongoDB with JSON Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae37fb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1d_bJ-kMqLDpO6iJ3U2REgS6a30wsr3yupAaSsutzD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706554983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "geoglify.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For over 8 years, I&amp;#39;ve been using Laravel daily. One of the gaps I currently feel in the Geoglify stack is the ease of organizing initial data and automatically populating the database, as I do with Laravel Seed. Therefore, I created a small example for MongoDB. This example allows for a straightforward addition of static data related to ships, ports, zones, and more to MongoDB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.geoglify.com/blog/initial-seed-mongodb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?auto=webp&amp;s=3cc2c6acaae95b98a6c8f39545bd7e504d78f55a", "width": 1479, "height": 827}, "resolutions": [{"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b837534f089c8820261cfcb889dc11563e14a1ef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c7d18d2b443100a6f929670b845f1fe883035b0", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a468b2ac0110bf2609f4c73260f416432055526", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4559439083657794f464663ef64746dbdbd4b90b", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0d605e28f3189116727cc02990acd2efc3a3c8a", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b351ac9b40ab82a3ab6e0790369310db29a4da8", "width": 1080, "height": 603}], "variants": {}, "id": "avzTdEtIlyrGjuhStoUYXVeh_K8zC4M5WXCGDLZ_9L8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ae37fb", "is_robot_indexable": true, "report_reasons": null, "author": "leoneljdias", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae37fb/initializing_mongodb_with_json_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.geoglify.com/blog/initial-seed-mongodb", "subreddit_subscribers": 156995, "created_utc": 1706554983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Okay so, here goes. I'm not a Data Engineer so please forgive me if I get a few things wrong. \n\nWe use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I'm lost as to what tools or \"pipeline\" (for lack of a better word) to suggest. \n\nIn my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? \n\nI'm open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don't mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I'm off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. \n\nTLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.", "author_fullname": "t2_tcnjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with an ETL process and MongoDB (Am not a Data Engineer)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aeufvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706636897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay so, here goes. I&amp;#39;m not a Data Engineer so please forgive me if I get a few things wrong. &lt;/p&gt;\n\n&lt;p&gt;We use azure as an organisation and we have a large (to us!) amount of data in a collection in MongoDB (around 40 Million documents). It includes a ton of nested fields and we would like to be able to aggregate this data, group on certain fields and output sum amounts (likely to be under 1.5M documents) - nothing too crazy. It is all indexed and I have tried grouping in python and extracting but it takes forever to run. This leads me to my next point, ideally we would like to run this monthly or quarterly (and adhoc too). We report in Power BI so it would be great if we could have some sort of connection there but I&amp;#39;m lost as to what tools or &amp;quot;pipeline&amp;quot; (for lack of a better word) to suggest. &lt;/p&gt;\n\n&lt;p&gt;In my mind I would use a tool like airflow to transform this data and then upload back into the MongoDB into a new collection? So the grouped data can be indexed and therefore it will be able to be retrieved faster? Is that correct? Or am I better off uploading it to an azure data factory rather than MongoDB as it has better connections in Power BI? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to learning new tech but we are quite a small team and would prefer a low cost solution and staying within either Mongo or Azure (I don&amp;#39;t mind learning open source stuff like airflow). Ideally as well I would like someone else to be able to run this (in the event that I&amp;#39;m off), whether that be pulling from github or something else. Am open to suggestions! Thanks in advance for reading my garbled explanation lol. &lt;/p&gt;\n\n&lt;p&gt;TLDR: We have a lot of data in MongoDB, and we report in Power BI - we want to transform this data and upload to either MongoDB or a low cost Azure hosted service but need advice on what tools to best approach this with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeufvx", "is_robot_indexable": true, "report_reasons": null, "author": "Zulowie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeufvx/need_help_with_an_etl_process_and_mongodb_am_not/", "subreddit_subscribers": 156995, "created_utc": 1706636897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, everyone. I'm doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,\n\nRight now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I'm appending the new ones (using polars write\\_database() and setting the if\\_exists parameter to \"append\"), without any double check, but I'm not sure how to do the update part. I mean, I know that I have to do something like:\n\nUPDATE schema.table\\_name SET col1=val1, col2 = val2 WHERE id = id\\_val;\n\nBut how to do that for tables with different schemas?\n\nThanks btw", "author_fullname": "t2_5i1nco5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generic Update Statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aesa2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706631697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, everyone. I&amp;#39;m doing an incremental ELT with python and Dagster. My problem is that, because there are a lot of tables to move, I created a SQLTable class to just add the details of new tables and the same logic is applied to every object, but doing that for the update part is driving me crazy,&lt;/p&gt;\n\n&lt;p&gt;Right now, I have my new records (those created after my last execution of the ELT) and the modified ones (those who has been created before my last execution but modified after that) separated. So I&amp;#39;m appending the new ones (using polars write_database() and setting the if_exists parameter to &amp;quot;append&amp;quot;), without any double check, but I&amp;#39;m not sure how to do the update part. I mean, I know that I have to do something like:&lt;/p&gt;\n\n&lt;p&gt;UPDATE schema.table_name SET col1=val1, col2 = val2 WHERE id = id_val;&lt;/p&gt;\n\n&lt;p&gt;But how to do that for tables with different schemas?&lt;/p&gt;\n\n&lt;p&gt;Thanks btw&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aesa2v", "is_robot_indexable": true, "report_reasons": null, "author": "gera0220", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aesa2v/generic_update_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aesa2v/generic_update_statement/", "subreddit_subscribers": 156995, "created_utc": 1706631697.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}