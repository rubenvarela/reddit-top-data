{"kind": "Listing", "data": {"after": "t3_1ads4nq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.\n\nDo you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?", "author_fullname": "t2_hnhowjrxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happened recently with Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adsbnd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 101, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 101, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706524483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.&lt;/p&gt;\n\n&lt;p&gt;Do you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adsbnd", "is_robot_indexable": true, "report_reasons": null, "author": "BubblyImpress7078", "discussion_type": null, "num_comments": 97, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsbnd/what_happened_recently_with_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsbnd/what_happened_recently_with_snowflake/", "subreddit_subscribers": 156882, "created_utc": 1706524483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "in general", "author_fullname": "t2_lno6576f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is programming in data engineering as complex as in software engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1advvbm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706536559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;in general&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1advvbm", "is_robot_indexable": true, "report_reasons": null, "author": "ryanwolfh", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1advvbm/is_programming_in_data_engineering_as_complex_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1advvbm/is_programming_in_data_engineering_as_complex_as/", "subreddit_subscribers": 156882, "created_utc": 1706536559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked as a BI Developer/SQL DBA/ SQL Developer for like 5 years.For the last 2 years I have been working as a Big data analyst working on Databricks, Python,Pyspark and some AWS services.\nAlso been working on Looker.\n\nI have been trying to switch to a pure DE role where I get to create pipelines and work on more advanced AWS services etc.But most do the DE roles out there requires lots of skills set and I don\u2019t even see many entry level DE roles.\n\nAny one in similar situation?", "author_fullname": "t2_slia7yw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is it so hard to land a DE role.?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae4wnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706559133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked as a BI Developer/SQL DBA/ SQL Developer for like 5 years.For the last 2 years I have been working as a Big data analyst working on Databricks, Python,Pyspark and some AWS services.\nAlso been working on Looker.&lt;/p&gt;\n\n&lt;p&gt;I have been trying to switch to a pure DE role where I get to create pipelines and work on more advanced AWS services etc.But most do the DE roles out there requires lots of skills set and I don\u2019t even see many entry level DE roles.&lt;/p&gt;\n\n&lt;p&gt;Any one in similar situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ae4wnv", "is_robot_indexable": true, "report_reasons": null, "author": "cruze_8907", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae4wnv/why_is_it_so_hard_to_land_a_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae4wnv/why_is_it_so_hard_to_land_a_de_role/", "subreddit_subscribers": 156882, "created_utc": 1706559133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys,\n\nWhich one is better to choose in your opinion? I am a self-taught data engineer, but I would like to get certified so that future employers stop questioning my Bachelor's degree in Economics.\n\ni know that the main diffrence is in the cloud platforms but i would like to know which one offers a greater and better depth into the field of data engineer? \n\nthank you in advance for ur help :D\n\n[https://aws.amazon.com/certification/certified-data-engineer-associate/](https://aws.amazon.com/certification/certified-data-engineer-associate/)\n\n[https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/)", "author_fullname": "t2_qmhxkds80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Certified Data Engineer - Associate VS Azure Data Engineer Associate (DP 203 exam)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae6h40", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706563015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;Which one is better to choose in your opinion? I am a self-taught data engineer, but I would like to get certified so that future employers stop questioning my Bachelor&amp;#39;s degree in Economics.&lt;/p&gt;\n\n&lt;p&gt;i know that the main diffrence is in the cloud platforms but i would like to know which one offers a greater and better depth into the field of data engineer? &lt;/p&gt;\n\n&lt;p&gt;thank you in advance for ur help :D&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/certification/certified-data-engineer-associate/\"&gt;https://aws.amazon.com/certification/certified-data-engineer-associate/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/\"&gt;https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?auto=webp&amp;s=8afacfc14dfed09cec0415cac7d36db9c3374c61", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a1b1322ed94c559cb213e6a08f3eb426a3fb0b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9edddfdb28bb0e92ceb041859aacef81ab9ed42e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de73cdc9da2d0b04938bb7d051ab1a3ceb783323", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60037829d2ce04de0705a2b45123d8ab7c12d41c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5a8f0da08b9281c578a8ab6f49a5b3f577ec9b8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9a9c1c38bd543a7ea6b718e139a9c1e6b62d18", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae6h40", "is_robot_indexable": true, "report_reasons": null, "author": "Comment_Error", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae6h40/aws_certified_data_engineer_associate_vs_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae6h40/aws_certified_data_engineer_associate_vs_azure/", "subreddit_subscribers": 156882, "created_utc": 1706563015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was testing out a nosql solution using cosmosDB on azure. I know this is not conventional. But is it wrong to go with this approach, where the database can be considered as a broker and the tables(or collection) considered as topics? \n\nThe producers add the data to the required  topics(table) and consumers can consume directly from the table. Assuming that there is auto scaling and that these can be upscaled based on the number of incoming messages, is this a viable alternative? And would this be cost effective in comparison to Kafka(Or event hubs)?\n\nAssume that my usecase is adding messages and consuming the data for different purposes. (Processing, dumping to data lake, visualizing data)\n\nAlternatively, is this how Kafka is designed in the backend or is there any other system?\n\nAny answers would be greatly appreciated! Thank you!", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use Kafka when you can use a database with multiple tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adzfya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706545955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing out a nosql solution using cosmosDB on azure. I know this is not conventional. But is it wrong to go with this approach, where the database can be considered as a broker and the tables(or collection) considered as topics? &lt;/p&gt;\n\n&lt;p&gt;The producers add the data to the required  topics(table) and consumers can consume directly from the table. Assuming that there is auto scaling and that these can be upscaled based on the number of incoming messages, is this a viable alternative? And would this be cost effective in comparison to Kafka(Or event hubs)?&lt;/p&gt;\n\n&lt;p&gt;Assume that my usecase is adding messages and consuming the data for different purposes. (Processing, dumping to data lake, visualizing data)&lt;/p&gt;\n\n&lt;p&gt;Alternatively, is this how Kafka is designed in the backend or is there any other system?&lt;/p&gt;\n\n&lt;p&gt;Any answers would be greatly appreciated! Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adzfya", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adzfya/why_use_kafka_when_you_can_use_a_database_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adzfya/why_use_kafka_when_you_can_use_a_database_with/", "subreddit_subscribers": 156882, "created_utc": 1706545955.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Shouldn't be a huge amount of data, &lt;200 GB currently in the DB.  \n\nI titled this as 'replicate', but it's really just ELT without the T.  This replication wouldn't have to be realtime, by the way.\n\nWe're hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?\n\nEdit: edited for clarity.", "author_fullname": "t2_j15uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to replicate from AWS Sql Server db to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae7u5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706566863.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706566365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Shouldn&amp;#39;t be a huge amount of data, &amp;lt;200 GB currently in the DB.  &lt;/p&gt;\n\n&lt;p&gt;I titled this as &amp;#39;replicate&amp;#39;, but it&amp;#39;s really just ELT without the T.  This replication wouldn&amp;#39;t have to be realtime, by the way.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re hoping to roll our own as much as possible in order to save money, but maybe that is unrealistic/unadvisable?&lt;/p&gt;\n\n&lt;p&gt;Edit: edited for clarity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae7u5p", "is_robot_indexable": true, "report_reasons": null, "author": "jbrune", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae7u5p/whats_the_best_way_to_replicate_from_aws_sql/", "subreddit_subscribers": 156882, "created_utc": 1706566365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There's been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there's an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. \n\n&amp;#x200B;\n\nMost of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I'm trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn't have a separate develop environment to develop models, like I'll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don't want to break anything.\n\n&amp;#x200B;\n\nBasically, I'm getting anxious about not knowing what I don't know regarding most things DE and I want to sack up and try to get better at it but I'm not quite sure where to start.", "author_fullname": "t2_4xc90hk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do to elevate your skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeefxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706584492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a mid-ish level analytics engineer that went from being a data analyst at a previous job to being hired almost exclusively to make data models in dbt at my current job (data stack is airflow, dbt, snowflake). There&amp;#39;s been some internal restructuring in my organization on top of my manager leaving (2.5 months after hiring me too) so now there&amp;#39;s an expectation that my work is going to include a lot more stuff outside my niche skillset, specifically leaning more towards DE stuff. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Most of my skills lie in data modeling and the front end aspect of data analytics like dashboarding, data exploration, creating reports etc. When I listen in on conversations involving our data architect and senior DE, it gets overwhelming for me as I&amp;#39;m trying to keep up. On top of this, I already have ideas I want to change/implement with how we go about using dbt and overall just have better ownership of it. For example, my current org doesn&amp;#39;t have a separate develop environment to develop models, like I&amp;#39;ll create a new branch in git but if I want to test the results and use the dbt run command, it literally will materialize the model straight to production. I want to change that but all of that back end stuff seems daunting and I don&amp;#39;t want to break anything.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, I&amp;#39;m getting anxious about not knowing what I don&amp;#39;t know regarding most things DE and I want to sack up and try to get better at it but I&amp;#39;m not quite sure where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeefxr", "is_robot_indexable": true, "report_reasons": null, "author": "thisisformeworking", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeefxr/what_would_you_do_to_elevate_your_skills/", "subreddit_subscribers": 156882, "created_utc": 1706584492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I am curios to know that what ideas and innovations you introduced in your data engineering project?", "author_fullname": "t2_uok38vei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas and innovations in data engineering project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae3glr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706555617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curios to know that what ideas and innovations you introduced in your data engineering project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae3glr", "is_robot_indexable": true, "report_reasons": null, "author": "loosernew7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae3glr/ideas_and_innovations_in_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae3glr/ideas_and_innovations_in_data_engineering_project/", "subreddit_subscribers": 156882, "created_utc": 1706555617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nCompany is currently in the process of designing a custom architecture for a data warehouse. Due to data residency requirements, our data is split across 7 different azure regions in seven different databases. In addition, the databases are semi multi-tenant \u2014 some clients have their own specialized schema, though the tables are constant in each schema. In short, there are \"50\" targets where data could be stored.\n\nAnyways, we rely on Presto to query this data, which is inefficient and expensive. We're looking into centralizing everything, which will be done in Databricks for the creation of the Silver/Bronze layers and Unity for ACL. However, to prevent vendor lock-in, we're looking to engineer a custom solution to get all the data from our databases, incrementally update via CDC it in a bronze layer stored in Azure S3, so we could move vendors and not re-invent the wheel if business priorities change.\n\nHere's the architecture I'm currently considering, based on a couple days of research.\n\nMariaDB Databases -&gt; CDC read through Debezium Server -&gt; Dumped to common FastAPI/Axum HTTP Server via Json/Avro (between all 7 databases) -&gt; Instructions for recreating data inserted into Delta Table -&gt; Reconstruct entire dataset based on Instructions (Delta or Iceberg, open question) -&gt; Create Silver/Gold layers through Databricks ([process here](https://docs.databricks.com/en/delta/clone-parquet.html))\n\nSome open questions I have\n\n\\- Is Iceberg or Delta the preferred solution here? I was mocking the HTTP server with Iceberg because it supports schema evolution. I was looking at delta-rs for this, but doesn't support schema evolution yet, which I would like to be intention since Iceberg supports this natively. I would like to avoid using Spark if possible.\n\n\\- I would be processing the CDC as they come in, which means I would constantly be inserting one row. I'd imagine these parquet-based solutions are not particularly good at inserts of one row. I could theoretically implement native batching. Is there any alternate solution, that is ideally file-based like Delta/Iceberg and open-source?\n\n\\- Am I overthinking it by storing the instructions for how to recreate the data rather than just translating the Debezium instructions directly into the relevant data?\n\nAny thoughts would be appreciated here. Thank you!", "author_fullname": "t2_v12atn7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Debezium-based Multimodal Datastore - critiques needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae36c3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706554911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;Company is currently in the process of designing a custom architecture for a data warehouse. Due to data residency requirements, our data is split across 7 different azure regions in seven different databases. In addition, the databases are semi multi-tenant \u2014 some clients have their own specialized schema, though the tables are constant in each schema. In short, there are &amp;quot;50&amp;quot; targets where data could be stored.&lt;/p&gt;\n\n&lt;p&gt;Anyways, we rely on Presto to query this data, which is inefficient and expensive. We&amp;#39;re looking into centralizing everything, which will be done in Databricks for the creation of the Silver/Bronze layers and Unity for ACL. However, to prevent vendor lock-in, we&amp;#39;re looking to engineer a custom solution to get all the data from our databases, incrementally update via CDC it in a bronze layer stored in Azure S3, so we could move vendors and not re-invent the wheel if business priorities change.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the architecture I&amp;#39;m currently considering, based on a couple days of research.&lt;/p&gt;\n\n&lt;p&gt;MariaDB Databases -&amp;gt; CDC read through Debezium Server -&amp;gt; Dumped to common FastAPI/Axum HTTP Server via Json/Avro (between all 7 databases) -&amp;gt; Instructions for recreating data inserted into Delta Table -&amp;gt; Reconstruct entire dataset based on Instructions (Delta or Iceberg, open question) -&amp;gt; Create Silver/Gold layers through Databricks (&lt;a href=\"https://docs.databricks.com/en/delta/clone-parquet.html\"&gt;process here&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Some open questions I have&lt;/p&gt;\n\n&lt;p&gt;- Is Iceberg or Delta the preferred solution here? I was mocking the HTTP server with Iceberg because it supports schema evolution. I was looking at delta-rs for this, but doesn&amp;#39;t support schema evolution yet, which I would like to be intention since Iceberg supports this natively. I would like to avoid using Spark if possible.&lt;/p&gt;\n\n&lt;p&gt;- I would be processing the CDC as they come in, which means I would constantly be inserting one row. I&amp;#39;d imagine these parquet-based solutions are not particularly good at inserts of one row. I could theoretically implement native batching. Is there any alternate solution, that is ideally file-based like Delta/Iceberg and open-source?&lt;/p&gt;\n\n&lt;p&gt;- Am I overthinking it by storing the instructions for how to recreate the data rather than just translating the Debezium instructions directly into the relevant data?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be appreciated here. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data/Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae36c3", "is_robot_indexable": true, "report_reasons": null, "author": "Dazzling-Reason-5140", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ae36c3/building_a_debeziumbased_multimodal_datastore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae36c3/building_a_debeziumbased_multimodal_datastore/", "subreddit_subscribers": 156882, "created_utc": 1706554911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In our current data transfer workflows, we heavily rely on Azure Data Factory Copy and Data Flows. To explore cost-effective alternatives and broaden our toolkit, what are other Azure tools or solutions that we should consider for data transfer? Additionally, are there any noteworthy engineering blogs or Twitter users who share insights and best practices in the realm of data transfer, especially focusing on cost optimization and efficient workflows? thank you", "author_fullname": "t2_7h5p8yg4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Azure DE costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aducno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706531897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In our current data transfer workflows, we heavily rely on Azure Data Factory Copy and Data Flows. To explore cost-effective alternatives and broaden our toolkit, what are other Azure tools or solutions that we should consider for data transfer? Additionally, are there any noteworthy engineering blogs or Twitter users who share insights and best practices in the realm of data transfer, especially focusing on cost optimization and efficient workflows? thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aducno", "is_robot_indexable": true, "report_reasons": null, "author": "West_Bank3045", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aducno/optimizing_azure_de_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aducno/optimizing_azure_de_costs/", "subreddit_subscribers": 156882, "created_utc": 1706531897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that we need to learn all these different tools to build a stack as a data engineer but i was wondering about whether there is a CI/CD process involved in a professional project. if so what additional tools are used for this.", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer as a Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adsmmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706525722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that we need to learn all these different tools to build a stack as a data engineer but i was wondering about whether there is a CI/CD process involved in a professional project. if so what additional tools are used for this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1adsmmu", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsmmu/data_engineer_as_a_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsmmu/data_engineer_as_a_professional/", "subreddit_subscribers": 156882, "created_utc": 1706525722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The challenge in optimizing the data lake for PowerBI analytics arises from distinct product dimensions with specific features in each department, causing complications in cross-department reporting due to inconsistent SKs assigned to the same product. \n\nTo address this issue, [a proposed solution involves creating a master product dimension for a unified source of truth](https://www.dataversity.net/what-is-master-data-management-and-why-is-it-important/#:%7E:text=Improved%20Data%20Quality%3A%20Master%20Data,businesses%20to%20make%20better%20decisions.).  Concerns have been raised about complexity, especially regarding varying product details in sales and marketing, and the inclusion of unique finance products. \n\nI undertook the following exercise to gain insight into the potential behavior of this master data. As you can observe below, the number of lines is anticipated to be substantial. This is due to the fact that, for every combination of product and features, an individual line would be generated, distinguished by a unique SKU identifier.\n\nhttps://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3\n\nSo here the questions arise:\n\n1. Are there any technical drawbacks or best practice violations associated with maintaining a global product table like that, considering the potential table size and repetition information, like the finance one, for example, across multiple lines?\n2. How does the proposed solution affect the ease of maintenance and updates to the global product table? Are there complexities in managing changes, additions, or deletions with different departments having distinct products or features?\n3. How would maintaining a large master product dimension impact system performance and scalability?", "author_fullname": "t2_kxhgl4or", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Dimension Models in Data Lakes: Master Product Dimension vs. Tailored Department Dimensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4royh9jaycfc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=198095c418c4de88cd6631fb31c4a194466ea26b"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a5872b58420ef2f6b3897ca956c274ec0f28527"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=557528dc4bce94b151f9e3ce010fc6e99c5ff7fd"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d69dea6f8fe9985a005893743be2e8246aa1c2bb"}, {"y": 420, "x": 960, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2277d703a368096561e52b557c0c2a6678c184d4"}, {"y": 473, "x": 1080, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a5b6bc31d3d09dbf4d571b7e807bef837721035"}], "s": {"y": 480, "x": 1095, "u": "https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3"}, "id": "4royh9jaycfc1"}}, "name": "t3_1adsfka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DlYDL1FDMhWqArc7ApoIzBWHhCEojYIAYqLv5D7xgxc.jpg", "edited": 1706525156.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1706524927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The challenge in optimizing the data lake for PowerBI analytics arises from distinct product dimensions with specific features in each department, causing complications in cross-department reporting due to inconsistent SKs assigned to the same product. &lt;/p&gt;\n\n&lt;p&gt;To address this issue, &lt;a href=\"https://www.dataversity.net/what-is-master-data-management-and-why-is-it-important/#:%7E:text=Improved%20Data%20Quality%3A%20Master%20Data,businesses%20to%20make%20better%20decisions.\"&gt;a proposed solution involves creating a master product dimension for a unified source of truth&lt;/a&gt;.  Concerns have been raised about complexity, especially regarding varying product details in sales and marketing, and the inclusion of unique finance products. &lt;/p&gt;\n\n&lt;p&gt;I undertook the following exercise to gain insight into the potential behavior of this master data. As you can observe below, the number of lines is anticipated to be substantial. This is due to the fact that, for every combination of product and features, an individual line would be generated, distinguished by a unique SKU identifier.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3\"&gt;https://preview.redd.it/4royh9jaycfc1.png?width=1095&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7aba8d04655382fbed01544ab2c498a728d82f3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So here the questions arise:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there any technical drawbacks or best practice violations associated with maintaining a global product table like that, considering the potential table size and repetition information, like the finance one, for example, across multiple lines?&lt;/li&gt;\n&lt;li&gt;How does the proposed solution affect the ease of maintenance and updates to the global product table? Are there complexities in managing changes, additions, or deletions with different departments having distinct products or features?&lt;/li&gt;\n&lt;li&gt;How would maintaining a large master product dimension impact system performance and scalability?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?auto=webp&amp;s=438a6872ab09140ec6dc9fb7b68ccc6c20d6259f", "width": 600, "height": 448}, "resolutions": [{"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea7fa6486e997424307e15991720d1f145d00397", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e59110c0f252a0f8bcb310a31575865828db2f37", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/wxpgTnLhCeyldkf2WSqJM28Sk8-KEEyjP-7t8g6F9dg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddaa41a3c5878fd9d0da4c1e932964d019e459cd", "width": 320, "height": 238}], "variants": {}, "id": "8Hfj_A51oBRRXTV3N6uczaapOWcexuKas5PYBkTU0uw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adsfka", "is_robot_indexable": true, "report_reasons": null, "author": "tazz_bh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adsfka/optimizing_dimension_models_in_data_lakes_master/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adsfka/optimizing_dimension_models_in_data_lakes_master/", "subreddit_subscribers": 156882, "created_utc": 1706524927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aegsqh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706592086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fellow architects, what\u2019s your favorite architect you\u2019ve built and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aegsqh", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aegsqh/solutions_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aegsqh/solutions_architect/", "subreddit_subscribers": 156882, "created_utc": 1706592086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Maps Web Scraping: 3 Practical Use Cases and How to Make the Most of Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1aedw84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/f8g_U5jBsJM2NEpGIUDWiYZcTFPXGTuOLB2SlNkfK3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706582904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?auto=webp&amp;s=46d33550cc81f9decd30fc91d60f6d2eeb67d612", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cac97fdd038cc2a317548d28ca7e8c67e25d359", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17bd1c7742d995fe6de3bdf8b35eeaf0fb7bbd5d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67aace18bb950eb12834c9d34e6c62d5ea85fec2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bcb8328a6c4d241ae19d9f1c42540e44059486e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=95819aa2b0595c6080f137c4b7c50287242c2787", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ngNy0twZ9PucNZE5BSURJzeMDm0NyXIJbY9y18tB90I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ec8499ad5ccce921e23e6417b7f23f5ba77d7c4", "width": 1080, "height": 567}], "variants": {}, "id": "XyWQAoZBD9EGqALl6yef2IcN-kJAsS4GMso9KUxnV-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aedw84", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aedw84/google_maps_web_scraping_3_practical_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://plainenglish.io/community/google-maps-web-scraping-3-practical-use-cases-and-how-to-make-the-most-of-them-marcoacavaco-rodrigues-gmail-com-1700744574311", "subreddit_subscribers": 156882, "created_utc": 1706582904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to work on a table with 50mil rows, but I'm not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with a sql table of 50mil records in pandas? Will sagemaker work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aeawka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706574401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to work on a table with 50mil rows, but I&amp;#39;m not able to load the table into pandas on my local system. Will it work if I use sagemaker or google colab? Does anyone have experience working with such big data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aeawka", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aeawka/working_with_a_sql_table_of_50mil_records_in/", "subreddit_subscribers": 156882, "created_utc": 1706574401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nSorry for the long post, but I am not sure how to sum this up into a tl;dr.\n\nI am fairly early in my career, and I have been tasked with setting up a monitoring system that is able to provide visibility at the row level.\n\nFor clarification, the lead of my team wants to be able to pick an arbitrary row in a table and know exactly how long it took to get from the source to destination and the details at each step in between.\n\nThe concept is simple at a high level. It's just subtracting timestamps. However, we use a lot of no-code solutions that don't provide APIs or access to what's actually going on behind the scenes, and the source system metrics are closed to external use. Getting a creation time and visibility into each step is extremely difficult as I see it.\n\nI feel like time would be better spent fixing the architecture that is unstable instead of chasing a level of monitoring that I am not sure is even possible to create reliably due to manual exports of data and hacky code due to closed systems.\n\nHowever, maybe I am missing something obvious due to a lack of knowledge. I am the only data engineer and my boss isn't technical, so I am mostly on my own here.\n\nAny advice?\n\nThe current pipeline is SAP -&gt; Qlik Replicate -&gt; Databricks DLT -&gt; Synapse serverless.", "author_fullname": "t2_96mteoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wasting Resources Chasing Dreams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae2x5u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706554292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post, but I am not sure how to sum this up into a tl;dr.&lt;/p&gt;\n\n&lt;p&gt;I am fairly early in my career, and I have been tasked with setting up a monitoring system that is able to provide visibility at the row level.&lt;/p&gt;\n\n&lt;p&gt;For clarification, the lead of my team wants to be able to pick an arbitrary row in a table and know exactly how long it took to get from the source to destination and the details at each step in between.&lt;/p&gt;\n\n&lt;p&gt;The concept is simple at a high level. It&amp;#39;s just subtracting timestamps. However, we use a lot of no-code solutions that don&amp;#39;t provide APIs or access to what&amp;#39;s actually going on behind the scenes, and the source system metrics are closed to external use. Getting a creation time and visibility into each step is extremely difficult as I see it.&lt;/p&gt;\n\n&lt;p&gt;I feel like time would be better spent fixing the architecture that is unstable instead of chasing a level of monitoring that I am not sure is even possible to create reliably due to manual exports of data and hacky code due to closed systems.&lt;/p&gt;\n\n&lt;p&gt;However, maybe I am missing something obvious due to a lack of knowledge. I am the only data engineer and my boss isn&amp;#39;t technical, so I am mostly on my own here.&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n\n&lt;p&gt;The current pipeline is SAP -&amp;gt; Qlik Replicate -&amp;gt; Databricks DLT -&amp;gt; Synapse serverless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ae2x5u", "is_robot_indexable": true, "report_reasons": null, "author": "kevrinth", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae2x5u/wasting_resources_chasing_dreams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae2x5u/wasting_resources_chasing_dreams/", "subreddit_subscribers": 156882, "created_utc": 1706554292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've been in DE/DA field for 2 years now, and I'd like to start working with AWS. I'd like to get the \"AWS Certified Data Engineer\" certification to improve my resume and to find a good job easily. I've done few simple pipelines at my previous job, but nothing big, mainly using Appflow, S3, Athena, Redshift and Quicksight. So no Spark, no Kafka, no Glue. \n\nMy maine issue actually is : what path should I follow and what ressources should I use to prepare this exam ?\n\nThe AWS SkillBuilder recommands to get 2 certifications before this one : Foundational Cloud Practitioner and Associate Solutions Architect. Do I really have to get these 2 certifications first (especially the Associate one which is probably harder and more expensive) ? Or it won't bring me any value for the DE one and I can skip them, saving time and money ?\n\nFor these 3 certifications, there are a bunch of courses provided by AWS SkillBuilder for 29$/mo, is it good and worth buying/following ? And is it enough ?", "author_fullname": "t2_1cdu2cz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare \"AWS Certified Data Engineer - Associate\" ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae2r8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706553894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been in DE/DA field for 2 years now, and I&amp;#39;d like to start working with AWS. I&amp;#39;d like to get the &amp;quot;AWS Certified Data Engineer&amp;quot; certification to improve my resume and to find a good job easily. I&amp;#39;ve done few simple pipelines at my previous job, but nothing big, mainly using Appflow, S3, Athena, Redshift and Quicksight. So no Spark, no Kafka, no Glue. &lt;/p&gt;\n\n&lt;p&gt;My maine issue actually is : what path should I follow and what ressources should I use to prepare this exam ?&lt;/p&gt;\n\n&lt;p&gt;The AWS SkillBuilder recommands to get 2 certifications before this one : Foundational Cloud Practitioner and Associate Solutions Architect. Do I really have to get these 2 certifications first (especially the Associate one which is probably harder and more expensive) ? Or it won&amp;#39;t bring me any value for the DE one and I can skip them, saving time and money ?&lt;/p&gt;\n\n&lt;p&gt;For these 3 certifications, there are a bunch of courses provided by AWS SkillBuilder for 29$/mo, is it good and worth buying/following ? And is it enough ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae2r8k", "is_robot_indexable": true, "report_reasons": null, "author": "imKrypex", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae2r8k/how_to_prepare_aws_certified_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae2r8k/how_to_prepare_aws_certified_data_engineer/", "subreddit_subscribers": 156882, "created_utc": 1706553894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you use data virtualization (using a tool to connect different data sources and wrangle them together without the need to physically move the data).\n\nWhat tools do you use? What do like or dislike about it? What do you wish existed?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Virtualization, do you use it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adzkjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706546281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you use data virtualization (using a tool to connect different data sources and wrangle them together without the need to physically move the data).&lt;/p&gt;\n\n&lt;p&gt;What tools do you use? What do like or dislike about it? What do you wish existed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adzkjs", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adzkjs/data_virtualization_do_you_use_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adzkjs/data_virtualization_do_you_use_it/", "subreddit_subscribers": 156882, "created_utc": 1706546281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working in a project to start doing analytics and reporting of our company data, the data is in a cockroach DB instance and I plan on making a pipeline to incrementally move the data to Snowflake. But I don't know what should be a good approach to making this pipeline, I took a look at snowpipe and snowpark to move the data, but all the documentation I found relies on the use of a staging area, such as S3. My main concerns are:\n\n\\- How to incrementally move the CockroachDB data to a staging area\n\n\\- How to make transformations to alter the table schemas and relationships? Right now the CockroachDB is used as a OLTP database, I want to implement a star schema on Snowflake.\n\n&amp;#x200B;", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data to Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adwv6q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706539269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working in a project to start doing analytics and reporting of our company data, the data is in a cockroach DB instance and I plan on making a pipeline to incrementally move the data to Snowflake. But I don&amp;#39;t know what should be a good approach to making this pipeline, I took a look at snowpipe and snowpark to move the data, but all the documentation I found relies on the use of a staging area, such as S3. My main concerns are:&lt;/p&gt;\n\n&lt;p&gt;- How to incrementally move the CockroachDB data to a staging area&lt;/p&gt;\n\n&lt;p&gt;- How to make transformations to alter the table schemas and relationships? Right now the CockroachDB is used as a OLTP database, I want to implement a star schema on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1adwv6q", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adwv6q/moving_data_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adwv6q/moving_data_to_snowflake/", "subreddit_subscribers": 156882, "created_utc": 1706539269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_46tlcjz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notes on Postgres user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1adrdbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ItXhhbPlVmWQpOS6kemUmpUWhRzTR3sZQvF1DJkso2A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706520564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "telablog.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://telablog.com/notes-on-postgres-user-management", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?auto=webp&amp;s=ff5fb6ff9a2f3334a2cd017ae15057f8307ebd39", "width": 800, "height": 533}, "resolutions": [{"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5af5b2f61917e95b77c9f334bc195fb37d0d884", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9dd1168e671cf200608d409f57acc42084cc3a7", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=32aecf9bd4dc2ea8d3654641774d4c7adf9b50f7", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BnhVkusdsWvb636QJSJHuIZwcklbI6-xfyvubvRwa8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41f6721dbb4c6b68e66b75dc35fbf1fad48b6c76", "width": 640, "height": 426}], "variants": {}, "id": "zN5Pqvs0hqwGFjxK4E-GpZ9fGcOAAObX-PeopO8JVGo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1adrdbg", "is_robot_indexable": true, "report_reasons": null, "author": "stjohn_piano", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adrdbg/notes_on_postgres_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://telablog.com/notes-on-postgres-user-management", "subreddit_subscribers": 156882, "created_utc": 1706520564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I need your insights about my problem in my current project.\n\nSo, we need to ingest data from a DB to S3 using AWS Glue. We're planning to use incremental reload after the first ingestion since our tables are large ( 200M rows).\n\nOur problem is that are tables do not have primary keys and a date column that will indicate when was the record is inserted in the DB.\n\nOne workaround is we do full reload, but this will be costly since we're planning to do our data refresh daily.\n\nDo you have any idea for a workaround?", "author_fullname": "t2_num46cmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is incremental reload possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1adqpth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706517793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I need your insights about my problem in my current project.&lt;/p&gt;\n\n&lt;p&gt;So, we need to ingest data from a DB to S3 using AWS Glue. We&amp;#39;re planning to use incremental reload after the first ingestion since our tables are large ( 200M rows).&lt;/p&gt;\n\n&lt;p&gt;Our problem is that are tables do not have primary keys and a date column that will indicate when was the record is inserted in the DB.&lt;/p&gt;\n\n&lt;p&gt;One workaround is we do full reload, but this will be costly since we&amp;#39;re planning to do our data refresh daily.&lt;/p&gt;\n\n&lt;p&gt;Do you have any idea for a workaround?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1adqpth", "is_robot_indexable": true, "report_reasons": null, "author": "matchaaa_latte", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1adqpth/is_incremental_reload_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1adqpth/is_incremental_reload_possible/", "subreddit_subscribers": 156882, "created_utc": 1706517793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey I am a SWE at a B2B SaaS company and my PM asked me to classify our customer feedback into Feature Requests, Bugs, and Reviews. I built a model but it does not work. Are there any models I can use? Any suggestions? I am on a time clock.\n\nThanks!", "author_fullname": "t2_nvnticro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Classify our customer feedback into Feature Requests, Bugs, and Reviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aehll2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706594942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I am a SWE at a B2B SaaS company and my PM asked me to classify our customer feedback into Feature Requests, Bugs, and Reviews. I built a model but it does not work. Are there any models I can use? Any suggestions? I am on a time clock.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aehll2", "is_robot_indexable": true, "report_reasons": null, "author": "Scary-Swing2852", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aehll2/classify_our_customer_feedback_into_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aehll2/classify_our_customer_feedback_into_feature/", "subreddit_subscribers": 156882, "created_utc": 1706594942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure what to expect, if any of you have gone through mid/senior interviews, what do you suggest I prepare for?  (e.g. DS&amp;Algo Leetcode style then Medium/Hard questions? etc)", "author_fullname": "t2_3tzpeuhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do data engineer interviews (mid/senior) look like at hedge funds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae5l9n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706560835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure what to expect, if any of you have gone through mid/senior interviews, what do you suggest I prepare for?  (e.g. DS&amp;amp;Algo Leetcode style then Medium/Hard questions? etc)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ae5l9n", "is_robot_indexable": true, "report_reasons": null, "author": "randomusicjunkie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae5l9n/what_do_data_engineer_interviews_midsenior_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ae5l9n/what_do_data_engineer_interviews_midsenior_look/", "subreddit_subscribers": 156882, "created_utc": 1706560835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For over 8 years, I've been using Laravel daily. One of the gaps I currently feel in the Geoglify stack is the ease of organizing initial data and automatically populating the database, as I do with Laravel Seed. Therefore, I created a small example for MongoDB. This example allows for a straightforward addition of static data related to ships, ports, zones, and more to MongoDB.", "author_fullname": "t2_7svy5qp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Initializing MongoDB with JSON Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1ae37fb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1d_bJ-kMqLDpO6iJ3U2REgS6a30wsr3yupAaSsutzD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706554983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "geoglify.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For over 8 years, I&amp;#39;ve been using Laravel daily. One of the gaps I currently feel in the Geoglify stack is the ease of organizing initial data and automatically populating the database, as I do with Laravel Seed. Therefore, I created a small example for MongoDB. This example allows for a straightforward addition of static data related to ships, ports, zones, and more to MongoDB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.geoglify.com/blog/initial-seed-mongodb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?auto=webp&amp;s=3cc2c6acaae95b98a6c8f39545bd7e504d78f55a", "width": 1479, "height": 827}, "resolutions": [{"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b837534f089c8820261cfcb889dc11563e14a1ef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c7d18d2b443100a6f929670b845f1fe883035b0", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a468b2ac0110bf2609f4c73260f416432055526", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4559439083657794f464663ef64746dbdbd4b90b", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0d605e28f3189116727cc02990acd2efc3a3c8a", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/eu2M4QjAnh9nLNf_PELZn0_FDJ8R4yN1XSoXHUjJVOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b351ac9b40ab82a3ab6e0790369310db29a4da8", "width": 1080, "height": 603}], "variants": {}, "id": "avzTdEtIlyrGjuhStoUYXVeh_K8zC4M5WXCGDLZ_9L8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ae37fb", "is_robot_indexable": true, "report_reasons": null, "author": "leoneljdias", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ae37fb/initializing_mongodb_with_json_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.geoglify.com/blog/initial-seed-mongodb", "subreddit_subscribers": 156882, "created_utc": 1706554983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "mod pls delete if not relevant.\n\ni was a AI Engineer trainee for about a year, then a Data Analyst for a year before that. my company is laying off people so im urgently looking for another job and surprisingly a mid-level DE position in a government-linked company invited me for an interview though i applied for a DA position. They told me my background was \"too technical\" for a DA role and now i guess i have a DE interview tomorrow?? \n\nI suspect the HR saw i used azure cloud and databricks and automatically assumed theyre the same as a DE skillset..i dont know man.\n\nAre there any transferable skills to be a DE from a DA/AI type role and any tips anyone here can give? im interested in Data Engineering but worried i will bomb the interview and not be up for the job. i have no clue how to design databases or tables or pipelines other than the usual star/snowflake schema basic stuff u learn in powerBI class lmao.", "author_fullname": "t2_8xqdvapu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for DE interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ads4nq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706523684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;mod pls delete if not relevant.&lt;/p&gt;\n\n&lt;p&gt;i was a AI Engineer trainee for about a year, then a Data Analyst for a year before that. my company is laying off people so im urgently looking for another job and surprisingly a mid-level DE position in a government-linked company invited me for an interview though i applied for a DA position. They told me my background was &amp;quot;too technical&amp;quot; for a DA role and now i guess i have a DE interview tomorrow?? &lt;/p&gt;\n\n&lt;p&gt;I suspect the HR saw i used azure cloud and databricks and automatically assumed theyre the same as a DE skillset..i dont know man.&lt;/p&gt;\n\n&lt;p&gt;Are there any transferable skills to be a DE from a DA/AI type role and any tips anyone here can give? im interested in Data Engineering but worried i will bomb the interview and not be up for the job. i have no clue how to design databases or tables or pipelines other than the usual star/snowflake schema basic stuff u learn in powerBI class lmao.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ads4nq", "is_robot_indexable": true, "report_reasons": null, "author": "raspberry6754", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ads4nq/advice_for_de_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ads4nq/advice_for_de_interview/", "subreddit_subscribers": 156882, "created_utc": 1706523684.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}