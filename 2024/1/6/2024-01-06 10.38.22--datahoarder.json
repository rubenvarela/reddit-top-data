{"kind": "Listing", "data": {"after": "t3_18zc8se", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is the LaCie Big Disk Extreme 2TB, model 301201U\n\nMy dad bought and used a couple of these, and I'm trying to gauge between scrapping this or whether it's worth the effort to sell or offer up (without the HDDs, since wiping them is definitely not worth my time). \n\nWith only Firewire and USB 2.0 interfaces, the enclosure would have limited appeal, but these were designed with onboard RAID0 to turn the two 1 TB drives into a \"single\" 2 TB drive. \n\nThe circuit boards of these circa 2009 enclosures sometimes failed, so perhaps there's a market for people who need to access the (striped) data from a failed unit? \n\nDoes anybody have a suggestion for which way I should proceed?", "author_fullname": "t2_suissw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LaCie external, worth the effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "name": "t3_18zhw9n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l0kklg28J6OWMZVgrtDbPd8uToT3m8eTY9YQOg0cuD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704489518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is the LaCie Big Disk Extreme 2TB, model 301201U&lt;/p&gt;\n\n&lt;p&gt;My dad bought and used a couple of these, and I&amp;#39;m trying to gauge between scrapping this or whether it&amp;#39;s worth the effort to sell or offer up (without the HDDs, since wiping them is definitely not worth my time). &lt;/p&gt;\n\n&lt;p&gt;With only Firewire and USB 2.0 interfaces, the enclosure would have limited appeal, but these were designed with onboard RAID0 to turn the two 1 TB drives into a &amp;quot;single&amp;quot; 2 TB drive. &lt;/p&gt;\n\n&lt;p&gt;The circuit boards of these circa 2009 enclosures sometimes failed, so perhaps there&amp;#39;s a market for people who need to access the (striped) data from a failed unit? &lt;/p&gt;\n\n&lt;p&gt;Does anybody have a suggestion for which way I should proceed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lca9a98guoac1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?auto=webp&amp;s=6e1821846edddd0f7f6d729a09e561e3a0e2b213", "width": 3744, "height": 2877}, "resolutions": [{"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2458c22394da4449e1f3148b681806fadbbda979", "width": 108, "height": 82}, {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de0af23c6e56717010f7af3442bb5a05d79721bc", "width": 216, "height": 165}, {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=432b5a9c9a6284e30fdc0f055455c4ef2292a75c", "width": 320, "height": 245}, {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a412023ac7f4ce1b8968ca3e68fa928b53216bc6", "width": 640, "height": 491}, {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad74f28da7a794934652eb8e658141176a48d459", "width": 960, "height": 737}, {"url": "https://preview.redd.it/lca9a98guoac1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c874771e20f6196896a3b23644231a36c1105c40", "width": 1080, "height": 829}], "variants": {}, "id": "iNXG2cDdjc4EKval2WAMAGBbuoKohqignNlGRBtU72I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zhw9n", "is_robot_indexable": true, "report_reasons": null, "author": "Redditations2u", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zhw9n/lacie_external_worth_the_effort/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lca9a98guoac1.jpeg", "subreddit_subscribers": 723297, "created_utc": 1704489518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been seeing some news indicating that SSD prices are and will continue to rise through 2024. I'm not very literate on the topic, so I'm wondering if buying now in anticipation of the price hikes is reasonable or not.\n\nI'm not really short on SSD storage, but I don't want the price hikes to bite me in the near future.", "author_fullname": "t2_179jmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you plan on buying SSDs in anticipation of price changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zs1bu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704517354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been seeing some news indicating that SSD prices are and will continue to rise through 2024. I&amp;#39;m not very literate on the topic, so I&amp;#39;m wondering if buying now in anticipation of the price hikes is reasonable or not.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not really short on SSD storage, but I don&amp;#39;t want the price hikes to bite me in the near future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zs1bu", "is_robot_indexable": true, "report_reasons": null, "author": "88sSSSs88", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zs1bu/do_you_plan_on_buying_ssds_in_anticipation_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zs1bu/do_you_plan_on_buying_ssds_in_anticipation_of/", "subreddit_subscribers": 723297, "created_utc": 1704517354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Do you guys run disk check utilities after buying a new drive? Or do you just use it out of the box?\n\nFrom what I understand, if there is a physical problem with a sector and the hard drive cannot write to it, it will mark it as a bad sector:\n\n[https://superuser.com/questions/1717234/how-does-a-hard-drive-know-if-its-writing-to-a-bad-sector-that-wasnt-bad-befor](https://superuser.com/questions/1717234/how-does-a-hard-drive-know-if-its-writing-to-a-bad-sector-that-wasnt-bad-befor)\n\n\"A bad block/sector is only detected on a read, and not on a write. The only error when writing would be if the sector/LBA could not be found.\"\n\nSo it seems unnecessary to do chkdsk or full format on a new hard drive as it won't write to a physically defective sector in the first place. I would like to hear your opinions thanks.", "author_fullname": "t2_hxw7xcf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you run chkdsk or full format on a new hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zel0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704515051.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704481302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you guys run disk check utilities after buying a new drive? Or do you just use it out of the box?&lt;/p&gt;\n\n&lt;p&gt;From what I understand, if there is a physical problem with a sector and the hard drive cannot write to it, it will mark it as a bad sector:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://superuser.com/questions/1717234/how-does-a-hard-drive-know-if-its-writing-to-a-bad-sector-that-wasnt-bad-befor\"&gt;https://superuser.com/questions/1717234/how-does-a-hard-drive-know-if-its-writing-to-a-bad-sector-that-wasnt-bad-befor&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A bad block/sector is only detected on a read, and not on a write. The only error when writing would be if the sector/LBA could not be found.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So it seems unnecessary to do chkdsk or full format on a new hard drive as it won&amp;#39;t write to a physically defective sector in the first place. I would like to hear your opinions thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?auto=webp&amp;s=f5c7d0b03f76df3af6a9365922d04d5bd2583811", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fac6ce2a333d0513b26437b0c86f4ad3a792fa5", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=136104314e6105ef277f5fa8147e92b5533b4014", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zel0r", "is_robot_indexable": true, "report_reasons": null, "author": "Average4B", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zel0r/do_you_run_chkdsk_or_full_format_on_a_new_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zel0r/do_you_run_chkdsk_or_full_format_on_a_new_hard/", "subreddit_subscribers": 723297, "created_utc": 1704481302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the right sub for this but imma ask anyways. I\u2019m getting a LSI 9207-8i for my server but after I ordered it I realized that all my remaining PCIE slots are all PCIE 1x slots. Will I be able to use the card or should I look into another option?", "author_fullname": "t2_51dfdxum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8x pcie LSI card in a 1x slot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zi2ya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704489987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the right sub for this but imma ask anyways. I\u2019m getting a LSI 9207-8i for my server but after I ordered it I realized that all my remaining PCIE slots are all PCIE 1x slots. Will I be able to use the card or should I look into another option?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zi2ya", "is_robot_indexable": true, "report_reasons": null, "author": "opi098514", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zi2ya/8x_pcie_lsi_card_in_a_1x_slot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zi2ya/8x_pcie_lsi_card_in_a_1x_slot/", "subreddit_subscribers": 723297, "created_utc": 1704489987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a PCI-E 5.0 port on my machine that I'd like to populate with U.2 drives (PM9A3 most likely).  I'm unable to find a decent enough option to split a 16x port into 4x4x that supports over Gen3 speeds (plenty of 3.0 PEX based cards).  My motherboard does not support bifurcation so it needs to be on the card.\n\nAre there PCI-E 5.0/4.0 16x to 4xU.2 options available?\n\nNote: Based in UK", "author_fullname": "t2_10y2kc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PCI-E 5.0 or 4.0 16x to 4x4 U.2?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zorln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704507362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a PCI-E 5.0 port on my machine that I&amp;#39;d like to populate with U.2 drives (PM9A3 most likely).  I&amp;#39;m unable to find a decent enough option to split a 16x port into 4x4x that supports over Gen3 speeds (plenty of 3.0 PEX based cards).  My motherboard does not support bifurcation so it needs to be on the card.&lt;/p&gt;\n\n&lt;p&gt;Are there PCI-E 5.0/4.0 16x to 4xU.2 options available?&lt;/p&gt;\n\n&lt;p&gt;Note: Based in UK&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "248TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zorln", "is_robot_indexable": true, "report_reasons": null, "author": "scs3jb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18zorln/pcie_50_or_40_16x_to_4x4_u2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zorln/pcie_50_or_40_16x_to_4x4_u2/", "subreddit_subscribers": 723297, "created_utc": 1704507362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I've been trying to download all of the newly-released court documents which contain information about a certain now-deceased evil billionaire and his island, from courtlistener.com.  \n\n\nHowever, when trying to 'wget -r \"https://www.courtlistener.com/docket/4355835/giuffre-v-maxwell\"', it gives me a 403 error; and it does so even when I try to feed it a cookies file.  \n\n\nShould I keep trying to use wget (if so, what other flags should I try). Or, since CourtListener is Open-Source, is there another way to go about downloading things-- I did find a questionably sorted Open Directory.  \n\n\nThanks.  \n\n\n&amp;#x200B;", "author_fullname": "t2_kh76tepm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk-Downloading from CourtListener", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zjqsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704494171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been trying to download all of the newly-released court documents which contain information about a certain now-deceased evil billionaire and his island, from courtlistener.com.  &lt;/p&gt;\n\n&lt;p&gt;However, when trying to &amp;#39;wget -r &amp;quot;&lt;a href=\"https://www.courtlistener.com/docket/4355835/giuffre-v-maxwell%22\"&gt;https://www.courtlistener.com/docket/4355835/giuffre-v-maxwell&amp;quot;&lt;/a&gt;&amp;#39;, it gives me a 403 error; and it does so even when I try to feed it a cookies file.  &lt;/p&gt;\n\n&lt;p&gt;Should I keep trying to use wget (if so, what other flags should I try). Or, since CourtListener is Open-Source, is there another way to go about downloading things-- I did find a questionably sorted Open Directory.  &lt;/p&gt;\n\n&lt;p&gt;Thanks.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zjqsq", "is_robot_indexable": true, "report_reasons": null, "author": "Bringback-T_D", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zjqsq/bulkdownloading_from_courtlistener/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zjqsq/bulkdownloading_from_courtlistener/", "subreddit_subscribers": 723297, "created_utc": 1704494171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "just a little friendly reminder, Guys, please sort your files! I recently decided to buy a new set of drives, and sort everything, because my previous system consisted of a bunch of drives just plugged into the computer, no raid, no backup and a total mess, I also have about 6 drives full of data that did not fit into my computer anymore, and one of the things I did was that every time I made space somewhere, or sorted my desktop, documents, etc. I always just moved them somewhere in the style of \"Old desktop\", \"dcim1,2,3\". Now I have tons of hard drives full of old desktops and stuff like that, I've been digging in it for about 3 hours now and I'm only about 1/30 of the way through. So guys, sort your files, make backups and if you want to use old drives (I have one from about 2005, on which I still have important data), use them only as a backup, ideally as a second backup.", "author_fullname": "t2_2pt8iy3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keep your data save and Organized", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zdy07", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704479682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just a little friendly reminder, Guys, please sort your files! I recently decided to buy a new set of drives, and sort everything, because my previous system consisted of a bunch of drives just plugged into the computer, no raid, no backup and a total mess, I also have about 6 drives full of data that did not fit into my computer anymore, and one of the things I did was that every time I made space somewhere, or sorted my desktop, documents, etc. I always just moved them somewhere in the style of &amp;quot;Old desktop&amp;quot;, &amp;quot;dcim1,2,3&amp;quot;. Now I have tons of hard drives full of old desktops and stuff like that, I&amp;#39;ve been digging in it for about 3 hours now and I&amp;#39;m only about 1/30 of the way through. So guys, sort your files, make backups and if you want to use old drives (I have one from about 2005, on which I still have important data), use them only as a backup, ideally as a second backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18zdy07", "is_robot_indexable": true, "report_reasons": null, "author": "Bago07", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zdy07/keep_your_data_save_and_organized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zdy07/keep_your_data_save_and_organized/", "subreddit_subscribers": 723297, "created_utc": 1704479682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All -\n\n I had an older WD Red drive start to report bad blocks.  Moved all the data off the drive without issues. Recommendation was to run a format /p:1 /x.\n\n I understand this is going to be slow. But I'm about 3 hours in and still showing \"0  percent.\"  \n\n Is the drive just dead?  Or should I really expect this to take \\~300 hours?\n\nThank you!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/c78qdn5g0oac1.png?width=711&amp;format=png&amp;auto=webp&amp;s=e400f1e914fb1a4a684fc57bea7166fcacaae6ab", "author_fullname": "t2_6oldcju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long to format a 2TB drive with /p:1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "media_metadata": {"c78qdn5g0oac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/c78qdn5g0oac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6907ed2a56182e836514cca667e5885f6bd6953"}, {"y": 116, "x": 216, "u": "https://preview.redd.it/c78qdn5g0oac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a426918bcabdb1d3a1bf51c035c31b70652fa97"}, {"y": 172, "x": 320, "u": "https://preview.redd.it/c78qdn5g0oac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f880e4519efac608865278c5aae5e1145cb1b664"}, {"y": 345, "x": 640, "u": "https://preview.redd.it/c78qdn5g0oac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8696fab74753cf3dbb59ee8fc5405c32c2b8fc1f"}], "s": {"y": 384, "x": 711, "u": "https://preview.redd.it/c78qdn5g0oac1.png?width=711&amp;format=png&amp;auto=webp&amp;s=e400f1e914fb1a4a684fc57bea7166fcacaae6ab"}, "id": "c78qdn5g0oac1"}}, "name": "t3_18zdun4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/RtHWob5i-nDa4oTpEy8QPcR4VN1lxWmqp6-aLgD9ZK4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704479447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All -&lt;/p&gt;\n\n&lt;p&gt;I had an older WD Red drive start to report bad blocks.  Moved all the data off the drive without issues. Recommendation was to run a format /p:1 /x.&lt;/p&gt;\n\n&lt;p&gt;I understand this is going to be slow. But I&amp;#39;m about 3 hours in and still showing &amp;quot;0  percent.&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Is the drive just dead?  Or should I really expect this to take ~300 hours?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c78qdn5g0oac1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e400f1e914fb1a4a684fc57bea7166fcacaae6ab\"&gt;https://preview.redd.it/c78qdn5g0oac1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e400f1e914fb1a4a684fc57bea7166fcacaae6ab&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zdun4", "is_robot_indexable": true, "report_reasons": null, "author": "nyc2pit", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zdun4/how_long_to_format_a_2tb_drive_with_p1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zdun4/how_long_to_format_a_2tb_drive_with_p1/", "subreddit_subscribers": 723297, "created_utc": 1704479447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI received two drives (Seagate Exos x18 18TB x 2) that were supposed to be brand new.  \n\nI went to Seagate's site to check on their warranty status using drives' serial numbers, WR5015CY &amp; WR5015FG, and got this message:\n\n\\*\\*\u201cThis product was originally sold as part of a larger system. Please contact the system manufacturer or your place of purchase for warranty support.\u201d\\*\\*\n\nI then checked the drives using HWInfo64. Here\u2019s one drives\u2019 read/write info (2nd drive is \\~ the same): \\*\\*Total Host Writes: 717,321 GB, Total Host Reads: 1,077,026 GB \\*\\*\n\nThe company has issued an RMA, but also says that the tech department will examine it, and if no fault is found, they may charge a testing fee (unspecified amount)\n\nI think the info I\u2019ve got on the drives proves that they can\u2019t be anything other than used, but I don\u2019t know that much about hard drives. So:\n\n1. Does what I have 100% prove that they are used drives?\n\n2. Is there some technical wiggle room that might allow them to declare them \u201cnew\u201d.\n\nThanks for any advice you can give", "author_fullname": "t2_ufrz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New or used 18TB hard drives? Info from Seagate's warranty site &amp; HWInfo64", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z5uhm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704457981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I received two drives (Seagate Exos x18 18TB x 2) that were supposed to be brand new.  &lt;/p&gt;\n\n&lt;p&gt;I went to Seagate&amp;#39;s site to check on their warranty status using drives&amp;#39; serial numbers, WR5015CY &amp;amp; WR5015FG, and got this message:&lt;/p&gt;\n\n&lt;p&gt;**\u201cThis product was originally sold as part of a larger system. Please contact the system manufacturer or your place of purchase for warranty support.\u201d**&lt;/p&gt;\n\n&lt;p&gt;I then checked the drives using HWInfo64. Here\u2019s one drives\u2019 read/write info (2nd drive is ~ the same): **Total Host Writes: 717,321 GB, Total Host Reads: 1,077,026 GB **&lt;/p&gt;\n\n&lt;p&gt;The company has issued an RMA, but also says that the tech department will examine it, and if no fault is found, they may charge a testing fee (unspecified amount)&lt;/p&gt;\n\n&lt;p&gt;I think the info I\u2019ve got on the drives proves that they can\u2019t be anything other than used, but I don\u2019t know that much about hard drives. So:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Does what I have 100% prove that they are used drives?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is there some technical wiggle room that might allow them to declare them \u201cnew\u201d.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for any advice you can give&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z5uhm", "is_robot_indexable": true, "report_reasons": null, "author": "IncisiveGuess", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z5uhm/new_or_used_18tb_hard_drives_info_from_seagates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z5uhm/new_or_used_18tb_hard_drives_info_from_seagates/", "subreddit_subscribers": 723297, "created_utc": 1704457981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to find a service or facility where I can get hundreds - possibly thousands - of old, individual photos digitized. It seems most people go about scanning them themselves but, frankly, this just seems more tedious and time-consuming than I'd prefer. I've tried scanning some photos on my parents' Canon scanner/printer combo, which was never purchased to scan photos, mind you, but it was always just a giant pain and the digital copies never turned out that well, anyway.\n\nAre there any facilities anyone knows of, such as Walgreens or Office Depot, where you can bring in your physical photos to have them digitized? I know there are some services where you can mail out your photos, such as ScanCafe, but I am a little nervous about something happening to the photos and never being able to get them back.\n\nAny advice would be greatly appreciated!", "author_fullname": "t2_e24pe4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I get tons of photos digitized without buying a scanner myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zsdgu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704518468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to find a service or facility where I can get hundreds - possibly thousands - of old, individual photos digitized. It seems most people go about scanning them themselves but, frankly, this just seems more tedious and time-consuming than I&amp;#39;d prefer. I&amp;#39;ve tried scanning some photos on my parents&amp;#39; Canon scanner/printer combo, which was never purchased to scan photos, mind you, but it was always just a giant pain and the digital copies never turned out that well, anyway.&lt;/p&gt;\n\n&lt;p&gt;Are there any facilities anyone knows of, such as Walgreens or Office Depot, where you can bring in your physical photos to have them digitized? I know there are some services where you can mail out your photos, such as ScanCafe, but I am a little nervous about something happening to the photos and never being able to get them back.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zsdgu", "is_robot_indexable": true, "report_reasons": null, "author": "Hannah_Lynn98", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zsdgu/where_can_i_get_tons_of_photos_digitized_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zsdgu/where_can_i_get_tons_of_photos_digitized_without/", "subreddit_subscribers": 723297, "created_utc": 1704518468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used a dual bay drive dock to clone an 8tb hdd to a 14tb hdd. I ended up with a target drive that only registered as 8tb in macOS Monterey, in the disk utility.\n\nI reformatted it as several different file systems, and in each case it said it's only an 8tb Hdd. Finally, I formatted it as APFS, and in the partition menu, it showed two partitions -- 8tb and 6tb -- though I didn't intentionally partition it. Apparently, that happens when you format as APFS.\n\ndiskutil list is showing a 14tb GUID partition scheme, but only an 8tb APFS container scheme.\n\nI'd like to clone my 8tb to the 14tb in a dual-bay dock so I don't have to do it as copy/paste, rsync, cp, etc. I have 4 dual-bay docks . . Weblink, Inatek, SSD, and Cavalry Retriever. I can't find specs on these things to indicate if they handle 16 TB drives. One says 16TB -- but that means total -- 8TB in each bay.\n\nI also don't know if the target disk will show the additional 6TB besides the 8TB copied.\n\nWill I have to manually copy all the files? Or is there a way to do this as a hardware clone?\n\nThanks.", "author_fullname": "t2_4tyix39s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloning 8tb to 14tb Hdd in dual-bay drive dock", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zoglu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704506495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used a dual bay drive dock to clone an 8tb hdd to a 14tb hdd. I ended up with a target drive that only registered as 8tb in macOS Monterey, in the disk utility.&lt;/p&gt;\n\n&lt;p&gt;I reformatted it as several different file systems, and in each case it said it&amp;#39;s only an 8tb Hdd. Finally, I formatted it as APFS, and in the partition menu, it showed two partitions -- 8tb and 6tb -- though I didn&amp;#39;t intentionally partition it. Apparently, that happens when you format as APFS.&lt;/p&gt;\n\n&lt;p&gt;diskutil list is showing a 14tb GUID partition scheme, but only an 8tb APFS container scheme.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to clone my 8tb to the 14tb in a dual-bay dock so I don&amp;#39;t have to do it as copy/paste, rsync, cp, etc. I have 4 dual-bay docks . . Weblink, Inatek, SSD, and Cavalry Retriever. I can&amp;#39;t find specs on these things to indicate if they handle 16 TB drives. One says 16TB -- but that means total -- 8TB in each bay.&lt;/p&gt;\n\n&lt;p&gt;I also don&amp;#39;t know if the target disk will show the additional 6TB besides the 8TB copied.&lt;/p&gt;\n\n&lt;p&gt;Will I have to manually copy all the files? Or is there a way to do this as a hardware clone?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zoglu", "is_robot_indexable": true, "report_reasons": null, "author": "ericlindellnyc", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zoglu/cloning_8tb_to_14tb_hdd_in_dualbay_drive_dock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zoglu/cloning_8tb_to_14tb_hdd_in_dualbay_drive_dock/", "subreddit_subscribers": 723297, "created_utc": 1704506495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currenty looking at different ways i coud create a backup of my data as day by data its getting bigger and all it will take is one of my drives dying and its gone. I was backing up to Mega.nz but my upload speed is rubbish to upload TB's of data. So i've been looking at Tapes as i've read that they have high capasity and dont have the same risks as HDD/SSD and if kept stored correctly have around 10+ year life span. But looking at LTO tapes theres LTO 9,8,7 etc other than probably the basic speed and storage size is there any difference between the verison.\n\n&amp;#x200B;\n\nAlso if you have any other ideas for back ideas I'd love to hear them. Just in planning stage atm", "author_fullname": "t2_ymrul07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different LTO tape versions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zcc3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704475709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currenty looking at different ways i coud create a backup of my data as day by data its getting bigger and all it will take is one of my drives dying and its gone. I was backing up to Mega.nz but my upload speed is rubbish to upload TB&amp;#39;s of data. So i&amp;#39;ve been looking at Tapes as i&amp;#39;ve read that they have high capasity and dont have the same risks as HDD/SSD and if kept stored correctly have around 10+ year life span. But looking at LTO tapes theres LTO 9,8,7 etc other than probably the basic speed and storage size is there any difference between the verison.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also if you have any other ideas for back ideas I&amp;#39;d love to hear them. Just in planning stage atm&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB \ud83c\udfe0 8TB \u2601\ufe0f", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zcc3v", "is_robot_indexable": true, "report_reasons": null, "author": "seleneVamp", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18zcc3v/different_lto_tape_versions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zcc3v/different_lto_tape_versions/", "subreddit_subscribers": 723297, "created_utc": 1704475709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen some of these questions being answered before but none of them had an answer for what I had in mind. \n\nI have a library of photos that is around 2tb large. In that library are a lot of duplicate photos because we save a variant of each photo in some different sizes (if that makes sense). If I ran a photo duplicate finder program it would show me every single one of these duplicate photos. But I only want to show the photos that are duplicates of one specific photo I have in another folder by itself. \n\nIs there any way to compare my library folder to my other folder with just the photo I want to find duplicates of without showing me every other duplicate that there is in the library folder? ", "author_fullname": "t2_ec3jqgob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Find every duplicate image that matches one specific image", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z763j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704462160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen some of these questions being answered before but none of them had an answer for what I had in mind. &lt;/p&gt;\n\n&lt;p&gt;I have a library of photos that is around 2tb large. In that library are a lot of duplicate photos because we save a variant of each photo in some different sizes (if that makes sense). If I ran a photo duplicate finder program it would show me every single one of these duplicate photos. But I only want to show the photos that are duplicates of one specific photo I have in another folder by itself. &lt;/p&gt;\n\n&lt;p&gt;Is there any way to compare my library folder to my other folder with just the photo I want to find duplicates of without showing me every other duplicate that there is in the library folder? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z763j", "is_robot_indexable": true, "report_reasons": null, "author": "TallDudeV2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z763j/find_every_duplicate_image_that_matches_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z763j/find_every_duplicate_image_that_matches_one/", "subreddit_subscribers": 723297, "created_utc": 1704462160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a friend who permanently rents out of a house that I own who desperately needs a MASSIVE storage solution for terabytes of video editing that they do. We are building this for fun just as much as the need for storage itself, so there isn't really a price ceiling on what we're willing to put into it but under $1,000 would be a good place to start. The storage solution must be on site, so off-site cloud storage services are a 100% no-go. We considered turning an old desktop into a Linux server and frankensteining a bunch of spin discs into it, but before I bite on the purchase we're looking for something with a little more fun or pizzazz. We talked about tape drives which seemed fun to figure out but I don't know where I would get one or even if it would be viable. If a Lennox server ends up being the best option, I'm looking for programs and features to put on to it since I'm kind of new to that field. I have no problem figuring out something new.\n\nFeatures and Requirements:\n\n- Must be on site. I mentioned it already but it is the main concern.\n- 20 terabytes at a bare minimum, but ideally it would have 100 terabytes or more.\n- Data should be shelf stable, that is, I should be able to leave the data storage unpowered for 10-20 years without any appreciable degradation\n- it would be convenient if it was network attached but the ratio of gigabytes per dollar is far more important\n- Space isn't too much of a concern but I would like to not have to give up an entire room for the project\n- It would be fun if I could play around with making it EMP safe, but that would really just be a toy that I don't need.\n- Noise isn't that much of an issue but it would be good if I could stop the sound with a wall and some soundproofing\n\nThank you in advance for all of you who took the time to read and reply, may your computers run smoothly and may your frame rates be high.\n\nEdit: noise limits", "author_fullname": "t2_70b7ow4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large Data Backup/Storage Recommendations Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zemxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704485997.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704481441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a friend who permanently rents out of a house that I own who desperately needs a MASSIVE storage solution for terabytes of video editing that they do. We are building this for fun just as much as the need for storage itself, so there isn&amp;#39;t really a price ceiling on what we&amp;#39;re willing to put into it but under $1,000 would be a good place to start. The storage solution must be on site, so off-site cloud storage services are a 100% no-go. We considered turning an old desktop into a Linux server and frankensteining a bunch of spin discs into it, but before I bite on the purchase we&amp;#39;re looking for something with a little more fun or pizzazz. We talked about tape drives which seemed fun to figure out but I don&amp;#39;t know where I would get one or even if it would be viable. If a Lennox server ends up being the best option, I&amp;#39;m looking for programs and features to put on to it since I&amp;#39;m kind of new to that field. I have no problem figuring out something new.&lt;/p&gt;\n\n&lt;p&gt;Features and Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be on site. I mentioned it already but it is the main concern.&lt;/li&gt;\n&lt;li&gt;20 terabytes at a bare minimum, but ideally it would have 100 terabytes or more.&lt;/li&gt;\n&lt;li&gt;Data should be shelf stable, that is, I should be able to leave the data storage unpowered for 10-20 years without any appreciable degradation&lt;/li&gt;\n&lt;li&gt;it would be convenient if it was network attached but the ratio of gigabytes per dollar is far more important&lt;/li&gt;\n&lt;li&gt;Space isn&amp;#39;t too much of a concern but I would like to not have to give up an entire room for the project&lt;/li&gt;\n&lt;li&gt;It would be fun if I could play around with making it EMP safe, but that would really just be a toy that I don&amp;#39;t need.&lt;/li&gt;\n&lt;li&gt;Noise isn&amp;#39;t that much of an issue but it would be good if I could stop the sound with a wall and some soundproofing&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you in advance for all of you who took the time to read and reply, may your computers run smoothly and may your frame rates be high.&lt;/p&gt;\n\n&lt;p&gt;Edit: noise limits&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zemxl", "is_robot_indexable": true, "report_reasons": null, "author": "Square_Style_2335", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zemxl/large_data_backupstorage_recommendations_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zemxl/large_data_backupstorage_recommendations_needed/", "subreddit_subscribers": 723297, "created_utc": 1704481441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've currently got a media library (the arrs) and all my drives in an UNRAID tower.  I'm going to be changing some things around and this time round, I'm just going to run plain Linux, probably Ubuntu server as that's what I'm familiar with.\n\nWhat RAID configuration would you recommend where I can start with 4 drives and add more as time goes on?  With regard to redundancy, I'm open minded.  This is replaceable data but it'd always be nice to not have to do that.  I'm a bit rusty on my RAID knowledge so I thought I might come here, asking advice on how best to set this machine up.  If I begin with two drives, I guess my only options are keeping full capacity with no drive fault tolerance, or halving my capacity with one drive fault tolerance.  There's got to be a better way to do this.\n\nThoughts?", "author_fullname": "t2_6eiisul2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question regarding Linux RAID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zcfld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704475947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve currently got a media library (the arrs) and all my drives in an UNRAID tower.  I&amp;#39;m going to be changing some things around and this time round, I&amp;#39;m just going to run plain Linux, probably Ubuntu server as that&amp;#39;s what I&amp;#39;m familiar with.&lt;/p&gt;\n\n&lt;p&gt;What RAID configuration would you recommend where I can start with 4 drives and add more as time goes on?  With regard to redundancy, I&amp;#39;m open minded.  This is replaceable data but it&amp;#39;d always be nice to not have to do that.  I&amp;#39;m a bit rusty on my RAID knowledge so I thought I might come here, asking advice on how best to set this machine up.  If I begin with two drives, I guess my only options are keeping full capacity with no drive fault tolerance, or halving my capacity with one drive fault tolerance.  There&amp;#39;s got to be a better way to do this.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zcfld", "is_robot_indexable": true, "report_reasons": null, "author": "Pleaseclap4", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zcfld/question_regarding_linux_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zcfld/question_regarding_linux_raid/", "subreddit_subscribers": 723297, "created_utc": 1704475947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got quite a few things saved to external drives but I'm looking to throw some M-discs into the mix for long-term cold storage. I'd like to catalog those the way I used to do with my DVDs and WhereIsIt so, for example, I could find the disc with a specific photo fairly easily through doing an EXIF search.\n\nI'd like to have a catalog that's not going to be locked into a proprietary format so I can export and possibly import it into another database program if the need arises.\n\nHow can I future-proof my catalogs? ", "author_fullname": "t2_f9q4jzo1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on catalog software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18za6jg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704470351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got quite a few things saved to external drives but I&amp;#39;m looking to throw some M-discs into the mix for long-term cold storage. I&amp;#39;d like to catalog those the way I used to do with my DVDs and WhereIsIt so, for example, I could find the disc with a specific photo fairly easily through doing an EXIF search.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have a catalog that&amp;#39;s not going to be locked into a proprietary format so I can export and possibly import it into another database program if the need arises.&lt;/p&gt;\n\n&lt;p&gt;How can I future-proof my catalogs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18za6jg", "is_robot_indexable": true, "report_reasons": null, "author": "chlorculo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18za6jg/question_on_catalog_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18za6jg/question_on_catalog_software/", "subreddit_subscribers": 723297, "created_utc": 1704470351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI've two pool (Stablebit) : 1 \"prod\" &amp; 1 for recovery in case of.Every week I power up the backup one and do a sync (with FreeFileSync).\n\n\\-&gt; oldest files I didn't touch for year haven't been refresh for years on the backup side-&gt; I'd like to, over the time, smoothly, get the oldest disks emptied to get them back on the physical test bench (and so avoid very long one shot emptying).\n\n**So I'd like to have a tool or a script to \"touch\" (= alter the modification date) of the x oldest files to force them to be included in the next sync run (the solution shall be able to handle +/- 5 millions files).**\n\nAny idea ?", "author_fullname": "t2_vukgz0f2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to force oldest files to be included in the next sync ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z9x1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704469684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve two pool (Stablebit) : 1 &amp;quot;prod&amp;quot; &amp;amp; 1 for recovery in case of.Every week I power up the backup one and do a sync (with FreeFileSync).&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; oldest files I didn&amp;#39;t touch for year haven&amp;#39;t been refresh for years on the backup side-&amp;gt; I&amp;#39;d like to, over the time, smoothly, get the oldest disks emptied to get them back on the physical test bench (and so avoid very long one shot emptying).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So I&amp;#39;d like to have a tool or a script to &amp;quot;touch&amp;quot; (= alter the modification date) of the x oldest files to force them to be included in the next sync run (the solution shall be able to handle +/- 5 millions files).&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any idea ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z9x1s", "is_robot_indexable": true, "report_reasons": null, "author": "JeanCompot", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z9x1s/how_to_force_oldest_files_to_be_included_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z9x1s/how_to_force_oldest_files_to_be_included_in_the/", "subreddit_subscribers": 723297, "created_utc": 1704469684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, what would be an easy and legal way to mass download all workouts and programs from [https://darebee.com/](https://darebee.com/) ? I would like them to be in their original resolution, not thumbnails please", "author_fullname": "t2_y9woc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all programs and workouts in the highest definition from DAREBEE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z8k35", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704466041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, what would be an easy and legal way to mass download all workouts and programs from &lt;a href=\"https://darebee.com/\"&gt;https://darebee.com/&lt;/a&gt; ? I would like them to be in their original resolution, not thumbnails please&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z8k35", "is_robot_indexable": true, "report_reasons": null, "author": "MavropaliasG", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z8k35/how_to_download_all_programs_and_workouts_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z8k35/how_to_download_all_programs_and_workouts_in_the/", "subreddit_subscribers": 723297, "created_utc": 1704466041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My setup involves: \n\n\\*\\*\\*Primary device\\*\\*\\*\n\nSynology Nas (1 drive)  \n\\*\\*\\*Back up device off-site\\*\\*\\*\n\nTruenas (3 disks in RaidZ1)\n\n\\*\\*\\*Backup Routine\\*\\*\\*\n\nEvery day I have a script that runs that rsyncs all my data from Synology to the Truenas.\n\n\\*\\*\\*My concern is\\*\\*\\*:\n\n should there be a RAM/HDD failure in the synology, all corrupted data will be copied over to the Truenas and I won't notice in time. \n\nGiven I have a very large distance between my off-site and my primary, I can't switch the devices. If I could, I'd trust the truenas over the synology given that it has error checking due to zfs.\n\nCan anyone suggest a mechanism to identify data rot? A potential option would be to have a 'canary file' - i.e. a file that sits on both Truenas and Synology that I'll never touch. If I see a change in the Canary File, then flag this up as data rot. However, this idea seems flawed in that data rot/corruption could occur anywhere in the drive, not just in my canary file. \n\nAppreciate this question may sound a little paranoid but I'd be very upset if hardware failures in my synology corrupted my data in my Truenas backups.\n\nPlease note,  I've turned on snapshots on my truenas which is very helpful, however, I don't view this as enough to guard against  datarot as it could take me some time to notice things have become corrupt on the synology. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  \n", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on hoarding data integrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z7ws0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704464239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My setup involves: &lt;/p&gt;\n\n&lt;p&gt;***Primary device***&lt;/p&gt;\n\n&lt;p&gt;Synology Nas (1 drive)&lt;br/&gt;\n***Back up device off-site***&lt;/p&gt;\n\n&lt;p&gt;Truenas (3 disks in RaidZ1)&lt;/p&gt;\n\n&lt;p&gt;***Backup Routine***&lt;/p&gt;\n\n&lt;p&gt;Every day I have a script that runs that rsyncs all my data from Synology to the Truenas.&lt;/p&gt;\n\n&lt;p&gt;***My concern is***:&lt;/p&gt;\n\n&lt;p&gt;should there be a RAM/HDD failure in the synology, all corrupted data will be copied over to the Truenas and I won&amp;#39;t notice in time. &lt;/p&gt;\n\n&lt;p&gt;Given I have a very large distance between my off-site and my primary, I can&amp;#39;t switch the devices. If I could, I&amp;#39;d trust the truenas over the synology given that it has error checking due to zfs.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest a mechanism to identify data rot? A potential option would be to have a &amp;#39;canary file&amp;#39; - i.e. a file that sits on both Truenas and Synology that I&amp;#39;ll never touch. If I see a change in the Canary File, then flag this up as data rot. However, this idea seems flawed in that data rot/corruption could occur anywhere in the drive, not just in my canary file. &lt;/p&gt;\n\n&lt;p&gt;Appreciate this question may sound a little paranoid but I&amp;#39;d be very upset if hardware failures in my synology corrupted my data in my Truenas backups.&lt;/p&gt;\n\n&lt;p&gt;Please note,  I&amp;#39;ve turned on snapshots on my truenas which is very helpful, however, I don&amp;#39;t view this as enough to guard against  datarot as it could take me some time to notice things have become corrupt on the synology. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z7ws0", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z7ws0/question_on_hoarding_data_integrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z7ws0/question_on_hoarding_data_integrity/", "subreddit_subscribers": 723297, "created_utc": 1704464239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm building a separate tower PC to house a bunch of drives, and I'm unsure how to organize my drives.\n\nDo I keep them as individual drives, not in Raid with their own drive letters mapped.  A storage pool with no mirroring or two way mirroring but lose half the capacity.\n\nI plan on backing up to two large external drives, one will be kept offsite while the other connected and running a PS script that will use robocopy to back up new data.  \n\nI'm in the mindset that my backup method should be ok, to forfeit mirroring and just go for max storage but also unsure if I go this way do I go storage pool or individual drives.", "author_fullname": "t2_e6t86cb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data backup and storage advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18z4br6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704452507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a separate tower PC to house a bunch of drives, and I&amp;#39;m unsure how to organize my drives.&lt;/p&gt;\n\n&lt;p&gt;Do I keep them as individual drives, not in Raid with their own drive letters mapped.  A storage pool with no mirroring or two way mirroring but lose half the capacity.&lt;/p&gt;\n\n&lt;p&gt;I plan on backing up to two large external drives, one will be kept offsite while the other connected and running a PS script that will use robocopy to back up new data.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the mindset that my backup method should be ok, to forfeit mirroring and just go for max storage but also unsure if I go this way do I go storage pool or individual drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18z4br6", "is_robot_indexable": true, "report_reasons": null, "author": "Endeavour1988", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18z4br6/data_backup_and_storage_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18z4br6/data_backup_and_storage_advice/", "subreddit_subscribers": 723297, "created_utc": 1704452507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a Netapp DS4243 with 24x 1TB for Christmas. I currently have it connected to my Dell Poweredge R730 through a PERC H830 in HBA mode. My server is running Proxmox and I'm primarily using it for a media server and a Pterodactyl server. ", "author_fullname": "t2_11fll4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm still kinda new to storage management. Could I get some help with deciding the best setup for my storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18zwn3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704534714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a Netapp DS4243 with 24x 1TB for Christmas. I currently have it connected to my Dell Poweredge R730 through a PERC H830 in HBA mode. My server is running Proxmox and I&amp;#39;m primarily using it for a media server and a Pterodactyl server. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zwn3u", "is_robot_indexable": true, "report_reasons": null, "author": "Megachuggayoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zwn3u/im_still_kinda_new_to_storage_management_could_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zwn3u/im_still_kinda_new_to_storage_management_could_i/", "subreddit_subscribers": 723297, "created_utc": 1704534714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are quite a few posts on here about scripts or sites that can batch download media or tweets, but is anyone aware of something that could *automatically* download media when a user posts?\n\nI have quite a few users' notifications on but it becomes tedious and distracting to save the media of each notification. \n\nThanks!", "author_fullname": "t2_t7ki2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program/Script to automatically download media when a Twitter/X user posts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18zwlax", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704534513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are quite a few posts on here about scripts or sites that can batch download media or tweets, but is anyone aware of something that could &lt;em&gt;automatically&lt;/em&gt; download media when a user posts?&lt;/p&gt;\n\n&lt;p&gt;I have quite a few users&amp;#39; notifications on but it becomes tedious and distracting to save the media of each notification. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zwlax", "is_robot_indexable": true, "report_reasons": null, "author": "socoolhage", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zwlax/programscript_to_automatically_download_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zwlax/programscript_to_automatically_download_media/", "subreddit_subscribers": 723297, "created_utc": 1704534513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, DataHoarding peeps!!\n\nLongtime watcher. I'm reaching out to this incredible community for some much-needed guidance and insights into a rather unique data storage challenge I'm facing. I've been in the adult video as a talent, producer, and editor for nearly a decade, but I've never found myself responsible for such an enormous amount of data until now.\n\nIn my current role, I oversee seven different studios, and part of my responsibilities involves backing up their raw ingest data, which is something like 100-110TB of data, and includes various RAW formats, a multitude of codes, raw photo shoots, videos, and 4k-8k VR footage from the past couple of years. Keep in mind, this is not a static volume, it is dynamic and will expand over time. As for how much, I would say at least terabytes over the years...\n\nI understand the significance of data integrity and the importance of robust backup solutions. I'd love to tap into the collective wisdom of this community to help me navigate this monumental data storage challenge.\n\nSpecifically, I've been contemplating two primary approaches: traditional RAID configurations (possibly RAID 5 or RAID 6), or opting for a software-based solution like StableBit DrivePool combined with SnapRAID. I'm running Windows, and I'm aware that the landscape of data storage has evolved considerably in recent years.\n\nHere are some thoughts and considerations that have been on my mind:\n\n* **RAID Configurations:** RAID 5 and RAID 6 offer a balance between storage capacity and redundancy. RAID 5 distributes data and parity information across multiple drives, allowing for the failure of one drive without data loss. However, with such a vast amount of data at stake, the risk of encountering a failed drive during a rebuild becomes a genuine concern. RAID 6, with its dual parity setup, allows for the simultaneous failure of two drives without data loss, but it does reduce usable storage capacity compared to RAID 5.\n* **Software-Based Solutions:** StableBit DrivePool provides flexibility by allowing you to pool drives of different sizes and types, creating a unified storage pool. It offers real-time duplication, ensuring data redundancy without the constraints of traditional RAID. SnapRAID, on the other hand, is a snapshot-based solution that complements DrivePool or other pooling solutions, providing recovery options in case of data corruption or drive failure.\n\nWhile I've outlined these considerations, I'm here to ask for your expertise and real-world experiences. Have any of you faced a similar data hoarding challenge or managed a colossal amount of data like this? If so, what approach did you take, and what were the results?\n\nUltimately, my goal is to ensure the safety and accessibility of this significant volume of valuable data while maintaining scalability and ease of maintenance. I'm eager to hear your thoughts, recommendations, and any practical insights you can share regarding RAID, software-based solutions, or any alternative approaches that might suit this unique situation...\n\nThank you all very much, I appreciate any info you can provide. Cheers.", "author_fullname": "t2_aueu7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adult Film Studios (raw data): Seeking Advice on Managing 120TB of Raw Video Data - RAID vs. StableBit DrivePool &amp; SnapRAID?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18zw106", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704532429.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704532140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, DataHoarding peeps!!&lt;/p&gt;\n\n&lt;p&gt;Longtime watcher. I&amp;#39;m reaching out to this incredible community for some much-needed guidance and insights into a rather unique data storage challenge I&amp;#39;m facing. I&amp;#39;ve been in the adult video as a talent, producer, and editor for nearly a decade, but I&amp;#39;ve never found myself responsible for such an enormous amount of data until now.&lt;/p&gt;\n\n&lt;p&gt;In my current role, I oversee seven different studios, and part of my responsibilities involves backing up their raw ingest data, which is something like 100-110TB of data, and includes various RAW formats, a multitude of codes, raw photo shoots, videos, and 4k-8k VR footage from the past couple of years. Keep in mind, this is not a static volume, it is dynamic and will expand over time. As for how much, I would say at least terabytes over the years...&lt;/p&gt;\n\n&lt;p&gt;I understand the significance of data integrity and the importance of robust backup solutions. I&amp;#39;d love to tap into the collective wisdom of this community to help me navigate this monumental data storage challenge.&lt;/p&gt;\n\n&lt;p&gt;Specifically, I&amp;#39;ve been contemplating two primary approaches: traditional RAID configurations (possibly RAID 5 or RAID 6), or opting for a software-based solution like StableBit DrivePool combined with SnapRAID. I&amp;#39;m running Windows, and I&amp;#39;m aware that the landscape of data storage has evolved considerably in recent years.&lt;/p&gt;\n\n&lt;p&gt;Here are some thoughts and considerations that have been on my mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;RAID Configurations:&lt;/strong&gt; RAID 5 and RAID 6 offer a balance between storage capacity and redundancy. RAID 5 distributes data and parity information across multiple drives, allowing for the failure of one drive without data loss. However, with such a vast amount of data at stake, the risk of encountering a failed drive during a rebuild becomes a genuine concern. RAID 6, with its dual parity setup, allows for the simultaneous failure of two drives without data loss, but it does reduce usable storage capacity compared to RAID 5.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Software-Based Solutions:&lt;/strong&gt; StableBit DrivePool provides flexibility by allowing you to pool drives of different sizes and types, creating a unified storage pool. It offers real-time duplication, ensuring data redundancy without the constraints of traditional RAID. SnapRAID, on the other hand, is a snapshot-based solution that complements DrivePool or other pooling solutions, providing recovery options in case of data corruption or drive failure.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While I&amp;#39;ve outlined these considerations, I&amp;#39;m here to ask for your expertise and real-world experiences. Have any of you faced a similar data hoarding challenge or managed a colossal amount of data like this? If so, what approach did you take, and what were the results?&lt;/p&gt;\n\n&lt;p&gt;Ultimately, my goal is to ensure the safety and accessibility of this significant volume of valuable data while maintaining scalability and ease of maintenance. I&amp;#39;m eager to hear your thoughts, recommendations, and any practical insights you can share regarding RAID, software-based solutions, or any alternative approaches that might suit this unique situation...&lt;/p&gt;\n\n&lt;p&gt;Thank you all very much, I appreciate any info you can provide. Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zw106", "is_robot_indexable": true, "report_reasons": null, "author": "PlatinumAero", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zw106/adult_film_studios_raw_data_seeking_advice_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zw106/adult_film_studios_raw_data_seeking_advice_on/", "subreddit_subscribers": 723297, "created_utc": 1704532140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, not great at this but I'm currently for my scraping I'm using JDownloader 2 but a problem I'm dealing with right now is rate limits for certain websites. \n\nI was wondering if I could use mutliple VM of a light weight OS with proxy software like PiHole as a means get around these rate rules?\n\nThe limit for one website is 4 simultaneous downloads. Anything more then everything grinds to a halt.", "author_fullname": "t2_8xjp0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VM Proxies help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zk6gt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704495256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, not great at this but I&amp;#39;m currently for my scraping I&amp;#39;m using JDownloader 2 but a problem I&amp;#39;m dealing with right now is rate limits for certain websites. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if I could use mutliple VM of a light weight OS with proxy software like PiHole as a means get around these rate rules?&lt;/p&gt;\n\n&lt;p&gt;The limit for one website is 4 simultaneous downloads. Anything more then everything grinds to a halt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zk6gt", "is_robot_indexable": true, "report_reasons": null, "author": "RedNapalm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zk6gt/vm_proxies_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zk6gt/vm_proxies_help/", "subreddit_subscribers": 723297, "created_utc": 1704495256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am currently trying to find a storage solution for around 10TB of research data. The primary data storage location will be a cloud storage service from my university and we would like to have it stored at a secondary physical external storage. \n\nI am leaning more towards SSD over HDD due to shock and drop resistance and general speed improvement. Upon further research, it seems like the highest consumer-grade SSD is 8TB (which we might exceed). Is there any way to properly integrate two SSDs together? Or we can only have a disk1 disk2 situation? Can a SATA docking station like the Sabrent 5-bay one help solve the issue?\n\nFurthermore, is there any performance and reliability difference between an external SSD and an internal SSD along with an SSD enclosure? And what enclosure should I look for since by looking at some of the posts here some SSDs might not work well with certain closures. \n\nPrice is not the utmost concern for us as long we can find a reliable, long-term, and fast-enough storage option. ", "author_fullname": "t2_6qmp8foz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for ~10TB storage solution for research data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zc8se", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704475475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am currently trying to find a storage solution for around 10TB of research data. The primary data storage location will be a cloud storage service from my university and we would like to have it stored at a secondary physical external storage. &lt;/p&gt;\n\n&lt;p&gt;I am leaning more towards SSD over HDD due to shock and drop resistance and general speed improvement. Upon further research, it seems like the highest consumer-grade SSD is 8TB (which we might exceed). Is there any way to properly integrate two SSDs together? Or we can only have a disk1 disk2 situation? Can a SATA docking station like the Sabrent 5-bay one help solve the issue?&lt;/p&gt;\n\n&lt;p&gt;Furthermore, is there any performance and reliability difference between an external SSD and an internal SSD along with an SSD enclosure? And what enclosure should I look for since by looking at some of the posts here some SSDs might not work well with certain closures. &lt;/p&gt;\n\n&lt;p&gt;Price is not the utmost concern for us as long we can find a reliable, long-term, and fast-enough storage option. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zc8se", "is_robot_indexable": true, "report_reasons": null, "author": "to-the-horizon", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zc8se/suggestions_for_10tb_storage_solution_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zc8se/suggestions_for_10tb_storage_solution_for/", "subreddit_subscribers": 723297, "created_utc": 1704475475.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}