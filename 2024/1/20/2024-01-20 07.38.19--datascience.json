{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I read that it\u2019s good to quantify your impact/savings in your resume, so I tried that. Is it too much? And yes these are all real savings(and not that much for an insurance company). \n\nThanks!", "author_fullname": "t2_495cn7pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Give me your worst", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_19anbhy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 124, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 124, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-7yf_yxuqw0Z5MB2N0sLN29HSGdvpCzLeGynzntooTU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705682389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read that it\u2019s good to quantify your impact/savings in your resume, so I tried that. Is it too much? And yes these are all real savings(and not that much for an insurance company). &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zrwtcwxgdfdc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?auto=webp&amp;s=9d0cb110383c43468c03a611678188d8f9910b0e", "width": 1170, "height": 1611}, "resolutions": [{"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e56b33379ba72c8e4d5e89661197af74af7104ff", "width": 108, "height": 148}, {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e15f6b4ef9c463a90efd8bc405d31c7765cdbc63", "width": 216, "height": 297}, {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff8c929285c34989f56ebe2a63d3ce2d68d49ff5", "width": 320, "height": 440}, {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=162d2d001a00b2137dc313366c55c94ec70f2c75", "width": 640, "height": 881}, {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=877c67b4706cbac3191554f39513f6fd4e4f2e90", "width": 960, "height": 1321}, {"url": "https://preview.redd.it/zrwtcwxgdfdc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=511ee39430902d08947e27e63a6bbc777ce733b4", "width": 1080, "height": 1487}], "variants": {}, "id": "aG3H7JAOyuAOVYKXvnAsycSGluY3wnZ7_MSZGIlml8k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19anbhy", "is_robot_indexable": true, "report_reasons": null, "author": "Throwawayforgainz99", "discussion_type": null, "num_comments": 109, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19anbhy/give_me_your_worst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zrwtcwxgdfdc1.jpeg", "subreddit_subscribers": 1260030, "created_utc": 1705682389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've run into this issue at work where a lot of my time is wasted on requesting access to some data, whether it be financial, HR or whatever, and having to file a formal request through my manager to get access to it. \n\nI'm wondering whether this has to do with my organisation not being very data-driven and am wondering if more data-literate organisations are giving data scientist more freedom to access data themselves? It sure feels like we would save a lot of everyones' time if I could just pull the data I want from a warehouse or lake instead. Am I being unrealistic?\n\nSidenote: I understand that you can't just give everyone free access to all the data, and things like HR-data might be reasonable to keep under lock and key. But I need to file a request for ANY type of data, including information on what I intend to use it for etc.", "author_fullname": "t2_24el30e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How common is overly strict access to data in your org. ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19aedoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705652779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve run into this issue at work where a lot of my time is wasted on requesting access to some data, whether it be financial, HR or whatever, and having to file a formal request through my manager to get access to it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering whether this has to do with my organisation not being very data-driven and am wondering if more data-literate organisations are giving data scientist more freedom to access data themselves? It sure feels like we would save a lot of everyones&amp;#39; time if I could just pull the data I want from a warehouse or lake instead. Am I being unrealistic?&lt;/p&gt;\n\n&lt;p&gt;Sidenote: I understand that you can&amp;#39;t just give everyone free access to all the data, and things like HR-data might be reasonable to keep under lock and key. But I need to file a request for ANY type of data, including information on what I intend to use it for etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19aedoc", "is_robot_indexable": true, "report_reasons": null, "author": "NipponPanda", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19aedoc/how_common_is_overly_strict_access_to_data_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19aedoc/how_common_is_overly_strict_access_to_data_in/", "subreddit_subscribers": 1260030, "created_utc": 1705652779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a data scientist at a small ecomm company. Because the lift we expect and observe is small (they just are dumb features to start but that's a different story), it takes a while to reach significance. So management often see AB tests as a throwaway thing that are irrelevant to the launch of features. And although the features are dumb, they take engineering and creative a long time to get to MVP (minimal viable product), so they tend to launch regardless of AB test results (sunk cost fallacy).\n\n\nI want advice from others on how I should/can approach AB testing -\nI've been approaching the tests as 'is it good enough to launch?'\nIs it bad to switch to 'does it do too much harm?'\nSo instead of needing significance to say, enough evidence to observe more than 2% lift in conversion beyond random chance, I switch to, this doesn't suck bad enough to observe 10% drop in conversion.\nIn reframing it as do-no-harm I can set the difference threshold to big enough that business would care to push the emergency-stop button. Or if this is a lose-lose approach? What else do I lose in switching the framing? I am afraid I'd regret the switch, and that management would see AB test as even less valuable (not that they care now...). I'm also afraid they come back needing prove that something was impactful, but we didn't test that way and now we lack data to prove/disprove anything.\n\nThanks in advance. Any suggestions/thoughts are sincerely appreciated.", "author_fullname": "t2_9r81nvqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AB testing e-commerce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19at46x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705696721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist at a small ecomm company. Because the lift we expect and observe is small (they just are dumb features to start but that&amp;#39;s a different story), it takes a while to reach significance. So management often see AB tests as a throwaway thing that are irrelevant to the launch of features. And although the features are dumb, they take engineering and creative a long time to get to MVP (minimal viable product), so they tend to launch regardless of AB test results (sunk cost fallacy).&lt;/p&gt;\n\n&lt;p&gt;I want advice from others on how I should/can approach AB testing -\nI&amp;#39;ve been approaching the tests as &amp;#39;is it good enough to launch?&amp;#39;\nIs it bad to switch to &amp;#39;does it do too much harm?&amp;#39;\nSo instead of needing significance to say, enough evidence to observe more than 2% lift in conversion beyond random chance, I switch to, this doesn&amp;#39;t suck bad enough to observe 10% drop in conversion.\nIn reframing it as do-no-harm I can set the difference threshold to big enough that business would care to push the emergency-stop button. Or if this is a lose-lose approach? What else do I lose in switching the framing? I am afraid I&amp;#39;d regret the switch, and that management would see AB test as even less valuable (not that they care now...). I&amp;#39;m also afraid they come back needing prove that something was impactful, but we didn&amp;#39;t test that way and now we lack data to prove/disprove anything.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance. Any suggestions/thoughts are sincerely appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19at46x", "is_robot_indexable": true, "report_reasons": null, "author": "Any-Progress-4570", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19at46x/ab_testing_ecommerce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19at46x/ab_testing_ecommerce/", "subreddit_subscribers": 1260030, "created_utc": 1705696721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm teaching a data ethics course. It encompasses all things data. The course takes a journey through ethics history for the first half of the semester, then focuses on modern case studies. Course is much more of a seminar based course with heavy discussion. Course is geared towards undergraduates, so please keep this in mind, just beginning their data science and data analytic careers. It is a 3 credit course so the put of class expectations are reasonable to allow for adding additional materials.\n\nSo that it's not all reading based, I have incorporated movies, documentaries, sci-fi fiction, and podcasts, in addition to current event snippets, journal articles (limited due to often dense nature), and selected book chapters. \n\nI have preped most of the course, but reddit always seems to amaze me. I also cross posted this in the R/privacy sub and got some great suggestions.\n\nSo R/datascience what topics, events, activities, etc do you feel should not be missed in a data ethics course? Specifically in the field of data ethics in rhe last 50 years!!\n\nI am open to all suggestions and plan to review EVERY suggestion offered.", "author_fullname": "t2_ye3qh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ethics course suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ah58z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705664248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m teaching a data ethics course. It encompasses all things data. The course takes a journey through ethics history for the first half of the semester, then focuses on modern case studies. Course is much more of a seminar based course with heavy discussion. Course is geared towards undergraduates, so please keep this in mind, just beginning their data science and data analytic careers. It is a 3 credit course so the put of class expectations are reasonable to allow for adding additional materials.&lt;/p&gt;\n\n&lt;p&gt;So that it&amp;#39;s not all reading based, I have incorporated movies, documentaries, sci-fi fiction, and podcasts, in addition to current event snippets, journal articles (limited due to often dense nature), and selected book chapters. &lt;/p&gt;\n\n&lt;p&gt;I have preped most of the course, but reddit always seems to amaze me. I also cross posted this in the R/privacy sub and got some great suggestions.&lt;/p&gt;\n\n&lt;p&gt;So R/datascience what topics, events, activities, etc do you feel should not be missed in a data ethics course? Specifically in the field of data ethics in rhe last 50 years!!&lt;/p&gt;\n\n&lt;p&gt;I am open to all suggestions and plan to review EVERY suggestion offered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19ah58z", "is_robot_indexable": true, "report_reasons": null, "author": "nohann", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19ah58z/data_ethics_course_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19ah58z/data_ethics_course_suggestions/", "subreddit_subscribers": 1260030, "created_utc": 1705664248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a7g9y0xm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake vs. Parquet Comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_19ak6qh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0O4cdjsb9SXE1oj5qCbQ3LjqBTA-SzipPjp5YoS8iUo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705674087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "delta.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://delta.io/blog/delta-lake-vs-parquet-comparison/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NdxybeGQV2u38qR1olVYGd4_IAvOxxwZAYX_3JiJHBw.jpg?auto=webp&amp;s=65ce2c6aa5771e326ab4c9d0630ab6efbb56952b", "width": 577, "height": 294}, "resolutions": [{"url": "https://external-preview.redd.it/NdxybeGQV2u38qR1olVYGd4_IAvOxxwZAYX_3JiJHBw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07c910344e467e79bb196e850b257168fba8af82", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/NdxybeGQV2u38qR1olVYGd4_IAvOxxwZAYX_3JiJHBw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d18b13ccc828c1a4b6ea4623f67e31d20da92055", "width": 216, "height": 110}, {"url": "https://external-preview.redd.it/NdxybeGQV2u38qR1olVYGd4_IAvOxxwZAYX_3JiJHBw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=36fbffa8570f7a8f9bbbad96e8c1215ce8e47e16", "width": 320, "height": 163}], "variants": {}, "id": "KSChB8n4rBYn1RsM1zU9ChqqJMIWSUvbS_0SjCovoXU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "19ak6qh", "is_robot_indexable": true, "report_reasons": null, "author": "balcell", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19ak6qh/delta_lake_vs_parquet_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://delta.io/blog/delta-lake-vs-parquet-comparison/", "subreddit_subscribers": 1260030, "created_utc": 1705674087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to predict the conversion rate of company defined groups of customers for various product offers. The company clusters customers into Q groups (these groups overlap) and want to know if we can predict conversion rate (using regression) when offering product Y. I can also potentially model the problem as binary classification  at the customer level and aggregate up later but at the moment I\u2019m modeling @ the customer clustering level (predict conversion rate based on features of each of the Q customer groups)\n\nI have access to historical purchase behaviors of customers and purchase volumes and what not for each product.\n\nEach row of my training set looks like this:\nDesign matrix:\n- Features describing last N days purchase volumes for the cluster of customers (historical purchases before time of marketing offer being made)\n- Features describing what proportion of customers bought similar products before the current product offer (similar products defined by nearest K neighbors of product description embedding)\n\nLabels:\n- the conversion rate of that customer clustering which can be interpreted as a probability\n\nMy question is should I split the training data to be product offers before CUTOFF date and test examples to be after CUTOFF date?\n\nI think that that is potentially the most realistic and anti leakage proof split but I only have 3 years of purchase data and 1 year was covid + 1 year was recession + inflation. Im afraid that doing time splits I will make the test set have substantial seasonal differences and also macroeconomic differences than train set\n\nConceptually speaking i see no issue with doing a true random train test split of all product offers in 2023. Since each training / testing data row only has features calculated from historical (before the label was ever calculated) purchases. But I am worried that I am missing something and may have leakage. I am not experienced in running and leading projects myself so I want to ensure I am being rigorous and thoughtful\n\nTo make things a little more interesting, i need to model both new products and existing products. So i might even make a separate dataset for new product campaigns vs existing campaigns but I am not sure what\u2019s the best approach here\n\nWould love tips and guidance here!!\n\nSide note: yes i could model this as a recommender system problem and build a complicated solution but my company is not a tech company and i am working with a single laptop no cloud. Id rather see if i can make progress in supervised learning framework first", "author_fullname": "t2_z1sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make a proper train test split", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19au18t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705699051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to predict the conversion rate of company defined groups of customers for various product offers. The company clusters customers into Q groups (these groups overlap) and want to know if we can predict conversion rate (using regression) when offering product Y. I can also potentially model the problem as binary classification  at the customer level and aggregate up later but at the moment I\u2019m modeling @ the customer clustering level (predict conversion rate based on features of each of the Q customer groups)&lt;/p&gt;\n\n&lt;p&gt;I have access to historical purchase behaviors of customers and purchase volumes and what not for each product.&lt;/p&gt;\n\n&lt;p&gt;Each row of my training set looks like this:\nDesign matrix:\n- Features describing last N days purchase volumes for the cluster of customers (historical purchases before time of marketing offer being made)\n- Features describing what proportion of customers bought similar products before the current product offer (similar products defined by nearest K neighbors of product description embedding)&lt;/p&gt;\n\n&lt;p&gt;Labels:\n- the conversion rate of that customer clustering which can be interpreted as a probability&lt;/p&gt;\n\n&lt;p&gt;My question is should I split the training data to be product offers before CUTOFF date and test examples to be after CUTOFF date?&lt;/p&gt;\n\n&lt;p&gt;I think that that is potentially the most realistic and anti leakage proof split but I only have 3 years of purchase data and 1 year was covid + 1 year was recession + inflation. Im afraid that doing time splits I will make the test set have substantial seasonal differences and also macroeconomic differences than train set&lt;/p&gt;\n\n&lt;p&gt;Conceptually speaking i see no issue with doing a true random train test split of all product offers in 2023. Since each training / testing data row only has features calculated from historical (before the label was ever calculated) purchases. But I am worried that I am missing something and may have leakage. I am not experienced in running and leading projects myself so I want to ensure I am being rigorous and thoughtful&lt;/p&gt;\n\n&lt;p&gt;To make things a little more interesting, i need to model both new products and existing products. So i might even make a separate dataset for new product campaigns vs existing campaigns but I am not sure what\u2019s the best approach here&lt;/p&gt;\n\n&lt;p&gt;Would love tips and guidance here!!&lt;/p&gt;\n\n&lt;p&gt;Side note: yes i could model this as a recommender system problem and build a complicated solution but my company is not a tech company and i am working with a single laptop no cloud. Id rather see if i can make progress in supervised learning framework first&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "19au18t", "is_robot_indexable": true, "report_reasons": null, "author": "slimsippin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19au18t/how_to_make_a_proper_train_test_split/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19au18t/how_to_make_a_proper_train_test_split/", "subreddit_subscribers": 1260030, "created_utc": 1705699051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, I was in a struggle to find any jobs related to data analytics, I live in a non-metropolitain area, but even applied outside my state, was willing to commute, but my lack of experience left most of my job applications denied for several months.\n\nI have a Bachelors in Psych/ Business and worked in healthcare front-end, was studying a masters in an unrelated humanities degree, then dropped out and finished a data analytics bootcamp online. I created many projects, a website, a portfolio, I networked, went to job fairs, and still ended up only finding entry-levels. \n\nCurrently I'm a couple months into a new role and it's not what thought. The job title was in healthcare, reimbursement analytics, working with insurance and finance to process insurance claims. Mostly customer service, administration and data entry. And while we do use tools like Excel, it's minimal. Practically nothing I learned in my bootcamp (Tableau, PowerBI, PPT, Python, SQL) is utilized here.\n\nThe only upside I can see to this role is it looks like am an analyst in finance, there's remote work most of the time (hybrid) and opportunity to go fully remote, as well as possible advancement within the company, and it might be a good resume booster when I decide to find a higher paying role. \n\nStill, I am having doubts about this position, what is your thoughts in my case, should I stick it through only a year then apply to other companies, or should I try to stay longer and seek advancement opportunities?", "author_fullname": "t2_6lgx6i0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Started new position in healthcare reimbursement analytics, having doubts..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19asgdi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705695068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was in a struggle to find any jobs related to data analytics, I live in a non-metropolitain area, but even applied outside my state, was willing to commute, but my lack of experience left most of my job applications denied for several months.&lt;/p&gt;\n\n&lt;p&gt;I have a Bachelors in Psych/ Business and worked in healthcare front-end, was studying a masters in an unrelated humanities degree, then dropped out and finished a data analytics bootcamp online. I created many projects, a website, a portfolio, I networked, went to job fairs, and still ended up only finding entry-levels. &lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m a couple months into a new role and it&amp;#39;s not what thought. The job title was in healthcare, reimbursement analytics, working with insurance and finance to process insurance claims. Mostly customer service, administration and data entry. And while we do use tools like Excel, it&amp;#39;s minimal. Practically nothing I learned in my bootcamp (Tableau, PowerBI, PPT, Python, SQL) is utilized here.&lt;/p&gt;\n\n&lt;p&gt;The only upside I can see to this role is it looks like am an analyst in finance, there&amp;#39;s remote work most of the time (hybrid) and opportunity to go fully remote, as well as possible advancement within the company, and it might be a good resume booster when I decide to find a higher paying role. &lt;/p&gt;\n\n&lt;p&gt;Still, I am having doubts about this position, what is your thoughts in my case, should I stick it through only a year then apply to other companies, or should I try to stay longer and seek advancement opportunities?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19asgdi", "is_robot_indexable": true, "report_reasons": null, "author": "the-alchemist-", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19asgdi/started_new_position_in_healthcare_reimbursement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19asgdi/started_new_position_in_healthcare_reimbursement/", "subreddit_subscribers": 1260030, "created_utc": 1705695068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I\u2019m working on this machine learning project to find improper payments. We will be using a population of every payments that\u2019s been audited. One feature I\u2019m using is how long ago was the vendor last audited. \n\nAn issue that\u2019s going to arise is that for the first instance a vendor is audited there will not be an audit history so the value returned would be null. \n\nSo my thought on it is to put the first possible day that the system was out in place to audit these vouchers as the last audit day. \n\nLet\u2019s say it was January 1, 2010. And this one particular row is the first instance a vendor has been audited which was July 12, 2023. So I could just subtract those two dates.\n\n\nWith January 1 kinda representing a maximum value without putting infinity. \n\nHowever, the issue with that is that\u2019s not all vendors were around on that date. Our assumption is that a longer a vendor goes without being audited then they may be riskier. However, a vendor that  came about last year is pretty new. So if they have never been audited it\u2019s hard to compare them to vendor that\u2019s been around since 2010. \n\nSo instead I was thinking about just using the date of the vendor becoming an entity. \n\nSo if a vendor was made yesterday, then it would just say one day ago was their last audit date. Even though they never been audited. But that was the earliest possible time. \n\nThoughts or suggestions on this?", "author_fullname": "t2_8cjp70ft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Null value work around for this scenario.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19aofgy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705685128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m working on this machine learning project to find improper payments. We will be using a population of every payments that\u2019s been audited. One feature I\u2019m using is how long ago was the vendor last audited. &lt;/p&gt;\n\n&lt;p&gt;An issue that\u2019s going to arise is that for the first instance a vendor is audited there will not be an audit history so the value returned would be null. &lt;/p&gt;\n\n&lt;p&gt;So my thought on it is to put the first possible day that the system was out in place to audit these vouchers as the last audit day. &lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say it was January 1, 2010. And this one particular row is the first instance a vendor has been audited which was July 12, 2023. So I could just subtract those two dates.&lt;/p&gt;\n\n&lt;p&gt;With January 1 kinda representing a maximum value without putting infinity. &lt;/p&gt;\n\n&lt;p&gt;However, the issue with that is that\u2019s not all vendors were around on that date. Our assumption is that a longer a vendor goes without being audited then they may be riskier. However, a vendor that  came about last year is pretty new. So if they have never been audited it\u2019s hard to compare them to vendor that\u2019s been around since 2010. &lt;/p&gt;\n\n&lt;p&gt;So instead I was thinking about just using the date of the vendor becoming an entity. &lt;/p&gt;\n\n&lt;p&gt;So if a vendor was made yesterday, then it would just say one day ago was their last audit date. Even though they never been audited. But that was the earliest possible time. &lt;/p&gt;\n\n&lt;p&gt;Thoughts or suggestions on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "19aofgy", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainVJ", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19aofgy/null_value_work_around_for_this_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19aofgy/null_value_work_around_for_this_scenario/", "subreddit_subscribers": 1260030, "created_utc": 1705685128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my volunteer position I have this project where I need to create an Excel sheet directory of local community resources (like churches, schools, mechanics, etc.) in a specific city area. The goal is to include business names, addresses, phone numbers, and ideally emails.\n\nAs someone new to coding, I've learned some basics of Python and Visual Studio Code, but this is my first real-life application. I'm thinking about web scraping for this task and have come across a tool called Beautiful Soup. Is this the right approach?\n\nIf anyone can spare a few minutes to guide me or suggest resources directly related to this type of project, I would greatly appreciate it. DM me or point me in the right direction.  \ud83d\ude4f", "author_fullname": "t2_7up88h6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scrape Info from Map", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19asziu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705696393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my volunteer position I have this project where I need to create an Excel sheet directory of local community resources (like churches, schools, mechanics, etc.) in a specific city area. The goal is to include business names, addresses, phone numbers, and ideally emails.&lt;/p&gt;\n\n&lt;p&gt;As someone new to coding, I&amp;#39;ve learned some basics of Python and Visual Studio Code, but this is my first real-life application. I&amp;#39;m thinking about web scraping for this task and have come across a tool called Beautiful Soup. Is this the right approach?&lt;/p&gt;\n\n&lt;p&gt;If anyone can spare a few minutes to guide me or suggest resources directly related to this type of project, I would greatly appreciate it. DM me or point me in the right direction.  \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "19asziu", "is_robot_indexable": true, "report_reasons": null, "author": "andraco95", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19asziu/scrape_info_from_map/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19asziu/scrape_info_from_map/", "subreddit_subscribers": 1260030, "created_utc": 1705696393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's an AI right now that has been trending in its ability to solve data science problems, it is called Julius AI, it allows for a free 10 questions per month to an account. \n\nI've personally used it and its ability to solve problems is incredible, but it is powered by GPT4. \n\nI've never used GPT4 to test the difference, so my question is for those who have used GPT4 or tried both AIs, is Julius worth it more than GPT4, does GPT4 has an advantage on Julius in data science or data analysis? \n\nAlso note that the proper plan of Julius is about 40$ per month for unlimited messages which is double of that for GPT4.\n\nSo what would you recommend? Should I go for it? \n\nThanks for your reply in advance\u270c\ufe0f", "author_fullname": "t2_l8jdxq43u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Julius AI worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19awedm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705705027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s an AI right now that has been trending in its ability to solve data science problems, it is called Julius AI, it allows for a free 10 questions per month to an account. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve personally used it and its ability to solve problems is incredible, but it is powered by GPT4. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never used GPT4 to test the difference, so my question is for those who have used GPT4 or tried both AIs, is Julius worth it more than GPT4, does GPT4 has an advantage on Julius in data science or data analysis? &lt;/p&gt;\n\n&lt;p&gt;Also note that the proper plan of Julius is about 40$ per month for unlimited messages which is double of that for GPT4.&lt;/p&gt;\n\n&lt;p&gt;So what would you recommend? Should I go for it? &lt;/p&gt;\n\n&lt;p&gt;Thanks for your reply in advance\u270c\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19awedm", "is_robot_indexable": true, "report_reasons": null, "author": "Darktrader21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19awedm/is_julius_ai_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19awedm/is_julius_ai_worth_it/", "subreddit_subscribers": 1260030, "created_utc": 1705705027.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}