{"kind": "Listing", "data": {"after": "t3_1904v17", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lately I have been archiving more media for my Plex library. I decided to download the entire One Piece show that's 1.3 tb. I started it a bit ago and it says it'll take over 3-5 days. I did download all of the Saturday Night Live seasons 2 years ago that took a week or less that was 1.68tbs.", "author_fullname": "t2_4trjihkg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the biggest thing you've downloaded in your time being a Datahoarder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1908up8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704571797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately I have been archiving more media for my Plex library. I decided to download the entire One Piece show that&amp;#39;s 1.3 tb. I started it a bit ago and it says it&amp;#39;ll take over 3-5 days. I did download all of the Saturday Night Live seasons 2 years ago that took a week or less that was 1.68tbs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "52TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1908up8", "is_robot_indexable": true, "report_reasons": null, "author": "Eskel5", "discussion_type": null, "num_comments": 115, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1908up8/whats_the_biggest_thing_youve_downloaded_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1908up8/whats_the_biggest_thing_youve_downloaded_in_your/", "subreddit_subscribers": 723434, "created_utc": 1704571797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_78n2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Meme] Average DigitalFAQ Forum Thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_1907rwb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/k2z32uelevac1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 544, "width": 1280, "scrubber_media_url": "https://v.redd.it/k2z32uelevac1/DASH_96.mp4", "dash_url": "https://v.redd.it/k2z32uelevac1/DASHPlaylist.mpd?a=1707208702%2COGY2NzkyMmRhYjdkNTQzOWYwYmM2ZTYwMWZjMTRmNWY5NTUyZGM5ZWM2ZWRjZGI5MWY3NzY0NDIwMDk1Y2NiOA%3D%3D&amp;v=1&amp;f=sd", "duration": 65, "hls_url": "https://v.redd.it/k2z32uelevac1/HLSPlaylist.m3u8?a=1707208702%2CNjI0ODlkNzU2MTNlOWEwMWM4N2I3M2E0NGM1ZWNiYmIyOTk3MmUzNThjMDRlMmEwNjc3NDUyNzI3NDdjYzk0Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=140&amp;height=59&amp;crop=140:59,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c413aa68682d0d4c97f88b82728982ffb9e05f5d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704569038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/k2z32uelevac1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?format=pjpg&amp;auto=webp&amp;s=bce44850f07342b55770e49b9dc74229ca1c0776", "width": 1920, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30edb531f0cf4a76e3d8fa5f083fac24e5f0b4d6", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=356a8f1356e0283ef130b7fc656c3d746826b129", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=235550b017ec21d5aae3e494b27c05a6e8129a79", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1bf5a4fbda2d77e7cf10d8dc47df1b53719ab764", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f6fb1360a21b12d868b1702a34c70cef96f944a4", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=119168848e82be396df3a14fd9be234106477d4a", "width": 1080, "height": 459}], "variants": {}, "id": "bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907rwb", "is_robot_indexable": true, "report_reasons": null, "author": "Nightowl3090", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907rwb/meme_average_digitalfaq_forum_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/k2z32uelevac1", "subreddit_subscribers": 723434, "created_utc": 1704569038.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/k2z32uelevac1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 544, "width": 1280, "scrubber_media_url": "https://v.redd.it/k2z32uelevac1/DASH_96.mp4", "dash_url": "https://v.redd.it/k2z32uelevac1/DASHPlaylist.mpd?a=1707208702%2COGY2NzkyMmRhYjdkNTQzOWYwYmM2ZTYwMWZjMTRmNWY5NTUyZGM5ZWM2ZWRjZGI5MWY3NzY0NDIwMDk1Y2NiOA%3D%3D&amp;v=1&amp;f=sd", "duration": 65, "hls_url": "https://v.redd.it/k2z32uelevac1/HLSPlaylist.m3u8?a=1707208702%2CNjI0ODlkNzU2MTNlOWEwMWM4N2I3M2E0NGM1ZWNiYmIyOTk3MmUzNThjMDRlMmEwNjc3NDUyNzI3NDdjYzk0Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a PC to encrypt and she has a Mac on her end. I do have access to a MacBook Air too. ", "author_fullname": "t2_119ak5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to password protect/encrypt an external drive with 500GB of photos and videos for shipping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1902mpo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704555641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a PC to encrypt and she has a Mac on her end. I do have access to a MacBook Air too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1902mpo", "is_robot_indexable": true, "report_reasons": null, "author": "nakkai", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1902mpo/best_way_to_password_protectencrypt_an_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1902mpo/best_way_to_password_protectencrypt_an_external/", "subreddit_subscribers": 723434, "created_utc": 1704555641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know other factors play when it comes to how much storage the video will be after recording like resolution, length of tape, and frames per second.\n\nI'm going to use Diamond VC500 Capture to convert the tapes. It will be 720x480 at 30fps for around 1-1:30 hours of tape.\n\nWhat would be a rough estimate of how much storage this would take up? I'm using Verbatim DVD+R 4.7GB Discs", "author_fullname": "t2_9rq959xs0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to digitize VHS tapes soon. I know what I need and how to do it, but how much storage does the recording of a VHS tape take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1900txf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704550384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know other factors play when it comes to how much storage the video will be after recording like resolution, length of tape, and frames per second.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to use Diamond VC500 Capture to convert the tapes. It will be 720x480 at 30fps for around 1-1:30 hours of tape.&lt;/p&gt;\n\n&lt;p&gt;What would be a rough estimate of how much storage this would take up? I&amp;#39;m using Verbatim DVD+R 4.7GB Discs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1900txf", "is_robot_indexable": true, "report_reasons": null, "author": "TrooperMann", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1900txf/i_want_to_digitize_vhs_tapes_soon_i_know_what_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1900txf/i_want_to_digitize_vhs_tapes_soon_i_know_what_i/", "subreddit_subscribers": 723434, "created_utc": 1704550384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The Western Digital website is having a sale right now, you can get two WD Red Pro 14TB NAS drives for $439.98 (free shipping) if anyone is in the market for some. Looks like the discount ends 1/7/2024.", "author_fullname": "t2_xagsn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bundle discount on WD Pro Red 14TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1904a1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3zQXU-3S4jeh3SEwSQaSwboZJNti_8jiS7D1cMUm3O0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704560131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Western Digital website is having a sale right now, you can get two WD Red Pro 14TB NAS drives for $439.98 (free shipping) if anyone is in the market for some. Looks like the discount ends 1/7/2024.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/34b9fq5fouac1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/34b9fq5fouac1.png?auto=webp&amp;s=c5a705840ad9031c5446de7f34aae1fc07448581", "width": 1080, "height": 2412}, "resolutions": [{"url": "https://preview.redd.it/34b9fq5fouac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab2416aca78843be6d4abd7f127fab9e112589a0", "width": 108, "height": 216}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734c47d6961c2ba9f4ee48ef1149226e67662e30", "width": 216, "height": 432}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4599776d3b396f0f1c7a7870b05a1d9251dc20bf", "width": 320, "height": 640}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7889b19d691644110378448897150e57443325e", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1b1fd88c0cd58a80c6ae415ad9839e461de0b54", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9123b5e52fa6831e1022e50d10f74819c7b051ff", "width": 1080, "height": 2160}], "variants": {}, "id": "2QqZ8gspdCb_mQ3lJxXBq0x0SDA6WfocGaGqIT6sf_A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1904a1m", "is_robot_indexable": true, "report_reasons": null, "author": "pillowserious", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904a1m/bundle_discount_on_wd_pro_red_14tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/34b9fq5fouac1.png", "subreddit_subscribers": 723434, "created_utc": 1704560131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_crd2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There comes a point where MEGA becomes unfeasible for a certain level of datahoarders :(", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_190hram", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ngwEo0bklrcZGim7cSMSgsLKghi-NCbOMUWyeThGkyI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704595960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/xfv0obghmxac1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/xfv0obghmxac1.png?auto=webp&amp;s=98e4bf632faec59a928397462a40f684b2a86910", "width": 1037, "height": 562}, "resolutions": [{"url": "https://preview.redd.it/xfv0obghmxac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41d193dac7fed1e96be4a68ceed4cd47f82a0e85", "width": 108, "height": 58}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=693fd298104a73bbe09aaf0742081bb25c49945c", "width": 216, "height": 117}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bda9ed080b9934c2aa64a8bb684a27ba225d361d", "width": 320, "height": 173}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27d292b33e645b9ab93aee14eb3534ab574d7ce3", "width": 640, "height": 346}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e62e7cd79d06ce2653222f363ea59235a0b9155", "width": 960, "height": 520}], "variants": {}, "id": "CCE8PnQ9w705IN7Kiaq56e5QB9EfV4iPOtnTX6jZQ28"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "190hram", "is_robot_indexable": true, "report_reasons": null, "author": "TheInzaneGamer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190hram/there_comes_a_point_where_mega_becomes_unfeasible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/xfv0obghmxac1.png", "subreddit_subscribers": 723434, "created_utc": 1704595960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title\n\nDue to not having anymore money after spending it all on my latest edition last year (Synology NAS DS1821+ with 8x22TB hard drives) I have been consumed with trying to go through my existing hard drives, amalgamating, using it as a man in the middle and robbing Peter to pay Paul in an effort to free up as much space as possible on my existing drives, but also due to cost of living pressures and inflation, 20-22TB internal drives (if you are extraordinarily lucky) are still $800 dollars, so I want as much space to last me as long as possible.\n\nI have mainly be backing up, transferring from drive to drive, and dealing with computer time, for how long it takes to transfer data.\n\nI feel guilty not doing anything and not going anywhere, but I feel like I have been productive and trying to recharge my own batteries to some extent. \n\nWhat have you all been doing?", "author_fullname": "t2_d7675", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explain what you have been doing re data hoarding over Christmas New Year break", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zz3cv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704544521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;Due to not having anymore money after spending it all on my latest edition last year (Synology NAS DS1821+ with 8x22TB hard drives) I have been consumed with trying to go through my existing hard drives, amalgamating, using it as a man in the middle and robbing Peter to pay Paul in an effort to free up as much space as possible on my existing drives, but also due to cost of living pressures and inflation, 20-22TB internal drives (if you are extraordinarily lucky) are still $800 dollars, so I want as much space to last me as long as possible.&lt;/p&gt;\n\n&lt;p&gt;I have mainly be backing up, transferring from drive to drive, and dealing with computer time, for how long it takes to transfer data.&lt;/p&gt;\n\n&lt;p&gt;I feel guilty not doing anything and not going anywhere, but I feel like I have been productive and trying to recharge my own batteries to some extent. &lt;/p&gt;\n\n&lt;p&gt;What have you all been doing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zz3cv", "is_robot_indexable": true, "report_reasons": null, "author": "DrWho345", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zz3cv/explain_what_you_have_been_doing_re_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zz3cv/explain_what_you_have_been_doing_re_data_hoarding/", "subreddit_subscribers": 723434, "created_utc": 1704544521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I find one problem with being a DataHoarder is having multiple versions of the same video clips. I've used programs such as DupeGuru to weed out identical videos, but the problem is when you have the same video, but different sizes. One possibility is that the video was popular and re-encoded by someone else at some point. \n\nMy goal is to only keep the best quality version of each video. \n\nSo I first thought I could just sort by video length, and then take out the smaller sized files. But I'm finding that some of the reencoded vids are larger than the original, so that doesn't always help. Right now I'm going through each potential dupe video, screenshotting a random frame, zooming in and comparing them manually. It works, but not the most efficient process.\n\nAre there any thoughts on a better way to do this? Thanks!", "author_fullname": "t2_h3tsi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video quality advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907qbg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find one problem with being a DataHoarder is having multiple versions of the same video clips. I&amp;#39;ve used programs such as DupeGuru to weed out identical videos, but the problem is when you have the same video, but different sizes. One possibility is that the video was popular and re-encoded by someone else at some point. &lt;/p&gt;\n\n&lt;p&gt;My goal is to only keep the best quality version of each video. &lt;/p&gt;\n\n&lt;p&gt;So I first thought I could just sort by video length, and then take out the smaller sized files. But I&amp;#39;m finding that some of the reencoded vids are larger than the original, so that doesn&amp;#39;t always help. Right now I&amp;#39;m going through each potential dupe video, screenshotting a random frame, zooming in and comparing them manually. It works, but not the most efficient process.&lt;/p&gt;\n\n&lt;p&gt;Are there any thoughts on a better way to do this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907qbg", "is_robot_indexable": true, "report_reasons": null, "author": "1doughnut", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907qbg/video_quality_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907qbg/video_quality_advice/", "subreddit_subscribers": 723434, "created_utc": 1704568921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible if so how?", "author_fullname": "t2_8zpe3mjoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting 3d models from 2gis.ru", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1908wle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704571925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible if so how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1908wle", "is_robot_indexable": true, "report_reasons": null, "author": "3dPrintMyThingi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1908wle/extracting_3d_models_from_2gisru/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1908wle/extracting_3d_models_from_2gisru/", "subreddit_subscribers": 723434, "created_utc": 1704571925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders. I have my trusty, tried and true original Truenas CORE machine just lying around anymore. It's a chenbro NR12000 w/ the E1250 CPU, and I think 16GB of RAM. It has about 30TB usable in a ZRAID3 config. In 2023 I upgraded to a genuine HP server machine w/ 100TB usable and a supermicro machine with 60TB usable, both with way more CPU and RAM. This never let me down and I hate to let it sit. Short of selling it, any handy ideas on what to do with it? ", "author_fullname": "t2_depdl36z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with 'spare' 30TB NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1901ifx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704552415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders. I have my trusty, tried and true original Truenas CORE machine just lying around anymore. It&amp;#39;s a chenbro NR12000 w/ the E1250 CPU, and I think 16GB of RAM. It has about 30TB usable in a ZRAID3 config. In 2023 I upgraded to a genuine HP server machine w/ 100TB usable and a supermicro machine with 60TB usable, both with way more CPU and RAM. This never let me down and I hate to let it sit. Short of selling it, any handy ideas on what to do with it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1901ifx", "is_robot_indexable": true, "report_reasons": null, "author": "MyTechAccount90210", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1901ifx/what_to_do_with_spare_30tb_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1901ifx/what_to_do_with_spare_30tb_nas/", "subreddit_subscribers": 723434, "created_utc": 1704552415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My main internal drive is a 4TB WD Black. (don't laugh) I want to upgrade to 8TB, so I bought an 8TB Seagate Barracuda from B&amp;H. I installed it into a Sabrent drive dock I've had a few years, but IS rated for 8TB. Formatted as Z, copied a couple TB from the internal drive, and about 600MB from an external drive of my wife's. The plan was to leave it in action for some weeks before swapping into the PC, to make sure it was working OK.\n\nStarted getting errors from Windows (W10) that the drive needed scanned and repaired. It would repeatedly hang about 30% through the scan, and the drive would disappear. Downloaded **Seatools**, the 2 minute self test would fail. But I did see some folders of data that I had copied. I have **HD Sentinel**, ran the short self-test and the conveyance self test (whatever that is) successfully with that. Disk surface test showed no bad or damaged sectors, but I was also left with 1000 \"found.nnn\" folders.\n\nQuestions: Could this be a problem with the dock or it's USB cable? I've already bought a new one with it's own USB 3.2 cable (which my PC supports). Is it worth installing it in the new dock, reformatting, and starting over? You can view screenshots of the [various test results at Imgur](https://imgur.com/a/4A7HZSP). Or should I RMA this drive? Thanks.", "author_fullname": "t2_fwy7u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I trust this drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907hg5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My main internal drive is a 4TB WD Black. (don&amp;#39;t laugh) I want to upgrade to 8TB, so I bought an 8TB Seagate Barracuda from B&amp;amp;H. I installed it into a Sabrent drive dock I&amp;#39;ve had a few years, but IS rated for 8TB. Formatted as Z, copied a couple TB from the internal drive, and about 600MB from an external drive of my wife&amp;#39;s. The plan was to leave it in action for some weeks before swapping into the PC, to make sure it was working OK.&lt;/p&gt;\n\n&lt;p&gt;Started getting errors from Windows (W10) that the drive needed scanned and repaired. It would repeatedly hang about 30% through the scan, and the drive would disappear. Downloaded &lt;strong&gt;Seatools&lt;/strong&gt;, the 2 minute self test would fail. But I did see some folders of data that I had copied. I have &lt;strong&gt;HD Sentinel&lt;/strong&gt;, ran the short self-test and the conveyance self test (whatever that is) successfully with that. Disk surface test showed no bad or damaged sectors, but I was also left with 1000 &amp;quot;found.nnn&amp;quot; folders.&lt;/p&gt;\n\n&lt;p&gt;Questions: Could this be a problem with the dock or it&amp;#39;s USB cable? I&amp;#39;ve already bought a new one with it&amp;#39;s own USB 3.2 cable (which my PC supports). Is it worth installing it in the new dock, reformatting, and starting over? You can view screenshots of the &lt;a href=\"https://imgur.com/a/4A7HZSP\"&gt;various test results at Imgur&lt;/a&gt;. Or should I RMA this drive? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?auto=webp&amp;s=bab6394dd8a233277e287d0b8817296aa6fac59c", "width": 566, "height": 573}, "resolutions": [{"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=28c234971dc8891add6528a783cf3c370b38e4d5", "width": 108, "height": 109}, {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e46399bdd3ab8a4224db150e857404ec6a2e542", "width": 216, "height": 218}, {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b5203b7783399d90b405f626f125f89f14af7b4", "width": 320, "height": 323}], "variants": {}, "id": "7HP71TV1dsuSeRkrcBo3iGMG7MffI_6vRYgs4-z3LgU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907hg5", "is_robot_indexable": true, "report_reasons": null, "author": "evildad53", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907hg5/can_i_trust_this_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907hg5/can_i_trust_this_drive/", "subreddit_subscribers": 723434, "created_utc": 1704568307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just wiped/formatted a 3TB drive I used in college 10 years ago, and feel so guilty\u2026 The data was backed up 10 years ago on to larger drives and has since been updated/added too.\n\nThe 3Tb drive was wiped to be repurposed, because it still has many useable hours on it yet.\n\nThe older drives data was obsolete, because I\u2019ve added and updated newer data on top of it.\n\nI feel so guilty as a data hoarder, wiping that drive\u2026 because like. ya know what if I needed it? .. I guess I\u2019m venting/looking for sympathy/and asking why I feel a little weird for wiping it?", "author_fullname": "t2_oly5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feel Guilty Wiping 10 yr old 3Tb Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190k5w4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704603492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just wiped/formatted a 3TB drive I used in college 10 years ago, and feel so guilty\u2026 The data was backed up 10 years ago on to larger drives and has since been updated/added too.&lt;/p&gt;\n\n&lt;p&gt;The 3Tb drive was wiped to be repurposed, because it still has many useable hours on it yet.&lt;/p&gt;\n\n&lt;p&gt;The older drives data was obsolete, because I\u2019ve added and updated newer data on top of it.&lt;/p&gt;\n\n&lt;p&gt;I feel so guilty as a data hoarder, wiping that drive\u2026 because like. ya know what if I needed it? .. I guess I\u2019m venting/looking for sympathy/and asking why I feel a little weird for wiping it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190k5w4", "is_robot_indexable": true, "report_reasons": null, "author": "BlueFuzzyBunny", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190k5w4/feel_guilty_wiping_10_yr_old_3tb_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190k5w4/feel_guilty_wiping_10_yr_old_3tb_drive/", "subreddit_subscribers": 723434, "created_utc": 1704603492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's the point of buying the latest version of a hard drive?  What is the difference between a new 4tb hard drive that came out in 2023 and a new 4tb hard drive that came out a long time ago?  (5, 10, 12 years, no idea, just as an example)\nAre there any bigger differences in terms of safety, longevity and reliability? \nOr is it just possible that even more data can fit in and some small upgrades? \n\nWill there be revolutions in the near future (next 5-10 years) related to data storage?", "author_fullname": "t2_9odcnu9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should I buy the latest version of hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190glwh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704592482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the point of buying the latest version of a hard drive?  What is the difference between a new 4tb hard drive that came out in 2023 and a new 4tb hard drive that came out a long time ago?  (5, 10, 12 years, no idea, just as an example)\nAre there any bigger differences in terms of safety, longevity and reliability? \nOr is it just possible that even more data can fit in and some small upgrades? &lt;/p&gt;\n\n&lt;p&gt;Will there be revolutions in the near future (next 5-10 years) related to data storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "20TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190glwh", "is_robot_indexable": true, "report_reasons": null, "author": "ServiceOk9043", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/190glwh/why_should_i_buy_the_latest_version_of_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190glwh/why_should_i_buy_the_latest_version_of_hard_drive/", "subreddit_subscribers": 723434, "created_utc": 1704592482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have searched around and so far nothing I find actually works. I have a video set I've paid for that I log in on their website and there is an embedded Vimeo playlist of the videos I want to download. I use the inspector in my web browser and look at the network tab while playing the video and just see a bunch of segment-number.m4s links. I try to copy that address into YTDL but it never works. What am I doing wrong?", "author_fullname": "t2_qbrku9p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download Embedded Vimeo Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1904xed", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704561810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have searched around and so far nothing I find actually works. I have a video set I&amp;#39;ve paid for that I log in on their website and there is an embedded Vimeo playlist of the videos I want to download. I use the inspector in my web browser and look at the network tab while playing the video and just see a bunch of segment-number.m4s links. I try to copy that address into YTDL but it never works. What am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1904xed", "is_robot_indexable": true, "report_reasons": null, "author": "Tumnus1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904xed/download_embedded_vimeo_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1904xed/download_embedded_vimeo_playlist/", "subreddit_subscribers": 723434, "created_utc": 1704561810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are quite a few posts on here about scripts or sites that can batch download media or tweets, but is anyone aware of something that could *automatically* download media when a user posts?\n\nI have quite a few users' notifications on but it becomes tedious and distracting to save the media of each notification. \n\nThanks!", "author_fullname": "t2_t7ki2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program/Script to automatically download media when a Twitter/X user posts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18zwlax", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704534513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are quite a few posts on here about scripts or sites that can batch download media or tweets, but is anyone aware of something that could &lt;em&gt;automatically&lt;/em&gt; download media when a user posts?&lt;/p&gt;\n\n&lt;p&gt;I have quite a few users&amp;#39; notifications on but it becomes tedious and distracting to save the media of each notification. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18zwlax", "is_robot_indexable": true, "report_reasons": null, "author": "socoolhage", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18zwlax/programscript_to_automatically_download_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18zwlax/programscript_to_automatically_download_media/", "subreddit_subscribers": 723434, "created_utc": 1704534513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a m3u8 link to a Pluto TV livestream and am able to watch it on VLC.\n\nHas anyone been successful saving episodes from a Pluto TV stream (with subtitles) to mp4? Here are things I have tried and issues I faced:\n\n\\- Using VLC's built-in recording function: things go OK until the stream enters or comes back from commercial breaks, at which point VLC stops capturing the video stream\n\n\\- Using ffmpeg: the stream is infinite, so I try to kill ffmpeg using CTRL+C manually after some time. The output file has the same problem.\n\n\\- Using yt-dlp: same problem.\n\nI am on a Mac and would also like to be able to save some of the commercials too, if possible. Thank you in advance for whatever help you may be able to provide!", "author_fullname": "t2_5ss0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving a Pluto TV livestream?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_190mtdc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704612779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a m3u8 link to a Pluto TV livestream and am able to watch it on VLC.&lt;/p&gt;\n\n&lt;p&gt;Has anyone been successful saving episodes from a Pluto TV stream (with subtitles) to mp4? Here are things I have tried and issues I faced:&lt;/p&gt;\n\n&lt;p&gt;- Using VLC&amp;#39;s built-in recording function: things go OK until the stream enters or comes back from commercial breaks, at which point VLC stops capturing the video stream&lt;/p&gt;\n\n&lt;p&gt;- Using ffmpeg: the stream is infinite, so I try to kill ffmpeg using CTRL+C manually after some time. The output file has the same problem.&lt;/p&gt;\n\n&lt;p&gt;- Using yt-dlp: same problem.&lt;/p&gt;\n\n&lt;p&gt;I am on a Mac and would also like to be able to save some of the commercials too, if possible. Thank you in advance for whatever help you may be able to provide!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190mtdc", "is_robot_indexable": true, "report_reasons": null, "author": "HolyShip", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190mtdc/saving_a_pluto_tv_livestream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190mtdc/saving_a_pluto_tv_livestream/", "subreddit_subscribers": 723434, "created_utc": 1704612779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello I have been storing some of my data on a WD My Book External HDD. I never really thought about the beeping noise it makes until now. Is this drive dying? Should I get another drive?\n\n[My Drive making a weird Beeping noise.](https://reddit.com/link/190lp4w/video/9rl122l5oyac1/player)\n\nAnd here is the SmartCTL: \n\n    smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-13-amd64] (local build)\n    Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n    \n    === START OF INFORMATION SECTION ===\n    Device Model:     WDC WD40EDAZ-11SLVB0\n    Serial Number:    WD-WX12DC13FRYS\n    LU WWN Device Id: 5 0014ee 214ebda6c\n    Firmware Version: 80.00A80\n    User Capacity:    4,000,787,030,016 bytes [4.00 TB]\n    Sector Sizes:     512 bytes logical, 4096 bytes physical\n    Rotation Rate:    5400 rpm\n    Form Factor:      3.5 inches\n    TRIM Command:     Available, deterministic, zeroed\n    Device is:        Not in smartctl database 7.3/5319\n    ATA Version is:   ACS-3 T13/2161-D revision 5\n    SATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\n    Local Time is:    Sat Jan  6 22:03:38 2024 PST\n    SMART support is: Available - device has SMART capability.\n    SMART support is: Enabled\n    AAM feature is:   Unavailable\n    APM feature is:   Unavailable\n    Rd look-ahead is: Enabled\n    Write cache is:   Enabled\n    DSN feature is:   Unavailable\n    ATA Security is:  Disabled, NOT FROZEN [SEC1]\n    Wt Cache Reorder: Enabled\n    \n    === START OF READ SMART DATA SECTION ===\n    SMART overall-health self-assessment test result: PASSED\n    \n    General SMART Values:\n    Offline data collection status:  (0x82)\tOffline data collection activity\n    \t\t\t\t\twas completed without error.\n    \t\t\t\t\tAuto Offline Data Collection: Enabled.\n    Self-test execution status:      (   0)\tThe previous self-test routine completed\n    \t\t\t\t\twithout error or no self-test has ever \n    \t\t\t\t\tbeen run.\n    Total time to complete Offline \n    data collection: \t\t(48104) seconds.\n    Offline data collection\n    capabilities: \t\t\t (0x7b) SMART execute Offline immediate.\n    \t\t\t\t\tAuto Offline data collection on/off support.\n    \t\t\t\t\tSuspend Offline collection upon new\n    \t\t\t\t\tcommand.\n    \t\t\t\t\tOffline surface scan supported.\n    \t\t\t\t\tSelf-test supported.\n    \t\t\t\t\tConveyance Self-test supported.\n    \t\t\t\t\tSelective Self-test supported.\n    SMART capabilities:            (0x0003)\tSaves SMART data before entering\n    \t\t\t\t\tpower-saving mode.\n    \t\t\t\t\tSupports SMART auto save timer.\n    Error logging capability:        (0x01)\tError logging supported.\n    \t\t\t\t\tGeneral Purpose Logging supported.\n    Short self-test routine \n    recommended polling time: \t (   2) minutes.\n    Extended self-test routine\n    recommended polling time: \t ( 186) minutes.\n    Conveyance self-test routine\n    recommended polling time: \t (   2) minutes.\n    SCT capabilities: \t       (0x3031)\tSCT Status supported.\n    \t\t\t\t\tSCT Feature Control supported.\n    \t\t\t\t\tSCT Data Table supported.\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n      1 Raw_Read_Error_Rate     POSR-K   200   200   051    -    0\n      3 Spin_Up_Time            POS--K   209   204   021    -    2516\n      4 Start_Stop_Count        -O--CK   097   097   000    -    3322\n      5 Reallocated_Sector_Ct   PO--CK   200   200   140    -    0\n      7 Seek_Error_Rate         -OSR-K   200   200   000    -    4\n      9 Power_On_Hours          -O--CK   092   092   000    -    6317\n     10 Spin_Retry_Count        -O--CK   100   100   000    -    0\n     11 Calibration_Retry_Count -O--CK   100   100   000    -    0\n     12 Power_Cycle_Count       -O--CK   097   097   000    -    3322\n    192 Power-Off_Retract_Count -O--CK   200   200   000    -    18\n    193 Load_Cycle_Count        -O--CK   195   195   000    -    15405\n    194 Temperature_Celsius     -O---K   124   098   000    -    23\n    196 Reallocated_Event_Count -O--CK   200   200   000    -    0\n    197 Current_Pending_Sector  -O--CK   200   200   000    -    1\n    198 Offline_Uncorrectable   ----CK   200   200   000    -    0\n    199 UDMA_CRC_Error_Count    -O--CK   200   200   000    -    0\n    200 Multi_Zone_Error_Rate   ---R--   200   200   000    -    0\n                                ||||||_ K auto-keep\n                                |||||__ C event count\n                                ||||___ R error rate\n                                |||____ S speed/performance\n                                ||_____ O updated online\n                                |______ P prefailure warning\n    \n    General Purpose Log Directory Version 1\n    SMART           Log Directory Version 1 [multi-sector log support]\n    Address    Access  R/W   Size  Description\n    0x00       GPL,SL  R/O      1  Log Directory\n    0x01           SL  R/O      1  Summary SMART error log\n    0x02           SL  R/O      5  Comprehensive SMART error log\n    0x03       GPL     R/O      6  Ext. Comprehensive SMART error log\n    0x04       GPL     R/O    256  Device Statistics log\n    0x04       SL      R/O      8  Device Statistics log\n    0x06           SL  R/O      1  SMART self-test log\n    0x07       GPL     R/O      1  Extended self-test log\n    0x09           SL  R/W      1  Selective self-test log\n    0x0c       GPL     R/O   2048  Pending Defects log\n    0x10       GPL     R/O      1  NCQ Command Error log\n    0x11       GPL     R/O      1  SATA Phy Event Counters log\n    0x24       GPL     R/O    294  Current Device Internal Status Data log\n    0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n    0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n    0xa0-0xa7  GPL,SL  VS      16  Device vendor specific log\n    0xa8-0xb6  GPL,SL  VS       1  Device vendor specific log\n    0xb7       GPL,SL  VS      78  Device vendor specific log\n    0xb9       GPL,SL  VS       4  Device vendor specific log\n    0xbd       GPL,SL  VS       1  Device vendor specific log\n    0xc0       GPL,SL  VS       1  Device vendor specific log\n    0xc1       GPL     VS      93  Device vendor specific log\n    0xe0       GPL,SL  R/W      1  SCT Command/Status\n    0xe1       GPL,SL  R/W      1  SCT Data Transfer\n    \n    SMART Extended Comprehensive Error Log Version: 1 (6 sectors)\n    Device Error Count: 1\n    \tCR     = Command Register\n    \tFEATR  = Features Register\n    \tCOUNT  = Count (was: Sector Count) Register\n    \tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    \tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n    \tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    \tLL     = LBA Low (was: Sector Number) Register     ]\n    \tDV     = Device (was: Device/Head) Register\n    \tDC     = Device Control Register\n    \tER     = Error register\n    \tST     = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    \n    Error 1 [0] occurred at disk power-on lifetime: 156 hours (6 days + 12 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      40 -- 51 08 00 00 00 00 00 bd 60 40 00  Error: UNC 2048 sectors at LBA = 0x0000bd60 = 48480\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      25 00 00 08 00 00 00 00 00 b8 00 40 00     00:00:41.901  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 a0 00 40 00     00:00:41.766  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 90 00 40 00     00:00:41.653  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 98 00 40 00     00:00:41.650  READ DMA EXT\n      25 00 00 00 08 00 00 00 00 10 00 40 00     00:00:41.625  READ DMA EXT\n    \n    SMART Extended Self-test Log Version: 1 (1 sectors)\n    No self-tests have been logged.  [To run self-tests, use: smartctl -t]\n    \n    SMART Selective self-test log data structure revision number 1\n     SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n        1        0        0  Not_testing\n        2        0        0  Not_testing\n        3        0        0  Not_testing\n        4        0        0  Not_testing\n        5        0        0  Not_testing\n    Selective self-test flags (0x0):\n      After scanning selected spans, do NOT read-scan remainder of disk.\n    If Selective self-test is pending on power-up, resume after 0 minute delay.\n    \n    SCT Status Version:                  3\n    SCT Version (vendor specific):       258 (0x0102)\n    Device State:                        Active (0)\n    Current Temperature:                    23 Celsius\n    Power Cycle Min/Max Temperature:     23/23 Celsius\n    Lifetime    Min/Max Temperature:     17/49 Celsius\n    Under/Over Temperature Limit Count:   0/0\n    Vendor specific:\n    01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n    \n    SCT Temperature History Version:     2\n    Temperature Sampling Period:         1 minute\n    Temperature Logging Interval:        1 minute\n    Min/Max recommended Temperature:      0/65 Celsius\n    Min/Max Temperature Limit:           -41/85 Celsius\n    Temperature History Size (Index):    478 (98)\n    \n    Index    Estimated Time   Temperature Celsius\n      99    2024-01-06 14:06    26  *******\n     100    2024-01-06 14:07    27  ********\n     ...    ..(  2 skipped).    ..  ********\n     103    2024-01-06 14:10    27  ********\n     104    2024-01-06 14:11    28  *********\n     ...    ..(  3 skipped).    ..  *********\n     108    2024-01-06 14:15    28  *********\n     109    2024-01-06 14:16    29  **********\n     ...    ..(  4 skipped).    ..  **********\n     114    2024-01-06 14:21    29  **********\n     115    2024-01-06 14:22    30  ***********\n     ...    ..(  4 skipped).    ..  ***********\n     120    2024-01-06 14:27    30  ***********\n     121    2024-01-06 14:28     ?  -\n     122    2024-01-06 14:29    22  ***\n     123    2024-01-06 14:30    22  ***\n     124    2024-01-06 14:31    23  ****\n     125    2024-01-06 14:32    23  ****\n     126    2024-01-06 14:33     ?  -\n     127    2024-01-06 14:34    22  ***\n     128    2024-01-06 14:35    22  ***\n     129    2024-01-06 14:36    22  ***\n     130    2024-01-06 14:37    23  ****\n     131    2024-01-06 14:38    24  *****\n     132    2024-01-06 14:39    24  *****\n     133    2024-01-06 14:40    25  ******\n     134    2024-01-06 14:41    25  ******\n     135    2024-01-06 14:42    25  ******\n     136    2024-01-06 14:43    26  *******\n     ...    ..(  3 skipped).    ..  *******\n     140    2024-01-06 14:47    26  *******\n     141    2024-01-06 14:48    27  ********\n     ...    ..(  3 skipped).    ..  ********\n     145    2024-01-06 14:52    27  ********\n     146    2024-01-06 14:53    28  *********\n     ...    ..(  3 skipped).    ..  *********\n     150    2024-01-06 14:57    28  *********\n     151    2024-01-06 14:58    29  **********\n     ...    ..(  6 skipped).    ..  **********\n     158    2024-01-06 15:05    29  **********\n     159    2024-01-06 15:06     ?  -\n     160    2024-01-06 15:07    17  -\n     161    2024-01-06 15:08    17  -\n     162    2024-01-06 15:09    17  -\n     163    2024-01-06 15:10     ?  -\n     164    2024-01-06 15:11    19  -\n     165    2024-01-06 15:12    19  -\n     166    2024-01-06 15:13    20  *\n     167    2024-01-06 15:14    20  *\n     168    2024-01-06 15:15    21  **\n     169    2024-01-06 15:16    21  **\n     170    2024-01-06 15:17    22  ***\n     171    2024-01-06 15:18    22  ***\n     172    2024-01-06 15:19    23  ****\n     173    2024-01-06 15:20    23  ****\n     174    2024-01-06 15:21    23  ****\n     175    2024-01-06 15:22    24  *****\n     176    2024-01-06 15:23    25  ******\n     177    2024-01-06 15:24    25  ******\n     178    2024-01-06 15:25    25  ******\n     179    2024-01-06 15:26    26  *******\n     ...    ..(  2 skipped).    ..  *******\n     182    2024-01-06 15:29    26  *******\n     183    2024-01-06 15:30    27  ********\n     184    2024-01-06 15:31    27  ********\n     185    2024-01-06 15:32    27  ********\n     186    2024-01-06 15:33    28  *********\n     187    2024-01-06 15:34    28  *********\n     188    2024-01-06 15:35    28  *********\n     189    2024-01-06 15:36    29  **********\n     ...    ..(  3 skipped).    ..  **********\n     193    2024-01-06 15:40    29  **********\n     194    2024-01-06 15:41    30  ***********\n     ...    ..(  3 skipped).    ..  ***********\n     198    2024-01-06 15:45    30  ***********\n     199    2024-01-06 15:46    31  ************\n     ...    ..(  4 skipped).    ..  ************\n     204    2024-01-06 15:51    31  ************\n     205    2024-01-06 15:52    32  *************\n     ...    ..(  5 skipped).    ..  *************\n     211    2024-01-06 15:58    32  *************\n     212    2024-01-06 15:59    33  **************\n     ...    ..(  6 skipped).    ..  **************\n     219    2024-01-06 16:06    33  **************\n     220    2024-01-06 16:07    34  ***************\n     ...    ..( 11 skipped).    ..  ***************\n     232    2024-01-06 16:19    34  ***************\n     233    2024-01-06 16:20    35  ****************\n     ...    ..( 15 skipped).    ..  ****************\n     249    2024-01-06 16:36    35  ****************\n     250    2024-01-06 16:37    36  *****************\n     ...    ..( 40 skipped).    ..  *****************\n     291    2024-01-06 17:18    36  *****************\n     292    2024-01-06 17:19    37  ******************\n     293    2024-01-06 17:20    36  *****************\n     294    2024-01-06 17:21    36  *****************\n     295    2024-01-06 17:22    37  ******************\n     296    2024-01-06 17:23    36  *****************\n     ...    ..(  4 skipped).    ..  *****************\n     301    2024-01-06 17:28    36  *****************\n     302    2024-01-06 17:29    37  ******************\n     303    2024-01-06 17:30    37  ******************\n     304    2024-01-06 17:31    36  *****************\n     305    2024-01-06 17:32    36  *****************\n     306    2024-01-06 17:33    37  ******************\n     ...    ..(  2 skipped).    ..  ******************\n     309    2024-01-06 17:36    37  ******************\n     310    2024-01-06 17:37    36  *****************\n     311    2024-01-06 17:38    37  ******************\n     312    2024-01-06 17:39     ?  -\n     313    2024-01-06 17:40    18  -\n     314    2024-01-06 17:41    18  -\n     315    2024-01-06 17:42    18  -\n     316    2024-01-06 17:43    19  -\n     317    2024-01-06 17:44    19  -\n     318    2024-01-06 17:45    20  *\n     319    2024-01-06 17:46    20  *\n     320    2024-01-06 17:47    21  **\n     321    2024-01-06 17:48    21  **\n     322    2024-01-06 17:49    21  **\n     323    2024-01-06 17:50     ?  -\n     324    2024-01-06 17:51    22  ***\n     325    2024-01-06 17:52    22  ***\n     326    2024-01-06 17:53     ?  -\n     327    2024-01-06 17:54    22  ***\n     328    2024-01-06 17:55    22  ***\n     329    2024-01-06 17:56    22  ***\n     330    2024-01-06 17:57    23  ****\n     331    2024-01-06 17:58    23  ****\n     332    2024-01-06 17:59     ?  -\n     333    2024-01-06 18:00    23  ****\n     334    2024-01-06 18:01    24  *****\n     335    2024-01-06 18:02    25  ******\n     336    2024-01-06 18:03    25  ******\n     337    2024-01-06 18:04    25  ******\n     338    2024-01-06 18:05    26  *******\n     339    2024-01-06 18:06    26  *******\n     340    2024-01-06 18:07     ?  -\n     341    2024-01-06 18:08    25  ******\n     ...    ..(  2 skipped).    ..  ******\n     344    2024-01-06 18:11    25  ******\n     345    2024-01-06 18:12    26  *******\n     346    2024-01-06 18:13     ?  -\n     347    2024-01-06 18:14    23  ****\n     348    2024-01-06 18:15    23  ****\n     349    2024-01-06 18:16    38  *******************\n     ...    ..( 46 skipped).    ..  *******************\n     396    2024-01-06 19:03    38  *******************\n     397    2024-01-06 19:04     ?  -\n     398    2024-01-06 19:05    23  ****\n     399    2024-01-06 19:06    23  ****\n     400    2024-01-06 19:07     ?  -\n     401    2024-01-06 19:08    24  *****\n     402    2024-01-06 19:09    24  *****\n     403    2024-01-06 19:10    24  *****\n     404    2024-01-06 19:11    25  ******\n     405    2024-01-06 19:12    25  ******\n     406    2024-01-06 19:13    25  ******\n     407    2024-01-06 19:14    26  *******\n     408    2024-01-06 19:15    26  *******\n     409    2024-01-06 19:16    27  ********\n     410    2024-01-06 19:17    27  ********\n     411    2024-01-06 19:18    28  *********\n     412    2024-01-06 19:19    28  *********\n     413    2024-01-06 19:20    28  *********\n     414    2024-01-06 19:21    29  **********\n     415    2024-01-06 19:22    29  **********\n     416    2024-01-06 19:23    29  **********\n     417    2024-01-06 19:24    30  ***********\n     418    2024-01-06 19:25    30  ***********\n     419    2024-01-06 19:26    30  ***********\n     420    2024-01-06 19:27    31  ************\n     421    2024-01-06 19:28    31  ************\n     422    2024-01-06 19:29    31  ************\n     423    2024-01-06 19:30     ?  -\n     424    2024-01-06 19:31    29  **********\n     ...    ..(  2 skipped).    ..  **********\n     427    2024-01-06 19:34    29  **********\n     428    2024-01-06 19:35    30  ***********\n     429    2024-01-06 19:36    30  ***********\n     430    2024-01-06 19:37    31  ************\n     ...    ..(  2 skipped).    ..  ************\n     433    2024-01-06 19:40    31  ************\n     434    2024-01-06 19:41    32  *************\n     ...    ..(  5 skipped).    ..  *************\n     440    2024-01-06 19:47    32  *************\n     441    2024-01-06 19:48     ?  -\n     442    2024-01-06 19:49    20  *\n     443    2024-01-06 19:50    20  *\n     444    2024-01-06 19:51    20  *\n     445    2024-01-06 19:52    21  **\n     446    2024-01-06 19:53    21  **\n     447    2024-01-06 19:54    22  ***\n     448    2024-01-06 19:55    22  ***\n     449    2024-01-06 19:56    23  ****\n     450    2024-01-06 19:57    23  ****\n     451    2024-01-06 19:58    24  *****\n     452    2024-01-06 19:59    24  *****\n     453    2024-01-06 20:00    25  ******\n     454    2024-01-06 20:01     ?  -\n     455    2024-01-06 20:02    25  ******\n     456    2024-01-06 20:03    25  ******\n     457    2024-01-06 20:04    25  ******\n     458    2024-01-06 20:05    26  *******\n     459    2024-01-06 20:06    26  *******\n     460    2024-01-06 20:07    26  *******\n     461    2024-01-06 20:08    27  ********\n     462    2024-01-06 20:09    27  ********\n     463    2024-01-06 20:10    27  ********\n     464    2024-01-06 20:11    28  *********\n     ...    ..(  2 skipped).    ..  *********\n     467    2024-01-06 20:14    28  *********\n     468    2024-01-06 20:15    29  **********\n     ...    ..(  2 skipped).    ..  **********\n     471    2024-01-06 20:18    29  **********\n     472    2024-01-06 20:19    30  ***********\n     ...    ..(  2 skipped).    ..  ***********\n     475    2024-01-06 20:22    30  ***********\n     476    2024-01-06 20:23    31  ************\n     ...    ..(  2 skipped).    ..  ************\n       1    2024-01-06 20:26    31  ************\n       2    2024-01-06 20:27    32  *************\n     ...    ..(  5 skipped).    ..  *************\n       8    2024-01-06 20:33    32  *************\n       9    2024-01-06 20:34    33  **************\n     ...    ..(  5 skipped).    ..  **************\n      15    2024-01-06 20:40    33  **************\n      16    2024-01-06 20:41    34  ***************\n     ...    ..(  8 skipped).    ..  ***************\n      25    2024-01-06 20:50    34  ***************\n      26    2024-01-06 20:51    35  ****************\n     ...    ..(  9 skipped).    ..  ****************\n      36    2024-01-06 21:01    35  ****************\n      37    2024-01-06 21:02    36  *****************\n     ...    ..( 11 skipped).    ..  *****************\n      49    2024-01-06 21:14    36  *****************\n      50    2024-01-06 21:15     ?  -\n      51    2024-01-06 21:16    21  **\n      52    2024-01-06 21:17    21  **\n      53    2024-01-06 21:18    22  ***\n      54    2024-01-06 21:19    22  ***\n      55    2024-01-06 21:20    23  ****\n      56    2024-01-06 21:21    23  ****\n      57    2024-01-06 21:22    24  *****\n      58    2024-01-06 21:23    24  *****\n      59    2024-01-06 21:24    25  ******\n     ...    ..(  2 skipped).    ..  ******\n      62    2024-01-06 21:27    25  ******\n      63    2024-01-06 21:28    26  *******\n     ...    ..(  3 skipped).    ..  *******\n      67    2024-01-06 21:32    26  *******\n      68    2024-01-06 21:33    27  ********\n     ...    ..(  4 skipped).    ..  ********\n      73    2024-01-06 21:38    27  ********\n      74    2024-01-06 21:39    28  *********\n     ...    ..(  3 skipped).    ..  *********\n      78    2024-01-06 21:43    28  *********\n      79    2024-01-06 21:44    29  **********\n     ...    ..(  2 skipped).    ..  **********\n      82    2024-01-06 21:47    29  **********\n      83    2024-01-06 21:48     ?  -\n      84    2024-01-06 21:49    28  *********\n     ...    ..(  2 skipped).    ..  *********\n      87    2024-01-06 21:52    28  *********\n      88    2024-01-06 21:53     ?  -\n      89    2024-01-06 21:54    23  ****\n      90    2024-01-06 21:55    23  ****\n      91    2024-01-06 21:56    23  ****\n      92    2024-01-06 21:57    24  *****\n      93    2024-01-06 21:58    24  *****\n      94    2024-01-06 21:59    25  ******\n      95    2024-01-06 22:00    26  *******\n     ...    ..(  2 skipped).    ..  *******\n      98    2024-01-06 22:03    26  *******\n    \n    SCT Error Recovery Control command not supported\n    \n    Device Statistics (GP Log 0x04)\n    Page  Offset Size        Value Flags Description\n    0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n    0x01  0x008  4            3322  ---  Lifetime Power-On Resets\n    0x01  0x010  4            6317  ---  Power-on Hours\n    0x01  0x018  6      5976322892  ---  Logical Sectors Written\n    0x01  0x020  6         9940809  ---  Number of Write Commands\n    0x01  0x028  6     14678492017  ---  Logical Sectors Read\n    0x01  0x030  6        43174921  ---  Number of Read Commands\n    0x01  0x038  6      1266363520  ---  Date and Time TimeStamp\n    0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n    0x03  0x008  4            6289  ---  Spindle Motor Power-on Hours\n    0x03  0x010  4            3747  ---  Head Flying Hours\n    0x03  0x018  4           15424  ---  Head Load Events\n    0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n    0x03  0x028  4            1071  ---  Read Recovery Attempts\n    0x03  0x030  4               0  ---  Number of Mechanical Start Failures\n    0x03  0x038  4              16  ---  Number of Realloc. Candidate Logical Sectors\n    0x03  0x040  4              18  ---  Number of High Priority Unload Events\n    0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n    0x04  0x008  4               1  ---  Number of Reported Uncorrectable Errors\n    0x04  0x010  4              37  ---  Resets Between Cmd Acceptance and Completion\n    0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n    0x05  0x008  1              23  ---  Current Temperature\n    0x05  0x010  1              39  ---  Average Short Term Temperature\n    0x05  0x018  1              41  ---  Average Long Term Temperature\n    0x05  0x020  1              49  ---  Highest Temperature\n    0x05  0x028  1              22  ---  Lowest Temperature\n    0x05  0x030  1              44  ---  Highest Average Short Term Temperature\n    0x05  0x038  1              30  ---  Lowest Average Short Term Temperature\n    0x05  0x040  1              41  ---  Highest Average Long Term Temperature\n    0x05  0x048  1              34  ---  Lowest Average Long Term Temperature\n    0x05  0x050  4               0  ---  Time in Over-Temperature\n    0x05  0x058  1              65  ---  Specified Maximum Operating Temperature\n    0x05  0x060  4               0  ---  Time in Under-Temperature\n    0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n    0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n    0x06  0x008  4            3365  ---  Number of Hardware Resets\n    0x06  0x010  4              42  ---  Number of ASR Events\n    0x06  0x018  4               0  ---  Number of Interface CRC Errors\n    0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n    0xff  0x008  7               0  ---  Vendor Specific\n    0xff  0x010  7               0  ---  Vendor Specific\n    0xff  0x018  7               0  ---  Vendor Specific\n                                    |||_ C monitored condition met\n                                    ||__ D supports DSN\n                                    |___ N normalized value\n    \n    Pending Defects log (GP Log 0x0c)\n    No Defects Logged\n    \n    SATA Phy Event Counters (GP Log 0x11)\n    ID      Size     Value  Description\n    0x0001  2            0  Command failed due to ICRC error\n    0x0002  2            0  R_ERR response for data FIS\n    0x0003  2            0  R_ERR response for device-to-host data FIS\n    0x0004  2            0  R_ERR response for host-to-device data FIS\n    0x0005  2            0  R_ERR response for non-data FIS\n    0x0006  2            0  R_ERR response for device-to-host non-data FIS\n    0x0007  2            0  R_ERR response for host-to-device non-data FIS\n    0x0008  2            0  Device-to-host non-data FIS retries\n    0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy\n    0x000a  2            1  Device-to-host register FISes sent due to a COMRESET\n    0x000b  2            0  CRC errors within host-to-device FIS\n    0x000d  2            0  Non-CRC errors within host-to-device FIS\n    0x000f  2            0  R_ERR response for host-to-device data FIS, CRC\n    0x0012  2            0  R_ERR response for host-to-device non-data FIS, CRC\n    0x8000  4           96  Vendor specific\n    \n    \n\n&amp;#x200B;", "author_fullname": "t2_6e895hxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a normal HDD sound?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"9rl122l5oyac1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/190lp4w/asset/9rl122l5oyac1/DASHPlaylist.mpd?a=1707208702%2CMDk1NGI0ZmZiNGI1YzllYTZhYTc5MzhhNDIwOGQ0OTMzOTNhYjNlNGY0NzkxNDc3MWIyYzYwMTY4YzgzMjNiZA%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 1920, "hlsUrl": "https://v.redd.it/link/190lp4w/asset/9rl122l5oyac1/HLSPlaylist.m3u8?a=1707208702%2CZWY4MDE0OTUyZTVmNDBlNWI0Nzk3Mzk0OTI4ODBkZjhlYzExZTcxZmIxNWYyMjQ3ZWUxNjliZTNkMGVhZWJjOA%3D%3D&amp;v=1&amp;f=sd", "id": "9rl122l5oyac1", "isGif": false}}, "name": "t3_190lp4w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704608543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I have been storing some of my data on a WD My Book External HDD. I never really thought about the beeping noise it makes until now. Is this drive dying? Should I get another drive?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/190lp4w/video/9rl122l5oyac1/player\"&gt;My Drive making a weird Beeping noise.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the SmartCTL: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-13-amd64] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     WDC WD40EDAZ-11SLVB0\nSerial Number:    WD-WX12DC13FRYS\nLU WWN Device Id: 5 0014ee 214ebda6c\nFirmware Version: 80.00A80\nUser Capacity:    4,000,787,030,016 bytes [4.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    5400 rpm\nForm Factor:      3.5 inches\nTRIM Command:     Available, deterministic, zeroed\nDevice is:        Not in smartctl database 7.3/5319\nATA Version is:   ACS-3 T13/2161-D revision 5\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Sat Jan  6 22:03:38 2024 PST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Unavailable\nAPM feature is:   Unavailable\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, NOT FROZEN [SEC1]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x82) Offline data collection activity\n                    was completed without error.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                    without error or no self-test has ever \n                    been run.\nTotal time to complete Offline \ndata collection:        (48104) seconds.\nOffline data collection\ncapabilities:            (0x7b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 186) minutes.\nConveyance self-test routine\nrecommended polling time:    (   2) minutes.\nSCT capabilities:          (0x3031) SCT Status supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     POSR-K   200   200   051    -    0\n  3 Spin_Up_Time            POS--K   209   204   021    -    2516\n  4 Start_Stop_Count        -O--CK   097   097   000    -    3322\n  5 Reallocated_Sector_Ct   PO--CK   200   200   140    -    0\n  7 Seek_Error_Rate         -OSR-K   200   200   000    -    4\n  9 Power_On_Hours          -O--CK   092   092   000    -    6317\n 10 Spin_Retry_Count        -O--CK   100   100   000    -    0\n 11 Calibration_Retry_Count -O--CK   100   100   000    -    0\n 12 Power_Cycle_Count       -O--CK   097   097   000    -    3322\n192 Power-Off_Retract_Count -O--CK   200   200   000    -    18\n193 Load_Cycle_Count        -O--CK   195   195   000    -    15405\n194 Temperature_Celsius     -O---K   124   098   000    -    23\n196 Reallocated_Event_Count -O--CK   200   200   000    -    0\n197 Current_Pending_Sector  -O--CK   200   200   000    -    1\n198 Offline_Uncorrectable   ----CK   200   200   000    -    0\n199 UDMA_CRC_Error_Count    -O--CK   200   200   000    -    0\n200 Multi_Zone_Error_Rate   ---R--   200   200   000    -    0\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      5  Comprehensive SMART error log\n0x03       GPL     R/O      6  Ext. Comprehensive SMART error log\n0x04       GPL     R/O    256  Device Statistics log\n0x04       SL      R/O      8  Device Statistics log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      1  Extended self-test log\n0x09           SL  R/W      1  Selective self-test log\n0x0c       GPL     R/O   2048  Pending Defects log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x24       GPL     R/O    294  Current Device Internal Status Data log\n0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xa0-0xa7  GPL,SL  VS      16  Device vendor specific log\n0xa8-0xb6  GPL,SL  VS       1  Device vendor specific log\n0xb7       GPL,SL  VS      78  Device vendor specific log\n0xb9       GPL,SL  VS       4  Device vendor specific log\n0xbd       GPL,SL  VS       1  Device vendor specific log\n0xc0       GPL,SL  VS       1  Device vendor specific log\n0xc1       GPL     VS      93  Device vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (6 sectors)\nDevice Error Count: 1\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 1 [0] occurred at disk power-on lifetime: 156 hours (6 days + 12 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 51 08 00 00 00 00 00 bd 60 40 00  Error: UNC 2048 sectors at LBA = 0x0000bd60 = 48480\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  25 00 00 08 00 00 00 00 00 b8 00 40 00     00:00:41.901  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 a0 00 40 00     00:00:41.766  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 90 00 40 00     00:00:41.653  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 98 00 40 00     00:00:41.650  READ DMA EXT\n  25 00 00 00 08 00 00 00 00 10 00 40 00     00:00:41.625  READ DMA EXT\n\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNo self-tests have been logged.  [To run self-tests, use: smartctl -t]\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  3\nSCT Version (vendor specific):       258 (0x0102)\nDevice State:                        Active (0)\nCurrent Temperature:                    23 Celsius\nPower Cycle Min/Max Temperature:     23/23 Celsius\nLifetime    Min/Max Temperature:     17/49 Celsius\nUnder/Over Temperature Limit Count:   0/0\nVendor specific:\n01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         1 minute\nTemperature Logging Interval:        1 minute\nMin/Max recommended Temperature:      0/65 Celsius\nMin/Max Temperature Limit:           -41/85 Celsius\nTemperature History Size (Index):    478 (98)\n\nIndex    Estimated Time   Temperature Celsius\n  99    2024-01-06 14:06    26  *******\n 100    2024-01-06 14:07    27  ********\n ...    ..(  2 skipped).    ..  ********\n 103    2024-01-06 14:10    27  ********\n 104    2024-01-06 14:11    28  *********\n ...    ..(  3 skipped).    ..  *********\n 108    2024-01-06 14:15    28  *********\n 109    2024-01-06 14:16    29  **********\n ...    ..(  4 skipped).    ..  **********\n 114    2024-01-06 14:21    29  **********\n 115    2024-01-06 14:22    30  ***********\n ...    ..(  4 skipped).    ..  ***********\n 120    2024-01-06 14:27    30  ***********\n 121    2024-01-06 14:28     ?  -\n 122    2024-01-06 14:29    22  ***\n 123    2024-01-06 14:30    22  ***\n 124    2024-01-06 14:31    23  ****\n 125    2024-01-06 14:32    23  ****\n 126    2024-01-06 14:33     ?  -\n 127    2024-01-06 14:34    22  ***\n 128    2024-01-06 14:35    22  ***\n 129    2024-01-06 14:36    22  ***\n 130    2024-01-06 14:37    23  ****\n 131    2024-01-06 14:38    24  *****\n 132    2024-01-06 14:39    24  *****\n 133    2024-01-06 14:40    25  ******\n 134    2024-01-06 14:41    25  ******\n 135    2024-01-06 14:42    25  ******\n 136    2024-01-06 14:43    26  *******\n ...    ..(  3 skipped).    ..  *******\n 140    2024-01-06 14:47    26  *******\n 141    2024-01-06 14:48    27  ********\n ...    ..(  3 skipped).    ..  ********\n 145    2024-01-06 14:52    27  ********\n 146    2024-01-06 14:53    28  *********\n ...    ..(  3 skipped).    ..  *********\n 150    2024-01-06 14:57    28  *********\n 151    2024-01-06 14:58    29  **********\n ...    ..(  6 skipped).    ..  **********\n 158    2024-01-06 15:05    29  **********\n 159    2024-01-06 15:06     ?  -\n 160    2024-01-06 15:07    17  -\n 161    2024-01-06 15:08    17  -\n 162    2024-01-06 15:09    17  -\n 163    2024-01-06 15:10     ?  -\n 164    2024-01-06 15:11    19  -\n 165    2024-01-06 15:12    19  -\n 166    2024-01-06 15:13    20  *\n 167    2024-01-06 15:14    20  *\n 168    2024-01-06 15:15    21  **\n 169    2024-01-06 15:16    21  **\n 170    2024-01-06 15:17    22  ***\n 171    2024-01-06 15:18    22  ***\n 172    2024-01-06 15:19    23  ****\n 173    2024-01-06 15:20    23  ****\n 174    2024-01-06 15:21    23  ****\n 175    2024-01-06 15:22    24  *****\n 176    2024-01-06 15:23    25  ******\n 177    2024-01-06 15:24    25  ******\n 178    2024-01-06 15:25    25  ******\n 179    2024-01-06 15:26    26  *******\n ...    ..(  2 skipped).    ..  *******\n 182    2024-01-06 15:29    26  *******\n 183    2024-01-06 15:30    27  ********\n 184    2024-01-06 15:31    27  ********\n 185    2024-01-06 15:32    27  ********\n 186    2024-01-06 15:33    28  *********\n 187    2024-01-06 15:34    28  *********\n 188    2024-01-06 15:35    28  *********\n 189    2024-01-06 15:36    29  **********\n ...    ..(  3 skipped).    ..  **********\n 193    2024-01-06 15:40    29  **********\n 194    2024-01-06 15:41    30  ***********\n ...    ..(  3 skipped).    ..  ***********\n 198    2024-01-06 15:45    30  ***********\n 199    2024-01-06 15:46    31  ************\n ...    ..(  4 skipped).    ..  ************\n 204    2024-01-06 15:51    31  ************\n 205    2024-01-06 15:52    32  *************\n ...    ..(  5 skipped).    ..  *************\n 211    2024-01-06 15:58    32  *************\n 212    2024-01-06 15:59    33  **************\n ...    ..(  6 skipped).    ..  **************\n 219    2024-01-06 16:06    33  **************\n 220    2024-01-06 16:07    34  ***************\n ...    ..( 11 skipped).    ..  ***************\n 232    2024-01-06 16:19    34  ***************\n 233    2024-01-06 16:20    35  ****************\n ...    ..( 15 skipped).    ..  ****************\n 249    2024-01-06 16:36    35  ****************\n 250    2024-01-06 16:37    36  *****************\n ...    ..( 40 skipped).    ..  *****************\n 291    2024-01-06 17:18    36  *****************\n 292    2024-01-06 17:19    37  ******************\n 293    2024-01-06 17:20    36  *****************\n 294    2024-01-06 17:21    36  *****************\n 295    2024-01-06 17:22    37  ******************\n 296    2024-01-06 17:23    36  *****************\n ...    ..(  4 skipped).    ..  *****************\n 301    2024-01-06 17:28    36  *****************\n 302    2024-01-06 17:29    37  ******************\n 303    2024-01-06 17:30    37  ******************\n 304    2024-01-06 17:31    36  *****************\n 305    2024-01-06 17:32    36  *****************\n 306    2024-01-06 17:33    37  ******************\n ...    ..(  2 skipped).    ..  ******************\n 309    2024-01-06 17:36    37  ******************\n 310    2024-01-06 17:37    36  *****************\n 311    2024-01-06 17:38    37  ******************\n 312    2024-01-06 17:39     ?  -\n 313    2024-01-06 17:40    18  -\n 314    2024-01-06 17:41    18  -\n 315    2024-01-06 17:42    18  -\n 316    2024-01-06 17:43    19  -\n 317    2024-01-06 17:44    19  -\n 318    2024-01-06 17:45    20  *\n 319    2024-01-06 17:46    20  *\n 320    2024-01-06 17:47    21  **\n 321    2024-01-06 17:48    21  **\n 322    2024-01-06 17:49    21  **\n 323    2024-01-06 17:50     ?  -\n 324    2024-01-06 17:51    22  ***\n 325    2024-01-06 17:52    22  ***\n 326    2024-01-06 17:53     ?  -\n 327    2024-01-06 17:54    22  ***\n 328    2024-01-06 17:55    22  ***\n 329    2024-01-06 17:56    22  ***\n 330    2024-01-06 17:57    23  ****\n 331    2024-01-06 17:58    23  ****\n 332    2024-01-06 17:59     ?  -\n 333    2024-01-06 18:00    23  ****\n 334    2024-01-06 18:01    24  *****\n 335    2024-01-06 18:02    25  ******\n 336    2024-01-06 18:03    25  ******\n 337    2024-01-06 18:04    25  ******\n 338    2024-01-06 18:05    26  *******\n 339    2024-01-06 18:06    26  *******\n 340    2024-01-06 18:07     ?  -\n 341    2024-01-06 18:08    25  ******\n ...    ..(  2 skipped).    ..  ******\n 344    2024-01-06 18:11    25  ******\n 345    2024-01-06 18:12    26  *******\n 346    2024-01-06 18:13     ?  -\n 347    2024-01-06 18:14    23  ****\n 348    2024-01-06 18:15    23  ****\n 349    2024-01-06 18:16    38  *******************\n ...    ..( 46 skipped).    ..  *******************\n 396    2024-01-06 19:03    38  *******************\n 397    2024-01-06 19:04     ?  -\n 398    2024-01-06 19:05    23  ****\n 399    2024-01-06 19:06    23  ****\n 400    2024-01-06 19:07     ?  -\n 401    2024-01-06 19:08    24  *****\n 402    2024-01-06 19:09    24  *****\n 403    2024-01-06 19:10    24  *****\n 404    2024-01-06 19:11    25  ******\n 405    2024-01-06 19:12    25  ******\n 406    2024-01-06 19:13    25  ******\n 407    2024-01-06 19:14    26  *******\n 408    2024-01-06 19:15    26  *******\n 409    2024-01-06 19:16    27  ********\n 410    2024-01-06 19:17    27  ********\n 411    2024-01-06 19:18    28  *********\n 412    2024-01-06 19:19    28  *********\n 413    2024-01-06 19:20    28  *********\n 414    2024-01-06 19:21    29  **********\n 415    2024-01-06 19:22    29  **********\n 416    2024-01-06 19:23    29  **********\n 417    2024-01-06 19:24    30  ***********\n 418    2024-01-06 19:25    30  ***********\n 419    2024-01-06 19:26    30  ***********\n 420    2024-01-06 19:27    31  ************\n 421    2024-01-06 19:28    31  ************\n 422    2024-01-06 19:29    31  ************\n 423    2024-01-06 19:30     ?  -\n 424    2024-01-06 19:31    29  **********\n ...    ..(  2 skipped).    ..  **********\n 427    2024-01-06 19:34    29  **********\n 428    2024-01-06 19:35    30  ***********\n 429    2024-01-06 19:36    30  ***********\n 430    2024-01-06 19:37    31  ************\n ...    ..(  2 skipped).    ..  ************\n 433    2024-01-06 19:40    31  ************\n 434    2024-01-06 19:41    32  *************\n ...    ..(  5 skipped).    ..  *************\n 440    2024-01-06 19:47    32  *************\n 441    2024-01-06 19:48     ?  -\n 442    2024-01-06 19:49    20  *\n 443    2024-01-06 19:50    20  *\n 444    2024-01-06 19:51    20  *\n 445    2024-01-06 19:52    21  **\n 446    2024-01-06 19:53    21  **\n 447    2024-01-06 19:54    22  ***\n 448    2024-01-06 19:55    22  ***\n 449    2024-01-06 19:56    23  ****\n 450    2024-01-06 19:57    23  ****\n 451    2024-01-06 19:58    24  *****\n 452    2024-01-06 19:59    24  *****\n 453    2024-01-06 20:00    25  ******\n 454    2024-01-06 20:01     ?  -\n 455    2024-01-06 20:02    25  ******\n 456    2024-01-06 20:03    25  ******\n 457    2024-01-06 20:04    25  ******\n 458    2024-01-06 20:05    26  *******\n 459    2024-01-06 20:06    26  *******\n 460    2024-01-06 20:07    26  *******\n 461    2024-01-06 20:08    27  ********\n 462    2024-01-06 20:09    27  ********\n 463    2024-01-06 20:10    27  ********\n 464    2024-01-06 20:11    28  *********\n ...    ..(  2 skipped).    ..  *********\n 467    2024-01-06 20:14    28  *********\n 468    2024-01-06 20:15    29  **********\n ...    ..(  2 skipped).    ..  **********\n 471    2024-01-06 20:18    29  **********\n 472    2024-01-06 20:19    30  ***********\n ...    ..(  2 skipped).    ..  ***********\n 475    2024-01-06 20:22    30  ***********\n 476    2024-01-06 20:23    31  ************\n ...    ..(  2 skipped).    ..  ************\n   1    2024-01-06 20:26    31  ************\n   2    2024-01-06 20:27    32  *************\n ...    ..(  5 skipped).    ..  *************\n   8    2024-01-06 20:33    32  *************\n   9    2024-01-06 20:34    33  **************\n ...    ..(  5 skipped).    ..  **************\n  15    2024-01-06 20:40    33  **************\n  16    2024-01-06 20:41    34  ***************\n ...    ..(  8 skipped).    ..  ***************\n  25    2024-01-06 20:50    34  ***************\n  26    2024-01-06 20:51    35  ****************\n ...    ..(  9 skipped).    ..  ****************\n  36    2024-01-06 21:01    35  ****************\n  37    2024-01-06 21:02    36  *****************\n ...    ..( 11 skipped).    ..  *****************\n  49    2024-01-06 21:14    36  *****************\n  50    2024-01-06 21:15     ?  -\n  51    2024-01-06 21:16    21  **\n  52    2024-01-06 21:17    21  **\n  53    2024-01-06 21:18    22  ***\n  54    2024-01-06 21:19    22  ***\n  55    2024-01-06 21:20    23  ****\n  56    2024-01-06 21:21    23  ****\n  57    2024-01-06 21:22    24  *****\n  58    2024-01-06 21:23    24  *****\n  59    2024-01-06 21:24    25  ******\n ...    ..(  2 skipped).    ..  ******\n  62    2024-01-06 21:27    25  ******\n  63    2024-01-06 21:28    26  *******\n ...    ..(  3 skipped).    ..  *******\n  67    2024-01-06 21:32    26  *******\n  68    2024-01-06 21:33    27  ********\n ...    ..(  4 skipped).    ..  ********\n  73    2024-01-06 21:38    27  ********\n  74    2024-01-06 21:39    28  *********\n ...    ..(  3 skipped).    ..  *********\n  78    2024-01-06 21:43    28  *********\n  79    2024-01-06 21:44    29  **********\n ...    ..(  2 skipped).    ..  **********\n  82    2024-01-06 21:47    29  **********\n  83    2024-01-06 21:48     ?  -\n  84    2024-01-06 21:49    28  *********\n ...    ..(  2 skipped).    ..  *********\n  87    2024-01-06 21:52    28  *********\n  88    2024-01-06 21:53     ?  -\n  89    2024-01-06 21:54    23  ****\n  90    2024-01-06 21:55    23  ****\n  91    2024-01-06 21:56    23  ****\n  92    2024-01-06 21:57    24  *****\n  93    2024-01-06 21:58    24  *****\n  94    2024-01-06 21:59    25  ******\n  95    2024-01-06 22:00    26  *******\n ...    ..(  2 skipped).    ..  *******\n  98    2024-01-06 22:03    26  *******\n\nSCT Error Recovery Control command not supported\n\nDevice Statistics (GP Log 0x04)\nPage  Offset Size        Value Flags Description\n0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n0x01  0x008  4            3322  ---  Lifetime Power-On Resets\n0x01  0x010  4            6317  ---  Power-on Hours\n0x01  0x018  6      5976322892  ---  Logical Sectors Written\n0x01  0x020  6         9940809  ---  Number of Write Commands\n0x01  0x028  6     14678492017  ---  Logical Sectors Read\n0x01  0x030  6        43174921  ---  Number of Read Commands\n0x01  0x038  6      1266363520  ---  Date and Time TimeStamp\n0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n0x03  0x008  4            6289  ---  Spindle Motor Power-on Hours\n0x03  0x010  4            3747  ---  Head Flying Hours\n0x03  0x018  4           15424  ---  Head Load Events\n0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n0x03  0x028  4            1071  ---  Read Recovery Attempts\n0x03  0x030  4               0  ---  Number of Mechanical Start Failures\n0x03  0x038  4              16  ---  Number of Realloc. Candidate Logical Sectors\n0x03  0x040  4              18  ---  Number of High Priority Unload Events\n0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n0x04  0x008  4               1  ---  Number of Reported Uncorrectable Errors\n0x04  0x010  4              37  ---  Resets Between Cmd Acceptance and Completion\n0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n0x05  0x008  1              23  ---  Current Temperature\n0x05  0x010  1              39  ---  Average Short Term Temperature\n0x05  0x018  1              41  ---  Average Long Term Temperature\n0x05  0x020  1              49  ---  Highest Temperature\n0x05  0x028  1              22  ---  Lowest Temperature\n0x05  0x030  1              44  ---  Highest Average Short Term Temperature\n0x05  0x038  1              30  ---  Lowest Average Short Term Temperature\n0x05  0x040  1              41  ---  Highest Average Long Term Temperature\n0x05  0x048  1              34  ---  Lowest Average Long Term Temperature\n0x05  0x050  4               0  ---  Time in Over-Temperature\n0x05  0x058  1              65  ---  Specified Maximum Operating Temperature\n0x05  0x060  4               0  ---  Time in Under-Temperature\n0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n0x06  0x008  4            3365  ---  Number of Hardware Resets\n0x06  0x010  4              42  ---  Number of ASR Events\n0x06  0x018  4               0  ---  Number of Interface CRC Errors\n0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n0xff  0x008  7               0  ---  Vendor Specific\n0xff  0x010  7               0  ---  Vendor Specific\n0xff  0x018  7               0  ---  Vendor Specific\n                                |||_ C monitored condition met\n                                ||__ D supports DSN\n                                |___ N normalized value\n\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  2            0  Command failed due to ICRC error\n0x0002  2            0  R_ERR response for data FIS\n0x0003  2            0  R_ERR response for device-to-host data FIS\n0x0004  2            0  R_ERR response for host-to-device data FIS\n0x0005  2            0  R_ERR response for non-data FIS\n0x0006  2            0  R_ERR response for device-to-host non-data FIS\n0x0007  2            0  R_ERR response for host-to-device non-data FIS\n0x0008  2            0  Device-to-host non-data FIS retries\n0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  2            1  Device-to-host register FISes sent due to a COMRESET\n0x000b  2            0  CRC errors within host-to-device FIS\n0x000d  2            0  Non-CRC errors within host-to-device FIS\n0x000f  2            0  R_ERR response for host-to-device data FIS, CRC\n0x0012  2            0  R_ERR response for host-to-device non-data FIS, CRC\n0x8000  4           96  Vendor specific\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190lp4w", "is_robot_indexable": true, "report_reasons": null, "author": "PitBikeViper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190lp4w/is_this_a_normal_hdd_sound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190lp4w/is_this_a_normal_hdd_sound/", "subreddit_subscribers": 723434, "created_utc": 1704608543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Plz delete if wrong sub...\n\nI there a legal way to save movies, TV shows, etc. locally?  Should I buy DVDs and rip them (if so, what program to rip?).  I haven't used torrents in a while and having the ISP interrupt the service would be a real problem at this point in life.", "author_fullname": "t2_sh0pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get movie files on computer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190lasy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704607165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Plz delete if wrong sub...&lt;/p&gt;\n\n&lt;p&gt;I there a legal way to save movies, TV shows, etc. locally?  Should I buy DVDs and rip them (if so, what program to rip?).  I haven&amp;#39;t used torrents in a while and having the ISP interrupt the service would be a real problem at this point in life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190lasy", "is_robot_indexable": true, "report_reasons": null, "author": "WeWillFigureItOut", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190lasy/how_to_get_movie_files_on_computer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190lasy/how_to_get_movie_files_on_computer/", "subreddit_subscribers": 723434, "created_utc": 1704607165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to figure out my options here for a basic and budget 4 bay NAS setup. At present I have an old WD 2 bay NAS with two 6TB drives in a Raid0 (I know, I know) that is running out of space and am looking to replace. Separately, I have a Lenovo M93P Tiny that is running Ubuntu as a headless server doing the usual home server stuff that is barely stressed the majority of the time.\n\nWas originally thinking of just getting a 4 bay DAS, throw in 4 8tb drives in a Raid5, and attach it via USB3.0. Problem one, any recommendations on a DAS? problem two, any recommendations on software that would run on Ubuntu to manage it?\n\nWould it be easier to just get a RaspberryPi and run TrueNAS on it?", "author_fullname": "t2_48p5okg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best homebrew NAS setup using DAS? Already have mini pc (no drive bays) running Ubuntu.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190jwgn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704602658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to figure out my options here for a basic and budget 4 bay NAS setup. At present I have an old WD 2 bay NAS with two 6TB drives in a Raid0 (I know, I know) that is running out of space and am looking to replace. Separately, I have a Lenovo M93P Tiny that is running Ubuntu as a headless server doing the usual home server stuff that is barely stressed the majority of the time.&lt;/p&gt;\n\n&lt;p&gt;Was originally thinking of just getting a 4 bay DAS, throw in 4 8tb drives in a Raid5, and attach it via USB3.0. Problem one, any recommendations on a DAS? problem two, any recommendations on software that would run on Ubuntu to manage it?&lt;/p&gt;\n\n&lt;p&gt;Would it be easier to just get a RaspberryPi and run TrueNAS on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190jwgn", "is_robot_indexable": true, "report_reasons": null, "author": "GiggityYay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190jwgn/best_homebrew_nas_setup_using_das_already_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190jwgn/best_homebrew_nas_setup_using_das_already_have/", "subreddit_subscribers": 723434, "created_utc": 1704602658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, i have many years of my family's videos on both of these. What would be the best way to convert each of these to digital? I will buy whatever products are required to get good output. Thank you.", "author_fullname": "t2_2v5p7f69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to convert VHS and DV to Digital, help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_190gjpe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Co2WmKcgN6jf_BMH1Y8_4PsZ6XmJu1rn1JnJA6p5yHM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704592292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have many years of my family&amp;#39;s videos on both of these. What would be the best way to convert each of these to digital? I will buy whatever products are required to get good output. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/2sxwexs1cxac1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/2sxwexs1cxac1.png?auto=webp&amp;s=965cd6df8131754112cbb10a55cafbe056bda637", "width": 1080, "height": 810}, "resolutions": [{"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=29b93b89ad64ab8c7c2b7d06b7289e61d5cea612", "width": 108, "height": 81}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=262e257432da8a00d582e2d937d820c94cb63dcd", "width": 216, "height": 162}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a7711c7becb3f692d5814fd7451646742b28bc1", "width": 320, "height": 240}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0379f297d720fd76ad630f06af017cdbaa109d4e", "width": 640, "height": 480}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbb5b14972f730890c0d843b6ef30bd2a34645b2", "width": 960, "height": 720}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82e88f9a9ffe83e389308263ea5989f00cd293f2", "width": 1080, "height": 810}], "variants": {}, "id": "a7JXjFFfk1yUsU-bn34R0dOfkSWpip6dzlw-xuIUD9c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190gjpe", "is_robot_indexable": true, "report_reasons": null, "author": "Xcedia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190gjpe/need_to_convert_vhs_and_dv_to_digital_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/2sxwexs1cxac1.png", "subreddit_subscribers": 723434, "created_utc": 1704592292.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to have rsync check the content of a file before synch? Like a go ahead to ensure that the directory is not encrypted by ransomware", "author_fullname": "t2_hrtc9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ransomware verification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190dlnm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704584043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to have rsync check the content of a file before synch? Like a go ahead to ensure that the directory is not encrypted by ransomware&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190dlnm", "is_robot_indexable": true, "report_reasons": null, "author": "py2gb", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190dlnm/ransomware_verification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190dlnm/ransomware_verification/", "subreddit_subscribers": 723434, "created_utc": 1704584043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I delete and send again new files that are 115-130 GB at least once a week, how long would this SSD last before degrading?", "author_fullname": "t2_vfl7y92b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long will Samsung T7 Shield 1TB SSD last?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190cc7j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704580728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I delete and send again new files that are 115-130 GB at least once a week, how long would this SSD last before degrading?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190cc7j", "is_robot_indexable": true, "report_reasons": null, "author": "leniwsek", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190cc7j/how_long_will_samsung_t7_shield_1tb_ssd_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190cc7j/how_long_will_samsung_t7_shield_1tb_ssd_last/", "subreddit_subscribers": 723434, "created_utc": 1704580728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, \n\nI\u2019m looking into buying my first DAS. Most of you are probably going to recommend going for a NAS, and Ideally I would like a NAS but the sort of thing I would want is out of my price range right now so my plan is to get a DAS for now then buy a NAS later down the line and use this DAS as a backup for that. (I would need a large capacity backup at some point anyway)\n\nMy question is, is it worth me buying an enclosure with 10gbps capability or will I not notice any difference in speed compared to a 5gbps enclosure, seeing as I\u2019ll be using 5400rpm drives, which max out at around 150MBps. I\u2019m planning on having 3 x 8-12tb seagate ironwolf or exos drives in a raid 5 configuration to begin, then add more drives as I need them later for a total of 6. So with the combined speed I presume the max speed I will ever get is roughly 725MBps? (5 x 150MBps, 1 x drive for parity). But realistically I won\u2019t need those extra drives for a long time so will be running off 3 or 4 drives for a while. So let\u2019s say 450MBps total with 4 drives, which the 5gbps enclosure can handle right? I had my eye on the terramaster D6-320 for the 10gbps enclosure, until i realised the transfer speeds would be restricted by the speed of the actual drives and i could get something for half the price if i go with a 5gbps enclosure. Is there anything I\u2019m missing here though?\n\nI have a 2017 iMac with thunderbolt 3 ports and I want to use this DAS to edit music and 4k video files from and also to have somewhere to put all my files in one place. My current setup is just a bunch of old external hard drives I\u2019ve collected over the years. I used to export the footage from my sd cards directly onto my Mac and edit from there, then back the files up to an external drive once I was finished, but my Mac is getting full and now I edit in 4k so I don\u2019t have the space for that anymore. Plus it\u2019s just a mess having multiple hdds, some of which have old footage on which I want to use for future edits and that means moving my files off those slow drives to do so. I haven\u2019t done much editing over the past few years but recently decided to get back into it and one of my external drives is struggling to boot so I need to pull my data off it and figured it\u2019s time to sort this mess out once and for all, rather than buying another external hdd and adding to the mess!\n\nI\u2019m also looking into what raid software is best to use with a DAS but not sure if that requires a new post? Softraid seems to be the best but it costs a fortune, then I can\u2019t see hardly any other options. I\u2019m not really keen on the look of terramaster\u2019s software, have any of you used it and what\u2019s it like? The main thing I need is the ability to clearly monitor the condition of my drives and the option to add more drives to a raid 5 when required. \n\nI think I have a good idea of what I need now but just wanted to run it past people who actually know what they\u2019re talking about before I drop a tonne of money on it. Any advise or recommendations would be appreciated. \n\nThanks", "author_fullname": "t2_5o6x09iy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5gbps vs 10gbps DAS with 5400rpm drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1909biz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704572993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking into buying my first DAS. Most of you are probably going to recommend going for a NAS, and Ideally I would like a NAS but the sort of thing I would want is out of my price range right now so my plan is to get a DAS for now then buy a NAS later down the line and use this DAS as a backup for that. (I would need a large capacity backup at some point anyway)&lt;/p&gt;\n\n&lt;p&gt;My question is, is it worth me buying an enclosure with 10gbps capability or will I not notice any difference in speed compared to a 5gbps enclosure, seeing as I\u2019ll be using 5400rpm drives, which max out at around 150MBps. I\u2019m planning on having 3 x 8-12tb seagate ironwolf or exos drives in a raid 5 configuration to begin, then add more drives as I need them later for a total of 6. So with the combined speed I presume the max speed I will ever get is roughly 725MBps? (5 x 150MBps, 1 x drive for parity). But realistically I won\u2019t need those extra drives for a long time so will be running off 3 or 4 drives for a while. So let\u2019s say 450MBps total with 4 drives, which the 5gbps enclosure can handle right? I had my eye on the terramaster D6-320 for the 10gbps enclosure, until i realised the transfer speeds would be restricted by the speed of the actual drives and i could get something for half the price if i go with a 5gbps enclosure. Is there anything I\u2019m missing here though?&lt;/p&gt;\n\n&lt;p&gt;I have a 2017 iMac with thunderbolt 3 ports and I want to use this DAS to edit music and 4k video files from and also to have somewhere to put all my files in one place. My current setup is just a bunch of old external hard drives I\u2019ve collected over the years. I used to export the footage from my sd cards directly onto my Mac and edit from there, then back the files up to an external drive once I was finished, but my Mac is getting full and now I edit in 4k so I don\u2019t have the space for that anymore. Plus it\u2019s just a mess having multiple hdds, some of which have old footage on which I want to use for future edits and that means moving my files off those slow drives to do so. I haven\u2019t done much editing over the past few years but recently decided to get back into it and one of my external drives is struggling to boot so I need to pull my data off it and figured it\u2019s time to sort this mess out once and for all, rather than buying another external hdd and adding to the mess!&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also looking into what raid software is best to use with a DAS but not sure if that requires a new post? Softraid seems to be the best but it costs a fortune, then I can\u2019t see hardly any other options. I\u2019m not really keen on the look of terramaster\u2019s software, have any of you used it and what\u2019s it like? The main thing I need is the ability to clearly monitor the condition of my drives and the option to add more drives to a raid 5 when required. &lt;/p&gt;\n\n&lt;p&gt;I think I have a good idea of what I need now but just wanted to run it past people who actually know what they\u2019re talking about before I drop a tonne of money on it. Any advise or recommendations would be appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1909biz", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Fox8080", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1909biz/5gbps_vs_10gbps_das_with_5400rpm_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1909biz/5gbps_vs_10gbps_das_with_5400rpm_drives/", "subreddit_subscribers": 723434, "created_utc": 1704572993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using MAkeMKV program. Here is the plan I have decided to do:\n\n(1) When it comes to fitness DVDs I plan to make them a iso and keep all the menus and such and just store the iso on my \"Anime\" External Hard Drive for now until I can get a second External Hard Drive this month or next. \n\n(2) Movies I plan to keep the main movie and Trailer for the main movie and the Subtitles for \"English\" just in case I do need the subtitles after all. But I can always turn them off or on. With the VLC Media Player that I use. \n\n(3) TV Shows I will be just keeping the \"Episodes\" so I plan to use makemkv to turn them into mkv files and then just keep them as is maybe it all depends on the screen size and such. I guess I could always fix the black square background with the screen size on VLC media player. \n\n&amp;#x200B;\n\nBut for now this is the easiest way I have found to keep the \"Fitness/Exercise\" videos without getting there names messed up and this way I can still customize the workouts. \n\n&amp;#x200B;\n\nBut the only issue I have is where I read that VLC will not play iso. The thing is I just played around with the iso and opened it in VLC fine without issue. I even took the iso and placed it inside a folder I just made on my secondary external hard drive I call \"Anime\" since the \"Anime\" hard drive has over 367GB left before full. I do have a bunch of random stuff on there for \"Anime\" I do still plan to delete since its gone from \"Subtitled\" only to \"Dubbed\" now for some of the \"Anime\". \n\n&amp;#x200B;\n\nSo I plan to go through the hard drive and remove some random stuff later. But for now I plan to \"Backup\" all the \"Fitness/Exercise\" DVDs to my windows 10 laptop 1 at a time and then move to the iso folder on the hard drive and then remove them from the computer once there moved to the hard drive. Then once I get my new hard drive I will move all the \"Fitness/Exercise\" to the new hard drive and keep them all as iso. But I will make a new folder for each DVD and rename each folder the title of the DVD and the year released. I know plex and such programs wont play iso. But that is fine I will use iso for only the fitness/exercise DVDs and then the Movies and TV Shows I will use with Kodi once I have all the DVDs copied. \n\n&amp;#x200B;\n\nSo for now that is the plan. \n\n&amp;#x200B;\n\nThis way I can still use my fitness DVDs on my PC while the PC is connect to my older 32\" TV and then play my fitness videos on there with customization and so on. But movies and TV Shows will be put on a different hard drive called \"Movies &amp; TV Shows\" and those will be used on Kodi for the same PC. \n\n&amp;#x200B;\n\nIf this sound like it might not work please comment below where I am going wrong. Thank You! ", "author_fullname": "t2_8gvbu38t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change of plan for fitness DVD vs Main Movie DVDs and TV Show DVDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907m3t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using MAkeMKV program. Here is the plan I have decided to do:&lt;/p&gt;\n\n&lt;p&gt;(1) When it comes to fitness DVDs I plan to make them a iso and keep all the menus and such and just store the iso on my &amp;quot;Anime&amp;quot; External Hard Drive for now until I can get a second External Hard Drive this month or next. &lt;/p&gt;\n\n&lt;p&gt;(2) Movies I plan to keep the main movie and Trailer for the main movie and the Subtitles for &amp;quot;English&amp;quot; just in case I do need the subtitles after all. But I can always turn them off or on. With the VLC Media Player that I use. &lt;/p&gt;\n\n&lt;p&gt;(3) TV Shows I will be just keeping the &amp;quot;Episodes&amp;quot; so I plan to use makemkv to turn them into mkv files and then just keep them as is maybe it all depends on the screen size and such. I guess I could always fix the black square background with the screen size on VLC media player. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But for now this is the easiest way I have found to keep the &amp;quot;Fitness/Exercise&amp;quot; videos without getting there names messed up and this way I can still customize the workouts. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But the only issue I have is where I read that VLC will not play iso. The thing is I just played around with the iso and opened it in VLC fine without issue. I even took the iso and placed it inside a folder I just made on my secondary external hard drive I call &amp;quot;Anime&amp;quot; since the &amp;quot;Anime&amp;quot; hard drive has over 367GB left before full. I do have a bunch of random stuff on there for &amp;quot;Anime&amp;quot; I do still plan to delete since its gone from &amp;quot;Subtitled&amp;quot; only to &amp;quot;Dubbed&amp;quot; now for some of the &amp;quot;Anime&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I plan to go through the hard drive and remove some random stuff later. But for now I plan to &amp;quot;Backup&amp;quot; all the &amp;quot;Fitness/Exercise&amp;quot; DVDs to my windows 10 laptop 1 at a time and then move to the iso folder on the hard drive and then remove them from the computer once there moved to the hard drive. Then once I get my new hard drive I will move all the &amp;quot;Fitness/Exercise&amp;quot; to the new hard drive and keep them all as iso. But I will make a new folder for each DVD and rename each folder the title of the DVD and the year released. I know plex and such programs wont play iso. But that is fine I will use iso for only the fitness/exercise DVDs and then the Movies and TV Shows I will use with Kodi once I have all the DVDs copied. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So for now that is the plan. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This way I can still use my fitness DVDs on my PC while the PC is connect to my older 32&amp;quot; TV and then play my fitness videos on there with customization and so on. But movies and TV Shows will be put on a different hard drive called &amp;quot;Movies &amp;amp; TV Shows&amp;quot; and those will be used on Kodi for the same PC. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If this sound like it might not work please comment below where I am going wrong. Thank You! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907m3t", "is_robot_indexable": true, "report_reasons": null, "author": "DawnRenee1988", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907m3t/change_of_plan_for_fitness_dvd_vs_main_movie_dvds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907m3t/change_of_plan_for_fitness_dvd_vs_main_movie_dvds/", "subreddit_subscribers": 723434, "created_utc": 1704568634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On the PC that will perform the operation, the encrypted HDD is unlocked.", "author_fullname": "t2_b65ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi! Will converting an external HDD from MBR to GPT make it lose BitLocker encryption? Thank you", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1904v17", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704561643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the PC that will perform the operation, the encrypted HDD is unlocked.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1904v17", "is_robot_indexable": true, "report_reasons": null, "author": "Salberyon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904v17/hi_will_converting_an_external_hdd_from_mbr_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1904v17/hi_will_converting_an_external_hdd_from_mbr_to/", "subreddit_subscribers": 723434, "created_utc": 1704561643.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}