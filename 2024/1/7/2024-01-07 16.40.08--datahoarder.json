{"kind": "Listing", "data": {"after": "t3_190d3xv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lately I have been archiving more media for my Plex library. I decided to download the entire One Piece show that's 1.3 tb. I started it a bit ago and it says it'll take over 3-5 days. I did download all of the Saturday Night Live seasons 2 years ago that took a week or less that was 1.68tbs.", "author_fullname": "t2_4trjihkg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the biggest thing you've downloaded in your time being a Datahoarder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1908up8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 192, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 192, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704571797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately I have been archiving more media for my Plex library. I decided to download the entire One Piece show that&amp;#39;s 1.3 tb. I started it a bit ago and it says it&amp;#39;ll take over 3-5 days. I did download all of the Saturday Night Live seasons 2 years ago that took a week or less that was 1.68tbs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "52TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1908up8", "is_robot_indexable": true, "report_reasons": null, "author": "Eskel5", "discussion_type": null, "num_comments": 178, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1908up8/whats_the_biggest_thing_youve_downloaded_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1908up8/whats_the_biggest_thing_youve_downloaded_in_your/", "subreddit_subscribers": 723567, "created_utc": 1704571797.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_78n2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Meme] Average DigitalFAQ Forum Thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_1907rwb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/k2z32uelevac1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 544, "width": 1280, "scrubber_media_url": "https://v.redd.it/k2z32uelevac1/DASH_96.mp4", "dash_url": "https://v.redd.it/k2z32uelevac1/DASHPlaylist.mpd?a=1707237608%2CZWE0OTFhM2M0NTg0MDQyZjRlZmEzMWNjNjcwMzZhMjQxZjUwNjM5N2ExNTFhMzg1MTFhZjVmZjgxMGZlY2FjNQ%3D%3D&amp;v=1&amp;f=sd", "duration": 65, "hls_url": "https://v.redd.it/k2z32uelevac1/HLSPlaylist.m3u8?a=1707237608%2CZjkzNTU1OTFiOWVhYTgyOTY1MDA5ZWNhMDkyNjBhNTlmYjA1NjYyNWM3MTE2NjFiNzBjNzZmNjEzMTVmZGI5Yg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=140&amp;height=59&amp;crop=140:59,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c413aa68682d0d4c97f88b82728982ffb9e05f5d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704569038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/k2z32uelevac1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?format=pjpg&amp;auto=webp&amp;s=bce44850f07342b55770e49b9dc74229ca1c0776", "width": 1920, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30edb531f0cf4a76e3d8fa5f083fac24e5f0b4d6", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=356a8f1356e0283ef130b7fc656c3d746826b129", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=235550b017ec21d5aae3e494b27c05a6e8129a79", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1bf5a4fbda2d77e7cf10d8dc47df1b53719ab764", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f6fb1360a21b12d868b1702a34c70cef96f944a4", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=119168848e82be396df3a14fd9be234106477d4a", "width": 1080, "height": 459}], "variants": {}, "id": "bnA2M3N5ZXdldmFjMZoWtHTUYQ8HmcDa3x7MlwMAKZ70Qm2G5Gu-Wnq4rOgY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907rwb", "is_robot_indexable": true, "report_reasons": null, "author": "Nightowl3090", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907rwb/meme_average_digitalfaq_forum_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/k2z32uelevac1", "subreddit_subscribers": 723567, "created_utc": 1704569038.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/k2z32uelevac1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 544, "width": 1280, "scrubber_media_url": "https://v.redd.it/k2z32uelevac1/DASH_96.mp4", "dash_url": "https://v.redd.it/k2z32uelevac1/DASHPlaylist.mpd?a=1707237608%2CZWE0OTFhM2M0NTg0MDQyZjRlZmEzMWNjNjcwMzZhMjQxZjUwNjM5N2ExNTFhMzg1MTFhZjVmZjgxMGZlY2FjNQ%3D%3D&amp;v=1&amp;f=sd", "duration": 65, "hls_url": "https://v.redd.it/k2z32uelevac1/HLSPlaylist.m3u8?a=1707237608%2CZjkzNTU1OTFiOWVhYTgyOTY1MDA5ZWNhMDkyNjBhNTlmYjA1NjYyNWM3MTE2NjFiNzBjNzZmNjEzMTVmZGI5Yg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_crd2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There comes a point where MEGA becomes unfeasible for a certain level of datahoarders :(", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_190hram", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ngwEo0bklrcZGim7cSMSgsLKghi-NCbOMUWyeThGkyI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704595960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/xfv0obghmxac1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/xfv0obghmxac1.png?auto=webp&amp;s=98e4bf632faec59a928397462a40f684b2a86910", "width": 1037, "height": 562}, "resolutions": [{"url": "https://preview.redd.it/xfv0obghmxac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41d193dac7fed1e96be4a68ceed4cd47f82a0e85", "width": 108, "height": 58}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=693fd298104a73bbe09aaf0742081bb25c49945c", "width": 216, "height": 117}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bda9ed080b9934c2aa64a8bb684a27ba225d361d", "width": 320, "height": 173}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27d292b33e645b9ab93aee14eb3534ab574d7ce3", "width": 640, "height": 346}, {"url": "https://preview.redd.it/xfv0obghmxac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e62e7cd79d06ce2653222f363ea59235a0b9155", "width": 960, "height": 520}], "variants": {}, "id": "CCE8PnQ9w705IN7Kiaq56e5QB9EfV4iPOtnTX6jZQ28"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "190hram", "is_robot_indexable": true, "report_reasons": null, "author": "TheInzaneGamer", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190hram/there_comes_a_point_where_mega_becomes_unfeasible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/xfv0obghmxac1.png", "subreddit_subscribers": 723567, "created_utc": 1704595960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The Western Digital website is having a sale right now, you can get two WD Red Pro 14TB NAS drives for $439.98 (free shipping) if anyone is in the market for some. Looks like the discount ends 1/7/2024.", "author_fullname": "t2_xagsn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bundle discount on WD Pro Red 14TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1904a1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3zQXU-3S4jeh3SEwSQaSwboZJNti_8jiS7D1cMUm3O0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704560131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Western Digital website is having a sale right now, you can get two WD Red Pro 14TB NAS drives for $439.98 (free shipping) if anyone is in the market for some. Looks like the discount ends 1/7/2024.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/34b9fq5fouac1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/34b9fq5fouac1.png?auto=webp&amp;s=c5a705840ad9031c5446de7f34aae1fc07448581", "width": 1080, "height": 2412}, "resolutions": [{"url": "https://preview.redd.it/34b9fq5fouac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab2416aca78843be6d4abd7f127fab9e112589a0", "width": 108, "height": 216}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734c47d6961c2ba9f4ee48ef1149226e67662e30", "width": 216, "height": 432}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4599776d3b396f0f1c7a7870b05a1d9251dc20bf", "width": 320, "height": 640}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7889b19d691644110378448897150e57443325e", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1b1fd88c0cd58a80c6ae415ad9839e461de0b54", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/34b9fq5fouac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9123b5e52fa6831e1022e50d10f74819c7b051ff", "width": 1080, "height": 2160}], "variants": {}, "id": "2QqZ8gspdCb_mQ3lJxXBq0x0SDA6WfocGaGqIT6sf_A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1904a1m", "is_robot_indexable": true, "report_reasons": null, "author": "pillowserious", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904a1m/bundle_discount_on_wd_pro_red_14tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/34b9fq5fouac1.png", "subreddit_subscribers": 723567, "created_utc": 1704560131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "All of my collections have been deleted and I can't login anymore. It's ridiculous that I never even got an email as to why I got banned. I had some deleted YouTube channel content on there. So glad I got some stuff backed up locally. Why would the Internet Archive of all people delete ALL content that a user has uploaded? And why the hell didn't I get an email??", "author_fullname": "t2_vnru8wzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Banned from Internet Archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190kfw0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704604359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All of my collections have been deleted and I can&amp;#39;t login anymore. It&amp;#39;s ridiculous that I never even got an email as to why I got banned. I had some deleted YouTube channel content on there. So glad I got some stuff backed up locally. Why would the Internet Archive of all people delete ALL content that a user has uploaded? And why the hell didn&amp;#39;t I get an email??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "52TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190kfw0", "is_robot_indexable": true, "report_reasons": null, "author": "funny_b0t2", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/190kfw0/banned_from_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190kfw0/banned_from_internet_archive/", "subreddit_subscribers": 723567, "created_utc": 1704604359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I find one problem with being a DataHoarder is having multiple versions of the same video clips. I've used programs such as DupeGuru to weed out identical videos, but the problem is when you have the same video, but different sizes. One possibility is that the video was popular and re-encoded by someone else at some point. \n\nMy goal is to only keep the best quality version of each video. \n\nSo I first thought I could just sort by video length, and then take out the smaller sized files. But I'm finding that some of the reencoded vids are larger than the original, so that doesn't always help. Right now I'm going through each potential dupe video, screenshotting a random frame, zooming in and comparing them manually. It works, but not the most efficient process.\n\nAre there any thoughts on a better way to do this? Thanks!", "author_fullname": "t2_h3tsi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video quality advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907qbg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find one problem with being a DataHoarder is having multiple versions of the same video clips. I&amp;#39;ve used programs such as DupeGuru to weed out identical videos, but the problem is when you have the same video, but different sizes. One possibility is that the video was popular and re-encoded by someone else at some point. &lt;/p&gt;\n\n&lt;p&gt;My goal is to only keep the best quality version of each video. &lt;/p&gt;\n\n&lt;p&gt;So I first thought I could just sort by video length, and then take out the smaller sized files. But I&amp;#39;m finding that some of the reencoded vids are larger than the original, so that doesn&amp;#39;t always help. Right now I&amp;#39;m going through each potential dupe video, screenshotting a random frame, zooming in and comparing them manually. It works, but not the most efficient process.&lt;/p&gt;\n\n&lt;p&gt;Are there any thoughts on a better way to do this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907qbg", "is_robot_indexable": true, "report_reasons": null, "author": "1doughnut", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907qbg/video_quality_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907qbg/video_quality_advice/", "subreddit_subscribers": 723567, "created_utc": 1704568921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible if so how?", "author_fullname": "t2_8zpe3mjoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting 3d models from 2gis.ru", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1908wle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704571925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible if so how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1908wle", "is_robot_indexable": true, "report_reasons": null, "author": "3dPrintMyThingi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1908wle/extracting_3d_models_from_2gisru/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1908wle/extracting_3d_models_from_2gisru/", "subreddit_subscribers": 723567, "created_utc": 1704571925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My main internal drive is a 4TB WD Black. (don't laugh) I want to upgrade to 8TB, so I bought an 8TB Seagate Barracuda from B&amp;H. I installed it into a Sabrent drive dock I've had a few years, but IS rated for 8TB. Formatted as Z, copied a couple TB from the internal drive, and about 600MB from an external drive of my wife's. The plan was to leave it in action for some weeks before swapping into the PC, to make sure it was working OK.\n\nStarted getting errors from Windows (W10) that the drive needed scanned and repaired. It would repeatedly hang about 30% through the scan, and the drive would disappear. Downloaded **Seatools**, the 2 minute self test would fail. But I did see some folders of data that I had copied. I have **HD Sentinel**, ran the short self-test and the conveyance self test (whatever that is) successfully with that. Disk surface test showed no bad or damaged sectors, but I was also left with 1000 \"found.nnn\" folders.\n\nQuestions: Could this be a problem with the dock or it's USB cable? I've already bought a new one with it's own USB 3.2 cable (which my PC supports). Is it worth installing it in the new dock, reformatting, and starting over? You can view screenshots of the [various test results at Imgur](https://imgur.com/a/4A7HZSP). Or should I RMA this drive? Thanks.", "author_fullname": "t2_fwy7u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I trust this drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907hg5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My main internal drive is a 4TB WD Black. (don&amp;#39;t laugh) I want to upgrade to 8TB, so I bought an 8TB Seagate Barracuda from B&amp;amp;H. I installed it into a Sabrent drive dock I&amp;#39;ve had a few years, but IS rated for 8TB. Formatted as Z, copied a couple TB from the internal drive, and about 600MB from an external drive of my wife&amp;#39;s. The plan was to leave it in action for some weeks before swapping into the PC, to make sure it was working OK.&lt;/p&gt;\n\n&lt;p&gt;Started getting errors from Windows (W10) that the drive needed scanned and repaired. It would repeatedly hang about 30% through the scan, and the drive would disappear. Downloaded &lt;strong&gt;Seatools&lt;/strong&gt;, the 2 minute self test would fail. But I did see some folders of data that I had copied. I have &lt;strong&gt;HD Sentinel&lt;/strong&gt;, ran the short self-test and the conveyance self test (whatever that is) successfully with that. Disk surface test showed no bad or damaged sectors, but I was also left with 1000 &amp;quot;found.nnn&amp;quot; folders.&lt;/p&gt;\n\n&lt;p&gt;Questions: Could this be a problem with the dock or it&amp;#39;s USB cable? I&amp;#39;ve already bought a new one with it&amp;#39;s own USB 3.2 cable (which my PC supports). Is it worth installing it in the new dock, reformatting, and starting over? You can view screenshots of the &lt;a href=\"https://imgur.com/a/4A7HZSP\"&gt;various test results at Imgur&lt;/a&gt;. Or should I RMA this drive? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?auto=webp&amp;s=bab6394dd8a233277e287d0b8817296aa6fac59c", "width": 566, "height": 573}, "resolutions": [{"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=28c234971dc8891add6528a783cf3c370b38e4d5", "width": 108, "height": 109}, {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e46399bdd3ab8a4224db150e857404ec6a2e542", "width": 216, "height": 218}, {"url": "https://external-preview.redd.it/aBdOe6upL_qzL8ZjcG7kXs1CRC_FD97QElI73GoKldQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b5203b7783399d90b405f626f125f89f14af7b4", "width": 320, "height": 323}], "variants": {}, "id": "7HP71TV1dsuSeRkrcBo3iGMG7MffI_6vRYgs4-z3LgU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907hg5", "is_robot_indexable": true, "report_reasons": null, "author": "evildad53", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907hg5/can_i_trust_this_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907hg5/can_i_trust_this_drive/", "subreddit_subscribers": 723567, "created_utc": 1704568307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry, I know it's partially on topic at best, but I figure you guys are likely the ones to chat to.\n\nI'm having issues with the playback of some of my, err hoarded data and what I really need is a variety of 'test files' so I can just spot a problem instantly, without having to question my eyes.\n\nI need some 30 and 24hz videos which specifically do panning, that kind of thing, just a general 'plex test' of content. Variety of frame rates and situations to spot if my playback is working flawlessly at resolutions / frame rates, with trancoding and so on.\n\n&amp;#x200B;\n\nAnyone seen anything like this that they've hoarded?", "author_fullname": "t2_vhliwffj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some test files, this seems the most likely place to find them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190of8d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704619483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry, I know it&amp;#39;s partially on topic at best, but I figure you guys are likely the ones to chat to.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having issues with the playback of some of my, err hoarded data and what I really need is a variety of &amp;#39;test files&amp;#39; so I can just spot a problem instantly, without having to question my eyes.&lt;/p&gt;\n\n&lt;p&gt;I need some 30 and 24hz videos which specifically do panning, that kind of thing, just a general &amp;#39;plex test&amp;#39; of content. Variety of frame rates and situations to spot if my playback is working flawlessly at resolutions / frame rates, with trancoding and so on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone seen anything like this that they&amp;#39;ve hoarded?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190of8d", "is_robot_indexable": true, "report_reasons": null, "author": "ChumpyCarvings", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190of8d/i_need_some_test_files_this_seems_the_most_likely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190of8d/i_need_some_test_files_this_seems_the_most_likely/", "subreddit_subscribers": 723567, "created_utc": 1704619483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's the point of buying the latest version of a hard drive?  What is the difference between a new 4tb hard drive that came out in 2023 and a new 4tb hard drive that came out a long time ago?  (5, 10, 12 years, no idea, just as an example)\nAre there any bigger differences in terms of safety, longevity and reliability? \nOr is it just possible that even more data can fit in and some small upgrades? \n\nWill there be revolutions in the near future (next 5-10 years) related to data storage?", "author_fullname": "t2_9odcnu9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should I buy the latest version of hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190glwh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704592482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the point of buying the latest version of a hard drive?  What is the difference between a new 4tb hard drive that came out in 2023 and a new 4tb hard drive that came out a long time ago?  (5, 10, 12 years, no idea, just as an example)\nAre there any bigger differences in terms of safety, longevity and reliability? \nOr is it just possible that even more data can fit in and some small upgrades? &lt;/p&gt;\n\n&lt;p&gt;Will there be revolutions in the near future (next 5-10 years) related to data storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "20TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190glwh", "is_robot_indexable": true, "report_reasons": null, "author": "ServiceOk9043", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/190glwh/why_should_i_buy_the_latest_version_of_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190glwh/why_should_i_buy_the_latest_version_of_hard_drive/", "subreddit_subscribers": 723567, "created_utc": 1704592482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have searched around and so far nothing I find actually works. I have a video set I've paid for that I log in on their website and there is an embedded Vimeo playlist of the videos I want to download. I use the inspector in my web browser and look at the network tab while playing the video and just see a bunch of segment-number.m4s links. I try to copy that address into YTDL but it never works. What am I doing wrong?", "author_fullname": "t2_qbrku9p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download Embedded Vimeo Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1904xed", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704561810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have searched around and so far nothing I find actually works. I have a video set I&amp;#39;ve paid for that I log in on their website and there is an embedded Vimeo playlist of the videos I want to download. I use the inspector in my web browser and look at the network tab while playing the video and just see a bunch of segment-number.m4s links. I try to copy that address into YTDL but it never works. What am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1904xed", "is_robot_indexable": true, "report_reasons": null, "author": "Tumnus1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904xed/download_embedded_vimeo_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1904xed/download_embedded_vimeo_playlist/", "subreddit_subscribers": 723567, "created_utc": 1704561810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There a few courses that I would like to download from Linkedin Learning. I want to ask if anyone can suggest a program that currently works? What is the easiest method to download full courses?\n\nI would appreciate any suggestions. Thank you. ", "author_fullname": "t2_gte3r2430", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you download full courses from Linkedin Learning? What method currently works?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190ozjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704621863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There a few courses that I would like to download from Linkedin Learning. I want to ask if anyone can suggest a program that currently works? What is the easiest method to download full courses?&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any suggestions. Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190ozjv", "is_robot_indexable": true, "report_reasons": null, "author": "bigbellybomac", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190ozjv/how_can_you_download_full_courses_from_linkedin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190ozjv/how_can_you_download_full_courses_from_linkedin/", "subreddit_subscribers": 723567, "created_utc": 1704621863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to figure out my options here for a basic and budget 4 bay NAS setup. At present I have an old WD 2 bay NAS with two 6TB drives in a Raid0 (I know, I know) that is running out of space and am looking to replace. Separately, I have a Lenovo M93P Tiny that is running Ubuntu as a headless server doing the usual home server stuff that is barely stressed the majority of the time.\n\nWas originally thinking of just getting a 4 bay DAS, throw in 4 8tb drives in a Raid5, and attach it via USB3.0. Problem one, any recommendations on a DAS? problem two, any recommendations on software that would run on Ubuntu to manage it?\n\nWould it be easier to just get a RaspberryPi and run TrueNAS on it?", "author_fullname": "t2_48p5okg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best homebrew NAS setup using DAS? Already have mini pc (no drive bays) running Ubuntu.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190jwgn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704602658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to figure out my options here for a basic and budget 4 bay NAS setup. At present I have an old WD 2 bay NAS with two 6TB drives in a Raid0 (I know, I know) that is running out of space and am looking to replace. Separately, I have a Lenovo M93P Tiny that is running Ubuntu as a headless server doing the usual home server stuff that is barely stressed the majority of the time.&lt;/p&gt;\n\n&lt;p&gt;Was originally thinking of just getting a 4 bay DAS, throw in 4 8tb drives in a Raid5, and attach it via USB3.0. Problem one, any recommendations on a DAS? problem two, any recommendations on software that would run on Ubuntu to manage it?&lt;/p&gt;\n\n&lt;p&gt;Would it be easier to just get a RaspberryPi and run TrueNAS on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190jwgn", "is_robot_indexable": true, "report_reasons": null, "author": "GiggityYay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190jwgn/best_homebrew_nas_setup_using_das_already_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190jwgn/best_homebrew_nas_setup_using_das_already_have/", "subreddit_subscribers": 723567, "created_utc": 1704602658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, i have many years of my family's videos on both of these. What would be the best way to convert each of these to digital? I will buy whatever products are required to get good output. Thank you.", "author_fullname": "t2_2v5p7f69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to convert VHS and DV to Digital, help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_190gjpe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Co2WmKcgN6jf_BMH1Y8_4PsZ6XmJu1rn1JnJA6p5yHM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704592292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have many years of my family&amp;#39;s videos on both of these. What would be the best way to convert each of these to digital? I will buy whatever products are required to get good output. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/2sxwexs1cxac1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/2sxwexs1cxac1.png?auto=webp&amp;s=965cd6df8131754112cbb10a55cafbe056bda637", "width": 1080, "height": 810}, "resolutions": [{"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=29b93b89ad64ab8c7c2b7d06b7289e61d5cea612", "width": 108, "height": 81}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=262e257432da8a00d582e2d937d820c94cb63dcd", "width": 216, "height": 162}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a7711c7becb3f692d5814fd7451646742b28bc1", "width": 320, "height": 240}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0379f297d720fd76ad630f06af017cdbaa109d4e", "width": 640, "height": 480}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbb5b14972f730890c0d843b6ef30bd2a34645b2", "width": 960, "height": 720}, {"url": "https://preview.redd.it/2sxwexs1cxac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82e88f9a9ffe83e389308263ea5989f00cd293f2", "width": 1080, "height": 810}], "variants": {}, "id": "a7JXjFFfk1yUsU-bn34R0dOfkSWpip6dzlw-xuIUD9c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190gjpe", "is_robot_indexable": true, "report_reasons": null, "author": "Xcedia", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190gjpe/need_to_convert_vhs_and_dv_to_digital_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/2sxwexs1cxac1.png", "subreddit_subscribers": 723567, "created_utc": 1704592292.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to have rsync check the content of a file before synch? Like a go ahead to ensure that the directory is not encrypted by ransomware", "author_fullname": "t2_hrtc9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ransomware verification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190dlnm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704584043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to have rsync check the content of a file before synch? Like a go ahead to ensure that the directory is not encrypted by ransomware&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190dlnm", "is_robot_indexable": true, "report_reasons": null, "author": "py2gb", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190dlnm/ransomware_verification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190dlnm/ransomware_verification/", "subreddit_subscribers": 723567, "created_utc": 1704584043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, \n\nI\u2019m looking into buying my first DAS. Most of you are probably going to recommend going for a NAS, and Ideally I would like a NAS but the sort of thing I would want is out of my price range right now so my plan is to get a DAS for now then buy a NAS later down the line and use this DAS as a backup for that. (I would need a large capacity backup at some point anyway)\n\nMy question is, is it worth me buying an enclosure with 10gbps capability or will I not notice any difference in speed compared to a 5gbps enclosure, seeing as I\u2019ll be using 5400rpm drives, which max out at around 150MBps. I\u2019m planning on having 3 x 8-12tb seagate ironwolf or exos drives in a raid 5 configuration to begin, then add more drives as I need them later for a total of 6. So with the combined speed I presume the max speed I will ever get is roughly 725MBps? (5 x 150MBps, 1 x drive for parity). But realistically I won\u2019t need those extra drives for a long time so will be running off 3 or 4 drives for a while. So let\u2019s say 450MBps total with 4 drives, which the 5gbps enclosure can handle right? I had my eye on the terramaster D6-320 for the 10gbps enclosure, until i realised the transfer speeds would be restricted by the speed of the actual drives and i could get something for half the price if i go with a 5gbps enclosure. Is there anything I\u2019m missing here though?\n\nI have a 2017 iMac with thunderbolt 3 ports and I want to use this DAS to edit music and 4k video files from and also to have somewhere to put all my files in one place. My current setup is just a bunch of old external hard drives I\u2019ve collected over the years. I used to export the footage from my sd cards directly onto my Mac and edit from there, then back the files up to an external drive once I was finished, but my Mac is getting full and now I edit in 4k so I don\u2019t have the space for that anymore. Plus it\u2019s just a mess having multiple hdds, some of which have old footage on which I want to use for future edits and that means moving my files off those slow drives to do so. I haven\u2019t done much editing over the past few years but recently decided to get back into it and one of my external drives is struggling to boot so I need to pull my data off it and figured it\u2019s time to sort this mess out once and for all, rather than buying another external hdd and adding to the mess!\n\nI\u2019m also looking into what raid software is best to use with a DAS but not sure if that requires a new post? Softraid seems to be the best but it costs a fortune, then I can\u2019t see hardly any other options. I\u2019m not really keen on the look of terramaster\u2019s software, have any of you used it and what\u2019s it like? The main thing I need is the ability to clearly monitor the condition of my drives and the option to add more drives to a raid 5 when required. \n\nI think I have a good idea of what I need now but just wanted to run it past people who actually know what they\u2019re talking about before I drop a tonne of money on it. Any advise or recommendations would be appreciated. \n\nThanks", "author_fullname": "t2_5o6x09iy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5gbps vs 10gbps DAS with 5400rpm drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1909biz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704572993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking into buying my first DAS. Most of you are probably going to recommend going for a NAS, and Ideally I would like a NAS but the sort of thing I would want is out of my price range right now so my plan is to get a DAS for now then buy a NAS later down the line and use this DAS as a backup for that. (I would need a large capacity backup at some point anyway)&lt;/p&gt;\n\n&lt;p&gt;My question is, is it worth me buying an enclosure with 10gbps capability or will I not notice any difference in speed compared to a 5gbps enclosure, seeing as I\u2019ll be using 5400rpm drives, which max out at around 150MBps. I\u2019m planning on having 3 x 8-12tb seagate ironwolf or exos drives in a raid 5 configuration to begin, then add more drives as I need them later for a total of 6. So with the combined speed I presume the max speed I will ever get is roughly 725MBps? (5 x 150MBps, 1 x drive for parity). But realistically I won\u2019t need those extra drives for a long time so will be running off 3 or 4 drives for a while. So let\u2019s say 450MBps total with 4 drives, which the 5gbps enclosure can handle right? I had my eye on the terramaster D6-320 for the 10gbps enclosure, until i realised the transfer speeds would be restricted by the speed of the actual drives and i could get something for half the price if i go with a 5gbps enclosure. Is there anything I\u2019m missing here though?&lt;/p&gt;\n\n&lt;p&gt;I have a 2017 iMac with thunderbolt 3 ports and I want to use this DAS to edit music and 4k video files from and also to have somewhere to put all my files in one place. My current setup is just a bunch of old external hard drives I\u2019ve collected over the years. I used to export the footage from my sd cards directly onto my Mac and edit from there, then back the files up to an external drive once I was finished, but my Mac is getting full and now I edit in 4k so I don\u2019t have the space for that anymore. Plus it\u2019s just a mess having multiple hdds, some of which have old footage on which I want to use for future edits and that means moving my files off those slow drives to do so. I haven\u2019t done much editing over the past few years but recently decided to get back into it and one of my external drives is struggling to boot so I need to pull my data off it and figured it\u2019s time to sort this mess out once and for all, rather than buying another external hdd and adding to the mess!&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also looking into what raid software is best to use with a DAS but not sure if that requires a new post? Softraid seems to be the best but it costs a fortune, then I can\u2019t see hardly any other options. I\u2019m not really keen on the look of terramaster\u2019s software, have any of you used it and what\u2019s it like? The main thing I need is the ability to clearly monitor the condition of my drives and the option to add more drives to a raid 5 when required. &lt;/p&gt;\n\n&lt;p&gt;I think I have a good idea of what I need now but just wanted to run it past people who actually know what they\u2019re talking about before I drop a tonne of money on it. Any advise or recommendations would be appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1909biz", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Fox8080", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1909biz/5gbps_vs_10gbps_das_with_5400rpm_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1909biz/5gbps_vs_10gbps_das_with_5400rpm_drives/", "subreddit_subscribers": 723567, "created_utc": 1704572993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using MAkeMKV program. Here is the plan I have decided to do:\n\n(1) When it comes to fitness DVDs I plan to make them a iso and keep all the menus and such and just store the iso on my \"Anime\" External Hard Drive for now until I can get a second External Hard Drive this month or next. \n\n(2) Movies I plan to keep the main movie and Trailer for the main movie and the Subtitles for \"English\" just in case I do need the subtitles after all. But I can always turn them off or on. With the VLC Media Player that I use. \n\n(3) TV Shows I will be just keeping the \"Episodes\" so I plan to use makemkv to turn them into mkv files and then just keep them as is maybe it all depends on the screen size and such. I guess I could always fix the black square background with the screen size on VLC media player. \n\n&amp;#x200B;\n\nBut for now this is the easiest way I have found to keep the \"Fitness/Exercise\" videos without getting there names messed up and this way I can still customize the workouts. \n\n&amp;#x200B;\n\nBut the only issue I have is where I read that VLC will not play iso. The thing is I just played around with the iso and opened it in VLC fine without issue. I even took the iso and placed it inside a folder I just made on my secondary external hard drive I call \"Anime\" since the \"Anime\" hard drive has over 367GB left before full. I do have a bunch of random stuff on there for \"Anime\" I do still plan to delete since its gone from \"Subtitled\" only to \"Dubbed\" now for some of the \"Anime\". \n\n&amp;#x200B;\n\nSo I plan to go through the hard drive and remove some random stuff later. But for now I plan to \"Backup\" all the \"Fitness/Exercise\" DVDs to my windows 10 laptop 1 at a time and then move to the iso folder on the hard drive and then remove them from the computer once there moved to the hard drive. Then once I get my new hard drive I will move all the \"Fitness/Exercise\" to the new hard drive and keep them all as iso. But I will make a new folder for each DVD and rename each folder the title of the DVD and the year released. I know plex and such programs wont play iso. But that is fine I will use iso for only the fitness/exercise DVDs and then the Movies and TV Shows I will use with Kodi once I have all the DVDs copied. \n\n&amp;#x200B;\n\nSo for now that is the plan. \n\n&amp;#x200B;\n\nThis way I can still use my fitness DVDs on my PC while the PC is connect to my older 32\" TV and then play my fitness videos on there with customization and so on. But movies and TV Shows will be put on a different hard drive called \"Movies &amp; TV Shows\" and those will be used on Kodi for the same PC. \n\n&amp;#x200B;\n\nIf this sound like it might not work please comment below where I am going wrong. Thank You! ", "author_fullname": "t2_8gvbu38t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change of plan for fitness DVD vs Main Movie DVDs and TV Show DVDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1907m3t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704568634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using MAkeMKV program. Here is the plan I have decided to do:&lt;/p&gt;\n\n&lt;p&gt;(1) When it comes to fitness DVDs I plan to make them a iso and keep all the menus and such and just store the iso on my &amp;quot;Anime&amp;quot; External Hard Drive for now until I can get a second External Hard Drive this month or next. &lt;/p&gt;\n\n&lt;p&gt;(2) Movies I plan to keep the main movie and Trailer for the main movie and the Subtitles for &amp;quot;English&amp;quot; just in case I do need the subtitles after all. But I can always turn them off or on. With the VLC Media Player that I use. &lt;/p&gt;\n\n&lt;p&gt;(3) TV Shows I will be just keeping the &amp;quot;Episodes&amp;quot; so I plan to use makemkv to turn them into mkv files and then just keep them as is maybe it all depends on the screen size and such. I guess I could always fix the black square background with the screen size on VLC media player. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But for now this is the easiest way I have found to keep the &amp;quot;Fitness/Exercise&amp;quot; videos without getting there names messed up and this way I can still customize the workouts. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But the only issue I have is where I read that VLC will not play iso. The thing is I just played around with the iso and opened it in VLC fine without issue. I even took the iso and placed it inside a folder I just made on my secondary external hard drive I call &amp;quot;Anime&amp;quot; since the &amp;quot;Anime&amp;quot; hard drive has over 367GB left before full. I do have a bunch of random stuff on there for &amp;quot;Anime&amp;quot; I do still plan to delete since its gone from &amp;quot;Subtitled&amp;quot; only to &amp;quot;Dubbed&amp;quot; now for some of the &amp;quot;Anime&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I plan to go through the hard drive and remove some random stuff later. But for now I plan to &amp;quot;Backup&amp;quot; all the &amp;quot;Fitness/Exercise&amp;quot; DVDs to my windows 10 laptop 1 at a time and then move to the iso folder on the hard drive and then remove them from the computer once there moved to the hard drive. Then once I get my new hard drive I will move all the &amp;quot;Fitness/Exercise&amp;quot; to the new hard drive and keep them all as iso. But I will make a new folder for each DVD and rename each folder the title of the DVD and the year released. I know plex and such programs wont play iso. But that is fine I will use iso for only the fitness/exercise DVDs and then the Movies and TV Shows I will use with Kodi once I have all the DVDs copied. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So for now that is the plan. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This way I can still use my fitness DVDs on my PC while the PC is connect to my older 32&amp;quot; TV and then play my fitness videos on there with customization and so on. But movies and TV Shows will be put on a different hard drive called &amp;quot;Movies &amp;amp; TV Shows&amp;quot; and those will be used on Kodi for the same PC. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If this sound like it might not work please comment below where I am going wrong. Thank You! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1907m3t", "is_robot_indexable": true, "report_reasons": null, "author": "DawnRenee1988", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1907m3t/change_of_plan_for_fitness_dvd_vs_main_movie_dvds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1907m3t/change_of_plan_for_fitness_dvd_vs_main_movie_dvds/", "subreddit_subscribers": 723567, "created_utc": 1704568634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On the PC that will perform the operation, the encrypted HDD is unlocked.", "author_fullname": "t2_b65ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi! Will converting an external HDD from MBR to GPT make it lose BitLocker encryption? Thank you", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1904v17", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704561643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the PC that will perform the operation, the encrypted HDD is unlocked.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1904v17", "is_robot_indexable": true, "report_reasons": null, "author": "Salberyon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1904v17/hi_will_converting_an_external_hdd_from_mbr_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1904v17/hi_will_converting_an_external_hdd_from_mbr_to/", "subreddit_subscribers": 723567, "created_utc": 1704561643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Please help!! I\u2019ve got a WD My Passport 4tb which is not mounting on my  MacBook Pro 13 (mid 2014) running on Big Sur 11.7.10. When I plug it in, it shows on the disk utility, but fails to mount (disk management dis enter error 0). I\u2019ve run the first aid, first section says no problems, second says File Ststem verify or repair failed (-64895). I am at a loss on how to force mount this thing.\n\n I will say, the passport is full, and if I can mount the drive I can then delete some files, but I have files on this drive I need for a project. I\u2019ve been trying to mount this for a week, and I need to get this project finished. Any advice?", "author_fullname": "t2_3jlwq21j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD My Passport not mounting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_190tlgw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704638600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please help!! I\u2019ve got a WD My Passport 4tb which is not mounting on my  MacBook Pro 13 (mid 2014) running on Big Sur 11.7.10. When I plug it in, it shows on the disk utility, but fails to mount (disk management dis enter error 0). I\u2019ve run the first aid, first section says no problems, second says File Ststem verify or repair failed (-64895). I am at a loss on how to force mount this thing.&lt;/p&gt;\n\n&lt;p&gt;I will say, the passport is full, and if I can mount the drive I can then delete some files, but I have files on this drive I need for a project. I\u2019ve been trying to mount this for a week, and I need to get this project finished. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190tlgw", "is_robot_indexable": true, "report_reasons": null, "author": "templetondean", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190tlgw/wd_my_passport_not_mounting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190tlgw/wd_my_passport_not_mounting/", "subreddit_subscribers": 723567, "created_utc": 1704638600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nI have been using OpenMediaVault on my setup for some time now, started with an oDroid XU4, now ended up using my old desktop.\n\n\nI noticed a strange issue that I initially blamed it on a few power outages. I am using the latest available OMV 6 with the latest Proxmox kernel 6.2.16-11-bpo11-pve. The HDD in question is new, it was purchased last year with another one. I know that the issues shown in dmesg should mean that the HDD turned bad and today I booted into GParted Live and Seatools Bootable to check all HDDs. None of the tools are showing any errors with the HDD in question, I let it run the Short Generic Test. SMART information is clean and I cannot see any dmesg errors in GParted Live, Seatools Bootable or Ubuntu 22.04 Live. I even tried changing the SATA port. I don't know when the issue appeared, but I noticed sometime at the end of last year that my snapraid sync and scrub jobs were hung for a long time and that the snapraid.content on the affected drive is different than the ones on the other content drives.\n\nCould this be related to the installation or the kernel? I have not tried booting into the mainline OMV kernel or any other older kernels. I also realized I didn't change the SATA cable with a new one to rule out a bad cable. What puzzles me the most is that booting into anything else but OMV shows no error on the affected drive. One strange behavior of the affected drive is that it appeared as not mounted at some point - mountpoint was completely empty. All data on it was visible after a reboot - that scared me the most and then I started doing some basic checks that led me to running Seatools Bootable and checking it in a Live CD environment. I can read and write on it now without any issues. I fear that if I were to send it for warranty I would just get it back as functional.\n\nLogs are available HERE.\n\nI booted with the non Proxmox kernel 6.1.0-0.deb11.13-amd64 and I cannot see any issue with any disks. I ran snapraid and I also copied a large file to the affected disk for testing. I need to be able to use OpenCL, that is why I want to use Proxmox kernel.", "author_fullname": "t2_gdcr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strange issue with new HDD and OpenMediaVault", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190pgvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1704623929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "logpaste.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using OpenMediaVault on my setup for some time now, started with an oDroid XU4, now ended up using my old desktop.&lt;/p&gt;\n\n&lt;p&gt;I noticed a strange issue that I initially blamed it on a few power outages. I am using the latest available OMV 6 with the latest Proxmox kernel 6.2.16-11-bpo11-pve. The HDD in question is new, it was purchased last year with another one. I know that the issues shown in dmesg should mean that the HDD turned bad and today I booted into GParted Live and Seatools Bootable to check all HDDs. None of the tools are showing any errors with the HDD in question, I let it run the Short Generic Test. SMART information is clean and I cannot see any dmesg errors in GParted Live, Seatools Bootable or Ubuntu 22.04 Live. I even tried changing the SATA port. I don&amp;#39;t know when the issue appeared, but I noticed sometime at the end of last year that my snapraid sync and scrub jobs were hung for a long time and that the snapraid.content on the affected drive is different than the ones on the other content drives.&lt;/p&gt;\n\n&lt;p&gt;Could this be related to the installation or the kernel? I have not tried booting into the mainline OMV kernel or any other older kernels. I also realized I didn&amp;#39;t change the SATA cable with a new one to rule out a bad cable. What puzzles me the most is that booting into anything else but OMV shows no error on the affected drive. One strange behavior of the affected drive is that it appeared as not mounted at some point - mountpoint was completely empty. All data on it was visible after a reboot - that scared me the most and then I started doing some basic checks that led me to running Seatools Bootable and checking it in a Live CD environment. I can read and write on it now without any issues. I fear that if I were to send it for warranty I would just get it back as functional.&lt;/p&gt;\n\n&lt;p&gt;Logs are available HERE.&lt;/p&gt;\n\n&lt;p&gt;I booted with the non Proxmox kernel 6.1.0-0.deb11.13-amd64 and I cannot see any issue with any disks. I ran snapraid and I also copied a large file to the affected disk for testing. I need to be able to use OpenCL, that is why I want to use Proxmox kernel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://logpaste.com/7o5YYWO5", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB OMV NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190pgvk", "is_robot_indexable": true, "report_reasons": null, "author": "tmihai20", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/190pgvk/strange_issue_with_new_hdd_and_openmediavault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://logpaste.com/7o5YYWO5", "subreddit_subscribers": 723567, "created_utc": 1704623929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to migrate a local backup from one application to another (in this case from Perfect-Backup to VEAM?) I'm hoping I can somehow make a new job with VEAM agent without having to re-copy all of my files again. Is there a way to do this? I'm just doing a standard backup to an external drive. Thanks for your help!", "author_fullname": "t2_jmi91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to migrate a local backup from one application to another (in this case from Perfect-Backup to VEAM?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190p7qz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704622837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to migrate a local backup from one application to another (in this case from Perfect-Backup to VEAM?) I&amp;#39;m hoping I can somehow make a new job with VEAM agent without having to re-copy all of my files again. Is there a way to do this? I&amp;#39;m just doing a standard backup to an external drive. Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190p7qz", "is_robot_indexable": true, "report_reasons": null, "author": "owenbenson", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190p7qz/is_it_possible_to_migrate_a_local_backup_from_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190p7qz/is_it_possible_to_migrate_a_local_backup_from_one/", "subreddit_subscribers": 723567, "created_utc": 1704622837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "edit: i think it would be much easier to just use raid for the main drives. so that's what i'll do\n\n&amp;#x200B;\n\n~~I asked if RAID was necessary for me here earlier and most people were saying that unless you care about downtime (I don't) then as long as you have a backup you're fine.~~\n\n~~But someone raised a good point saying that whatever you had put on your NAS between the last backup sync and the point where your HDD failed would be completely lost without RAID. Is there a way to prevent this? Maybe something where you have two SSDs in RAID (which I'm already considering getting to put regular software on) and then having files stay there until they're ready to be synced?~~", "author_fullname": "t2_mqskseq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to prevent recently downloaded data loss without using RAID?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19077rq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704596545.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704567635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;edit: i think it would be much easier to just use raid for the main drives. so that&amp;#39;s what i&amp;#39;ll do&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I asked if RAID was necessary for me here earlier and most people were saying that unless you care about downtime (I don&amp;#39;t) then as long as you have a backup you&amp;#39;re fine.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;But someone raised a good point saying that whatever you had put on your NAS between the last backup sync and the point where your HDD failed would be completely lost without RAID. Is there a way to prevent this? Maybe something where you have two SSDs in RAID (which I&amp;#39;m already considering getting to put regular software on) and then having files stay there until they&amp;#39;re ready to be synced?&lt;/del&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19077rq", "is_robot_indexable": true, "report_reasons": null, "author": "OneSteelTank", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19077rq/is_there_a_way_to_prevent_recently_downloaded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19077rq/is_there_a_way_to_prevent_recently_downloaded/", "subreddit_subscribers": 723567, "created_utc": 1704567635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(This is bootleg-crossposted from [this post](https://www.reddit.com/r/datarecovery/comments/190oajn/do_any_disk_copying_programs_for_windows_10_allow/) I made to r/datarecovery, moved here as I thought this community may be a better fit, in turn partially continued off of [this post](https://www.reddit.com/r/datarecovery/comments/18xtmi8/what_software_for_windows_10_supporting/) I made on January 3^(rd).)\n\nSo, for context, I had wanted to make a backup image of the drives of three of my (or associated) devices:\n\n1. My laptop, as mentioned in the previous post. Specifically, a sector-by-sector image. (\\~475 GiB, \\~30 GiB free)\n2. A brother of mine's laptop (the one I'm using at this moment), largely as he's sending it into repair and they make no promises on data integrity. He prefers a sector-by-sector copy, too. (\\~475 GiB, \\~40 GiB free)\n3. An external hard drive that, while internally fine, has a *really* unreliable connection and is probably best to transition away from. (That, in fact, was the external drive I was referring to transferring my laptop's disk image to, which I talked myself out of.) *And for consistency's sake...* (2 TB; \\~1.81 TiB, \\~1 TiB free)\n\nThe big problem is, *I only have a 2 TB SSD to back those up to* (I, uhh, don't have much resources and was given it rather than having purchased it myself), and while that would be sufficient conventionally, AFAIK it's not sufficient if I want a sector-by-sector copy, which would be identically sized to the full space of the drive.\n\nSo, is there any way (with a program on Windows 10) to reduce the size of these while still containing *all information* on the drive? I'd assume something as simple as run-length compression could suffice. It would effectively have to be done dynamically, as otherwise the external hard drive will have to be entirely filled to transfer Image #3 uncompressed first, and the time constraints of tasks #1 and #2 massively advise against that.", "author_fullname": "t2_12oi4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any disk copying programs (for Windows 10) allow the (dynamic) compression of a sector-by-sector disk image/copy as it is being saved? If so, which ones?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_190ooqr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704620562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(This is bootleg-crossposted from &lt;a href=\"https://www.reddit.com/r/datarecovery/comments/190oajn/do_any_disk_copying_programs_for_windows_10_allow/\"&gt;this post&lt;/a&gt; I made to &lt;a href=\"/r/datarecovery\"&gt;r/datarecovery&lt;/a&gt;, moved here as I thought this community may be a better fit, in turn partially continued off of &lt;a href=\"https://www.reddit.com/r/datarecovery/comments/18xtmi8/what_software_for_windows_10_supporting/\"&gt;this post&lt;/a&gt; I made on January 3&lt;sup&gt;rd&lt;/sup&gt;.)&lt;/p&gt;\n\n&lt;p&gt;So, for context, I had wanted to make a backup image of the drives of three of my (or associated) devices:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My laptop, as mentioned in the previous post. Specifically, a sector-by-sector image. (~475 GiB, ~30 GiB free)&lt;/li&gt;\n&lt;li&gt;A brother of mine&amp;#39;s laptop (the one I&amp;#39;m using at this moment), largely as he&amp;#39;s sending it into repair and they make no promises on data integrity. He prefers a sector-by-sector copy, too. (~475 GiB, ~40 GiB free)&lt;/li&gt;\n&lt;li&gt;An external hard drive that, while internally fine, has a &lt;em&gt;really&lt;/em&gt; unreliable connection and is probably best to transition away from. (That, in fact, was the external drive I was referring to transferring my laptop&amp;#39;s disk image to, which I talked myself out of.) &lt;em&gt;And for consistency&amp;#39;s sake...&lt;/em&gt; (2 TB; ~1.81 TiB, ~1 TiB free)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The big problem is, &lt;em&gt;I only have a 2 TB SSD to back those up to&lt;/em&gt; (I, uhh, don&amp;#39;t have much resources and was given it rather than having purchased it myself), and while that would be sufficient conventionally, AFAIK it&amp;#39;s not sufficient if I want a sector-by-sector copy, which would be identically sized to the full space of the drive.&lt;/p&gt;\n\n&lt;p&gt;So, is there any way (with a program on Windows 10) to reduce the size of these while still containing &lt;em&gt;all information&lt;/em&gt; on the drive? I&amp;#39;d assume something as simple as run-length compression could suffice. It would effectively have to be done dynamically, as otherwise the external hard drive will have to be entirely filled to transfer Image #3 uncompressed first, and the time constraints of tasks #1 and #2 massively advise against that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190ooqr", "is_robot_indexable": true, "report_reasons": null, "author": "GrantExploit", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190ooqr/do_any_disk_copying_programs_for_windows_10_allow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190ooqr/do_any_disk_copying_programs_for_windows_10_allow/", "subreddit_subscribers": 723567, "created_utc": 1704620562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello I have been storing some of my data on a WD My Book External HDD. I never really thought about the beeping noise it makes until now. Is this drive dying? Should I get another drive?\n\n[My Drive making a weird Beeping noise.](https://reddit.com/link/190lp4w/video/9rl122l5oyac1/player)\n\nAnd here is the SmartCTL: \n\n    smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-13-amd64] (local build)\n    Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n    \n    === START OF INFORMATION SECTION ===\n    Device Model:     WDC WD40EDAZ-11SLVB0\n    Serial Number:    WD-WX12DC13FRYS\n    LU WWN Device Id: 5 0014ee 214ebda6c\n    Firmware Version: 80.00A80\n    User Capacity:    4,000,787,030,016 bytes [4.00 TB]\n    Sector Sizes:     512 bytes logical, 4096 bytes physical\n    Rotation Rate:    5400 rpm\n    Form Factor:      3.5 inches\n    TRIM Command:     Available, deterministic, zeroed\n    Device is:        Not in smartctl database 7.3/5319\n    ATA Version is:   ACS-3 T13/2161-D revision 5\n    SATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\n    Local Time is:    Sat Jan  6 22:03:38 2024 PST\n    SMART support is: Available - device has SMART capability.\n    SMART support is: Enabled\n    AAM feature is:   Unavailable\n    APM feature is:   Unavailable\n    Rd look-ahead is: Enabled\n    Write cache is:   Enabled\n    DSN feature is:   Unavailable\n    ATA Security is:  Disabled, NOT FROZEN [SEC1]\n    Wt Cache Reorder: Enabled\n    \n    === START OF READ SMART DATA SECTION ===\n    SMART overall-health self-assessment test result: PASSED\n    \n    General SMART Values:\n    Offline data collection status:  (0x82)\tOffline data collection activity\n    \t\t\t\t\twas completed without error.\n    \t\t\t\t\tAuto Offline Data Collection: Enabled.\n    Self-test execution status:      (   0)\tThe previous self-test routine completed\n    \t\t\t\t\twithout error or no self-test has ever \n    \t\t\t\t\tbeen run.\n    Total time to complete Offline \n    data collection: \t\t(48104) seconds.\n    Offline data collection\n    capabilities: \t\t\t (0x7b) SMART execute Offline immediate.\n    \t\t\t\t\tAuto Offline data collection on/off support.\n    \t\t\t\t\tSuspend Offline collection upon new\n    \t\t\t\t\tcommand.\n    \t\t\t\t\tOffline surface scan supported.\n    \t\t\t\t\tSelf-test supported.\n    \t\t\t\t\tConveyance Self-test supported.\n    \t\t\t\t\tSelective Self-test supported.\n    SMART capabilities:            (0x0003)\tSaves SMART data before entering\n    \t\t\t\t\tpower-saving mode.\n    \t\t\t\t\tSupports SMART auto save timer.\n    Error logging capability:        (0x01)\tError logging supported.\n    \t\t\t\t\tGeneral Purpose Logging supported.\n    Short self-test routine \n    recommended polling time: \t (   2) minutes.\n    Extended self-test routine\n    recommended polling time: \t ( 186) minutes.\n    Conveyance self-test routine\n    recommended polling time: \t (   2) minutes.\n    SCT capabilities: \t       (0x3031)\tSCT Status supported.\n    \t\t\t\t\tSCT Feature Control supported.\n    \t\t\t\t\tSCT Data Table supported.\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n      1 Raw_Read_Error_Rate     POSR-K   200   200   051    -    0\n      3 Spin_Up_Time            POS--K   209   204   021    -    2516\n      4 Start_Stop_Count        -O--CK   097   097   000    -    3322\n      5 Reallocated_Sector_Ct   PO--CK   200   200   140    -    0\n      7 Seek_Error_Rate         -OSR-K   200   200   000    -    4\n      9 Power_On_Hours          -O--CK   092   092   000    -    6317\n     10 Spin_Retry_Count        -O--CK   100   100   000    -    0\n     11 Calibration_Retry_Count -O--CK   100   100   000    -    0\n     12 Power_Cycle_Count       -O--CK   097   097   000    -    3322\n    192 Power-Off_Retract_Count -O--CK   200   200   000    -    18\n    193 Load_Cycle_Count        -O--CK   195   195   000    -    15405\n    194 Temperature_Celsius     -O---K   124   098   000    -    23\n    196 Reallocated_Event_Count -O--CK   200   200   000    -    0\n    197 Current_Pending_Sector  -O--CK   200   200   000    -    1\n    198 Offline_Uncorrectable   ----CK   200   200   000    -    0\n    199 UDMA_CRC_Error_Count    -O--CK   200   200   000    -    0\n    200 Multi_Zone_Error_Rate   ---R--   200   200   000    -    0\n                                ||||||_ K auto-keep\n                                |||||__ C event count\n                                ||||___ R error rate\n                                |||____ S speed/performance\n                                ||_____ O updated online\n                                |______ P prefailure warning\n    \n    General Purpose Log Directory Version 1\n    SMART           Log Directory Version 1 [multi-sector log support]\n    Address    Access  R/W   Size  Description\n    0x00       GPL,SL  R/O      1  Log Directory\n    0x01           SL  R/O      1  Summary SMART error log\n    0x02           SL  R/O      5  Comprehensive SMART error log\n    0x03       GPL     R/O      6  Ext. Comprehensive SMART error log\n    0x04       GPL     R/O    256  Device Statistics log\n    0x04       SL      R/O      8  Device Statistics log\n    0x06           SL  R/O      1  SMART self-test log\n    0x07       GPL     R/O      1  Extended self-test log\n    0x09           SL  R/W      1  Selective self-test log\n    0x0c       GPL     R/O   2048  Pending Defects log\n    0x10       GPL     R/O      1  NCQ Command Error log\n    0x11       GPL     R/O      1  SATA Phy Event Counters log\n    0x24       GPL     R/O    294  Current Device Internal Status Data log\n    0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n    0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n    0xa0-0xa7  GPL,SL  VS      16  Device vendor specific log\n    0xa8-0xb6  GPL,SL  VS       1  Device vendor specific log\n    0xb7       GPL,SL  VS      78  Device vendor specific log\n    0xb9       GPL,SL  VS       4  Device vendor specific log\n    0xbd       GPL,SL  VS       1  Device vendor specific log\n    0xc0       GPL,SL  VS       1  Device vendor specific log\n    0xc1       GPL     VS      93  Device vendor specific log\n    0xe0       GPL,SL  R/W      1  SCT Command/Status\n    0xe1       GPL,SL  R/W      1  SCT Data Transfer\n    \n    SMART Extended Comprehensive Error Log Version: 1 (6 sectors)\n    Device Error Count: 1\n    \tCR     = Command Register\n    \tFEATR  = Features Register\n    \tCOUNT  = Count (was: Sector Count) Register\n    \tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    \tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n    \tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    \tLL     = LBA Low (was: Sector Number) Register     ]\n    \tDV     = Device (was: Device/Head) Register\n    \tDC     = Device Control Register\n    \tER     = Error register\n    \tST     = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    \n    Error 1 [0] occurred at disk power-on lifetime: 156 hours (6 days + 12 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      40 -- 51 08 00 00 00 00 00 bd 60 40 00  Error: UNC 2048 sectors at LBA = 0x0000bd60 = 48480\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      25 00 00 08 00 00 00 00 00 b8 00 40 00     00:00:41.901  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 a0 00 40 00     00:00:41.766  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 90 00 40 00     00:00:41.653  READ DMA EXT\n      25 00 00 08 00 00 00 00 00 98 00 40 00     00:00:41.650  READ DMA EXT\n      25 00 00 00 08 00 00 00 00 10 00 40 00     00:00:41.625  READ DMA EXT\n    \n    SMART Extended Self-test Log Version: 1 (1 sectors)\n    No self-tests have been logged.  [To run self-tests, use: smartctl -t]\n    \n    SMART Selective self-test log data structure revision number 1\n     SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n        1        0        0  Not_testing\n        2        0        0  Not_testing\n        3        0        0  Not_testing\n        4        0        0  Not_testing\n        5        0        0  Not_testing\n    Selective self-test flags (0x0):\n      After scanning selected spans, do NOT read-scan remainder of disk.\n    If Selective self-test is pending on power-up, resume after 0 minute delay.\n    \n    SCT Status Version:                  3\n    SCT Version (vendor specific):       258 (0x0102)\n    Device State:                        Active (0)\n    Current Temperature:                    23 Celsius\n    Power Cycle Min/Max Temperature:     23/23 Celsius\n    Lifetime    Min/Max Temperature:     17/49 Celsius\n    Under/Over Temperature Limit Count:   0/0\n    Vendor specific:\n    01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n    \n    SCT Temperature History Version:     2\n    Temperature Sampling Period:         1 minute\n    Temperature Logging Interval:        1 minute\n    Min/Max recommended Temperature:      0/65 Celsius\n    Min/Max Temperature Limit:           -41/85 Celsius\n    Temperature History Size (Index):    478 (98)\n    \n    Index    Estimated Time   Temperature Celsius\n      99    2024-01-06 14:06    26  *******\n     100    2024-01-06 14:07    27  ********\n     ...    ..(  2 skipped).    ..  ********\n     103    2024-01-06 14:10    27  ********\n     104    2024-01-06 14:11    28  *********\n     ...    ..(  3 skipped).    ..  *********\n     108    2024-01-06 14:15    28  *********\n     109    2024-01-06 14:16    29  **********\n     ...    ..(  4 skipped).    ..  **********\n     114    2024-01-06 14:21    29  **********\n     115    2024-01-06 14:22    30  ***********\n     ...    ..(  4 skipped).    ..  ***********\n     120    2024-01-06 14:27    30  ***********\n     121    2024-01-06 14:28     ?  -\n     122    2024-01-06 14:29    22  ***\n     123    2024-01-06 14:30    22  ***\n     124    2024-01-06 14:31    23  ****\n     125    2024-01-06 14:32    23  ****\n     126    2024-01-06 14:33     ?  -\n     127    2024-01-06 14:34    22  ***\n     128    2024-01-06 14:35    22  ***\n     129    2024-01-06 14:36    22  ***\n     130    2024-01-06 14:37    23  ****\n     131    2024-01-06 14:38    24  *****\n     132    2024-01-06 14:39    24  *****\n     133    2024-01-06 14:40    25  ******\n     134    2024-01-06 14:41    25  ******\n     135    2024-01-06 14:42    25  ******\n     136    2024-01-06 14:43    26  *******\n     ...    ..(  3 skipped).    ..  *******\n     140    2024-01-06 14:47    26  *******\n     141    2024-01-06 14:48    27  ********\n     ...    ..(  3 skipped).    ..  ********\n     145    2024-01-06 14:52    27  ********\n     146    2024-01-06 14:53    28  *********\n     ...    ..(  3 skipped).    ..  *********\n     150    2024-01-06 14:57    28  *********\n     151    2024-01-06 14:58    29  **********\n     ...    ..(  6 skipped).    ..  **********\n     158    2024-01-06 15:05    29  **********\n     159    2024-01-06 15:06     ?  -\n     160    2024-01-06 15:07    17  -\n     161    2024-01-06 15:08    17  -\n     162    2024-01-06 15:09    17  -\n     163    2024-01-06 15:10     ?  -\n     164    2024-01-06 15:11    19  -\n     165    2024-01-06 15:12    19  -\n     166    2024-01-06 15:13    20  *\n     167    2024-01-06 15:14    20  *\n     168    2024-01-06 15:15    21  **\n     169    2024-01-06 15:16    21  **\n     170    2024-01-06 15:17    22  ***\n     171    2024-01-06 15:18    22  ***\n     172    2024-01-06 15:19    23  ****\n     173    2024-01-06 15:20    23  ****\n     174    2024-01-06 15:21    23  ****\n     175    2024-01-06 15:22    24  *****\n     176    2024-01-06 15:23    25  ******\n     177    2024-01-06 15:24    25  ******\n     178    2024-01-06 15:25    25  ******\n     179    2024-01-06 15:26    26  *******\n     ...    ..(  2 skipped).    ..  *******\n     182    2024-01-06 15:29    26  *******\n     183    2024-01-06 15:30    27  ********\n     184    2024-01-06 15:31    27  ********\n     185    2024-01-06 15:32    27  ********\n     186    2024-01-06 15:33    28  *********\n     187    2024-01-06 15:34    28  *********\n     188    2024-01-06 15:35    28  *********\n     189    2024-01-06 15:36    29  **********\n     ...    ..(  3 skipped).    ..  **********\n     193    2024-01-06 15:40    29  **********\n     194    2024-01-06 15:41    30  ***********\n     ...    ..(  3 skipped).    ..  ***********\n     198    2024-01-06 15:45    30  ***********\n     199    2024-01-06 15:46    31  ************\n     ...    ..(  4 skipped).    ..  ************\n     204    2024-01-06 15:51    31  ************\n     205    2024-01-06 15:52    32  *************\n     ...    ..(  5 skipped).    ..  *************\n     211    2024-01-06 15:58    32  *************\n     212    2024-01-06 15:59    33  **************\n     ...    ..(  6 skipped).    ..  **************\n     219    2024-01-06 16:06    33  **************\n     220    2024-01-06 16:07    34  ***************\n     ...    ..( 11 skipped).    ..  ***************\n     232    2024-01-06 16:19    34  ***************\n     233    2024-01-06 16:20    35  ****************\n     ...    ..( 15 skipped).    ..  ****************\n     249    2024-01-06 16:36    35  ****************\n     250    2024-01-06 16:37    36  *****************\n     ...    ..( 40 skipped).    ..  *****************\n     291    2024-01-06 17:18    36  *****************\n     292    2024-01-06 17:19    37  ******************\n     293    2024-01-06 17:20    36  *****************\n     294    2024-01-06 17:21    36  *****************\n     295    2024-01-06 17:22    37  ******************\n     296    2024-01-06 17:23    36  *****************\n     ...    ..(  4 skipped).    ..  *****************\n     301    2024-01-06 17:28    36  *****************\n     302    2024-01-06 17:29    37  ******************\n     303    2024-01-06 17:30    37  ******************\n     304    2024-01-06 17:31    36  *****************\n     305    2024-01-06 17:32    36  *****************\n     306    2024-01-06 17:33    37  ******************\n     ...    ..(  2 skipped).    ..  ******************\n     309    2024-01-06 17:36    37  ******************\n     310    2024-01-06 17:37    36  *****************\n     311    2024-01-06 17:38    37  ******************\n     312    2024-01-06 17:39     ?  -\n     313    2024-01-06 17:40    18  -\n     314    2024-01-06 17:41    18  -\n     315    2024-01-06 17:42    18  -\n     316    2024-01-06 17:43    19  -\n     317    2024-01-06 17:44    19  -\n     318    2024-01-06 17:45    20  *\n     319    2024-01-06 17:46    20  *\n     320    2024-01-06 17:47    21  **\n     321    2024-01-06 17:48    21  **\n     322    2024-01-06 17:49    21  **\n     323    2024-01-06 17:50     ?  -\n     324    2024-01-06 17:51    22  ***\n     325    2024-01-06 17:52    22  ***\n     326    2024-01-06 17:53     ?  -\n     327    2024-01-06 17:54    22  ***\n     328    2024-01-06 17:55    22  ***\n     329    2024-01-06 17:56    22  ***\n     330    2024-01-06 17:57    23  ****\n     331    2024-01-06 17:58    23  ****\n     332    2024-01-06 17:59     ?  -\n     333    2024-01-06 18:00    23  ****\n     334    2024-01-06 18:01    24  *****\n     335    2024-01-06 18:02    25  ******\n     336    2024-01-06 18:03    25  ******\n     337    2024-01-06 18:04    25  ******\n     338    2024-01-06 18:05    26  *******\n     339    2024-01-06 18:06    26  *******\n     340    2024-01-06 18:07     ?  -\n     341    2024-01-06 18:08    25  ******\n     ...    ..(  2 skipped).    ..  ******\n     344    2024-01-06 18:11    25  ******\n     345    2024-01-06 18:12    26  *******\n     346    2024-01-06 18:13     ?  -\n     347    2024-01-06 18:14    23  ****\n     348    2024-01-06 18:15    23  ****\n     349    2024-01-06 18:16    38  *******************\n     ...    ..( 46 skipped).    ..  *******************\n     396    2024-01-06 19:03    38  *******************\n     397    2024-01-06 19:04     ?  -\n     398    2024-01-06 19:05    23  ****\n     399    2024-01-06 19:06    23  ****\n     400    2024-01-06 19:07     ?  -\n     401    2024-01-06 19:08    24  *****\n     402    2024-01-06 19:09    24  *****\n     403    2024-01-06 19:10    24  *****\n     404    2024-01-06 19:11    25  ******\n     405    2024-01-06 19:12    25  ******\n     406    2024-01-06 19:13    25  ******\n     407    2024-01-06 19:14    26  *******\n     408    2024-01-06 19:15    26  *******\n     409    2024-01-06 19:16    27  ********\n     410    2024-01-06 19:17    27  ********\n     411    2024-01-06 19:18    28  *********\n     412    2024-01-06 19:19    28  *********\n     413    2024-01-06 19:20    28  *********\n     414    2024-01-06 19:21    29  **********\n     415    2024-01-06 19:22    29  **********\n     416    2024-01-06 19:23    29  **********\n     417    2024-01-06 19:24    30  ***********\n     418    2024-01-06 19:25    30  ***********\n     419    2024-01-06 19:26    30  ***********\n     420    2024-01-06 19:27    31  ************\n     421    2024-01-06 19:28    31  ************\n     422    2024-01-06 19:29    31  ************\n     423    2024-01-06 19:30     ?  -\n     424    2024-01-06 19:31    29  **********\n     ...    ..(  2 skipped).    ..  **********\n     427    2024-01-06 19:34    29  **********\n     428    2024-01-06 19:35    30  ***********\n     429    2024-01-06 19:36    30  ***********\n     430    2024-01-06 19:37    31  ************\n     ...    ..(  2 skipped).    ..  ************\n     433    2024-01-06 19:40    31  ************\n     434    2024-01-06 19:41    32  *************\n     ...    ..(  5 skipped).    ..  *************\n     440    2024-01-06 19:47    32  *************\n     441    2024-01-06 19:48     ?  -\n     442    2024-01-06 19:49    20  *\n     443    2024-01-06 19:50    20  *\n     444    2024-01-06 19:51    20  *\n     445    2024-01-06 19:52    21  **\n     446    2024-01-06 19:53    21  **\n     447    2024-01-06 19:54    22  ***\n     448    2024-01-06 19:55    22  ***\n     449    2024-01-06 19:56    23  ****\n     450    2024-01-06 19:57    23  ****\n     451    2024-01-06 19:58    24  *****\n     452    2024-01-06 19:59    24  *****\n     453    2024-01-06 20:00    25  ******\n     454    2024-01-06 20:01     ?  -\n     455    2024-01-06 20:02    25  ******\n     456    2024-01-06 20:03    25  ******\n     457    2024-01-06 20:04    25  ******\n     458    2024-01-06 20:05    26  *******\n     459    2024-01-06 20:06    26  *******\n     460    2024-01-06 20:07    26  *******\n     461    2024-01-06 20:08    27  ********\n     462    2024-01-06 20:09    27  ********\n     463    2024-01-06 20:10    27  ********\n     464    2024-01-06 20:11    28  *********\n     ...    ..(  2 skipped).    ..  *********\n     467    2024-01-06 20:14    28  *********\n     468    2024-01-06 20:15    29  **********\n     ...    ..(  2 skipped).    ..  **********\n     471    2024-01-06 20:18    29  **********\n     472    2024-01-06 20:19    30  ***********\n     ...    ..(  2 skipped).    ..  ***********\n     475    2024-01-06 20:22    30  ***********\n     476    2024-01-06 20:23    31  ************\n     ...    ..(  2 skipped).    ..  ************\n       1    2024-01-06 20:26    31  ************\n       2    2024-01-06 20:27    32  *************\n     ...    ..(  5 skipped).    ..  *************\n       8    2024-01-06 20:33    32  *************\n       9    2024-01-06 20:34    33  **************\n     ...    ..(  5 skipped).    ..  **************\n      15    2024-01-06 20:40    33  **************\n      16    2024-01-06 20:41    34  ***************\n     ...    ..(  8 skipped).    ..  ***************\n      25    2024-01-06 20:50    34  ***************\n      26    2024-01-06 20:51    35  ****************\n     ...    ..(  9 skipped).    ..  ****************\n      36    2024-01-06 21:01    35  ****************\n      37    2024-01-06 21:02    36  *****************\n     ...    ..( 11 skipped).    ..  *****************\n      49    2024-01-06 21:14    36  *****************\n      50    2024-01-06 21:15     ?  -\n      51    2024-01-06 21:16    21  **\n      52    2024-01-06 21:17    21  **\n      53    2024-01-06 21:18    22  ***\n      54    2024-01-06 21:19    22  ***\n      55    2024-01-06 21:20    23  ****\n      56    2024-01-06 21:21    23  ****\n      57    2024-01-06 21:22    24  *****\n      58    2024-01-06 21:23    24  *****\n      59    2024-01-06 21:24    25  ******\n     ...    ..(  2 skipped).    ..  ******\n      62    2024-01-06 21:27    25  ******\n      63    2024-01-06 21:28    26  *******\n     ...    ..(  3 skipped).    ..  *******\n      67    2024-01-06 21:32    26  *******\n      68    2024-01-06 21:33    27  ********\n     ...    ..(  4 skipped).    ..  ********\n      73    2024-01-06 21:38    27  ********\n      74    2024-01-06 21:39    28  *********\n     ...    ..(  3 skipped).    ..  *********\n      78    2024-01-06 21:43    28  *********\n      79    2024-01-06 21:44    29  **********\n     ...    ..(  2 skipped).    ..  **********\n      82    2024-01-06 21:47    29  **********\n      83    2024-01-06 21:48     ?  -\n      84    2024-01-06 21:49    28  *********\n     ...    ..(  2 skipped).    ..  *********\n      87    2024-01-06 21:52    28  *********\n      88    2024-01-06 21:53     ?  -\n      89    2024-01-06 21:54    23  ****\n      90    2024-01-06 21:55    23  ****\n      91    2024-01-06 21:56    23  ****\n      92    2024-01-06 21:57    24  *****\n      93    2024-01-06 21:58    24  *****\n      94    2024-01-06 21:59    25  ******\n      95    2024-01-06 22:00    26  *******\n     ...    ..(  2 skipped).    ..  *******\n      98    2024-01-06 22:03    26  *******\n    \n    SCT Error Recovery Control command not supported\n    \n    Device Statistics (GP Log 0x04)\n    Page  Offset Size        Value Flags Description\n    0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n    0x01  0x008  4            3322  ---  Lifetime Power-On Resets\n    0x01  0x010  4            6317  ---  Power-on Hours\n    0x01  0x018  6      5976322892  ---  Logical Sectors Written\n    0x01  0x020  6         9940809  ---  Number of Write Commands\n    0x01  0x028  6     14678492017  ---  Logical Sectors Read\n    0x01  0x030  6        43174921  ---  Number of Read Commands\n    0x01  0x038  6      1266363520  ---  Date and Time TimeStamp\n    0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n    0x03  0x008  4            6289  ---  Spindle Motor Power-on Hours\n    0x03  0x010  4            3747  ---  Head Flying Hours\n    0x03  0x018  4           15424  ---  Head Load Events\n    0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n    0x03  0x028  4            1071  ---  Read Recovery Attempts\n    0x03  0x030  4               0  ---  Number of Mechanical Start Failures\n    0x03  0x038  4              16  ---  Number of Realloc. Candidate Logical Sectors\n    0x03  0x040  4              18  ---  Number of High Priority Unload Events\n    0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n    0x04  0x008  4               1  ---  Number of Reported Uncorrectable Errors\n    0x04  0x010  4              37  ---  Resets Between Cmd Acceptance and Completion\n    0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n    0x05  0x008  1              23  ---  Current Temperature\n    0x05  0x010  1              39  ---  Average Short Term Temperature\n    0x05  0x018  1              41  ---  Average Long Term Temperature\n    0x05  0x020  1              49  ---  Highest Temperature\n    0x05  0x028  1              22  ---  Lowest Temperature\n    0x05  0x030  1              44  ---  Highest Average Short Term Temperature\n    0x05  0x038  1              30  ---  Lowest Average Short Term Temperature\n    0x05  0x040  1              41  ---  Highest Average Long Term Temperature\n    0x05  0x048  1              34  ---  Lowest Average Long Term Temperature\n    0x05  0x050  4               0  ---  Time in Over-Temperature\n    0x05  0x058  1              65  ---  Specified Maximum Operating Temperature\n    0x05  0x060  4               0  ---  Time in Under-Temperature\n    0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n    0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n    0x06  0x008  4            3365  ---  Number of Hardware Resets\n    0x06  0x010  4              42  ---  Number of ASR Events\n    0x06  0x018  4               0  ---  Number of Interface CRC Errors\n    0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n    0xff  0x008  7               0  ---  Vendor Specific\n    0xff  0x010  7               0  ---  Vendor Specific\n    0xff  0x018  7               0  ---  Vendor Specific\n                                    |||_ C monitored condition met\n                                    ||__ D supports DSN\n                                    |___ N normalized value\n    \n    Pending Defects log (GP Log 0x0c)\n    No Defects Logged\n    \n    SATA Phy Event Counters (GP Log 0x11)\n    ID      Size     Value  Description\n    0x0001  2            0  Command failed due to ICRC error\n    0x0002  2            0  R_ERR response for data FIS\n    0x0003  2            0  R_ERR response for device-to-host data FIS\n    0x0004  2            0  R_ERR response for host-to-device data FIS\n    0x0005  2            0  R_ERR response for non-data FIS\n    0x0006  2            0  R_ERR response for device-to-host non-data FIS\n    0x0007  2            0  R_ERR response for host-to-device non-data FIS\n    0x0008  2            0  Device-to-host non-data FIS retries\n    0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy\n    0x000a  2            1  Device-to-host register FISes sent due to a COMRESET\n    0x000b  2            0  CRC errors within host-to-device FIS\n    0x000d  2            0  Non-CRC errors within host-to-device FIS\n    0x000f  2            0  R_ERR response for host-to-device data FIS, CRC\n    0x0012  2            0  R_ERR response for host-to-device non-data FIS, CRC\n    0x8000  4           96  Vendor specific\n    \n    \n\n&amp;#x200B;", "author_fullname": "t2_6e895hxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a normal HDD sound?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"9rl122l5oyac1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/190lp4w/asset/9rl122l5oyac1/DASHPlaylist.mpd?a=1707237608%2COWViZmVmMzMwOTcwMWQ0YmRhMWEwYzZiZDlmMWQ0MmU0MmRjYzgzYjIxY2I3ZjI5Yzk4YTI3MWQ3ZjA3NzAxYQ%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 1920, "hlsUrl": "https://v.redd.it/link/190lp4w/asset/9rl122l5oyac1/HLSPlaylist.m3u8?a=1707237608%2COTNlMzc0NTVlYTc5ZDA3Y2U4ODhmM2ZlNWViOTU4YTY4Y2IwNWUwOGI0ZTg2M2NlNDUxOGZiYWQ3MjNlMjE4ZQ%3D%3D&amp;v=1&amp;f=sd", "id": "9rl122l5oyac1", "isGif": false}}, "name": "t3_190lp4w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704608543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I have been storing some of my data on a WD My Book External HDD. I never really thought about the beeping noise it makes until now. Is this drive dying? Should I get another drive?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/190lp4w/video/9rl122l5oyac1/player\"&gt;My Drive making a weird Beeping noise.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the SmartCTL: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-13-amd64] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     WDC WD40EDAZ-11SLVB0\nSerial Number:    WD-WX12DC13FRYS\nLU WWN Device Id: 5 0014ee 214ebda6c\nFirmware Version: 80.00A80\nUser Capacity:    4,000,787,030,016 bytes [4.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    5400 rpm\nForm Factor:      3.5 inches\nTRIM Command:     Available, deterministic, zeroed\nDevice is:        Not in smartctl database 7.3/5319\nATA Version is:   ACS-3 T13/2161-D revision 5\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Sat Jan  6 22:03:38 2024 PST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Unavailable\nAPM feature is:   Unavailable\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, NOT FROZEN [SEC1]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x82) Offline data collection activity\n                    was completed without error.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                    without error or no self-test has ever \n                    been run.\nTotal time to complete Offline \ndata collection:        (48104) seconds.\nOffline data collection\ncapabilities:            (0x7b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 186) minutes.\nConveyance self-test routine\nrecommended polling time:    (   2) minutes.\nSCT capabilities:          (0x3031) SCT Status supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     POSR-K   200   200   051    -    0\n  3 Spin_Up_Time            POS--K   209   204   021    -    2516\n  4 Start_Stop_Count        -O--CK   097   097   000    -    3322\n  5 Reallocated_Sector_Ct   PO--CK   200   200   140    -    0\n  7 Seek_Error_Rate         -OSR-K   200   200   000    -    4\n  9 Power_On_Hours          -O--CK   092   092   000    -    6317\n 10 Spin_Retry_Count        -O--CK   100   100   000    -    0\n 11 Calibration_Retry_Count -O--CK   100   100   000    -    0\n 12 Power_Cycle_Count       -O--CK   097   097   000    -    3322\n192 Power-Off_Retract_Count -O--CK   200   200   000    -    18\n193 Load_Cycle_Count        -O--CK   195   195   000    -    15405\n194 Temperature_Celsius     -O---K   124   098   000    -    23\n196 Reallocated_Event_Count -O--CK   200   200   000    -    0\n197 Current_Pending_Sector  -O--CK   200   200   000    -    1\n198 Offline_Uncorrectable   ----CK   200   200   000    -    0\n199 UDMA_CRC_Error_Count    -O--CK   200   200   000    -    0\n200 Multi_Zone_Error_Rate   ---R--   200   200   000    -    0\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      5  Comprehensive SMART error log\n0x03       GPL     R/O      6  Ext. Comprehensive SMART error log\n0x04       GPL     R/O    256  Device Statistics log\n0x04       SL      R/O      8  Device Statistics log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      1  Extended self-test log\n0x09           SL  R/W      1  Selective self-test log\n0x0c       GPL     R/O   2048  Pending Defects log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x24       GPL     R/O    294  Current Device Internal Status Data log\n0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xa0-0xa7  GPL,SL  VS      16  Device vendor specific log\n0xa8-0xb6  GPL,SL  VS       1  Device vendor specific log\n0xb7       GPL,SL  VS      78  Device vendor specific log\n0xb9       GPL,SL  VS       4  Device vendor specific log\n0xbd       GPL,SL  VS       1  Device vendor specific log\n0xc0       GPL,SL  VS       1  Device vendor specific log\n0xc1       GPL     VS      93  Device vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (6 sectors)\nDevice Error Count: 1\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 1 [0] occurred at disk power-on lifetime: 156 hours (6 days + 12 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 51 08 00 00 00 00 00 bd 60 40 00  Error: UNC 2048 sectors at LBA = 0x0000bd60 = 48480\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  25 00 00 08 00 00 00 00 00 b8 00 40 00     00:00:41.901  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 a0 00 40 00     00:00:41.766  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 90 00 40 00     00:00:41.653  READ DMA EXT\n  25 00 00 08 00 00 00 00 00 98 00 40 00     00:00:41.650  READ DMA EXT\n  25 00 00 00 08 00 00 00 00 10 00 40 00     00:00:41.625  READ DMA EXT\n\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNo self-tests have been logged.  [To run self-tests, use: smartctl -t]\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  3\nSCT Version (vendor specific):       258 (0x0102)\nDevice State:                        Active (0)\nCurrent Temperature:                    23 Celsius\nPower Cycle Min/Max Temperature:     23/23 Celsius\nLifetime    Min/Max Temperature:     17/49 Celsius\nUnder/Over Temperature Limit Count:   0/0\nVendor specific:\n01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         1 minute\nTemperature Logging Interval:        1 minute\nMin/Max recommended Temperature:      0/65 Celsius\nMin/Max Temperature Limit:           -41/85 Celsius\nTemperature History Size (Index):    478 (98)\n\nIndex    Estimated Time   Temperature Celsius\n  99    2024-01-06 14:06    26  *******\n 100    2024-01-06 14:07    27  ********\n ...    ..(  2 skipped).    ..  ********\n 103    2024-01-06 14:10    27  ********\n 104    2024-01-06 14:11    28  *********\n ...    ..(  3 skipped).    ..  *********\n 108    2024-01-06 14:15    28  *********\n 109    2024-01-06 14:16    29  **********\n ...    ..(  4 skipped).    ..  **********\n 114    2024-01-06 14:21    29  **********\n 115    2024-01-06 14:22    30  ***********\n ...    ..(  4 skipped).    ..  ***********\n 120    2024-01-06 14:27    30  ***********\n 121    2024-01-06 14:28     ?  -\n 122    2024-01-06 14:29    22  ***\n 123    2024-01-06 14:30    22  ***\n 124    2024-01-06 14:31    23  ****\n 125    2024-01-06 14:32    23  ****\n 126    2024-01-06 14:33     ?  -\n 127    2024-01-06 14:34    22  ***\n 128    2024-01-06 14:35    22  ***\n 129    2024-01-06 14:36    22  ***\n 130    2024-01-06 14:37    23  ****\n 131    2024-01-06 14:38    24  *****\n 132    2024-01-06 14:39    24  *****\n 133    2024-01-06 14:40    25  ******\n 134    2024-01-06 14:41    25  ******\n 135    2024-01-06 14:42    25  ******\n 136    2024-01-06 14:43    26  *******\n ...    ..(  3 skipped).    ..  *******\n 140    2024-01-06 14:47    26  *******\n 141    2024-01-06 14:48    27  ********\n ...    ..(  3 skipped).    ..  ********\n 145    2024-01-06 14:52    27  ********\n 146    2024-01-06 14:53    28  *********\n ...    ..(  3 skipped).    ..  *********\n 150    2024-01-06 14:57    28  *********\n 151    2024-01-06 14:58    29  **********\n ...    ..(  6 skipped).    ..  **********\n 158    2024-01-06 15:05    29  **********\n 159    2024-01-06 15:06     ?  -\n 160    2024-01-06 15:07    17  -\n 161    2024-01-06 15:08    17  -\n 162    2024-01-06 15:09    17  -\n 163    2024-01-06 15:10     ?  -\n 164    2024-01-06 15:11    19  -\n 165    2024-01-06 15:12    19  -\n 166    2024-01-06 15:13    20  *\n 167    2024-01-06 15:14    20  *\n 168    2024-01-06 15:15    21  **\n 169    2024-01-06 15:16    21  **\n 170    2024-01-06 15:17    22  ***\n 171    2024-01-06 15:18    22  ***\n 172    2024-01-06 15:19    23  ****\n 173    2024-01-06 15:20    23  ****\n 174    2024-01-06 15:21    23  ****\n 175    2024-01-06 15:22    24  *****\n 176    2024-01-06 15:23    25  ******\n 177    2024-01-06 15:24    25  ******\n 178    2024-01-06 15:25    25  ******\n 179    2024-01-06 15:26    26  *******\n ...    ..(  2 skipped).    ..  *******\n 182    2024-01-06 15:29    26  *******\n 183    2024-01-06 15:30    27  ********\n 184    2024-01-06 15:31    27  ********\n 185    2024-01-06 15:32    27  ********\n 186    2024-01-06 15:33    28  *********\n 187    2024-01-06 15:34    28  *********\n 188    2024-01-06 15:35    28  *********\n 189    2024-01-06 15:36    29  **********\n ...    ..(  3 skipped).    ..  **********\n 193    2024-01-06 15:40    29  **********\n 194    2024-01-06 15:41    30  ***********\n ...    ..(  3 skipped).    ..  ***********\n 198    2024-01-06 15:45    30  ***********\n 199    2024-01-06 15:46    31  ************\n ...    ..(  4 skipped).    ..  ************\n 204    2024-01-06 15:51    31  ************\n 205    2024-01-06 15:52    32  *************\n ...    ..(  5 skipped).    ..  *************\n 211    2024-01-06 15:58    32  *************\n 212    2024-01-06 15:59    33  **************\n ...    ..(  6 skipped).    ..  **************\n 219    2024-01-06 16:06    33  **************\n 220    2024-01-06 16:07    34  ***************\n ...    ..( 11 skipped).    ..  ***************\n 232    2024-01-06 16:19    34  ***************\n 233    2024-01-06 16:20    35  ****************\n ...    ..( 15 skipped).    ..  ****************\n 249    2024-01-06 16:36    35  ****************\n 250    2024-01-06 16:37    36  *****************\n ...    ..( 40 skipped).    ..  *****************\n 291    2024-01-06 17:18    36  *****************\n 292    2024-01-06 17:19    37  ******************\n 293    2024-01-06 17:20    36  *****************\n 294    2024-01-06 17:21    36  *****************\n 295    2024-01-06 17:22    37  ******************\n 296    2024-01-06 17:23    36  *****************\n ...    ..(  4 skipped).    ..  *****************\n 301    2024-01-06 17:28    36  *****************\n 302    2024-01-06 17:29    37  ******************\n 303    2024-01-06 17:30    37  ******************\n 304    2024-01-06 17:31    36  *****************\n 305    2024-01-06 17:32    36  *****************\n 306    2024-01-06 17:33    37  ******************\n ...    ..(  2 skipped).    ..  ******************\n 309    2024-01-06 17:36    37  ******************\n 310    2024-01-06 17:37    36  *****************\n 311    2024-01-06 17:38    37  ******************\n 312    2024-01-06 17:39     ?  -\n 313    2024-01-06 17:40    18  -\n 314    2024-01-06 17:41    18  -\n 315    2024-01-06 17:42    18  -\n 316    2024-01-06 17:43    19  -\n 317    2024-01-06 17:44    19  -\n 318    2024-01-06 17:45    20  *\n 319    2024-01-06 17:46    20  *\n 320    2024-01-06 17:47    21  **\n 321    2024-01-06 17:48    21  **\n 322    2024-01-06 17:49    21  **\n 323    2024-01-06 17:50     ?  -\n 324    2024-01-06 17:51    22  ***\n 325    2024-01-06 17:52    22  ***\n 326    2024-01-06 17:53     ?  -\n 327    2024-01-06 17:54    22  ***\n 328    2024-01-06 17:55    22  ***\n 329    2024-01-06 17:56    22  ***\n 330    2024-01-06 17:57    23  ****\n 331    2024-01-06 17:58    23  ****\n 332    2024-01-06 17:59     ?  -\n 333    2024-01-06 18:00    23  ****\n 334    2024-01-06 18:01    24  *****\n 335    2024-01-06 18:02    25  ******\n 336    2024-01-06 18:03    25  ******\n 337    2024-01-06 18:04    25  ******\n 338    2024-01-06 18:05    26  *******\n 339    2024-01-06 18:06    26  *******\n 340    2024-01-06 18:07     ?  -\n 341    2024-01-06 18:08    25  ******\n ...    ..(  2 skipped).    ..  ******\n 344    2024-01-06 18:11    25  ******\n 345    2024-01-06 18:12    26  *******\n 346    2024-01-06 18:13     ?  -\n 347    2024-01-06 18:14    23  ****\n 348    2024-01-06 18:15    23  ****\n 349    2024-01-06 18:16    38  *******************\n ...    ..( 46 skipped).    ..  *******************\n 396    2024-01-06 19:03    38  *******************\n 397    2024-01-06 19:04     ?  -\n 398    2024-01-06 19:05    23  ****\n 399    2024-01-06 19:06    23  ****\n 400    2024-01-06 19:07     ?  -\n 401    2024-01-06 19:08    24  *****\n 402    2024-01-06 19:09    24  *****\n 403    2024-01-06 19:10    24  *****\n 404    2024-01-06 19:11    25  ******\n 405    2024-01-06 19:12    25  ******\n 406    2024-01-06 19:13    25  ******\n 407    2024-01-06 19:14    26  *******\n 408    2024-01-06 19:15    26  *******\n 409    2024-01-06 19:16    27  ********\n 410    2024-01-06 19:17    27  ********\n 411    2024-01-06 19:18    28  *********\n 412    2024-01-06 19:19    28  *********\n 413    2024-01-06 19:20    28  *********\n 414    2024-01-06 19:21    29  **********\n 415    2024-01-06 19:22    29  **********\n 416    2024-01-06 19:23    29  **********\n 417    2024-01-06 19:24    30  ***********\n 418    2024-01-06 19:25    30  ***********\n 419    2024-01-06 19:26    30  ***********\n 420    2024-01-06 19:27    31  ************\n 421    2024-01-06 19:28    31  ************\n 422    2024-01-06 19:29    31  ************\n 423    2024-01-06 19:30     ?  -\n 424    2024-01-06 19:31    29  **********\n ...    ..(  2 skipped).    ..  **********\n 427    2024-01-06 19:34    29  **********\n 428    2024-01-06 19:35    30  ***********\n 429    2024-01-06 19:36    30  ***********\n 430    2024-01-06 19:37    31  ************\n ...    ..(  2 skipped).    ..  ************\n 433    2024-01-06 19:40    31  ************\n 434    2024-01-06 19:41    32  *************\n ...    ..(  5 skipped).    ..  *************\n 440    2024-01-06 19:47    32  *************\n 441    2024-01-06 19:48     ?  -\n 442    2024-01-06 19:49    20  *\n 443    2024-01-06 19:50    20  *\n 444    2024-01-06 19:51    20  *\n 445    2024-01-06 19:52    21  **\n 446    2024-01-06 19:53    21  **\n 447    2024-01-06 19:54    22  ***\n 448    2024-01-06 19:55    22  ***\n 449    2024-01-06 19:56    23  ****\n 450    2024-01-06 19:57    23  ****\n 451    2024-01-06 19:58    24  *****\n 452    2024-01-06 19:59    24  *****\n 453    2024-01-06 20:00    25  ******\n 454    2024-01-06 20:01     ?  -\n 455    2024-01-06 20:02    25  ******\n 456    2024-01-06 20:03    25  ******\n 457    2024-01-06 20:04    25  ******\n 458    2024-01-06 20:05    26  *******\n 459    2024-01-06 20:06    26  *******\n 460    2024-01-06 20:07    26  *******\n 461    2024-01-06 20:08    27  ********\n 462    2024-01-06 20:09    27  ********\n 463    2024-01-06 20:10    27  ********\n 464    2024-01-06 20:11    28  *********\n ...    ..(  2 skipped).    ..  *********\n 467    2024-01-06 20:14    28  *********\n 468    2024-01-06 20:15    29  **********\n ...    ..(  2 skipped).    ..  **********\n 471    2024-01-06 20:18    29  **********\n 472    2024-01-06 20:19    30  ***********\n ...    ..(  2 skipped).    ..  ***********\n 475    2024-01-06 20:22    30  ***********\n 476    2024-01-06 20:23    31  ************\n ...    ..(  2 skipped).    ..  ************\n   1    2024-01-06 20:26    31  ************\n   2    2024-01-06 20:27    32  *************\n ...    ..(  5 skipped).    ..  *************\n   8    2024-01-06 20:33    32  *************\n   9    2024-01-06 20:34    33  **************\n ...    ..(  5 skipped).    ..  **************\n  15    2024-01-06 20:40    33  **************\n  16    2024-01-06 20:41    34  ***************\n ...    ..(  8 skipped).    ..  ***************\n  25    2024-01-06 20:50    34  ***************\n  26    2024-01-06 20:51    35  ****************\n ...    ..(  9 skipped).    ..  ****************\n  36    2024-01-06 21:01    35  ****************\n  37    2024-01-06 21:02    36  *****************\n ...    ..( 11 skipped).    ..  *****************\n  49    2024-01-06 21:14    36  *****************\n  50    2024-01-06 21:15     ?  -\n  51    2024-01-06 21:16    21  **\n  52    2024-01-06 21:17    21  **\n  53    2024-01-06 21:18    22  ***\n  54    2024-01-06 21:19    22  ***\n  55    2024-01-06 21:20    23  ****\n  56    2024-01-06 21:21    23  ****\n  57    2024-01-06 21:22    24  *****\n  58    2024-01-06 21:23    24  *****\n  59    2024-01-06 21:24    25  ******\n ...    ..(  2 skipped).    ..  ******\n  62    2024-01-06 21:27    25  ******\n  63    2024-01-06 21:28    26  *******\n ...    ..(  3 skipped).    ..  *******\n  67    2024-01-06 21:32    26  *******\n  68    2024-01-06 21:33    27  ********\n ...    ..(  4 skipped).    ..  ********\n  73    2024-01-06 21:38    27  ********\n  74    2024-01-06 21:39    28  *********\n ...    ..(  3 skipped).    ..  *********\n  78    2024-01-06 21:43    28  *********\n  79    2024-01-06 21:44    29  **********\n ...    ..(  2 skipped).    ..  **********\n  82    2024-01-06 21:47    29  **********\n  83    2024-01-06 21:48     ?  -\n  84    2024-01-06 21:49    28  *********\n ...    ..(  2 skipped).    ..  *********\n  87    2024-01-06 21:52    28  *********\n  88    2024-01-06 21:53     ?  -\n  89    2024-01-06 21:54    23  ****\n  90    2024-01-06 21:55    23  ****\n  91    2024-01-06 21:56    23  ****\n  92    2024-01-06 21:57    24  *****\n  93    2024-01-06 21:58    24  *****\n  94    2024-01-06 21:59    25  ******\n  95    2024-01-06 22:00    26  *******\n ...    ..(  2 skipped).    ..  *******\n  98    2024-01-06 22:03    26  *******\n\nSCT Error Recovery Control command not supported\n\nDevice Statistics (GP Log 0x04)\nPage  Offset Size        Value Flags Description\n0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n0x01  0x008  4            3322  ---  Lifetime Power-On Resets\n0x01  0x010  4            6317  ---  Power-on Hours\n0x01  0x018  6      5976322892  ---  Logical Sectors Written\n0x01  0x020  6         9940809  ---  Number of Write Commands\n0x01  0x028  6     14678492017  ---  Logical Sectors Read\n0x01  0x030  6        43174921  ---  Number of Read Commands\n0x01  0x038  6      1266363520  ---  Date and Time TimeStamp\n0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n0x03  0x008  4            6289  ---  Spindle Motor Power-on Hours\n0x03  0x010  4            3747  ---  Head Flying Hours\n0x03  0x018  4           15424  ---  Head Load Events\n0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n0x03  0x028  4            1071  ---  Read Recovery Attempts\n0x03  0x030  4               0  ---  Number of Mechanical Start Failures\n0x03  0x038  4              16  ---  Number of Realloc. Candidate Logical Sectors\n0x03  0x040  4              18  ---  Number of High Priority Unload Events\n0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n0x04  0x008  4               1  ---  Number of Reported Uncorrectable Errors\n0x04  0x010  4              37  ---  Resets Between Cmd Acceptance and Completion\n0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n0x05  0x008  1              23  ---  Current Temperature\n0x05  0x010  1              39  ---  Average Short Term Temperature\n0x05  0x018  1              41  ---  Average Long Term Temperature\n0x05  0x020  1              49  ---  Highest Temperature\n0x05  0x028  1              22  ---  Lowest Temperature\n0x05  0x030  1              44  ---  Highest Average Short Term Temperature\n0x05  0x038  1              30  ---  Lowest Average Short Term Temperature\n0x05  0x040  1              41  ---  Highest Average Long Term Temperature\n0x05  0x048  1              34  ---  Lowest Average Long Term Temperature\n0x05  0x050  4               0  ---  Time in Over-Temperature\n0x05  0x058  1              65  ---  Specified Maximum Operating Temperature\n0x05  0x060  4               0  ---  Time in Under-Temperature\n0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n0x06  0x008  4            3365  ---  Number of Hardware Resets\n0x06  0x010  4              42  ---  Number of ASR Events\n0x06  0x018  4               0  ---  Number of Interface CRC Errors\n0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n0xff  0x008  7               0  ---  Vendor Specific\n0xff  0x010  7               0  ---  Vendor Specific\n0xff  0x018  7               0  ---  Vendor Specific\n                                |||_ C monitored condition met\n                                ||__ D supports DSN\n                                |___ N normalized value\n\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  2            0  Command failed due to ICRC error\n0x0002  2            0  R_ERR response for data FIS\n0x0003  2            0  R_ERR response for device-to-host data FIS\n0x0004  2            0  R_ERR response for host-to-device data FIS\n0x0005  2            0  R_ERR response for non-data FIS\n0x0006  2            0  R_ERR response for device-to-host non-data FIS\n0x0007  2            0  R_ERR response for host-to-device non-data FIS\n0x0008  2            0  Device-to-host non-data FIS retries\n0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  2            1  Device-to-host register FISes sent due to a COMRESET\n0x000b  2            0  CRC errors within host-to-device FIS\n0x000d  2            0  Non-CRC errors within host-to-device FIS\n0x000f  2            0  R_ERR response for host-to-device data FIS, CRC\n0x0012  2            0  R_ERR response for host-to-device non-data FIS, CRC\n0x8000  4           96  Vendor specific\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190lp4w", "is_robot_indexable": true, "report_reasons": null, "author": "PitBikeViper", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190lp4w/is_this_a_normal_hdd_sound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/190lp4w/is_this_a_normal_hdd_sound/", "subreddit_subscribers": 723567, "created_utc": 1704608543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seagate has these exos x24 drives on their website but after checking every single retailer website and even going to microcenter in person, it seems these drives don\u2019t exist or are only available to large datacenters.\n\nIs there anyway to get these or an expected release date?\n\nWhat is the next best option? 20tb exos? Should I just settle for the 20tb ones..", "author_fullname": "t2_8pz5uty0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to buy Exos x24 drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_190d3xv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6SqRH424YWfAFMmJTPRh9WRUD3246C3PtK034b-gnRA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704582715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seagate has these exos x24 drives on their website but after checking every single retailer website and even going to microcenter in person, it seems these drives don\u2019t exist or are only available to large datacenters.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to get these or an expected release date?&lt;/p&gt;\n\n&lt;p&gt;What is the next best option? 20tb exos? Should I just settle for the 20tb ones..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/w11nq7rkjwac1.jpeg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/w11nq7rkjwac1.jpeg?auto=webp&amp;s=c54fd96a08398d2c4808acea06ed31caffc48ca8", "width": 640, "height": 640}, "resolutions": [{"url": "https://preview.redd.it/w11nq7rkjwac1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c070865e27763d60c9c554cdbcb6ff6eeb79d8f", "width": 108, "height": 108}, {"url": "https://preview.redd.it/w11nq7rkjwac1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=628fe2fa75d8cb0c427f1d5d20a457fad7ed5282", "width": 216, "height": 216}, {"url": "https://preview.redd.it/w11nq7rkjwac1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=053a8d8196d5262c910d9eb55083b90661a9934e", "width": 320, "height": 320}, {"url": "https://preview.redd.it/w11nq7rkjwac1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0be90f6ec91fd87ce14dc5d6836edd0b685c533f", "width": 640, "height": 640}], "variants": {}, "id": "Vw7SBxpo0jwLWP3x-UpseZ1sv-WLBMcyAPCpBwLFcMM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "190d3xv", "is_robot_indexable": true, "report_reasons": null, "author": "cxrey_", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/190d3xv/where_to_buy_exos_x24_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/w11nq7rkjwac1.jpeg", "subreddit_subscribers": 723567, "created_utc": 1704582715.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}