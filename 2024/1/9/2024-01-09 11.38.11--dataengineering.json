{"kind": "Listing", "data": {"after": "t3_191lomk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a junior Data Analyst/Scientist consultant, mostly doing analytics and BI, but I also work with machine learning.\n\n\nI want to move into Data Engineering. I've started learning on my own and I'm curious about how others have done it and would recommend doing it.\n\n\nMy plan is to learn the basics on my own, replicate 3 full projects from the web, then do my own project on something I like. \n\n\nDoes this sound good? What do you think? Do you know some end to end project that you would consider worth learning from?", "author_fullname": "t2_tyzkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People who transitioned to DE, how did you study?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191m6ke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704723368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a junior Data Analyst/Scientist consultant, mostly doing analytics and BI, but I also work with machine learning.&lt;/p&gt;\n\n&lt;p&gt;I want to move into Data Engineering. I&amp;#39;ve started learning on my own and I&amp;#39;m curious about how others have done it and would recommend doing it.&lt;/p&gt;\n\n&lt;p&gt;My plan is to learn the basics on my own, replicate 3 full projects from the web, then do my own project on something I like. &lt;/p&gt;\n\n&lt;p&gt;Does this sound good? What do you think? Do you know some end to end project that you would consider worth learning from?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "191m6ke", "is_robot_indexable": true, "report_reasons": null, "author": "Deiice", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191m6ke/people_who_transitioned_to_de_how_did_you_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191m6ke/people_who_transitioned_to_de_how_did_you_study/", "subreddit_subscribers": 151411, "created_utc": 1704723368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the middle of scoping and costing the heaviest video, heaviest content project I\u2019ve ever done. I\u2019ve actually started having nightmares about it crashing and it\u2019s still 6-7 months out from launching.  \n\n\nSpent about 2 hours going through blogspam and am right back where I started.  \n\n\nAny advice on what data streaming solution to scope out so I can sleep?", "author_fullname": "t2_oke7cr1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What real-time data streaming tool have you seen work under heavy stress?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1924rbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704770770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the middle of scoping and costing the heaviest video, heaviest content project I\u2019ve ever done. I\u2019ve actually started having nightmares about it crashing and it\u2019s still 6-7 months out from launching.  &lt;/p&gt;\n\n&lt;p&gt;Spent about 2 hours going through blogspam and am right back where I started.  &lt;/p&gt;\n\n&lt;p&gt;Any advice on what data streaming solution to scope out so I can sleep?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1924rbb", "is_robot_indexable": true, "report_reasons": null, "author": "kiddinglyvacuous99", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1924rbb/what_realtime_data_streaming_tool_have_you_seen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1924rbb/what_realtime_data_streaming_tool_have_you_seen/", "subreddit_subscribers": 151411, "created_utc": 1704770770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a data engineering project with specific demands for ensuring and validating data consistency. My database is structured into three tiers: a schema with raw data tables, a schema with intermediate tables, and a schema with tables that are 'ready to query'. Therefore, validations need to be performed between these levels, considering various values and conditions.\n\nI attempted to create some expectations using Great Expectations (GE), but I encountered several complexities specific to my use case. For instance, I need to verify whether the sum of the revenue from orders with status equals to \"approved\" in my orders table matches the sum in the approved revenue column of my revenue table. This is a basic example for which I couldn't find any solution using GE.\n\nMy question is: should I create a simplified version of Great Expectations to address all my use cases deeply? I feel that I'm going to amount of Toil in either scenario, possibly even more if I choose to implement it using Great Expectations in its entirety.", "author_fullname": "t2_qb9fcc1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use Great Expectations or build it myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191y5xq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704753336.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704753073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a data engineering project with specific demands for ensuring and validating data consistency. My database is structured into three tiers: a schema with raw data tables, a schema with intermediate tables, and a schema with tables that are &amp;#39;ready to query&amp;#39;. Therefore, validations need to be performed between these levels, considering various values and conditions.&lt;/p&gt;\n\n&lt;p&gt;I attempted to create some expectations using Great Expectations (GE), but I encountered several complexities specific to my use case. For instance, I need to verify whether the sum of the revenue from orders with status equals to &amp;quot;approved&amp;quot; in my orders table matches the sum in the approved revenue column of my revenue table. This is a basic example for which I couldn&amp;#39;t find any solution using GE.&lt;/p&gt;\n\n&lt;p&gt;My question is: should I create a simplified version of Great Expectations to address all my use cases deeply? I feel that I&amp;#39;m going to amount of Toil in either scenario, possibly even more if I choose to implement it using Great Expectations in its entirety.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191y5xq", "is_robot_indexable": true, "report_reasons": null, "author": "Feisty_Albatross_893", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191y5xq/should_i_use_great_expectations_or_build_it_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191y5xq/should_i_use_great_expectations_or_build_it_myself/", "subreddit_subscribers": 151411, "created_utc": 1704753073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, so I'm wondering if there are any major differences between the two roles and what they might look like from others with similar experience around them.  I know data engineers are more implementation and data architect is design, but I was wondering how they relate within the field and what possible career trajectory there might be for each", "author_fullname": "t2_egr49q43v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architect vs Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922w0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704765474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, so I&amp;#39;m wondering if there are any major differences between the two roles and what they might look like from others with similar experience around them.  I know data engineers are more implementation and data architect is design, but I was wondering how they relate within the field and what possible career trajectory there might be for each&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1922w0r", "is_robot_indexable": true, "report_reasons": null, "author": "morkborkus", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922w0r/data_architect_vs_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922w0r/data_architect_vs_data_engineer/", "subreddit_subscribers": 151411, "created_utc": 1704765474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.\n\nI believe we are more consumers than producers of data.\n\n\\- You depend on the data the developers write in PostgreSQL.  \n\\- You depend on the data salespeople input in Salesforce.  \n\\- You depend on the data accountants add to Xero.\n\nIf these people produce low-quality data, there\u2019s almost no chance for you to build high-quality data products.\n\nTrying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.\n\nYou need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.\n\nI'm not saying data engineers are not producers for analysts, but I think we are primarily consumers.\n\nWhat's your opinion?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the data team a data producer or a consumer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191s99i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704738951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.&lt;/p&gt;\n\n&lt;p&gt;I believe we are more consumers than producers of data.&lt;/p&gt;\n\n&lt;p&gt;- You depend on the data the developers write in PostgreSQL.&lt;br/&gt;\n- You depend on the data salespeople input in Salesforce.&lt;br/&gt;\n- You depend on the data accountants add to Xero.&lt;/p&gt;\n\n&lt;p&gt;If these people produce low-quality data, there\u2019s almost no chance for you to build high-quality data products.&lt;/p&gt;\n\n&lt;p&gt;Trying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.&lt;/p&gt;\n\n&lt;p&gt;You need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not saying data engineers are not producers for analysts, but I think we are primarily consumers.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "191s99i", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/191s99i/is_the_data_team_a_data_producer_or_a_consumer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191s99i/is_the_data_team_a_data_producer_or_a_consumer/", "subreddit_subscribers": 151411, "created_utc": 1704738951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have heard great things and wanted to give it a shot. Currently have a Postgres database handling data aggregations and ETL. I wanted to pick a separate technology for the warehouse so that down the road migrating to something like Snowflake would be less painful (no shortcuts, lazy coding, etc) and to keep end users from accidentally stalling out the resources of the ETL pipelines. Snowflake and anything similar would be overkill for my current data volume, and trying to avoid cloud for simplifying security environment.\n\nI was curious how DuckDB could work in this scenario with multiple end users. The data in Postgres will be properly transformed into gold level and star schema before landing in the warehouse. I\u2019m really wanting the warehouse to be a ready to consume analytics layer rather than a mess of transformations and staging tables (already using DBT in ETL pipelines).\n\nWould I surface a common .db file on a local network for end users with read only access? Or is there another configuration to consider/another tool completely that would be better suited for the job.", "author_fullname": "t2_ao7u40a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB as a data warehouse for multiple users?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922n89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704764802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have heard great things and wanted to give it a shot. Currently have a Postgres database handling data aggregations and ETL. I wanted to pick a separate technology for the warehouse so that down the road migrating to something like Snowflake would be less painful (no shortcuts, lazy coding, etc) and to keep end users from accidentally stalling out the resources of the ETL pipelines. Snowflake and anything similar would be overkill for my current data volume, and trying to avoid cloud for simplifying security environment.&lt;/p&gt;\n\n&lt;p&gt;I was curious how DuckDB could work in this scenario with multiple end users. The data in Postgres will be properly transformed into gold level and star schema before landing in the warehouse. I\u2019m really wanting the warehouse to be a ready to consume analytics layer rather than a mess of transformations and staging tables (already using DBT in ETL pipelines).&lt;/p&gt;\n\n&lt;p&gt;Would I surface a common .db file on a local network for end users with read only access? Or is there another configuration to consider/another tool completely that would be better suited for the job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1922n89", "is_robot_indexable": true, "report_reasons": null, "author": "minormisgnomer", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922n89/duckdb_as_a_data_warehouse_for_multiple_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922n89/duckdb_as_a_data_warehouse_for_multiple_users/", "subreddit_subscribers": 151411, "created_utc": 1704764802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data engineering can mean a lot of things depending on what company you work at. At all of the companies I've worked at, all data engineering disciplines had the title of data engineer.\n\n\n\n\n\nFor example, I see data engineering as falling under the following areas: software engineer - data, data pipeline development, ML engineering, database admin.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you make it clear on your resume what area of data engineering you work in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191rwwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704738111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data engineering can mean a lot of things depending on what company you work at. At all of the companies I&amp;#39;ve worked at, all data engineering disciplines had the title of data engineer.&lt;/p&gt;\n\n&lt;p&gt;For example, I see data engineering as falling under the following areas: software engineer - data, data pipeline development, ML engineering, database admin.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "191rwwx", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/191rwwx/how_do_you_make_it_clear_on_your_resume_what_area/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191rwwx/how_do_you_make_it_clear_on_your_resume_what_area/", "subreddit_subscribers": 151411, "created_utc": 1704738111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking to experiment with various big data tooling like Databricks and Microsoft Fabric. What would be good datasets to play with to build out projects with? \n\nI think Kaggle is likely too small scale. I\u2019d like to be able to flesh out ETLs all the way through to Analytics and ETLs. So looking for large scale freely available data sets that could be interesting to work with. \n\nThanks!", "author_fullname": "t2_srabrhe6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good datasets for personal learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191snt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704739951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to experiment with various big data tooling like Databricks and Microsoft Fabric. What would be good datasets to play with to build out projects with? &lt;/p&gt;\n\n&lt;p&gt;I think Kaggle is likely too small scale. I\u2019d like to be able to flesh out ETLs all the way through to Analytics and ETLs. So looking for large scale freely available data sets that could be interesting to work with. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191snt1", "is_robot_indexable": true, "report_reasons": null, "author": "i-kn0w-n0thing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191snt1/good_datasets_for_personal_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191snt1/good_datasets_for_personal_learning/", "subreddit_subscribers": 151411, "created_utc": 1704739951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w6hkluod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_191irxm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NvqcJrCh5KE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NvqcJrCh5KE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/NvqcJrCh5KE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NvqcJrCh5KE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/191irxm", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S8GB40eCwMDr0-dKe3_wvzR6RDjR59LebaYMtd4NH3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704711932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/NvqcJrCh5KE?si=mKWWU1biapj7UPmS", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iZLlO3Xi_VC4PUeEQuIFHmJ38fbvB7Sv1SAnj9Co4AE.jpg?auto=webp&amp;s=cbce3834c26c53418251094a84bc66b7f205da3d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/iZLlO3Xi_VC4PUeEQuIFHmJ38fbvB7Sv1SAnj9Co4AE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f913b8849ff93640f85e2efad7fc6b5a57bdbc2e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/iZLlO3Xi_VC4PUeEQuIFHmJ38fbvB7Sv1SAnj9Co4AE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=73a7ddbcb77775efbac4bf816364a987c10636bc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/iZLlO3Xi_VC4PUeEQuIFHmJ38fbvB7Sv1SAnj9Co4AE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88df0acb3bf7c506eda7b0ec2313eee5a773b470", "width": 320, "height": 240}], "variants": {}, "id": "DDjFBscc8fz4PdFw0MYf9ssvISGa2PIanU_R9xTfdGI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "191irxm", "is_robot_indexable": true, "report_reasons": null, "author": "danipudani", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191irxm/core_principles_of_scikit_learn_gael_varoquaux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/NvqcJrCh5KE?si=mKWWU1biapj7UPmS", "subreddit_subscribers": 151411, "created_utc": 1704711932.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NvqcJrCh5KE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/NvqcJrCh5KE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nWe have an interesting problem and wanted to get reddit's take on it. I am keeping it intentionally vague for privacy, but will add relevant details and can answer questions in the comments. \n\nWe are building an application that segments consumers based on whether they have or have not done activities. Ultimately the application supports combinations of boolean logic and whether they have or have not done something to arrive at a list of IDs and a count of consumers. It also supports non-dynamic date ranges, whether they have done it in the last 30 days, 1 year, or all time. One example request would be \"Get me all the people who have done ((activity X AND activity Y) OR activity Z) in the last 30 days\", another could be \"Get me all the people who have done activity X AND NOT activity Y\". We delivered this functionality though our architecture has some issues. \n\nPrimary issue is that the we are using event tables, and as a result we have records of consumers who HAVE participated in an activity but we don't maintain people who HAVE NOT done the activity, thus we cannot query consumers who have done an activity 0 times. To achieve the NOT logic, we exclude people who have done the activity to get the final result set. If we were to run a query that has activity X OR NOT activity Y, we would get unintended results. \n\nSecondary issue is that we have preset date ranges, which is useful at times but not very flexible or dynamic. Ideally a user could input a custom date range in the application to be applied to the backend queries. \n\nThis is a very large data set. The total possible consumers is \\~3B and they all participate in (some) activity. We 1) want to maintain a pool of consumers such that our NOT logic can query for records where participation count = 0 and 2) want to provide a flexible date range option (or mimic that functionality) \n\nFirst thought is to join a datedim table to our table of consumers to have one record for each combination of consumer and date. However this results in an ever increasing amount of data, for 2 years we would have \\~2Trillion records and counting. Then we have to apply boolean logic to filter down the records and in some cases join subsets of records, which simply gets expensive and is computationally intensive for a user facing application. \n\nWe are using **snowflake** for data storage and **python** for custom query generation, I am trying to think of creative ways to add a flexible date range and support potentially massive computation without requests from the application being too slow. For a user application, does it make more sense to compute dynamic date ranges on the fly after a request has been submitted, or precompute a massive dataset and filter it down after the fact? Or a mix of both? \n\nIf anyone has any wisdom or similar experience please let me know, I am also happy to answer any questions in the comments. ", "author_fullname": "t2_60ux6m11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Really big data set, combining pool of consumers with datedim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191x2nj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704750442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;We have an interesting problem and wanted to get reddit&amp;#39;s take on it. I am keeping it intentionally vague for privacy, but will add relevant details and can answer questions in the comments. &lt;/p&gt;\n\n&lt;p&gt;We are building an application that segments consumers based on whether they have or have not done activities. Ultimately the application supports combinations of boolean logic and whether they have or have not done something to arrive at a list of IDs and a count of consumers. It also supports non-dynamic date ranges, whether they have done it in the last 30 days, 1 year, or all time. One example request would be &amp;quot;Get me all the people who have done ((activity X AND activity Y) OR activity Z) in the last 30 days&amp;quot;, another could be &amp;quot;Get me all the people who have done activity X AND NOT activity Y&amp;quot;. We delivered this functionality though our architecture has some issues. &lt;/p&gt;\n\n&lt;p&gt;Primary issue is that the we are using event tables, and as a result we have records of consumers who HAVE participated in an activity but we don&amp;#39;t maintain people who HAVE NOT done the activity, thus we cannot query consumers who have done an activity 0 times. To achieve the NOT logic, we exclude people who have done the activity to get the final result set. If we were to run a query that has activity X OR NOT activity Y, we would get unintended results. &lt;/p&gt;\n\n&lt;p&gt;Secondary issue is that we have preset date ranges, which is useful at times but not very flexible or dynamic. Ideally a user could input a custom date range in the application to be applied to the backend queries. &lt;/p&gt;\n\n&lt;p&gt;This is a very large data set. The total possible consumers is ~3B and they all participate in (some) activity. We 1) want to maintain a pool of consumers such that our NOT logic can query for records where participation count = 0 and 2) want to provide a flexible date range option (or mimic that functionality) &lt;/p&gt;\n\n&lt;p&gt;First thought is to join a datedim table to our table of consumers to have one record for each combination of consumer and date. However this results in an ever increasing amount of data, for 2 years we would have ~2Trillion records and counting. Then we have to apply boolean logic to filter down the records and in some cases join subsets of records, which simply gets expensive and is computationally intensive for a user facing application. &lt;/p&gt;\n\n&lt;p&gt;We are using &lt;strong&gt;snowflake&lt;/strong&gt; for data storage and &lt;strong&gt;python&lt;/strong&gt; for custom query generation, I am trying to think of creative ways to add a flexible date range and support potentially massive computation without requests from the application being too slow. For a user application, does it make more sense to compute dynamic date ranges on the fly after a request has been submitted, or precompute a massive dataset and filter it down after the fact? Or a mix of both? &lt;/p&gt;\n\n&lt;p&gt;If anyone has any wisdom or similar experience please let me know, I am also happy to answer any questions in the comments. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191x2nj", "is_robot_indexable": true, "report_reasons": null, "author": "lt-96", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191x2nj/really_big_data_set_combining_pool_of_consumers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191x2nj/really_big_data_set_combining_pool_of_consumers/", "subreddit_subscribers": 151411, "created_utc": 1704750442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,   \n\n\n[dlt](https://dlthub.com/docs/intro) is the python library for data loading with schema evolution. Our contributors recently created 2 very interesting articles on event ingestion which I wanted to share.  \n1. [Event ingestion on AWS](https://dlthub.com/docs/blog/dlt-aws-taktile-blog) with Lambda; Cost of 7.5 USD/1m events.  \n2. [Event ingestion on GCP](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp) via PubSub; Cost of 16usd/month for 2 workers that can process 1400 events/second.  \n\n\nWe are also working on doing our own similar pipeline on gcp and will share it when ready.\n\n&amp;#x200B;\n\ndlt library added an early release of [data contracts](https://dlthub.com/docs/general-usage/schema-contracts) with the following options for handling bad events:  \n\\- evolve schema  \n\\- reject load package   \n\\- truncate columns and ingest (discard columns)  \n\\- discard event (discard row)  \nWe will next add bad data handling so if you have requirements how you would like that to work please comment [here](https://github.com/dlt-hub/dlt/issues/780).  \n\n\nThanks all! and let 2024 be a year with more open source data solutions :)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event ingestion for cheap on AWS and GCP: 2 demos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191lf3o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704721119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/intro\"&gt;dlt&lt;/a&gt; is the python library for data loading with schema evolution. Our contributors recently created 2 very interesting articles on event ingestion which I wanted to share.&lt;br/&gt;\n1. &lt;a href=\"https://dlthub.com/docs/blog/dlt-aws-taktile-blog\"&gt;Event ingestion on AWS&lt;/a&gt; with Lambda; Cost of 7.5 USD/1m events.&lt;br/&gt;\n2. &lt;a href=\"https://dlthub.com/docs/blog/streaming-pubsub-json-gcp\"&gt;Event ingestion on GCP&lt;/a&gt; via PubSub; Cost of 16usd/month for 2 workers that can process 1400 events/second.  &lt;/p&gt;\n\n&lt;p&gt;We are also working on doing our own similar pipeline on gcp and will share it when ready.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;dlt library added an early release of &lt;a href=\"https://dlthub.com/docs/general-usage/schema-contracts\"&gt;data contracts&lt;/a&gt; with the following options for handling bad events:&lt;br/&gt;\n- evolve schema&lt;br/&gt;\n- reject load package&lt;br/&gt;\n- truncate columns and ingest (discard columns)&lt;br/&gt;\n- discard event (discard row)&lt;br/&gt;\nWe will next add bad data handling so if you have requirements how you would like that to work please comment &lt;a href=\"https://github.com/dlt-hub/dlt/issues/780\"&gt;here&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;Thanks all! and let 2024 be a year with more open source data solutions :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "191lf3o", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191lf3o/event_ingestion_for_cheap_on_aws_and_gcp_2_demos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191lf3o/event_ingestion_for_cheap_on_aws_and_gcp_2_demos/", "subreddit_subscribers": 151411, "created_utc": 1704721119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. \n\n&amp;#x200B;\n\nMy question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn't a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them \"Are you happy here? If not, what can we do to make it better?\" questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. \n\n&amp;#x200B;", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Data) Engineering Managers - What Makes You Stay At A Company Long", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1927hdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704779296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn&amp;#39;t a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them &amp;quot;Are you happy here? If not, what can we do to make it better?&amp;quot; questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1927hdr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "subreddit_subscribers": 151411, "created_utc": 1704779296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just quit without anything lined up due to extreme burnout and being pushed in a management direction on a very small team that doesn\u2019t want or need it, when my career interests at this point are technical. \n\nMy manager came back asking if I\u2019d be interested in contract work to help finish out the project set to span Q1 &amp; Q2 of this year. With my departure, they\u2019re now planning on using in-house consultants of our 1-2 main vendors and asked if I would either assist with the technical component or managing the relationship with the consultant(s). \n\nHonestly, I would definitely be interested if it meant I could focus on just the scope given and didn\u2019t have to be included in other meetings or pulled into the side quests that were pushing me over 40h a week, but trying to figure out a reasonable proposal. The main thing I care about is keeping my hours below 20 (ideally 12-16) as I try to heal this burnout. \n\nIs it as simple as a multiplier of current hourly rate? Or should I factor in my own experience, proprietary knowledge, education, etc? Should I propose hourly or estimate the hours and lump sum for the project? \n\nIn case it\u2019s relevant, my role is possibly more closely aligned with Analytics Engineering (heavy in dbt, I\u2019m only one who knows the tool/our project well) and BI development (implementing dbt for end users), but feel this community has good insight. Anyone have experience here? Thanks in advance!", "author_fullname": "t2_spwsseep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Employer asked me to contract after I gave my notice. Best way to calculate rate for proposal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191t6ed", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704741458.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704741157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just quit without anything lined up due to extreme burnout and being pushed in a management direction on a very small team that doesn\u2019t want or need it, when my career interests at this point are technical. &lt;/p&gt;\n\n&lt;p&gt;My manager came back asking if I\u2019d be interested in contract work to help finish out the project set to span Q1 &amp;amp; Q2 of this year. With my departure, they\u2019re now planning on using in-house consultants of our 1-2 main vendors and asked if I would either assist with the technical component or managing the relationship with the consultant(s). &lt;/p&gt;\n\n&lt;p&gt;Honestly, I would definitely be interested if it meant I could focus on just the scope given and didn\u2019t have to be included in other meetings or pulled into the side quests that were pushing me over 40h a week, but trying to figure out a reasonable proposal. The main thing I care about is keeping my hours below 20 (ideally 12-16) as I try to heal this burnout. &lt;/p&gt;\n\n&lt;p&gt;Is it as simple as a multiplier of current hourly rate? Or should I factor in my own experience, proprietary knowledge, education, etc? Should I propose hourly or estimate the hours and lump sum for the project? &lt;/p&gt;\n\n&lt;p&gt;In case it\u2019s relevant, my role is possibly more closely aligned with Analytics Engineering (heavy in dbt, I\u2019m only one who knows the tool/our project well) and BI development (implementing dbt for end users), but feel this community has good insight. Anyone have experience here? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "191t6ed", "is_robot_indexable": true, "report_reasons": null, "author": "hairc-ut", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191t6ed/employer_asked_me_to_contract_after_i_gave_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191t6ed/employer_asked_me_to_contract_after_i_gave_my/", "subreddit_subscribers": 151411, "created_utc": 1704741157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some excel files and I need to do some data quality checks using python\n\n1) Null count and Treatment\n    if a particular column has &gt;25% , it should throw an email\n\n2) Duplicate Treatment\n\n3) Accepted values in Column \n   e.g State column should have only 50 states\n\n4) Expected Datatype check for columns\n\n5) Variance check for Numeric column compared to previous month \n\netc\n\nFor now I have inplemented using pandas inbuilt functions.\n\n\nI thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.", "author_fullname": "t2_l6b7kbp7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality python Modules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19253mf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704771759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some excel files and I need to do some data quality checks using python&lt;/p&gt;\n\n&lt;p&gt;1) Null count and Treatment\n    if a particular column has &amp;gt;25% , it should throw an email&lt;/p&gt;\n\n&lt;p&gt;2) Duplicate Treatment&lt;/p&gt;\n\n&lt;p&gt;3) Accepted values in Column \n   e.g State column should have only 50 states&lt;/p&gt;\n\n&lt;p&gt;4) Expected Datatype check for columns&lt;/p&gt;\n\n&lt;p&gt;5) Variance check for Numeric column compared to previous month &lt;/p&gt;\n\n&lt;p&gt;etc&lt;/p&gt;\n\n&lt;p&gt;For now I have inplemented using pandas inbuilt functions.&lt;/p&gt;\n\n&lt;p&gt;I thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19253mf", "is_robot_indexable": true, "report_reasons": null, "author": "vainothisside", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19253mf/data_quality_python_modules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19253mf/data_quality_python_modules/", "subreddit_subscribers": 151411, "created_utc": 1704771759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.\n\nThis will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.", "author_fullname": "t2_vlvq8b3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what graph DB to use for knowledge graphs for our LLM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192abwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704790358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.&lt;/p&gt;\n\n&lt;p&gt;This will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192abwa", "is_robot_indexable": true, "report_reasons": null, "author": "0xkiichiro", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "subreddit_subscribers": 151411, "created_utc": 1704790358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Probably only one thing that is more painful than working with legacy ETL stored procedures/DMLs is migrating them.\n\nNew data teams normally start with a data analyst or data scientist come up with a number of SQLs, then setting up cron jobs materializing views into tables. Over time, it becomes a pile of mess. Data leanage exists nowhere but in their minds. Tables being updated by god-knows DMLs.\n\nI have seen a lot of efforts in migrating existing SQL codes and DML to DAG-based solution like DBT. But doing so manually is extremely tedious. I wonder there must be automation service for this already.\n\nHas anyone come across this problem?", "author_fullname": "t2_oqbpxpulm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrate legacy database ETL to newer DAG-based", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922oee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704764890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Probably only one thing that is more painful than working with legacy ETL stored procedures/DMLs is migrating them.&lt;/p&gt;\n\n&lt;p&gt;New data teams normally start with a data analyst or data scientist come up with a number of SQLs, then setting up cron jobs materializing views into tables. Over time, it becomes a pile of mess. Data leanage exists nowhere but in their minds. Tables being updated by god-knows DMLs.&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of efforts in migrating existing SQL codes and DML to DAG-based solution like DBT. But doing so manually is extremely tedious. I wonder there must be automation service for this already.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1922oee", "is_robot_indexable": true, "report_reasons": null, "author": "JayDoDr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922oee/migrate_legacy_database_etl_to_newer_dagbased/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922oee/migrate_legacy_database_etl_to_newer_dagbased/", "subreddit_subscribers": 151411, "created_utc": 1704764890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am trying to think of ways to approximate the count of records returned on a series of queries without processing the query. \n\nI am using snowflake, and we create a number of temporary tables that are either inner or outer joined together to support AND / OR logic. \n\nFor example, say we have Temp Table A, Temp Table B, Temp Table C and we want a count of records for ids that appear in ((A AND B) OR C). Is it remotely possible to approximate a count without running a query? My first thought is to use **HyperLogLog** algorithm ([https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:\\~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization](https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization)), however this seems to be more applicable for interacting with 2 tables and finding the approximate intersection. \n\nWe don't need to be very precise, just a ballpark count. Any ideas out there to estimate the count of multiple tables being joined together?\n\n&amp;#x200B;", "author_fullname": "t2_60ux6m11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get multi-table join results without running the query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191x6w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704750724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am trying to think of ways to approximate the count of records returned on a series of queries without processing the query. &lt;/p&gt;\n\n&lt;p&gt;I am using snowflake, and we create a number of temporary tables that are either inner or outer joined together to support AND / OR logic. &lt;/p&gt;\n\n&lt;p&gt;For example, say we have Temp Table A, Temp Table B, Temp Table C and we want a count of records for ids that appear in ((A AND B) OR C). Is it remotely possible to approximate a count without running a query? My first thought is to use &lt;strong&gt;HyperLogLog&lt;/strong&gt; algorithm (&lt;a href=\"https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:%7E:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization\"&gt;https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization&lt;/a&gt;), however this seems to be more applicable for interacting with 2 tables and finding the approximate intersection. &lt;/p&gt;\n\n&lt;p&gt;We don&amp;#39;t need to be very precise, just a ballpark count. Any ideas out there to estimate the count of multiple tables being joined together?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191x6w3", "is_robot_indexable": true, "report_reasons": null, "author": "lt-96", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191x6w3/get_multitable_join_results_without_running_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191x6w3/get_multitable_join_results_without_running_the/", "subreddit_subscribers": 151411, "created_utc": 1704750724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, community! \ud83d\ude80 I'm eager to kickstart my journey into Apache Airflow and looking for suggestions on beginner-friendly projects. Any insights, recommendations, or hands-on learning ideas that can help me grasp the ropes of Airflow effectively? Thanks in advance for your valuable input!", "author_fullname": "t2_3fyuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Beginner-Friendly Projects to Dive into Apache Airflow Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191k13k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704716546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, community! \ud83d\ude80 I&amp;#39;m eager to kickstart my journey into Apache Airflow and looking for suggestions on beginner-friendly projects. Any insights, recommendations, or hands-on learning ideas that can help me grasp the ropes of Airflow effectively? Thanks in advance for your valuable input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "191k13k", "is_robot_indexable": true, "report_reasons": null, "author": "ramesh4f", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191k13k/seeking_advice_beginnerfriendly_projects_to_dive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191k13k/seeking_advice_beginnerfriendly_projects_to_dive/", "subreddit_subscribers": 151411, "created_utc": 1704716546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The [project](https://github.com/JawaharRamis/PL-Football-ETL-with-Airflow-AWS) orchestrated using Airflow retrieve raw data from an api saving into an S3 bucket which is then transformed using a glue job and then stored onto another S3 bucket. The processed data is fed into Quicksight for visualizations. \n\nI have been trying to build up my DE portfolio hoping to land a job in this field. You can find more of my projects in my [Github](https://github.com/JawaharRamis). Most of these projects helped familiarize myself with the tools. My goal is to develop a really good DE project within the next few months which could help me really stand out. \n\nShare me your advices/suggestions on my project and also how I could build up my portfolio further.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Premier League football data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191jbc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704713923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;a href=\"https://github.com/JawaharRamis/PL-Football-ETL-with-Airflow-AWS\"&gt;project&lt;/a&gt; orchestrated using Airflow retrieve raw data from an api saving into an S3 bucket which is then transformed using a glue job and then stored onto another S3 bucket. The processed data is fed into Quicksight for visualizations. &lt;/p&gt;\n\n&lt;p&gt;I have been trying to build up my DE portfolio hoping to land a job in this field. You can find more of my projects in my &lt;a href=\"https://github.com/JawaharRamis\"&gt;Github&lt;/a&gt;. Most of these projects helped familiarize myself with the tools. My goal is to develop a really good DE project within the next few months which could help me really stand out. &lt;/p&gt;\n\n&lt;p&gt;Share me your advices/suggestions on my project and also how I could build up my portfolio further.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?auto=webp&amp;s=347d9668cc8c2ec7a8c04fee6e77cba3a4afc79d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ad3e33471e7a8bb581236afaed67a1bbab4303b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86777486d7455d06995fbd5614f91dacfb14f5fb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6940e9a9e525dcefada6b5a37dec973eeb2ae372", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f72576fe2aecfc199f7eb58c169264038fd0666d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1da77c69cd2bd166ec975ae5f545204aff8ec91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/1Ij6NXV0VmqGQbreihbdbU2WYEnc12GrQAqjtLmC4mw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c6a017189e1a21c9ae88c5767560b118c9b055e", "width": 1080, "height": 540}], "variants": {}, "id": "IDYD8vpVUNEhiFy6td-x6vZG_MLHzaQ1ccBksElAl60"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "191jbc9", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191jbc9/premier_league_football_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191jbc9/premier_league_football_data_pipeline/", "subreddit_subscribers": 151411, "created_utc": 1704713923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I've been tasked with pulling out a view of the data lineage for a number of variables from a series of SAS scripts. \n\nThe scripts look like standard SAS code with data and proc steps. \n\nI need to pull out what tables the variables come from, the transformation steps and the what tables the variables end up in. \n\nI'm thinking about building a parser in SAS but was wondering about any tips anyone had? \n\nThank you all in advance.", "author_fullname": "t2_6plpm3k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Static Code Analysis for Data Lineage in SAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_192bova", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704796055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve been tasked with pulling out a view of the data lineage for a number of variables from a series of SAS scripts. &lt;/p&gt;\n\n&lt;p&gt;The scripts look like standard SAS code with data and proc steps. &lt;/p&gt;\n\n&lt;p&gt;I need to pull out what tables the variables come from, the transformation steps and the what tables the variables end up in. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking about building a parser in SAS but was wondering about any tips anyone had? &lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192bova", "is_robot_indexable": true, "report_reasons": null, "author": "KarmaIssues", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192bova/static_code_analysis_for_data_lineage_in_sas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192bova/static_code_analysis_for_data_lineage_in_sas/", "subreddit_subscribers": 151411, "created_utc": 1704796055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have been trying to solve this problem for several weeks. First of all I'll summarize the case and then I'll do it with all the details. I come here because I can not find anything that solves the problem in forums or even chatGPT, because spark connect was implemented in Spark version 3.4, so it is quite recent.\n\nMy goal is to create a spark cluster with docker containers and then use spark connect on a jupyter notebook but this time installed on the host machine (not a container) so I can practice with spark code, while seeing how the tasks run on the container spark cluster. I work on windows, which I think is important to solve the problem I will expose at the end with containers.\n\nWell, being as brief as possible, I am able to install spark on the containers with ubuntu, run the master and connect the workers to the master. When I run ./spark-connect-server.sh -packages... as it says in the documentation I don't know if that spark server is connected or related to the cluster I have set up and I would like to understand the details and steps to follow for this configuration.The other idea that comes to me is that spark connect is independent of the cluster running in the containers but I wouldn't understand its use either.\n\nOn the other hand, if in the jupyter notebook with python I use import pyspark and then pyspark.sql following the steps in the documentation, when using sparkSession.builder.remote(\"ip and port of the container\") I do not know which is the ip to use, if the one of sparkconnect executed (and how to know this ip), or the one of the master \"ip container:7077\". This is the part that loses me the most because I don't even understand the errors that the program returns. It may be important that I run jupyter from VSCode.\n\n\nFinally, I would like to explain a problem of connections with the ports of Docker containers. I always have to map the container ports, for example the 8080 of the container running the master, with those of the host machine because otherwise I can't use the container ip 127.17.0.1:8080 from the browser to access the Spark web UI, I can only access as [localhost:8080](http://localhost:8080) with the port mapped in the container creation.\n\n\nThanks for reading the post regardless of whether you can provide info or not, and thanks also if you comment providing a solution to one or more of the problems exposed.\n\n\nBest regards.", "author_fullname": "t2_4dzozo3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark connect + Jupuyter Notebook + Docker Cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_192blx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704797143.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704795719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been trying to solve this problem for several weeks. First of all I&amp;#39;ll summarize the case and then I&amp;#39;ll do it with all the details. I come here because I can not find anything that solves the problem in forums or even chatGPT, because spark connect was implemented in Spark version 3.4, so it is quite recent.&lt;/p&gt;\n\n&lt;p&gt;My goal is to create a spark cluster with docker containers and then use spark connect on a jupyter notebook but this time installed on the host machine (not a container) so I can practice with spark code, while seeing how the tasks run on the container spark cluster. I work on windows, which I think is important to solve the problem I will expose at the end with containers.&lt;/p&gt;\n\n&lt;p&gt;Well, being as brief as possible, I am able to install spark on the containers with ubuntu, run the master and connect the workers to the master. When I run ./spark-connect-server.sh -packages... as it says in the documentation I don&amp;#39;t know if that spark server is connected or related to the cluster I have set up and I would like to understand the details and steps to follow for this configuration.The other idea that comes to me is that spark connect is independent of the cluster running in the containers but I wouldn&amp;#39;t understand its use either.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, if in the jupyter notebook with python I use import pyspark and then pyspark.sql following the steps in the documentation, when using sparkSession.builder.remote(&amp;quot;ip and port of the container&amp;quot;) I do not know which is the ip to use, if the one of sparkconnect executed (and how to know this ip), or the one of the master &amp;quot;ip container:7077&amp;quot;. This is the part that loses me the most because I don&amp;#39;t even understand the errors that the program returns. It may be important that I run jupyter from VSCode.&lt;/p&gt;\n\n&lt;p&gt;Finally, I would like to explain a problem of connections with the ports of Docker containers. I always have to map the container ports, for example the 8080 of the container running the master, with those of the host machine because otherwise I can&amp;#39;t use the container ip 127.17.0.1:8080 from the browser to access the Spark web UI, I can only access as &lt;a href=\"http://localhost:8080\"&gt;localhost:8080&lt;/a&gt; with the port mapped in the container creation.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading the post regardless of whether you can provide info or not, and thanks also if you comment providing a solution to one or more of the problems exposed.&lt;/p&gt;\n\n&lt;p&gt;Best regards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192blx8", "is_robot_indexable": true, "report_reasons": null, "author": "Omaroto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192blx8/spark_connect_jupuyter_notebook_docker_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192blx8/spark_connect_jupuyter_notebook_docker_cluster/", "subreddit_subscribers": 151411, "created_utc": 1704795719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?  \nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using the DBT semantic layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_192b61u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704793915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?&lt;br/&gt;\nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192b61u", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "subreddit_subscribers": 151411, "created_utc": 1704793915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not sure what I'm doing wrong.\n\nI'm trying to setup Airflow using a fairly standard `pip install apache-airflow`, but it somehow has a dependency on unicodecsv that I have a hard time installing.\n\nThe only version I found only had support up to 3.5, which I was able to install on Python 3.5, but how does Airflow support 3.8 when one of its dependency only goes up to 3.5?", "author_fullname": "t2_4yd5ms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dependencies of apache-airflow on unicodecsv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192ad4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704790517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure what I&amp;#39;m doing wrong.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to setup Airflow using a fairly standard &lt;code&gt;pip install apache-airflow&lt;/code&gt;, but it somehow has a dependency on unicodecsv that I have a hard time installing.&lt;/p&gt;\n\n&lt;p&gt;The only version I found only had support up to 3.5, which I was able to install on Python 3.5, but how does Airflow support 3.8 when one of its dependency only goes up to 3.5?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192ad4h", "is_robot_indexable": true, "report_reasons": null, "author": "ZirePhiinix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192ad4h/dependencies_of_apacheairflow_on_unicodecsv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192ad4h/dependencies_of_apacheairflow_on_unicodecsv/", "subreddit_subscribers": 151411, "created_utc": 1704790517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say I have normalised a table to create my database schema. However, how would new data coming in via the original denormalised format be appended to the normalised version of the table?", "author_fullname": "t2_d9jl5vn6o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data appending task help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191m9g1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704723595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I have normalised a table to create my database schema. However, how would new data coming in via the original denormalised format be appended to the normalised version of the table?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191m9g1", "is_robot_indexable": true, "report_reasons": null, "author": "No-Pineapple7188", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191m9g1/data_appending_task_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191m9g1/data_appending_task_help/", "subreddit_subscribers": 151411, "created_utc": 1704723595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i have learned that creating a separate database for development is better than using the live database -- for which i was doing for a while, so i am applying in my database this practice that i learned. Now, whenever there are changes in my development database, I just perform exactly the same (changes in columns, create new table, alter procedure, etc) to the live database.  \nIt is somehow tedious, so I think, there might be a better way than what I was doing right now. Can you share how do you do it? I am using postgre", "author_fullname": "t2_de0trs4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to apply same changes from dev database to prod?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191lomk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704721934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have learned that creating a separate database for development is better than using the live database -- for which i was doing for a while, so i am applying in my database this practice that i learned. Now, whenever there are changes in my development database, I just perform exactly the same (changes in columns, create new table, alter procedure, etc) to the live database.&lt;br/&gt;\nIt is somehow tedious, so I think, there might be a better way than what I was doing right now. Can you share how do you do it? I am using postgre&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191lomk", "is_robot_indexable": true, "report_reasons": null, "author": "WildNumber7303", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191lomk/how_to_apply_same_changes_from_dev_database_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191lomk/how_to_apply_same_changes_from_dev_database_to/", "subreddit_subscribers": 151411, "created_utc": 1704721934.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}