{"kind": "Listing", "data": {"after": "t3_192blx8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the middle of scoping and costing the heaviest video, heaviest content project I\u2019ve ever done. I\u2019ve actually started having nightmares about it crashing and it\u2019s still 6-7 months out from launching.  \n\n\nSpent about 2 hours going through blogspam and am right back where I started.  \n\n\nAny advice on what data streaming solution to scope out so I can sleep?", "author_fullname": "t2_oke7cr1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What real-time data streaming tool have you seen work under heavy stress?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1924rbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704770770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the middle of scoping and costing the heaviest video, heaviest content project I\u2019ve ever done. I\u2019ve actually started having nightmares about it crashing and it\u2019s still 6-7 months out from launching.  &lt;/p&gt;\n\n&lt;p&gt;Spent about 2 hours going through blogspam and am right back where I started.  &lt;/p&gt;\n\n&lt;p&gt;Any advice on what data streaming solution to scope out so I can sleep?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1924rbb", "is_robot_indexable": true, "report_reasons": null, "author": "kiddinglyvacuous99", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1924rbb/what_realtime_data_streaming_tool_have_you_seen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1924rbb/what_realtime_data_streaming_tool_have_you_seen/", "subreddit_subscribers": 151477, "created_utc": 1704770770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If so, how do you deal with it?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you feel the tension between \"data democratization\" and the business need for high quality curated data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192f47n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704808143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so, how do you deal with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192f47n", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/", "subreddit_subscribers": 151477, "created_utc": 1704808143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, so I'm wondering if there are any major differences between the two roles and what they might look like from others with similar experience around them.  I know data engineers are more implementation and data architect is design, but I was wondering how they relate within the field and what possible career trajectory there might be for each", "author_fullname": "t2_egr49q43v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architect vs Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922w0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704765474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, so I&amp;#39;m wondering if there are any major differences between the two roles and what they might look like from others with similar experience around them.  I know data engineers are more implementation and data architect is design, but I was wondering how they relate within the field and what possible career trajectory there might be for each&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1922w0r", "is_robot_indexable": true, "report_reasons": null, "author": "morkborkus", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922w0r/data_architect_vs_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922w0r/data_architect_vs_data_engineer/", "subreddit_subscribers": 151477, "created_utc": 1704765474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a data engineering project with specific demands for ensuring and validating data consistency. My database is structured into three tiers: a schema with raw data tables, a schema with intermediate tables, and a schema with tables that are 'ready to query'. Therefore, validations need to be performed between these levels, considering various values and conditions.\n\nI attempted to create some expectations using Great Expectations (GE), but I encountered several complexities specific to my use case. For instance, I need to verify whether the sum of the revenue from orders with status equals to \"approved\" in my orders table matches the sum in the approved revenue column of my revenue table. This is a basic example for which I couldn't find any solution using GE.\n\nMy question is: should I create a simplified version of Great Expectations to address all my use cases deeply? I feel that I'm going to amount of Toil in either scenario, possibly even more if I choose to implement it using Great Expectations in its entirety.", "author_fullname": "t2_qb9fcc1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use Great Expectations or build it myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191y5xq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704753336.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704753073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a data engineering project with specific demands for ensuring and validating data consistency. My database is structured into three tiers: a schema with raw data tables, a schema with intermediate tables, and a schema with tables that are &amp;#39;ready to query&amp;#39;. Therefore, validations need to be performed between these levels, considering various values and conditions.&lt;/p&gt;\n\n&lt;p&gt;I attempted to create some expectations using Great Expectations (GE), but I encountered several complexities specific to my use case. For instance, I need to verify whether the sum of the revenue from orders with status equals to &amp;quot;approved&amp;quot; in my orders table matches the sum in the approved revenue column of my revenue table. This is a basic example for which I couldn&amp;#39;t find any solution using GE.&lt;/p&gt;\n\n&lt;p&gt;My question is: should I create a simplified version of Great Expectations to address all my use cases deeply? I feel that I&amp;#39;m going to amount of Toil in either scenario, possibly even more if I choose to implement it using Great Expectations in its entirety.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191y5xq", "is_robot_indexable": true, "report_reasons": null, "author": "Feisty_Albatross_893", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191y5xq/should_i_use_great_expectations_or_build_it_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191y5xq/should_i_use_great_expectations_or_build_it_myself/", "subreddit_subscribers": 151477, "created_utc": 1704753073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.\n\nI believe we are more consumers than producers of data.\n\n\\- You depend on the data the developers write in PostgreSQL.  \n\\- You depend on the data salespeople input in Salesforce.  \n\\- You depend on the data accountants add to Xero.\n\nIf these people produce low-quality data, there\u2019s almost no chance for you to build high-quality data products.\n\nTrying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.\n\nYou need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.\n\nI'm not saying data engineers are not producers for analysts, but I think we are primarily consumers.\n\nWhat's your opinion?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the data team a data producer or a consumer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191s99i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704738951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.&lt;/p&gt;\n\n&lt;p&gt;I believe we are more consumers than producers of data.&lt;/p&gt;\n\n&lt;p&gt;- You depend on the data the developers write in PostgreSQL.&lt;br/&gt;\n- You depend on the data salespeople input in Salesforce.&lt;br/&gt;\n- You depend on the data accountants add to Xero.&lt;/p&gt;\n\n&lt;p&gt;If these people produce low-quality data, there\u2019s almost no chance for you to build high-quality data products.&lt;/p&gt;\n\n&lt;p&gt;Trying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.&lt;/p&gt;\n\n&lt;p&gt;You need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not saying data engineers are not producers for analysts, but I think we are primarily consumers.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "191s99i", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/191s99i/is_the_data_team_a_data_producer_or_a_consumer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191s99i/is_the_data_team_a_data_producer_or_a_consumer/", "subreddit_subscribers": 151477, "created_utc": 1704738951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have heard great things and wanted to give it a shot. Currently have a Postgres database handling data aggregations and ETL. I wanted to pick a separate technology for the warehouse so that down the road migrating to something like Snowflake would be less painful (no shortcuts, lazy coding, etc) and to keep end users from accidentally stalling out the resources of the ETL pipelines. Snowflake and anything similar would be overkill for my current data volume, and trying to avoid cloud for simplifying security environment.\n\nI was curious how DuckDB could work in this scenario with multiple end users. The data in Postgres will be properly transformed into gold level and star schema before landing in the warehouse. I\u2019m really wanting the warehouse to be a ready to consume analytics layer rather than a mess of transformations and staging tables (already using DBT in ETL pipelines).\n\nWould I surface a common .db file on a local network for end users with read only access? Or is there another configuration to consider/another tool completely that would be better suited for the job.\n\nEdit: thanks for all the replies. It seems like local data warehouse solutions meant for multiple users is in short supply. May use the Postgres columnar extension instead and use DuckDb as a backbone for superduperdb which seems more in spirit with the \u201cembeddedness\u201d", "author_fullname": "t2_ao7u40a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB as a data warehouse for multiple users?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922n89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704808048.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704764802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have heard great things and wanted to give it a shot. Currently have a Postgres database handling data aggregations and ETL. I wanted to pick a separate technology for the warehouse so that down the road migrating to something like Snowflake would be less painful (no shortcuts, lazy coding, etc) and to keep end users from accidentally stalling out the resources of the ETL pipelines. Snowflake and anything similar would be overkill for my current data volume, and trying to avoid cloud for simplifying security environment.&lt;/p&gt;\n\n&lt;p&gt;I was curious how DuckDB could work in this scenario with multiple end users. The data in Postgres will be properly transformed into gold level and star schema before landing in the warehouse. I\u2019m really wanting the warehouse to be a ready to consume analytics layer rather than a mess of transformations and staging tables (already using DBT in ETL pipelines).&lt;/p&gt;\n\n&lt;p&gt;Would I surface a common .db file on a local network for end users with read only access? Or is there another configuration to consider/another tool completely that would be better suited for the job.&lt;/p&gt;\n\n&lt;p&gt;Edit: thanks for all the replies. It seems like local data warehouse solutions meant for multiple users is in short supply. May use the Postgres columnar extension instead and use DuckDb as a backbone for superduperdb which seems more in spirit with the \u201cembeddedness\u201d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1922n89", "is_robot_indexable": true, "report_reasons": null, "author": "minormisgnomer", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922n89/duckdb_as_a_data_warehouse_for_multiple_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922n89/duckdb_as_a_data_warehouse_for_multiple_users/", "subreddit_subscribers": 151477, "created_utc": 1704764802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data engineering can mean a lot of things depending on what company you work at. At all of the companies I've worked at, all data engineering disciplines had the title of data engineer.\n\n\n\n\n\nFor example, I see data engineering as falling under the following areas: software engineer - data, data pipeline development, ML engineering, database admin.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you make it clear on your resume what area of data engineering you work in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191rwwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704738111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data engineering can mean a lot of things depending on what company you work at. At all of the companies I&amp;#39;ve worked at, all data engineering disciplines had the title of data engineer.&lt;/p&gt;\n\n&lt;p&gt;For example, I see data engineering as falling under the following areas: software engineer - data, data pipeline development, ML engineering, database admin.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "191rwwx", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/191rwwx/how_do_you_make_it_clear_on_your_resume_what_area/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191rwwx/how_do_you_make_it_clear_on_your_resume_what_area/", "subreddit_subscribers": 151477, "created_utc": 1704738111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking to experiment with various big data tooling like Databricks and Microsoft Fabric. What would be good datasets to play with to build out projects with? \n\nI think Kaggle is likely too small scale. I\u2019d like to be able to flesh out ETLs all the way through to Analytics and ETLs. So looking for large scale freely available data sets that could be interesting to work with. \n\nThanks!", "author_fullname": "t2_srabrhe6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good datasets for personal learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191snt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704739951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to experiment with various big data tooling like Databricks and Microsoft Fabric. What would be good datasets to play with to build out projects with? &lt;/p&gt;\n\n&lt;p&gt;I think Kaggle is likely too small scale. I\u2019d like to be able to flesh out ETLs all the way through to Analytics and ETLs. So looking for large scale freely available data sets that could be interesting to work with. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191snt1", "is_robot_indexable": true, "report_reasons": null, "author": "i-kn0w-n0thing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191snt1/good_datasets_for_personal_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191snt1/good_datasets_for_personal_learning/", "subreddit_subscribers": 151477, "created_utc": 1704739951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. \n\n&amp;#x200B;\n\nMy question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn't a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them \"Are you happy here? If not, what can we do to make it better?\" questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. \n\n&amp;#x200B;", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Data) Engineering Managers - What Makes You Stay At A Company Long", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1927hdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704779296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn&amp;#39;t a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them &amp;quot;Are you happy here? If not, what can we do to make it better?&amp;quot; questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1927hdr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/", "subreddit_subscribers": 151477, "created_utc": 1704779296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.\n\nWe all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern\n\nBecause users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors \n\nI agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python\n\nWhy don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we are ignoring Julia for data engineering and going for rust??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192fhgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704809199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.&lt;/p&gt;\n\n&lt;p&gt;We all agree python has issues and major of them are\n1. Its slow as hell\n2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem \n3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern&lt;/p&gt;\n\n&lt;p&gt;Because users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors &lt;/p&gt;\n\n&lt;p&gt;I agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn\u2019t solve the packaging problem of python&lt;/p&gt;\n\n&lt;p&gt;Why don\u2019t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192fhgg", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/", "subreddit_subscribers": 151477, "created_utc": 1704809199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nWe have an interesting problem and wanted to get reddit's take on it. I am keeping it intentionally vague for privacy, but will add relevant details and can answer questions in the comments. \n\nWe are building an application that segments consumers based on whether they have or have not done activities. Ultimately the application supports combinations of boolean logic and whether they have or have not done something to arrive at a list of IDs and a count of consumers. It also supports non-dynamic date ranges, whether they have done it in the last 30 days, 1 year, or all time. One example request would be \"Get me all the people who have done ((activity X AND activity Y) OR activity Z) in the last 30 days\", another could be \"Get me all the people who have done activity X AND NOT activity Y\". We delivered this functionality though our architecture has some issues. \n\nPrimary issue is that the we are using event tables, and as a result we have records of consumers who HAVE participated in an activity but we don't maintain people who HAVE NOT done the activity, thus we cannot query consumers who have done an activity 0 times. To achieve the NOT logic, we exclude people who have done the activity to get the final result set. If we were to run a query that has activity X OR NOT activity Y, we would get unintended results. \n\nSecondary issue is that we have preset date ranges, which is useful at times but not very flexible or dynamic. Ideally a user could input a custom date range in the application to be applied to the backend queries. \n\nThis is a very large data set. The total possible consumers is \\~3B and they all participate in (some) activity. We 1) want to maintain a pool of consumers such that our NOT logic can query for records where participation count = 0 and 2) want to provide a flexible date range option (or mimic that functionality) \n\nFirst thought is to join a datedim table to our table of consumers to have one record for each combination of consumer and date. However this results in an ever increasing amount of data, for 2 years we would have \\~2Trillion records and counting. Then we have to apply boolean logic to filter down the records and in some cases join subsets of records, which simply gets expensive and is computationally intensive for a user facing application. \n\nWe are using **snowflake** for data storage and **python** for custom query generation, I am trying to think of creative ways to add a flexible date range and support potentially massive computation without requests from the application being too slow. For a user application, does it make more sense to compute dynamic date ranges on the fly after a request has been submitted, or precompute a massive dataset and filter it down after the fact? Or a mix of both? \n\nIf anyone has any wisdom or similar experience please let me know, I am also happy to answer any questions in the comments. ", "author_fullname": "t2_60ux6m11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Really big data set, combining pool of consumers with datedim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191x2nj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704750442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;We have an interesting problem and wanted to get reddit&amp;#39;s take on it. I am keeping it intentionally vague for privacy, but will add relevant details and can answer questions in the comments. &lt;/p&gt;\n\n&lt;p&gt;We are building an application that segments consumers based on whether they have or have not done activities. Ultimately the application supports combinations of boolean logic and whether they have or have not done something to arrive at a list of IDs and a count of consumers. It also supports non-dynamic date ranges, whether they have done it in the last 30 days, 1 year, or all time. One example request would be &amp;quot;Get me all the people who have done ((activity X AND activity Y) OR activity Z) in the last 30 days&amp;quot;, another could be &amp;quot;Get me all the people who have done activity X AND NOT activity Y&amp;quot;. We delivered this functionality though our architecture has some issues. &lt;/p&gt;\n\n&lt;p&gt;Primary issue is that the we are using event tables, and as a result we have records of consumers who HAVE participated in an activity but we don&amp;#39;t maintain people who HAVE NOT done the activity, thus we cannot query consumers who have done an activity 0 times. To achieve the NOT logic, we exclude people who have done the activity to get the final result set. If we were to run a query that has activity X OR NOT activity Y, we would get unintended results. &lt;/p&gt;\n\n&lt;p&gt;Secondary issue is that we have preset date ranges, which is useful at times but not very flexible or dynamic. Ideally a user could input a custom date range in the application to be applied to the backend queries. &lt;/p&gt;\n\n&lt;p&gt;This is a very large data set. The total possible consumers is ~3B and they all participate in (some) activity. We 1) want to maintain a pool of consumers such that our NOT logic can query for records where participation count = 0 and 2) want to provide a flexible date range option (or mimic that functionality) &lt;/p&gt;\n\n&lt;p&gt;First thought is to join a datedim table to our table of consumers to have one record for each combination of consumer and date. However this results in an ever increasing amount of data, for 2 years we would have ~2Trillion records and counting. Then we have to apply boolean logic to filter down the records and in some cases join subsets of records, which simply gets expensive and is computationally intensive for a user facing application. &lt;/p&gt;\n\n&lt;p&gt;We are using &lt;strong&gt;snowflake&lt;/strong&gt; for data storage and &lt;strong&gt;python&lt;/strong&gt; for custom query generation, I am trying to think of creative ways to add a flexible date range and support potentially massive computation without requests from the application being too slow. For a user application, does it make more sense to compute dynamic date ranges on the fly after a request has been submitted, or precompute a massive dataset and filter it down after the fact? Or a mix of both? &lt;/p&gt;\n\n&lt;p&gt;If anyone has any wisdom or similar experience please let me know, I am also happy to answer any questions in the comments. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191x2nj", "is_robot_indexable": true, "report_reasons": null, "author": "lt-96", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191x2nj/really_big_data_set_combining_pool_of_consumers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191x2nj/really_big_data_set_combining_pool_of_consumers/", "subreddit_subscribers": 151477, "created_utc": 1704750442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just quit without anything lined up due to extreme burnout and being pushed in a management direction on a very small team that doesn\u2019t want or need it, when my career interests at this point are technical. \n\nMy manager came back asking if I\u2019d be interested in contract work to help finish out the project set to span Q1 &amp; Q2 of this year. With my departure, they\u2019re now planning on using in-house consultants of our 1-2 main vendors and asked if I would either assist with the technical component or managing the relationship with the consultant(s). \n\nHonestly, I would definitely be interested if it meant I could focus on just the scope given and didn\u2019t have to be included in other meetings or pulled into the side quests that were pushing me over 40h a week, but trying to figure out a reasonable proposal. The main thing I care about is keeping my hours below 20 (ideally 12-16) as I try to heal this burnout. \n\nIs it as simple as a multiplier of current hourly rate? Or should I factor in my own experience, proprietary knowledge, education, etc? Should I propose hourly or estimate the hours and lump sum for the project? \n\nIn case it\u2019s relevant, my role is possibly more closely aligned with Analytics Engineering (heavy in dbt, I\u2019m only one who knows the tool/our project well) and BI development (implementing dbt for end users), but feel this community has good insight. Anyone have experience here? Thanks in advance!", "author_fullname": "t2_spwsseep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Employer asked me to contract after I gave my notice. Best way to calculate rate for proposal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191t6ed", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704741458.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704741157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just quit without anything lined up due to extreme burnout and being pushed in a management direction on a very small team that doesn\u2019t want or need it, when my career interests at this point are technical. &lt;/p&gt;\n\n&lt;p&gt;My manager came back asking if I\u2019d be interested in contract work to help finish out the project set to span Q1 &amp;amp; Q2 of this year. With my departure, they\u2019re now planning on using in-house consultants of our 1-2 main vendors and asked if I would either assist with the technical component or managing the relationship with the consultant(s). &lt;/p&gt;\n\n&lt;p&gt;Honestly, I would definitely be interested if it meant I could focus on just the scope given and didn\u2019t have to be included in other meetings or pulled into the side quests that were pushing me over 40h a week, but trying to figure out a reasonable proposal. The main thing I care about is keeping my hours below 20 (ideally 12-16) as I try to heal this burnout. &lt;/p&gt;\n\n&lt;p&gt;Is it as simple as a multiplier of current hourly rate? Or should I factor in my own experience, proprietary knowledge, education, etc? Should I propose hourly or estimate the hours and lump sum for the project? &lt;/p&gt;\n\n&lt;p&gt;In case it\u2019s relevant, my role is possibly more closely aligned with Analytics Engineering (heavy in dbt, I\u2019m only one who knows the tool/our project well) and BI development (implementing dbt for end users), but feel this community has good insight. Anyone have experience here? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "191t6ed", "is_robot_indexable": true, "report_reasons": null, "author": "hairc-ut", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191t6ed/employer_asked_me_to_contract_after_i_gave_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191t6ed/employer_asked_me_to_contract_after_i_gave_my/", "subreddit_subscribers": 151477, "created_utc": 1704741157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All of us **want to move fast**, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don't use them.\n\nWe built a quick tool that infers tests/documents for you. \n\nWhat are people's **thoughts on a tool that infers tests and docs and helps you add them to your schema definition**? \n\n&amp;#x200B;\n\n[Schema inference](https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player)", "author_fullname": "t2_q5jcx79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT test and documentation generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mv0k6icgofbc1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/DASHPlaylist.mpd?a=1707417486%2COTc0ZGM4YTAxOWM0YTQxNWY4ZWI5YzRhZjIxYmRhOGVhNGYzNTBmYTU4NDJlYTM2NTAyMjY2MTY3NzIwOTEzMA%3D%3D&amp;v=1&amp;f=sd", "x": 1730, "y": 1080, "hlsUrl": "https://v.redd.it/link/192he2z/asset/mv0k6icgofbc1/HLSPlaylist.m3u8?a=1707417486%2CYjllMTI3MDZiNGUzYmJmOGUwYjQyZTVlZmNmZjNiM2I0YWJkOTVjNzY1OGVhNjgyNDcxMmY1MDkxYzA1NDgxMg%3D%3D&amp;v=1&amp;f=sd", "id": "mv0k6icgofbc1", "isGif": false}}, "name": "t3_192he2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704814481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All of us &lt;strong&gt;want to move fast&lt;/strong&gt;, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don&amp;#39;t use them.&lt;/p&gt;\n\n&lt;p&gt;We built a quick tool that infers tests/documents for you. &lt;/p&gt;\n\n&lt;p&gt;What are people&amp;#39;s &lt;strong&gt;thoughts on a tool that infers tests and docs and helps you add them to your schema definition&lt;/strong&gt;? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player\"&gt;Schema inference&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192he2z", "is_robot_indexable": true, "report_reasons": null, "author": "bk1007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/", "subreddit_subscribers": 151477, "created_utc": 1704814481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.\n\nThis will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.", "author_fullname": "t2_vlvq8b3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what graph DB to use for knowledge graphs for our LLM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192abwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704790358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.&lt;/p&gt;\n\n&lt;p&gt;This will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192abwa", "is_robot_indexable": true, "report_reasons": null, "author": "0xkiichiro", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/", "subreddit_subscribers": 151477, "created_utc": 1704790358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some excel files and I need to do some data quality checks using python\n\n1) Null count and Treatment\n    if a particular column has &gt;25% , it should throw an email\n\n2) Duplicate Treatment\n\n3) Accepted values in Column \n   e.g State column should have only 50 states\n\n4) Expected Datatype check for columns\n\n5) Variance check for Numeric column compared to previous month \n\netc\n\nFor now I have inplemented using pandas inbuilt functions.\n\n\nI thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.", "author_fullname": "t2_l6b7kbp7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality python Modules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19253mf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704771759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some excel files and I need to do some data quality checks using python&lt;/p&gt;\n\n&lt;p&gt;1) Null count and Treatment\n    if a particular column has &amp;gt;25% , it should throw an email&lt;/p&gt;\n\n&lt;p&gt;2) Duplicate Treatment&lt;/p&gt;\n\n&lt;p&gt;3) Accepted values in Column \n   e.g State column should have only 50 states&lt;/p&gt;\n\n&lt;p&gt;4) Expected Datatype check for columns&lt;/p&gt;\n\n&lt;p&gt;5) Variance check for Numeric column compared to previous month &lt;/p&gt;\n\n&lt;p&gt;etc&lt;/p&gt;\n\n&lt;p&gt;For now I have inplemented using pandas inbuilt functions.&lt;/p&gt;\n\n&lt;p&gt;I thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19253mf", "is_robot_indexable": true, "report_reasons": null, "author": "vainothisside", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19253mf/data_quality_python_modules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19253mf/data_quality_python_modules/", "subreddit_subscribers": 151477, "created_utc": 1704771759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did you know that? I'll just add that to the \"why I prefer SQL to Pandas\" points' list.  ", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You can't enforce a schema when creating a df in Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192iwrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704818275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you know that? I&amp;#39;ll just add that to the &amp;quot;why I prefer SQL to Pandas&amp;quot; points&amp;#39; list.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192iwrh", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192iwrh/you_cant_enforce_a_schema_when_creating_a_df_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192iwrh/you_cant_enforce_a_schema_when_creating_a_df_in/", "subreddit_subscribers": 151477, "created_utc": 1704818275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?  \nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using the DBT semantic layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192b61u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704793915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?&lt;br/&gt;\nTheir google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192b61u", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/", "subreddit_subscribers": 151477, "created_utc": 1704793915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We're sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &gt; Meltano &gt; DBT &gt; Openmetadata &gt; Tableau with custom data models.  Our head of infrastructure is a \"Microsoft can solve everything\" guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We're now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the \"wild west\" approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We're dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?", "author_fullname": "t2_1hb8tav0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Fabric - cost and viability in a large org with shrinking budget and limited usage of MS stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192epn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704806924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We&amp;#39;re sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase &amp;gt; Meltano &amp;gt; DBT &amp;gt; Openmetadata &amp;gt; Tableau with custom data models.  Our head of infrastructure is a &amp;quot;Microsoft can solve everything&amp;quot; guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We&amp;#39;re now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the &amp;quot;wild west&amp;quot; approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We&amp;#39;re dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192epn6", "is_robot_indexable": true, "report_reasons": null, "author": "FrebTheRat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/", "subreddit_subscribers": 151477, "created_utc": 1704806924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://github.com/EzgiKorkmaz/generalization-reinforcement-learning](https://github.com/EzgiKorkmaz/generalization-reinforcement-learning) ", "author_fullname": "t2_dxnb75vp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing Reinforcement Learning Generalization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192chgw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704799202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/EzgiKorkmaz/generalization-reinforcement-learning\"&gt;https://github.com/EzgiKorkmaz/generalization-reinforcement-learning&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?auto=webp&amp;s=a307011e6d4e8cd83b1f8039ab18cd797bf54f7f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cbd692d64c5865e109dcac0536578e96e7b8b9dc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d40596348cc3ffacd8e505f90d53ca9e7e5422fa", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80b9ccb6cb445bc0e713ae03d4ba14b0a5208763", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=011e2e08124c57a32a8eaf26dfcbef8e4ccc503f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11e398d2b4006ee054ecf4b53f9fe08ef9dd4978", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Y-lN7fQfNIzz7z5auEaCqRim2P7XfXgYOqJcofAP9Pg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ad34b233db77aeafa025ca3c2a613e8cbadf124", "width": 1080, "height": 540}], "variants": {}, "id": "2VV733y5RSMC2bXV8KwN3DvYFj_HKl-SHo_DFHYQ_2U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "192chgw", "is_robot_indexable": true, "report_reasons": null, "author": "ml_dnn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192chgw/analyzing_reinforcement_learning_generalization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192chgw/analyzing_reinforcement_learning_generalization/", "subreddit_subscribers": 151477, "created_utc": 1704799202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Probably only one thing that is more painful than working with legacy ETL stored procedures/DMLs is migrating them.\n\nNew data teams normally start with a data analyst or data scientist come up with a number of SQLs, then setting up cron jobs materializing views into tables. Over time, it becomes a pile of mess. Data leanage exists nowhere but in their minds. Tables being updated by god-knows DMLs.\n\nI have seen a lot of efforts in migrating existing SQL codes and DML to DAG-based solution like DBT. But doing so manually is extremely tedious. I wonder there must be automation service for this already.\n\nHas anyone come across this problem?", "author_fullname": "t2_oqbpxpulm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrate legacy database ETL to newer DAG-based", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1922oee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704764890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Probably only one thing that is more painful than working with legacy ETL stored procedures/DMLs is migrating them.&lt;/p&gt;\n\n&lt;p&gt;New data teams normally start with a data analyst or data scientist come up with a number of SQLs, then setting up cron jobs materializing views into tables. Over time, it becomes a pile of mess. Data leanage exists nowhere but in their minds. Tables being updated by god-knows DMLs.&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of efforts in migrating existing SQL codes and DML to DAG-based solution like DBT. But doing so manually is extremely tedious. I wonder there must be automation service for this already.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1922oee", "is_robot_indexable": true, "report_reasons": null, "author": "JayDoDr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1922oee/migrate_legacy_database_etl_to_newer_dagbased/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1922oee/migrate_legacy_database_etl_to_newer_dagbased/", "subreddit_subscribers": 151477, "created_utc": 1704764890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am trying to think of ways to approximate the count of records returned on a series of queries without processing the query. \n\nI am using snowflake, and we create a number of temporary tables that are either inner or outer joined together to support AND / OR logic. \n\nFor example, say we have Temp Table A, Temp Table B, Temp Table C and we want a count of records for ids that appear in ((A AND B) OR C). Is it remotely possible to approximate a count without running a query? My first thought is to use **HyperLogLog** algorithm ([https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:\\~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization](https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization)), however this seems to be more applicable for interacting with 2 tables and finding the approximate intersection. \n\nWe don't need to be very precise, just a ballpark count. Any ideas out there to estimate the count of multiple tables being joined together?\n\n&amp;#x200B;", "author_fullname": "t2_60ux6m11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get multi-table join results without running the query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_191x6w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704750724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am trying to think of ways to approximate the count of records returned on a series of queries without processing the query. &lt;/p&gt;\n\n&lt;p&gt;I am using snowflake, and we create a number of temporary tables that are either inner or outer joined together to support AND / OR logic. &lt;/p&gt;\n\n&lt;p&gt;For example, say we have Temp Table A, Temp Table B, Temp Table C and we want a count of records for ids that appear in ((A AND B) OR C). Is it remotely possible to approximate a count without running a query? My first thought is to use &lt;strong&gt;HyperLogLog&lt;/strong&gt; algorithm (&lt;a href=\"https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:%7E:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization\"&gt;https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization&lt;/a&gt;), however this seems to be more applicable for interacting with 2 tables and finding the approximate intersection. &lt;/p&gt;\n\n&lt;p&gt;We don&amp;#39;t need to be very precise, just a ballpark count. Any ideas out there to estimate the count of multiple tables being joined together?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "191x6w3", "is_robot_indexable": true, "report_reasons": null, "author": "lt-96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/191x6w3/get_multitable_join_results_without_running_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/191x6w3/get_multitable_join_results_without_running_the/", "subreddit_subscribers": 151477, "created_utc": 1704750724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a question wrt Meltano, as I'm considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api's on a daily basis with full overwrites to the target. Based on what I'm reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?", "author_fullname": "t2_9knk666s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch with Meltano", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192gc7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704811623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a question wrt Meltano, as I&amp;#39;m considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api&amp;#39;s on a daily basis with full overwrites to the target. Based on what I&amp;#39;m reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192gc7y", "is_robot_indexable": true, "report_reasons": null, "author": "limartje", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192gc7y/batch_with_meltano/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192gc7y/batch_with_meltano/", "subreddit_subscribers": 151477, "created_utc": 1704811623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n* I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.\n* I then download these files from GCS and perform data transformations locally.\n* Once data is cleaned, I upload it to a BigQuery table.\n* After the transformation, the local file is deleted, and the file in GCS is moved to a \"processed\" folder.\n\n**The Issue**:\n\nWhen multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)\n\n**Potential Cause**:\n\nThrough my research, it seems like this might be related to a \"race condition,\" although I\u2019m not entirely certain.\n\n**Request for Assistance**:\n\nI'm seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.\n\nCould anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?", "author_fullname": "t2_47qmqn8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Resolving File Handling Issue in Python ETL Process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192eege", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704805948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;I initiate an API call to a vendor that triggers their crawlers, resulting in CSV files being sent to a Google Cloud Storage (GCS) bucket.&lt;/li&gt;\n&lt;li&gt;I then download these files from GCS and perform data transformations locally.&lt;/li&gt;\n&lt;li&gt;Once data is cleaned, I upload it to a BigQuery table.&lt;/li&gt;\n&lt;li&gt;After the transformation, the local file is deleted, and the file in GCS is moved to a &amp;quot;processed&amp;quot; folder.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Issue&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;When multiple files are downloaded simultaneously, the Python program may encounter a situation where, during the transformation phase, it mistakenly reports that certain files don\u2019t exist, leading to those specific files remaining in the local system, while others are successfully transformed, uploaded to BigQuery, and deleted from the local system. ( for example transformation starts for file A but later shows file B does not exist)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Potential Cause&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Through my research, it seems like this might be related to a &amp;quot;race condition,&amp;quot; although I\u2019m not entirely certain.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Request for Assistance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on how to precisely identify and resolve this issue. I suspect it might have something to do with handling multiple file downloads simultaneously and the subsequent file processing in Python.&lt;/p&gt;\n\n&lt;p&gt;Could anyone provide insights or strategies to handle such concurrent file processing scenarios more effectively in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192eege", "is_robot_indexable": true, "report_reasons": null, "author": "polonium_biscuit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192eege/need_help_resolving_file_handling_issue_in_python/", "subreddit_subscribers": 151477, "created_utc": 1704805948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?\n\nAlso, do you guys prefer multicloud solution or does your service provide a good enough SLA?\n\nMy case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.", "author_fullname": "t2_12ff33zm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys approach High Availability and DR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192c93r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704798302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?&lt;/p&gt;\n\n&lt;p&gt;Also, do you guys prefer multicloud solution or does your service provide a good enough SLA?&lt;/p&gt;\n\n&lt;p&gt;My case: my stack basically consist of BigQuery/Airflow/PowerBI.\nFor instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "192c93r", "is_robot_indexable": true, "report_reasons": null, "author": "RobvicRJ", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/", "subreddit_subscribers": 151477, "created_utc": 1704798302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have been trying to solve this problem for several weeks. First of all I'll summarize the case and then I'll do it with all the details. I come here because I can not find anything that solves the problem in forums or even chatGPT, because spark connect was implemented in Spark version 3.4, so it is quite recent.\n\nMy goal is to create a spark cluster with docker containers and then use spark connect on a jupyter notebook but this time installed on the host machine (not a container) so I can practice with spark code, while seeing how the tasks run on the container spark cluster. I work on windows, which I think is important to solve the problem I will expose at the end with containers.\n\nWell, being as brief as possible, I am able to install spark on the containers with ubuntu, run the master and connect the workers to the master. When I run ./spark-connect-server.sh -packages... as it says in the documentation I don't know if that spark server is connected or related to the cluster I have set up and I would like to understand the details and steps to follow for this configuration.The other idea that comes to me is that spark connect is independent of the cluster running in the containers but I wouldn't understand its use either.\n\nOn the other hand, if in the jupyter notebook with python I use import pyspark and then pyspark.sql following the steps in the documentation, when using sparkSession.builder.remote(\"ip and port of the container\") I do not know which is the ip to use, if the one of sparkconnect executed (and how to know this ip), or the one of the master \"ip container:7077\". This is the part that loses me the most because I don't even understand the errors that the program returns. It may be important that I run jupyter from VSCode.\n\n\nFinally, I would like to explain a problem of connections with the ports of Docker containers. I always have to map the container ports, for example the 8080 of the container running the master, with those of the host machine because otherwise I can't use the container ip 127.17.0.1:8080 from the browser to access the Spark web UI, I can only access as [localhost:8080](http://localhost:8080) with the port mapped in the container creation.\n\n\nThanks for reading the post regardless of whether you can provide info or not, and thanks also if you comment providing a solution to one or more of the problems exposed.\n\n\nBest regards.", "author_fullname": "t2_4dzozo3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark connect + Jupuyter Notebook + Docker Cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_192blx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704797143.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704795719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been trying to solve this problem for several weeks. First of all I&amp;#39;ll summarize the case and then I&amp;#39;ll do it with all the details. I come here because I can not find anything that solves the problem in forums or even chatGPT, because spark connect was implemented in Spark version 3.4, so it is quite recent.&lt;/p&gt;\n\n&lt;p&gt;My goal is to create a spark cluster with docker containers and then use spark connect on a jupyter notebook but this time installed on the host machine (not a container) so I can practice with spark code, while seeing how the tasks run on the container spark cluster. I work on windows, which I think is important to solve the problem I will expose at the end with containers.&lt;/p&gt;\n\n&lt;p&gt;Well, being as brief as possible, I am able to install spark on the containers with ubuntu, run the master and connect the workers to the master. When I run ./spark-connect-server.sh -packages... as it says in the documentation I don&amp;#39;t know if that spark server is connected or related to the cluster I have set up and I would like to understand the details and steps to follow for this configuration.The other idea that comes to me is that spark connect is independent of the cluster running in the containers but I wouldn&amp;#39;t understand its use either.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, if in the jupyter notebook with python I use import pyspark and then pyspark.sql following the steps in the documentation, when using sparkSession.builder.remote(&amp;quot;ip and port of the container&amp;quot;) I do not know which is the ip to use, if the one of sparkconnect executed (and how to know this ip), or the one of the master &amp;quot;ip container:7077&amp;quot;. This is the part that loses me the most because I don&amp;#39;t even understand the errors that the program returns. It may be important that I run jupyter from VSCode.&lt;/p&gt;\n\n&lt;p&gt;Finally, I would like to explain a problem of connections with the ports of Docker containers. I always have to map the container ports, for example the 8080 of the container running the master, with those of the host machine because otherwise I can&amp;#39;t use the container ip 127.17.0.1:8080 from the browser to access the Spark web UI, I can only access as &lt;a href=\"http://localhost:8080\"&gt;localhost:8080&lt;/a&gt; with the port mapped in the container creation.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading the post regardless of whether you can provide info or not, and thanks also if you comment providing a solution to one or more of the problems exposed.&lt;/p&gt;\n\n&lt;p&gt;Best regards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "192blx8", "is_robot_indexable": true, "report_reasons": null, "author": "Omaroto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/192blx8/spark_connect_jupuyter_notebook_docker_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/192blx8/spark_connect_jupuyter_notebook_docker_cluster/", "subreddit_subscribers": 151477, "created_utc": 1704795719.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}