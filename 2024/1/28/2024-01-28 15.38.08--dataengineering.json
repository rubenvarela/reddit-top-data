{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!  \n\n\nFirst of all, all my apologies if that question has been asked before.\n\nI need to design a data platform on GCP.     \nI gave some thoughts about the possible ingestion and aggregation approaches.    \nIn the past I used a full BigQuery approach (Option 2 below).    \nI was wondering if keeping the raw and staging data in Cloud Storage and using Spark for transformation could make sense.    \nI think it can be great because of cost and being cloud agnostic. In that scenario the Data Warehouse will be split between Cloud Storage and BigQuery.\n\nDo you guys have any experience with using external tables and Cloud Storage + Spark compared to a full Big Query Data Warehouse approach?  \nHow about the cost?\n\n## 1) First Option\n\n* The csv/json files will be ingested in a Cloud Storage Bucket.\n* The csv/json files can be read and written in parquet format in another bucket (staging area).\n* Data transformations are done with Spark.\n* The resulting transformations (usable by data users &amp; BI tools) are accessible through BigQuery external (or internal) tables.\n\n## 2) Second Option\n\n* The csv/json files will be ingested in a Cloud Storage Bucket.\n* The data are ingested into BigQuery as raw tables (staging area).\n* Data transformations are done with BigQuery (SQL) directly.\n* The resulting transformations (usable by data users &amp; BI tools) are accessible through BigQuery tables.\n\n## 3) Third Option\n\n* The csv/json files will be ingested in a Cloud Storage Bucket.\n* The data are ingested into BigQuery as raw external tables (staging area).\n* Data transformations are done with BigQuery (SQL) directly.\n* The resulting transformations (usable by data users &amp; BI tools) are accessible through BigQuery external tables.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/db1jzh7l23fc1.png?width=1357&amp;format=png&amp;auto=webp&amp;s=9d404ee6da16310d3f7b05a62b9b6429f5bbaf79", "author_fullname": "t2_3wj092gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse approaches on GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "media_metadata": {"db1jzh7l23fc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46f4dbb73b260d388f526fd095ece8f50b807f08"}, {"y": 86, "x": 216, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=af7caa338cc489513d1258f458bd86d13554c4d7"}, {"y": 128, "x": 320, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=061f6cba8f257bbf529ebf9e29d5d891b4f8a8fb"}, {"y": 257, "x": 640, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=510d4b629ae13823fd9a44f98feca35eaa0ce3f9"}, {"y": 386, "x": 960, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f8ce3633358005a008cd3b761cbad7ff99317cdb"}, {"y": 434, "x": 1080, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d38cb33c54b958ad46fa605f3650a90ac935164"}], "s": {"y": 546, "x": 1357, "u": "https://preview.redd.it/db1jzh7l23fc1.png?width=1357&amp;format=png&amp;auto=webp&amp;s=9d404ee6da16310d3f7b05a62b9b6429f5bbaf79"}, "id": "db1jzh7l23fc1"}}, "name": "t3_1acqprq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/y_gFdwHAOULgxo5xxRYrKm0mmBevHsnxN4PNFAfVSxk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706405232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!  &lt;/p&gt;\n\n&lt;p&gt;First of all, all my apologies if that question has been asked before.&lt;/p&gt;\n\n&lt;p&gt;I need to design a data platform on GCP.&lt;br/&gt;\nI gave some thoughts about the possible ingestion and aggregation approaches.&lt;br/&gt;\nIn the past I used a full BigQuery approach (Option 2 below).&lt;br/&gt;\nI was wondering if keeping the raw and staging data in Cloud Storage and using Spark for transformation could make sense.&lt;br/&gt;\nI think it can be great because of cost and being cloud agnostic. In that scenario the Data Warehouse will be split between Cloud Storage and BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any experience with using external tables and Cloud Storage + Spark compared to a full Big Query Data Warehouse approach?&lt;br/&gt;\nHow about the cost?&lt;/p&gt;\n\n&lt;h2&gt;1) First Option&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The csv/json files will be ingested in a Cloud Storage Bucket.&lt;/li&gt;\n&lt;li&gt;The csv/json files can be read and written in parquet format in another bucket (staging area).&lt;/li&gt;\n&lt;li&gt;Data transformations are done with Spark.&lt;/li&gt;\n&lt;li&gt;The resulting transformations (usable by data users &amp;amp; BI tools) are accessible through BigQuery external (or internal) tables.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;2) Second Option&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The csv/json files will be ingested in a Cloud Storage Bucket.&lt;/li&gt;\n&lt;li&gt;The data are ingested into BigQuery as raw tables (staging area).&lt;/li&gt;\n&lt;li&gt;Data transformations are done with BigQuery (SQL) directly.&lt;/li&gt;\n&lt;li&gt;The resulting transformations (usable by data users &amp;amp; BI tools) are accessible through BigQuery tables.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;3) Third Option&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The csv/json files will be ingested in a Cloud Storage Bucket.&lt;/li&gt;\n&lt;li&gt;The data are ingested into BigQuery as raw external tables (staging area).&lt;/li&gt;\n&lt;li&gt;Data transformations are done with BigQuery (SQL) directly.&lt;/li&gt;\n&lt;li&gt;The resulting transformations (usable by data users &amp;amp; BI tools) are accessible through BigQuery external tables.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/db1jzh7l23fc1.png?width=1357&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d404ee6da16310d3f7b05a62b9b6429f5bbaf79\"&gt;https://preview.redd.it/db1jzh7l23fc1.png?width=1357&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d404ee6da16310d3f7b05a62b9b6429f5bbaf79&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1acqprq", "is_robot_indexable": true, "report_reasons": null, "author": "yinshangyi", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acqprq/data_warehouse_approaches_on_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acqprq/data_warehouse_approaches_on_gcp/", "subreddit_subscribers": 156436, "created_utc": 1706405232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let me tell you all my story. \nI am currently a mid level data engineer in a middle size retail company. The company is quite stable and have public stock. \n\nThe reason why I considering cloud data engineer  is because the market where I am right now is very heavy into cloud and spark. So if i need to advance my paygrade I need to have experience with these field.\n\nThe problem is my company does everything on premise. So most thing will do via python, SQL with some legacy cron but mostly using Airflow to orchestrate thing.   Also some use CDC Debezium, Kafka for some critical streaming data.   I also provide dashboard with alert daily in the form of Superset  also.\n\nThere is no spark usage because the data is not that big and the cost and infra need for it outweight ours need at the moment. Some data do exist in Google BigQuery but thats the reponsibility of other team. I do ingest it for some case.\n\nHow would I gain 'actual' experience for those fields? I do study and does some demo workshop pipeline with AWS, Google and Azure but its all very basic with static data.\n\nAs for spark i do pratice with my personal project and make it as custom docker image. But again it not the same as what you will face in actual work environment right?\n\nThank for taking your time to read and for any advice. Sorry if something might seem weird as english is not my native language.", "author_fullname": "t2_5hb3w8bs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you break into cloud data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1accv8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706368216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let me tell you all my story. \nI am currently a mid level data engineer in a middle size retail company. The company is quite stable and have public stock. &lt;/p&gt;\n\n&lt;p&gt;The reason why I considering cloud data engineer  is because the market where I am right now is very heavy into cloud and spark. So if i need to advance my paygrade I need to have experience with these field.&lt;/p&gt;\n\n&lt;p&gt;The problem is my company does everything on premise. So most thing will do via python, SQL with some legacy cron but mostly using Airflow to orchestrate thing.   Also some use CDC Debezium, Kafka for some critical streaming data.   I also provide dashboard with alert daily in the form of Superset  also.&lt;/p&gt;\n\n&lt;p&gt;There is no spark usage because the data is not that big and the cost and infra need for it outweight ours need at the moment. Some data do exist in Google BigQuery but thats the reponsibility of other team. I do ingest it for some case.&lt;/p&gt;\n\n&lt;p&gt;How would I gain &amp;#39;actual&amp;#39; experience for those fields? I do study and does some demo workshop pipeline with AWS, Google and Azure but its all very basic with static data.&lt;/p&gt;\n\n&lt;p&gt;As for spark i do pratice with my personal project and make it as custom docker image. But again it not the same as what you will face in actual work environment right?&lt;/p&gt;\n\n&lt;p&gt;Thank for taking your time to read and for any advice. Sorry if something might seem weird as english is not my native language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1accv8x", "is_robot_indexable": true, "report_reasons": null, "author": "vclz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1accv8x/how_would_you_break_into_cloud_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1accv8x/how_would_you_break_into_cloud_data_engineer/", "subreddit_subscribers": 156436, "created_utc": 1706368216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn even more about data engineering and would really like to connect to enthusiastic data folks. Does anybody has recommendations for good data related conference across europe in 2024?", "author_fullname": "t2_lj5pjrd6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best data conferences in Europe - 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad0vkk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706442110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn even more about data engineering and would really like to connect to enthusiastic data folks. Does anybody has recommendations for good data related conference across europe in 2024?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ad0vkk", "is_robot_indexable": true, "report_reasons": null, "author": "HolidayCritical3665", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad0vkk/best_data_conferences_in_europe_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad0vkk/best_data_conferences_in_europe_2024/", "subreddit_subscribers": 156436, "created_utc": 1706442110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say I have source team that can write data to Kafka for the data team\u2019s consumption so data team can build a DW. What approach from the below approaches would be the best approxh for long term success and why?\n\nSource\u2014&gt; Kafka\u2014&gt;Redshift (DW)\n\nSource \u2014&gt; Kafka \u2014&gt; S3 \u2014&gt; Redshift?\n\n\nI like storing data in S3 first and  then copy it into Redshift. This way your S3 can be your Data Lake and the DL data can be used bu other tools. \n\nI am trying to see what are some other benefits of storing data on S3 or should we just write data to Redshift?", "author_fullname": "t2_5cjr5v2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lake preferred approach.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acvcqo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706420119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have source team that can write data to Kafka for the data team\u2019s consumption so data team can build a DW. What approach from the below approaches would be the best approxh for long term success and why?&lt;/p&gt;\n\n&lt;p&gt;Source\u2014&amp;gt; Kafka\u2014&amp;gt;Redshift (DW)&lt;/p&gt;\n\n&lt;p&gt;Source \u2014&amp;gt; Kafka \u2014&amp;gt; S3 \u2014&amp;gt; Redshift?&lt;/p&gt;\n\n&lt;p&gt;I like storing data in S3 first and  then copy it into Redshift. This way your S3 can be your Data Lake and the DL data can be used bu other tools. &lt;/p&gt;\n\n&lt;p&gt;I am trying to see what are some other benefits of storing data on S3 or should we just write data to Redshift?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1acvcqo", "is_robot_indexable": true, "report_reasons": null, "author": "captut", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acvcqo/data_lake_preferred_approach/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acvcqo/data_lake_preferred_approach/", "subreddit_subscribers": 156436, "created_utc": 1706420119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some context : I'm working on a predictive maintenance prototype on Azure. Essentially, the sensors send in readings periodically every 30s (Temperature, Vibrations, Pressure, Noice, etc.). The data is added into an event hub. The data is then processed and dumped into ADLS V2. The readings are passed into an ML model and run against some basic checks(If temp exceeds, send an email notification to the asset owner, etc...) The notifications (for now) are processed via logic apps(When a blob is created within the datalake)\n\nCan these events directly be processed via an event driven architecture instead of using Kafka? Or processing the data through serverless functions? \n\nAlso, what are some good visualization tools that can let me monotor this data in near real time?\n\nI've just started learning to use Kafka, and would appreciate any answers. ", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the purpose of using Kafka, when the same can be processed though an Event Driven Architecuture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad1xqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706446067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some context : I&amp;#39;m working on a predictive maintenance prototype on Azure. Essentially, the sensors send in readings periodically every 30s (Temperature, Vibrations, Pressure, Noice, etc.). The data is added into an event hub. The data is then processed and dumped into ADLS V2. The readings are passed into an ML model and run against some basic checks(If temp exceeds, send an email notification to the asset owner, etc...) The notifications (for now) are processed via logic apps(When a blob is created within the datalake)&lt;/p&gt;\n\n&lt;p&gt;Can these events directly be processed via an event driven architecture instead of using Kafka? Or processing the data through serverless functions? &lt;/p&gt;\n\n&lt;p&gt;Also, what are some good visualization tools that can let me monotor this data in near real time?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just started learning to use Kafka, and would appreciate any answers. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ad1xqt", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad1xqt/whats_the_purpose_of_using_kafka_when_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad1xqt/whats_the_purpose_of_using_kafka_when_the_same/", "subreddit_subscribers": 156436, "created_utc": 1706446067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, software engineer in the trading space here. Currently, we dump a load of very useful data into Kdb and never look at it again, q is designed for Kx consultants, not for the lay-person. Took some time over Christmas to make a simple system that goes:\n\nRaw JSON events in Kdb -[Java]-&gt; ClickHouse Raw -[Materialised View]-&gt; ClickHouse Facts -&gt; Superset\n\nDemo reactions were similar to how I imagine cavemen reacted to seeing fire for the first time, and news of this shiny new tool has flown up the chain of command. Needless to say we're now very keen on building out a SQL-based OLAP system. As we move towards a proper POC, I've refined the setup slightly to:\n\n* dbt for building the facts table from the raw table. \n  * Materialised views felt too magic, at least how I was using them - lots of JSON parsing and self joins to build up order summaries, etc. Feel they're more suited for simple running aggregations?\n* Jenkins for running the Kdb ingest and dbt transforms every 10m.\n  * Don't shoot me, had it already and I'm familiar with it as a developer.\n\nSome key goals/facts:\n\n* Must be low maintenance.\n  * We're turning our backs on the corporate data strategy. No dedicated DE staff, just developers with lots of other things to be doing.\n* Cost is secondary.\n  * Insights from this data are worth $$$.\n* Keep everything, forever (ELT &gt; ETL). \n  * It's unacceptable to say \"we don't have this piece of data anymore\" or \"we forgot to parse that new field\" in 2024.\n* Full Prod volumes are c.10bn rows a day, 99.9% of which is market data.\n  * Raw JSON market data events can reside in Kdb per previous tenet, but we'll still need to store every tick (under a proper schema) for order analysis.\n\nImagine the final production deployment will be managed Superset/ClickHouse, then self hosted orchestration/dbt. Things I'm particularly interested in hearing your thoughts on are (but not exclusively):\n\n* Is ClickHouse a suitable database choice? Simple deployment was a driving factor for my toy project; could Snowflake, StarRocks, Pinot/Druid, etc etc be a better option as we move towards production?\n* What orchestration tool should I use? Airflow/Dagster/Prefect seem to be the big 3, any distinguishing features? Do forsee needing to run more complicated transformations than just dbt SQL (eg. the quants will probably want to get involved running ML models etc).\n* Any other useful tools I'm missing (although more tools = more things to maintain).\n* What should we watch out for as we scale this up from a toy to a POC and finally to production? \n\nThanks!!", "author_fullname": "t2_snsq45lm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a greenfield OLAP system for finance/trading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ad00tv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706438633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, software engineer in the trading space here. Currently, we dump a load of very useful data into Kdb and never look at it again, q is designed for Kx consultants, not for the lay-person. Took some time over Christmas to make a simple system that goes:&lt;/p&gt;\n\n&lt;p&gt;Raw JSON events in Kdb -[Java]-&amp;gt; ClickHouse Raw -[Materialised View]-&amp;gt; ClickHouse Facts -&amp;gt; Superset&lt;/p&gt;\n\n&lt;p&gt;Demo reactions were similar to how I imagine cavemen reacted to seeing fire for the first time, and news of this shiny new tool has flown up the chain of command. Needless to say we&amp;#39;re now very keen on building out a SQL-based OLAP system. As we move towards a proper POC, I&amp;#39;ve refined the setup slightly to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dbt for building the facts table from the raw table. \n\n&lt;ul&gt;\n&lt;li&gt;Materialised views felt too magic, at least how I was using them - lots of JSON parsing and self joins to build up order summaries, etc. Feel they&amp;#39;re more suited for simple running aggregations?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Jenkins for running the Kdb ingest and dbt transforms every 10m.\n\n&lt;ul&gt;\n&lt;li&gt;Don&amp;#39;t shoot me, had it already and I&amp;#39;m familiar with it as a developer.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Some key goals/facts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be low maintenance.\n\n&lt;ul&gt;\n&lt;li&gt;We&amp;#39;re turning our backs on the corporate data strategy. No dedicated DE staff, just developers with lots of other things to be doing.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Cost is secondary.\n\n&lt;ul&gt;\n&lt;li&gt;Insights from this data are worth $$$.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Keep everything, forever (ELT &amp;gt; ETL). \n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s unacceptable to say &amp;quot;we don&amp;#39;t have this piece of data anymore&amp;quot; or &amp;quot;we forgot to parse that new field&amp;quot; in 2024.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Full Prod volumes are c.10bn rows a day, 99.9% of which is market data.\n\n&lt;ul&gt;\n&lt;li&gt;Raw JSON market data events can reside in Kdb per previous tenet, but we&amp;#39;ll still need to store every tick (under a proper schema) for order analysis.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Imagine the final production deployment will be managed Superset/ClickHouse, then self hosted orchestration/dbt. Things I&amp;#39;m particularly interested in hearing your thoughts on are (but not exclusively):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is ClickHouse a suitable database choice? Simple deployment was a driving factor for my toy project; could Snowflake, StarRocks, Pinot/Druid, etc etc be a better option as we move towards production?&lt;/li&gt;\n&lt;li&gt;What orchestration tool should I use? Airflow/Dagster/Prefect seem to be the big 3, any distinguishing features? Do forsee needing to run more complicated transformations than just dbt SQL (eg. the quants will probably want to get involved running ML models etc).&lt;/li&gt;\n&lt;li&gt;Any other useful tools I&amp;#39;m missing (although more tools = more things to maintain).&lt;/li&gt;\n&lt;li&gt;What should we watch out for as we scale this up from a toy to a POC and finally to production? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ad00tv", "is_robot_indexable": true, "report_reasons": null, "author": "Alternative_Push_948", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad00tv/building_a_greenfield_olap_system_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad00tv/building_a_greenfield_olap_system_for/", "subreddit_subscribers": 156436, "created_utc": 1706438633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm considering a data Engineer job in Stockholm and wondering what salary one might expect. I have around 3-4 years of experience in the area. I don't want to get lowballed and there is not a lot of information on the sites that I have checked.", "author_fullname": "t2_16rehat2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer salary in sweden", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aczxgc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706438262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering a data Engineer job in Stockholm and wondering what salary one might expect. I have around 3-4 years of experience in the area. I don&amp;#39;t want to get lowballed and there is not a lot of information on the sites that I have checked.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aczxgc", "is_robot_indexable": true, "report_reasons": null, "author": "fjurgo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aczxgc/data_engineer_salary_in_sweden/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aczxgc/data_engineer_salary_in_sweden/", "subreddit_subscribers": 156436, "created_utc": 1706438262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After seeing a few people on this subreddit respond to [Gunnar Morling's 1BRC](https://www.morling.dev/blog/one-billion-row-challenge/) with unofficial SQL solutions, we decided to give it a try, and after a few rounds of optimizing our parsing code, Pansynchro's performance is equivalent to the top 20% of 1BRC contest entrants.  Not bad for a general-purpose data integration framework that's not able to apply many of the hyper-specific optimization tricks used by top-performing entries!\n\n[https://pansynchro.tech/the-one-billion-row-challenge-optimizing-csv-performance/](https://pansynchro.tech/the-one-billion-row-challenge-optimizing-csv-performance/)", "author_fullname": "t2_otqnx9sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One Billion Row Challenge - Pansynchro", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acuj21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706417290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After seeing a few people on this subreddit respond to &lt;a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\"&gt;Gunnar Morling&amp;#39;s 1BRC&lt;/a&gt; with unofficial SQL solutions, we decided to give it a try, and after a few rounds of optimizing our parsing code, Pansynchro&amp;#39;s performance is equivalent to the top 20% of 1BRC contest entrants.  Not bad for a general-purpose data integration framework that&amp;#39;s not able to apply many of the hyper-specific optimization tricks used by top-performing entries!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pansynchro.tech/the-one-billion-row-challenge-optimizing-csv-performance/\"&gt;https://pansynchro.tech/the-one-billion-row-challenge-optimizing-csv-performance/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1acuj21", "is_robot_indexable": true, "report_reasons": null, "author": "Pansynchro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acuj21/one_billion_row_challenge_pansynchro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acuj21/one_billion_row_challenge_pansynchro/", "subreddit_subscribers": 156436, "created_utc": 1706417290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have converted some domain-specific name vectors into embeddings, with a dataset size of 200k words. All the embeddings were generated using OpenAI's embedding model 3 (3072 dim per embedding) . Now I am planning to implement semantic search similarity. Given a domain keyword, I want to find the top 5 most similar matches. After embedding all 280k words, the size of the JSON file containing the embeddings is around 30GB.\n\nI am new to this domain and evaluating the best options.\n\n1. Should I use a cloud vector database like Pinecone or Typsense, or host locally on DigitalOcean?\n2. If I go with a cloud option like Typsense, what configuration (RAM, etc.) would I need for 280k embeddings (30GB in size)? And how much would it likely cost?\n\nI have been confused for the past few days and unable to find useful resources. Any help or advice you could provide would be greatly appreciated.", "author_fullname": "t2_kgr7e5qng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Semantic Search on 200k vectors (30GB) Worth of Embeddings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acxy0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706430039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have converted some domain-specific name vectors into embeddings, with a dataset size of 200k words. All the embeddings were generated using OpenAI&amp;#39;s embedding model 3 (3072 dim per embedding) . Now I am planning to implement semantic search similarity. Given a domain keyword, I want to find the top 5 most similar matches. After embedding all 280k words, the size of the JSON file containing the embeddings is around 30GB.&lt;/p&gt;\n\n&lt;p&gt;I am new to this domain and evaluating the best options.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I use a cloud vector database like Pinecone or Typsense, or host locally on DigitalOcean?&lt;/li&gt;\n&lt;li&gt;If I go with a cloud option like Typsense, what configuration (RAM, etc.) would I need for 280k embeddings (30GB in size)? And how much would it likely cost?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have been confused for the past few days and unable to find useful resources. Any help or advice you could provide would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1acxy0q", "is_robot_indexable": true, "report_reasons": null, "author": "stoicbats_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acxy0q/best_practices_for_semantic_search_on_200k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acxy0q/best_practices_for_semantic_search_on_200k/", "subreddit_subscribers": 156436, "created_utc": 1706430039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys our pipelines are migrating to the azure stack so ADF with Databricks spark transforms and then output to delta tables ADLS gen 2 storage and on to consumption mainly in Power BI.\n\nBut it looks like we'll be building the models very slightly differently in each PBI report which looks crazy inefficient to me. I think we should be building out common reusable data models for each function in our large corporate business.\n\nAre Common Data Models the answer here or is there another best practice we should be doing? No one seems to be thinking about shared data models in my org", "author_fullname": "t2_8lw2xwwp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to use for shared data modelling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acghfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706377752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys our pipelines are migrating to the azure stack so ADF with Databricks spark transforms and then output to delta tables ADLS gen 2 storage and on to consumption mainly in Power BI.&lt;/p&gt;\n\n&lt;p&gt;But it looks like we&amp;#39;ll be building the models very slightly differently in each PBI report which looks crazy inefficient to me. I think we should be building out common reusable data models for each function in our large corporate business.&lt;/p&gt;\n\n&lt;p&gt;Are Common Data Models the answer here or is there another best practice we should be doing? No one seems to be thinking about shared data models in my org&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1acghfv", "is_robot_indexable": true, "report_reasons": null, "author": "Secure_Bandicoot_576", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acghfv/what_to_use_for_shared_data_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acghfv/what_to_use_for_shared_data_modelling/", "subreddit_subscribers": 156436, "created_utc": 1706377752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nHi,\nI currently work as a Business intelligence developer with my cogito,  clarity, caboodle, and revenue data model certs. I know that EPIC will be transitioning to azure in a couple years. I know BID cap out somewhere around 120k in my area.\n\nHow can I best prepare myself to be a data engineer within Healthcare? Should I prepare for DP 203 Microsoft certified azure data engineer and learn python? \nI want to be able to reach at least starting 125k+ fully remote salary in the midwest.\n\nThanks", "author_fullname": "t2_5x2dzayp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Epic report writer to data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acda11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706369355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI currently work as a Business intelligence developer with my cogito,  clarity, caboodle, and revenue data model certs. I know that EPIC will be transitioning to azure in a couple years. I know BID cap out somewhere around 120k in my area.&lt;/p&gt;\n\n&lt;p&gt;How can I best prepare myself to be a data engineer within Healthcare? Should I prepare for DP 203 Microsoft certified azure data engineer and learn python? \nI want to be able to reach at least starting 125k+ fully remote salary in the midwest.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1acda11", "is_robot_indexable": true, "report_reasons": null, "author": "Bionic50", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acda11/epic_report_writer_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acda11/epic_report_writer_to_data_engineer/", "subreddit_subscribers": 156436, "created_utc": 1706369355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have the first round of interviews coming up in a week. The interviewer asked to solve HackerRank hard SQL and Medium Python. Have gone through the SQL questions but I would like to have some more practice. Any tips on where to find more relevant SQL exercises for this role? \n\nAlso, would highly appreciate if anyone with experience of going through this interview or with experience of working in this role leave any comments/tips for the interview. Thanks in advance!", "author_fullname": "t2_9axqyq8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips for the technical round prep (SQL, Python) for Amazon Sr. BIE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ad3mxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706451522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have the first round of interviews coming up in a week. The interviewer asked to solve HackerRank hard SQL and Medium Python. Have gone through the SQL questions but I would like to have some more practice. Any tips on where to find more relevant SQL exercises for this role? &lt;/p&gt;\n\n&lt;p&gt;Also, would highly appreciate if anyone with experience of going through this interview or with experience of working in this role leave any comments/tips for the interview. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ad3mxb", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult-Big-3890", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ad3mxb/any_tips_for_the_technical_round_prep_sql_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ad3mxb/any_tips_for_the_technical_round_prep_sql_python/", "subreddit_subscribers": 156436, "created_utc": 1706451522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let\u2019s say you have an application that communicates with your database. You scaled out your frontend so that it\u2019s highly available. However, a process that occurs from your frontend is very noisy and results in too much chatter going from your database back to that one instance.\n\nMaybe the question is fundamentally flawed, but does it make technical sense in any manner to scale out the requests your server receives from its backend? How would you do that? Would it require proxying the complex request through a backend service and awaiting the result at the instance that needs it? Are there any algorithms that do this without requiring changes to the underlying application?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you horizontally scale for incoming requests?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acx6je", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706426958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let\u2019s say you have an application that communicates with your database. You scaled out your frontend so that it\u2019s highly available. However, a process that occurs from your frontend is very noisy and results in too much chatter going from your database back to that one instance.&lt;/p&gt;\n\n&lt;p&gt;Maybe the question is fundamentally flawed, but does it make technical sense in any manner to scale out the requests your server receives from its backend? How would you do that? Would it require proxying the complex request through a backend service and awaiting the result at the instance that needs it? Are there any algorithms that do this without requiring changes to the underlying application?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1acx6je", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acx6je/how_do_you_horizontally_scale_for_incoming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acx6je/how_do_you_horizontally_scale_for_incoming/", "subreddit_subscribers": 156436, "created_utc": 1706426958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! Does anyone have experience with using Confluent Cloud\u2019s fully-managed S3 Sink Connector to move data from Kafka topics to AWS S3? Currently I am struggling to simulate whether its scheduled rotations (through rotate.schedule.interval.ms config parameter) actually leads to duplicate records in the records uploaded to S3, as the documentation says **\u201cUsing the rotate.schedule.interval.ms property results in a non-deterministic environment and invalidates exactly-once guarantees.\u201d** \n\nIn my attempt to simulate a scenario that produces duplicates, I am referencing the example in this article ([Confluent S3 Sink Connector EOS (declarativesystems.com)](https://www.declarativesystems.com/2023/08/18/confluent-s3-sink-connector-eos.html) which illustrates that duplicates can occur when the producer writes to the Kafka topic at the same time as when the connector is scheduled to upload. So what I did was schedule a confluent\\_kafka Producer (in Python) to produce records every 10 minutes, while my S3 Sink Connector is configured to also upload every 10 minutes. So far I have tried this with 1,500 records and 6,000 records. However, I have yet to see any duplicate records when I try to parse the output stored in S3. One thing I did forget until now was the flush.size is still set to 1000 (default) meaning an upload should trigger once a kafka partition reaches 1000 records, but I have not seen any early uploads so I think it has not affected my simulation so far, but sharing it in case my understanding is wrong.", "author_fullname": "t2_3j4k9pzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Confluent Cloud's fully managed S3 Sink Connector, do duplicates still occur?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acur5b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706418063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Does anyone have experience with using Confluent Cloud\u2019s fully-managed S3 Sink Connector to move data from Kafka topics to AWS S3? Currently I am struggling to simulate whether its scheduled rotations (through rotate.schedule.interval.ms config parameter) actually leads to duplicate records in the records uploaded to S3, as the documentation says &lt;strong&gt;\u201cUsing the rotate.schedule.interval.ms property results in a non-deterministic environment and invalidates exactly-once guarantees.\u201d&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;In my attempt to simulate a scenario that produces duplicates, I am referencing the example in this article (&lt;a href=\"https://www.declarativesystems.com/2023/08/18/confluent-s3-sink-connector-eos.html\"&gt;Confluent S3 Sink Connector EOS (declarativesystems.com)&lt;/a&gt; which illustrates that duplicates can occur when the producer writes to the Kafka topic at the same time as when the connector is scheduled to upload. So what I did was schedule a confluent_kafka Producer (in Python) to produce records every 10 minutes, while my S3 Sink Connector is configured to also upload every 10 minutes. So far I have tried this with 1,500 records and 6,000 records. However, I have yet to see any duplicate records when I try to parse the output stored in S3. One thing I did forget until now was the flush.size is still set to 1000 (default) meaning an upload should trigger once a kafka partition reaches 1000 records, but I have not seen any early uploads so I think it has not affected my simulation so far, but sharing it in case my understanding is wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1acur5b", "is_robot_indexable": true, "report_reasons": null, "author": "Nagusameta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acur5b/using_confluent_clouds_fully_managed_s3_sink/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acur5b/using_confluent_clouds_fully_managed_s3_sink/", "subreddit_subscribers": 156436, "created_utc": 1706418063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could anyone recommend university-affiliated certifications for a budding data analyst? Looking for options beyond Coursera.\n\nThanks!", "author_fullname": "t2_82djul76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "University Data Analyst Certification Recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acu35b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706415774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could anyone recommend university-affiliated certifications for a budding data analyst? Looking for options beyond Coursera.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1acu35b", "is_robot_indexable": true, "report_reasons": null, "author": "ledangkhoa1220", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acu35b/university_data_analyst_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acu35b/university_data_analyst_certification/", "subreddit_subscribers": 156436, "created_utc": 1706415774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Inputs\n\n* multiple teams with various hours available per month.\n* multiple projects with various hours to complete and months from the deadline.\n\nAssumption\n\n* project hours/month will not exceed team capacity.\n* month 1 in solution is next month (June 2024)\n\nIn the data below team A has 3 projects. The projects require 1000 monthly hours each (3000 hours divided by 3 months). Team A has 2000 monthly capacity hours to dedicate to any number of projects. I want to write code that will define the start month and then smartly know when to start the next project with that team until all projects are done. In the example, team A can do projects 1 and 2 simultaneously because it is below their capacity and start on project 3 in month 4 as project 1 wraps up and their capacity increases to a point where they can start working on project 3.\n\nProject Data\n\n|Project|Team|Priority|Month|Project Hours|\n|:-|:-|:-|:-|:-|\n|1|A|1|3|3000|\n|2|A|2|6|6000|\n|3|A|3|3|3000|\n|4|B|1|6|1500|\n\nTeam Capacity Dimension\n\n|Team|Monthly Capacity|\n|:-|:-|\n|a|2000|\n|b|2000|\n\nOutput\n\n|Project|Team|Month|\n|:-|:-|:-|\n|1|a|1|\n|1|a|2|\n|1|a|3|\n|2|a|1|\n|2|a|2|\n|2|a|3|\n|2|a|4|\n|2|a|5|\n|2|a|6|\n|3|a|4|\n|3|a|5|\n|3|a|6|\n|4|b|1|\n|4|b|2|\n|4|b|3|\n|4|b|4|\n|4|b|5|\n|4|b|6|\n\nI\u2019m thinking a loop and/ or an over (partition by,  order) would be my best option. Thoughts?\n\nThanks in advance, jamkgrif", "author_fullname": "t2_7dkzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Define project start and end time by team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acse7q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706412368.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706410368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inputs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;multiple teams with various hours available per month.&lt;/li&gt;\n&lt;li&gt;multiple projects with various hours to complete and months from the deadline.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Assumption&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;project hours/month will not exceed team capacity.&lt;/li&gt;\n&lt;li&gt;month 1 in solution is next month (June 2024)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the data below team A has 3 projects. The projects require 1000 monthly hours each (3000 hours divided by 3 months). Team A has 2000 monthly capacity hours to dedicate to any number of projects. I want to write code that will define the start month and then smartly know when to start the next project with that team until all projects are done. In the example, team A can do projects 1 and 2 simultaneously because it is below their capacity and start on project 3 in month 4 as project 1 wraps up and their capacity increases to a point where they can start working on project 3.&lt;/p&gt;\n\n&lt;p&gt;Project Data&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Project&lt;/th&gt;\n&lt;th align=\"left\"&gt;Team&lt;/th&gt;\n&lt;th align=\"left\"&gt;Priority&lt;/th&gt;\n&lt;th align=\"left\"&gt;Month&lt;/th&gt;\n&lt;th align=\"left\"&gt;Project Hours&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;6000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Team Capacity Dimension&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Team&lt;/th&gt;\n&lt;th align=\"left\"&gt;Monthly Capacity&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Output&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Project&lt;/th&gt;\n&lt;th align=\"left\"&gt;Team&lt;/th&gt;\n&lt;th align=\"left\"&gt;Month&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I\u2019m thinking a loop and/ or an over (partition by,  order) would be my best option. Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance, jamkgrif&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1acse7q", "is_robot_indexable": true, "report_reasons": null, "author": "jamkgrif", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acse7q/define_project_start_and_end_time_by_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acse7q/define_project_start_and_end_time_by_team/", "subreddit_subscribers": 156436, "created_utc": 1706410368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHi all, coming for QA background, wanted to switch to data engineering field, I'm curious to know - what are the differences between a data engineer and a cloud engineer in terms of job roles and skills sets? Which one is better accordingly? \n\nSpoke to cloud engineer and he told that DE skills also comes in his field, so refered me to consider Cloud engineering. What's your take ont his\n\nThanks for any helpful responses!", "author_fullname": "t2_bpk9d5w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering vs Cloud Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acwxnp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706425963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, coming for QA background, wanted to switch to data engineering field, I&amp;#39;m curious to know - what are the differences between a data engineer and a cloud engineer in terms of job roles and skills sets? Which one is better accordingly? &lt;/p&gt;\n\n&lt;p&gt;Spoke to cloud engineer and he told that DE skills also comes in his field, so refered me to consider Cloud engineering. What&amp;#39;s your take ont his&lt;/p&gt;\n\n&lt;p&gt;Thanks for any helpful responses!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1acwxnp", "is_robot_indexable": true, "report_reasons": null, "author": "Zestyclose_Web_6331", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acwxnp/data_engineering_vs_cloud_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acwxnp/data_engineering_vs_cloud_engineering/", "subreddit_subscribers": 156436, "created_utc": 1706425963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nI\u2019m sorry to be mean , but offlate some of the weekly zoom calls have really bad speakers . There is a guy who wears Blippi glasses and when asked questions , he says the tutorial is in progress and asks the audience to follow his LinkedIn profile . It sounds more like he is interested in getting followers rather than he knowing the subject . I doubt he knows things about langchain, llama index , etc , when asked about it , he gives a nod and lame smile . These zoom calls were excellent when Akmal ran the show . I never missed Akmal\u2019s calls. Nowadays I get out of the call when that blippi joins . \n#singlestore #weeklycalls", "author_fullname": "t2_7v604kty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Singlestore Zoom call reviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aciszo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706383793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sorry to be mean , but offlate some of the weekly zoom calls have really bad speakers . There is a guy who wears Blippi glasses and when asked questions , he says the tutorial is in progress and asks the audience to follow his LinkedIn profile . It sounds more like he is interested in getting followers rather than he knowing the subject . I doubt he knows things about langchain, llama index , etc , when asked about it , he gives a nod and lame smile . These zoom calls were excellent when Akmal ran the show . I never missed Akmal\u2019s calls. Nowadays I get out of the call when that blippi joins . &lt;/p&gt;\n\n&lt;h1&gt;singlestore #weeklycalls&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aciszo", "is_robot_indexable": true, "report_reasons": null, "author": "plutobot-0203", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aciszo/singlestore_zoom_call_reviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aciszo/singlestore_zoom_call_reviews/", "subreddit_subscribers": 156436, "created_utc": 1706383793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Forget boring textbooks and stuffy seminars! Welcome to the data jungle, where mountains of information await your exploration. \nYou, armed with the mighty PySpark, are no ordinary tourist. You\u2019re a data explorer, a fearless decoder of secrets hidden within numbers. \nBrace yourself for twists and turns, thrilling transformations, and jaw-dropping insights as you conquer the ever-expanding horizons of big data.", "author_fullname": "t2_a34lyspp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Your Data-Driven Adventures: A Beginner\u2019s Guide to PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 49, "top_awarded_type": null, "hide_score": false, "name": "t3_1acwv3f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2GHlsxJCT2tFnN6HB08jnHEAgn0oT7mGXHRmQ-ba5Og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706425685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Forget boring textbooks and stuffy seminars! Welcome to the data jungle, where mountains of information await your exploration. \nYou, armed with the mighty PySpark, are no ordinary tourist. You\u2019re a data explorer, a fearless decoder of secrets hidden within numbers. \nBrace yourself for twists and turns, thrilling transformations, and jaw-dropping insights as you conquer the ever-expanding horizons of big data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@prakhar740/spark-your-data-driven-adventures-a-beginners-guide-to-pyspark-18695f077966", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uLxucKr6lqO8j4m_8vGvLAhwGmTCGzkZsTEjDmCjt44.jpg?auto=webp&amp;s=547c5fec498631ea2d56fa5b71ea711a5f047910", "width": 785, "height": 280}, "resolutions": [{"url": "https://external-preview.redd.it/uLxucKr6lqO8j4m_8vGvLAhwGmTCGzkZsTEjDmCjt44.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb255f7176d5f87c4eb92645dfc96df576176862", "width": 108, "height": 38}, {"url": "https://external-preview.redd.it/uLxucKr6lqO8j4m_8vGvLAhwGmTCGzkZsTEjDmCjt44.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56b8536643cca7908b0df5fe4d57e09d443b44c7", "width": 216, "height": 77}, {"url": "https://external-preview.redd.it/uLxucKr6lqO8j4m_8vGvLAhwGmTCGzkZsTEjDmCjt44.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=56bb776ede29d91c4985f46f8145e4f6e79c3d68", "width": 320, "height": 114}, {"url": "https://external-preview.redd.it/uLxucKr6lqO8j4m_8vGvLAhwGmTCGzkZsTEjDmCjt44.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e5b0132d44f91be370886e2d349b07407f2bbe75", "width": 640, "height": 228}], "variants": {}, "id": "4O4TpjN82e-arGYAMO_532wFcQFvoFqKSpcb_o-98So"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1acwv3f", "is_robot_indexable": true, "report_reasons": null, "author": "NoWarthog3988", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acwv3f/spark_your_datadriven_adventures_a_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@prakhar740/spark-your-data-driven-adventures-a-beginners-guide-to-pyspark-18695f077966", "subreddit_subscribers": 156436, "created_utc": 1706425685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know it depends but say you're on a Junior role. How much of overtime or staying behind can I expect? \n\nAlso, how much of the job is about coding? Like how many hours do you spend programming? \n\nThanks", "author_fullname": "t2_rrtbk3gwh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE is the dream job. How common are overtime?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1acgj4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.26, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706377873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it depends but say you&amp;#39;re on a Junior role. How much of overtime or staying behind can I expect? &lt;/p&gt;\n\n&lt;p&gt;Also, how much of the job is about coding? Like how many hours do you spend programming? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1acgj4m", "is_robot_indexable": true, "report_reasons": null, "author": "Timeframe98", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1acgj4m/de_is_the_dream_job_how_common_are_overtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1acgj4m/de_is_the_dream_job_how_common_are_overtime/", "subreddit_subscribers": 156436, "created_utc": 1706377873.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}