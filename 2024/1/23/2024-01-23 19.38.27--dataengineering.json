{"kind": "Listing", "data": {"after": "t3_19dniju", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some reason, everytime I try to learn I see new tools and how they ease the existing work. And I end up wasting more time where if I spent that on actually learning, I would be way ahead. How do you know which tool to pick and choose(from the noise in the market) ?\n\nhttps://preview.redd.it/ji5thy5f05ec1.png?width=2013&amp;format=png&amp;auto=webp&amp;s=167f4e2afce621cc135d5a0ff7d5c484fedaa032", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Data Space really this Complicated or am I just overthinking?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ji5thy5f05ec1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=407ea76e6c58768be8a3c4ec83b1e1c94a7f1ccd"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9fc662ada79e21acb1a19ca2d720fa384fa5b80"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa8a34aed3f57410d9ec1e07e19d12870b4eaaa6"}, {"y": 330, "x": 640, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a5a2c5452da28f4a51f554df8ace1370adf7bb6"}, {"y": 496, "x": 960, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=79d4e991f16ad3e962f34f36b200936b6c240e63"}, {"y": 558, "x": 1080, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=007ab4b69e95d325e31ea4357d1e22ee0f4a7d60"}], "s": {"y": 1041, "x": 2013, "u": "https://preview.redd.it/ji5thy5f05ec1.png?width=2013&amp;format=png&amp;auto=webp&amp;s=167f4e2afce621cc135d5a0ff7d5c484fedaa032"}, "id": "ji5thy5f05ec1"}}, "name": "t3_19difxp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NL2dUEcAl7BNg-wBWQxQmSI85U9S7otuZLi8-pjwXbA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705992902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason, everytime I try to learn I see new tools and how they ease the existing work. And I end up wasting more time where if I spent that on actually learning, I would be way ahead. How do you know which tool to pick and choose(from the noise in the market) ?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ji5thy5f05ec1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=167f4e2afce621cc135d5a0ff7d5c484fedaa032\"&gt;https://preview.redd.it/ji5thy5f05ec1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=167f4e2afce621cc135d5a0ff7d5c484fedaa032&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19difxp", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19difxp/is_the_data_space_really_this_complicated_or_am_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19difxp/is_the_data_space_really_this_complicated_or_am_i/", "subreddit_subscribers": 155169, "created_utc": 1705992902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backstory - First let me say I\u2019m a newish BI analyst and in a department of me. We are starting to develop a data culture but I need to show value. I\u2019m using PBI and made several reports from on prem databases. \n\nMy goal is to connect to an API and put the acquired data into a MS Access database. The API returns the data in JSON format. \n\nI\u2019ve written a very simple Python program that pulls the data from the API on a daily basis and places it in a JSON file to be consumed by PBI. \n\nI\u2019m not a data engineer by any stretch of the imagination but I figured this would be the right sub to ask this question. \n\nIs it possible for Python to pull the data from the API and insert into an Access database while also performing some ETL? Am I in way over my head if I\u2019m very new to Python?", "author_fullname": "t2_3hma2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First time connecting to an API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d6f86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705957729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backstory - First let me say I\u2019m a newish BI analyst and in a department of me. We are starting to develop a data culture but I need to show value. I\u2019m using PBI and made several reports from on prem databases. &lt;/p&gt;\n\n&lt;p&gt;My goal is to connect to an API and put the acquired data into a MS Access database. The API returns the data in JSON format. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve written a very simple Python program that pulls the data from the API on a daily basis and places it in a JSON file to be consumed by PBI. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not a data engineer by any stretch of the imagination but I figured this would be the right sub to ask this question. &lt;/p&gt;\n\n&lt;p&gt;Is it possible for Python to pull the data from the API and insert into an Access database while also performing some ETL? Am I in way over my head if I\u2019m very new to Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19d6f86", "is_robot_indexable": true, "report_reasons": null, "author": "jebert32", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19d6f86/first_time_connecting_to_an_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19d6f86/first_time_connecting_to_an_api/", "subreddit_subscribers": 155169, "created_utc": 1705957729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My goal is to get a fully-remote, entry-level job in data engineering. How unrealistic is that?\n\nBe brutally honest... better to crush my dreams now rather than study for months only to find out later.\n\nI have no experience in data engineering, but I do have a phd in a stem field and 4 years working as a data scientist", "author_fullname": "t2_fq68u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Entry level + Fully Remote... How unrealistic?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19de5wg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705978332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to get a fully-remote, entry-level job in data engineering. How unrealistic is that?&lt;/p&gt;\n\n&lt;p&gt;Be brutally honest... better to crush my dreams now rather than study for months only to find out later.&lt;/p&gt;\n\n&lt;p&gt;I have no experience in data engineering, but I do have a phd in a stem field and 4 years working as a data scientist&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19de5wg", "is_robot_indexable": true, "report_reasons": null, "author": "KimchiFitness", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19de5wg/entry_level_fully_remote_how_unrealistic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19de5wg/entry_level_fully_remote_how_unrealistic/", "subreddit_subscribers": 155169, "created_utc": 1705978332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently working on an ETL pipeline to move tables from SQL Server on a server to Postgres on another server, and then do operations on those tables in Postgres on a daily schedule. Some of those tables contain geospatial data.\n\nI currently implemented the pipeline through python scripts using pandas and sqlalchemy\n\nFor tables with spatial data I use geopandas and geosqlalchemy\n\nThe process is functional so far but it takes too long to move large tables \n\nHow can I speed up moving tables from SQL Server to Postgres? What kind of tools/ideas would be better to implement?", "author_fullname": "t2_b0bsz436o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL pipeline from SQL Server to Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19djant", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705996388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently working on an ETL pipeline to move tables from SQL Server on a server to Postgres on another server, and then do operations on those tables in Postgres on a daily schedule. Some of those tables contain geospatial data.&lt;/p&gt;\n\n&lt;p&gt;I currently implemented the pipeline through python scripts using pandas and sqlalchemy&lt;/p&gt;\n\n&lt;p&gt;For tables with spatial data I use geopandas and geosqlalchemy&lt;/p&gt;\n\n&lt;p&gt;The process is functional so far but it takes too long to move large tables &lt;/p&gt;\n\n&lt;p&gt;How can I speed up moving tables from SQL Server to Postgres? What kind of tools/ideas would be better to implement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19djant", "is_robot_indexable": true, "report_reasons": null, "author": "bolt_runner", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19djant/etl_pipeline_from_sql_server_to_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19djant/etl_pipeline_from_sql_server_to_postgres/", "subreddit_subscribers": 155169, "created_utc": 1705996388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "have an education in software development check.\n\ndo allot of courses on courserea and build a portfolio\n\nget a data analyst job\n\nget a data engineer job after a few years.\n\n&amp;#x200B;\n\ndont know if it matters but im in canada.\n\n&amp;#x200B;\n\nedit:just wanted to say thank you all for being so positive.", "author_fullname": "t2_szy83vbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "my path on being a data engineer, advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d4lq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705988887.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705953260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have an education in software development check.&lt;/p&gt;\n\n&lt;p&gt;do allot of courses on courserea and build a portfolio&lt;/p&gt;\n\n&lt;p&gt;get a data analyst job&lt;/p&gt;\n\n&lt;p&gt;get a data engineer job after a few years.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;dont know if it matters but im in canada.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit:just wanted to say thank you all for being so positive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19d4lq3", "is_robot_indexable": true, "report_reasons": null, "author": "BornYoghurt8710", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19d4lq3/my_path_on_being_a_data_engineer_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19d4lq3/my_path_on_being_a_data_engineer_advice/", "subreddit_subscribers": 155169, "created_utc": 1705953260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently launched a POC data pipeline and now that it\u2019s seeing production workloads I want to refactor it. It\u2019s like a combination of 3 data pipelines, synchronously working, and I want to turn it into 3 smaller pipelines that run asynchronously.\n\nI figure this will result in three smaller projects that steal code from the POC and depreciation of the POC. So, what is typically done to the POC?\n\nI use git like most.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do with depreciated applications in your code base?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dde12", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705976026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently launched a POC data pipeline and now that it\u2019s seeing production workloads I want to refactor it. It\u2019s like a combination of 3 data pipelines, synchronously working, and I want to turn it into 3 smaller pipelines that run asynchronously.&lt;/p&gt;\n\n&lt;p&gt;I figure this will result in three smaller projects that steal code from the POC and depreciation of the POC. So, what is typically done to the POC?&lt;/p&gt;\n\n&lt;p&gt;I use git like most.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dde12", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dde12/what_do_you_do_with_depreciated_applications_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dde12/what_do_you_do_with_depreciated_applications_in/", "subreddit_subscribers": 155169, "created_utc": 1705976026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.\n\nIt has been an Apache top-level project since 2018 ([https://pulsar.apache.org](https://pulsar.apache.org/)). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.\n\nUnlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.\n\nWhen I first heard about the Pulsar, I thought my dreams had come true. \ud83c\udf08\ud83e\udd84\n\nNow I'm trying to understand why there is not much buzz in public about it:\n\n* Is it a marketing flaw and people just didn't ever hear about it?\n* Or is there something wrong or missing in Pulsar?\n* Maybe it is just an overkill for most new projects?\n\nI would greatly appreciate hearing about your experience and thoughts on Pulsar.", "author_fullname": "t2_15hg9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think about Apache Pulsar?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dry3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706026742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706026337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.&lt;/p&gt;\n\n&lt;p&gt;It has been an Apache top-level project since 2018 (&lt;a href=\"https://pulsar.apache.org/\"&gt;https://pulsar.apache.org&lt;/a&gt;). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.&lt;/p&gt;\n\n&lt;p&gt;Unlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.&lt;/p&gt;\n\n&lt;p&gt;When I first heard about the Pulsar, I thought my dreams had come true. \ud83c\udf08\ud83e\udd84&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m trying to understand why there is not much buzz in public about it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it a marketing flaw and people just didn&amp;#39;t ever hear about it?&lt;/li&gt;\n&lt;li&gt;Or is there something wrong or missing in Pulsar?&lt;/li&gt;\n&lt;li&gt;Maybe it is just an overkill for most new projects?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would greatly appreciate hearing about your experience and thoughts on Pulsar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?auto=webp&amp;s=1a58c711911d84b2f519a1ebd08ffaf45660be13", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01aacbda20b76f446c0b68d0f0bcee12860a82ed", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca04e0b0d8c0f2f204fa4d37a4e9513be4b5d06d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a90d61ab4a25f6f305d2ef4c1b13d9096f42412b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b154af8ec3ffb86dffb7caf2c7b2afa2e48bca6e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=421a03368d603cfbce7be51e351c8581fbce11ee", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79cdb62ae01d72d03f7dce263d7d04f41e345207", "width": 1080, "height": 567}], "variants": {}, "id": "IpR2rPjTijRtTpBc0LhDqAYBTgh-AKSxmasMwlwTsew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dry3i", "is_robot_indexable": true, "report_reasons": null, "author": "visortelle", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dry3i/what_do_you_think_about_apache_pulsar/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dry3i/what_do_you_think_about_apache_pulsar/", "subreddit_subscribers": 155169, "created_utc": 1706026337.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Summary:\n\n* B.S. in Microbiology &amp; Chemistry \n* 10 years of work experience - clinical research, teaching, &amp; sales\n* No coding experience \n* Strong soft skills\n* Currently working fully remote\n\nDebating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it's positive or negative.  \n\nAm I turning this career path into a \"feel good\" dream or is it feasible? \n\nMuch love. ", "author_fullname": "t2_ex7mq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I romanticizing DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dp67h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706018753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Summary:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;B.S. in Microbiology &amp;amp; Chemistry &lt;/li&gt;\n&lt;li&gt;10 years of work experience - clinical research, teaching, &amp;amp; sales&lt;/li&gt;\n&lt;li&gt;No coding experience &lt;/li&gt;\n&lt;li&gt;Strong soft skills&lt;/li&gt;\n&lt;li&gt;Currently working fully remote&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Debating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it&amp;#39;s positive or negative.  &lt;/p&gt;\n\n&lt;p&gt;Am I turning this career path into a &amp;quot;feel good&amp;quot; dream or is it feasible? &lt;/p&gt;\n\n&lt;p&gt;Much love. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dp67h", "is_robot_indexable": true, "report_reasons": null, "author": "ItsWetInPortland", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dp67h/am_i_romanticizing_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dp67h/am_i_romanticizing_de/", "subreddit_subscribers": 155169, "created_utc": 1706018753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Partitioning isn't always a win, like any optimization it's a mix balancing trade-offs in favor of the type of workloads that higher priority and seen more often.  \n\n\nWhen determining how large data set should be partitioned, what questions do you ask yourself?  \n\n\nWhen determining how large a data set should be partitioned, what questions do you ask yourself?  \n", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What questions to ask when deciding on partitioning strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d44ul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705952091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Partitioning isn&amp;#39;t always a win, like any optimization it&amp;#39;s a mix balancing trade-offs in favor of the type of workloads that higher priority and seen more often.  &lt;/p&gt;\n\n&lt;p&gt;When determining how large data set should be partitioned, what questions do you ask yourself?  &lt;/p&gt;\n\n&lt;p&gt;When determining how large a data set should be partitioned, what questions do you ask yourself?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19d44ul", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19d44ul/what_questions_to_ask_when_deciding_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19d44ul/what_questions_to_ask_when_deciding_on/", "subreddit_subscribers": 155169, "created_utc": 1705952091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to create a python app that will output data e.g. every 5s (ideally should be parametrizable to e.g. 10s or 1 day). Input is realtime data stream from RabbitMQ/Kafka. Every 5s app should look into the queue and take all data between defined start and end time - i.e. every run instance should be idempotent as it is defined by static start/end. Job should have some retrying logic in case something fails. App should run in on-premise Kubernetes cluster.\n\nI thought of 3 possible solutions:\n\na) custom python app with some scheduler such as Airflow that could schedule the app in cluster every 5s\n\nb) custom python app that would run in an infinite loop as standalone pod in the cluster and every 5s or so would do the job\n\nc) use some existing solution that would solve both issues (data logic and scheduling) for me\n\nThe issue with a) is we need a robust scheduler that could handle all the job spawns (each 5s, each 10s etc.)\n\nThe issue with b) is there is less transparency in case app crashes and also since its only one app in case it crashes all runs that should run after the crash won't run until the app is fixed (scheduler would schedule all the following runs despite previous fails)\n\nThe issue with c) is the only potential solution is Prefect but it has memory leak so when we run it for inifity it will eventually run out of memory.\n\nWhat would be the best approach to implement this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do design a near real-time data job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dtn2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706030793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to create a python app that will output data e.g. every 5s (ideally should be parametrizable to e.g. 10s or 1 day). Input is realtime data stream from RabbitMQ/Kafka. Every 5s app should look into the queue and take all data between defined start and end time - i.e. every run instance should be idempotent as it is defined by static start/end. Job should have some retrying logic in case something fails. App should run in on-premise Kubernetes cluster.&lt;/p&gt;\n\n&lt;p&gt;I thought of 3 possible solutions:&lt;/p&gt;\n\n&lt;p&gt;a) custom python app with some scheduler such as Airflow that could schedule the app in cluster every 5s&lt;/p&gt;\n\n&lt;p&gt;b) custom python app that would run in an infinite loop as standalone pod in the cluster and every 5s or so would do the job&lt;/p&gt;\n\n&lt;p&gt;c) use some existing solution that would solve both issues (data logic and scheduling) for me&lt;/p&gt;\n\n&lt;p&gt;The issue with a) is we need a robust scheduler that could handle all the job spawns (each 5s, each 10s etc.)&lt;/p&gt;\n\n&lt;p&gt;The issue with b) is there is less transparency in case app crashes and also since its only one app in case it crashes all runs that should run after the crash won&amp;#39;t run until the app is fixed (scheduler would schedule all the following runs despite previous fails)&lt;/p&gt;\n\n&lt;p&gt;The issue with c) is the only potential solution is Prefect but it has memory leak so when we run it for inifity it will eventually run out of memory.&lt;/p&gt;\n\n&lt;p&gt;What would be the best approach to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dtn2u", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dtn2u/how_to_do_design_a_near_realtime_data_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dtn2u/how_to_do_design_a_near_realtime_data_job/", "subreddit_subscribers": 155169, "created_utc": 1706030793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to figure out how to size resources for our data pipeline and warehouse. My boss wants me to calculate what each pipeline needs in terms of compute and RAM. Any ideas on how to evaluate and track these requirements? Would love to hear what's worked for you.", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you decide on resource sizes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dor5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706017540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out how to size resources for our data pipeline and warehouse. My boss wants me to calculate what each pipeline needs in terms of compute and RAM. Any ideas on how to evaluate and track these requirements? Would love to hear what&amp;#39;s worked for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dor5g", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dor5g/how_do_you_decide_on_resource_sizes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dor5g/how_do_you_decide_on_resource_sizes/", "subreddit_subscribers": 155169, "created_utc": 1706017540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, to provide some background, I currently run a very small data team in an industry that\u2019s heavily biased towards on-prem. After facing some limitations with SSIS, we\u2019re looking to move to a more modern ETL solution. My issue is we are currently built to run on-prem ETL while modern architecture seems to focus cloud ELT.\n\nMy current setup is as follows: \n1. Extract data from a variety of structured SQL servers (most on-prem, some accessed through VPN. All have direct SQL access)\n2. Transform that data using SQL into a more usable, de-normalized format\n3. Upload the data into our on-prem data warehouse for analytics. \n\nA few complicating factors:\n1. Several of our vendors are moving to the cloud and I\u2019ll need to export the data via API (and one vendor through snowflake)\n2. Our largest data table is under 30 million rows, I don\u2019t expect I need anything more complicated than polars/pandas and SQL\n3. In the future, we\u2019d like to be able to ingest and process high frequency (~5-15s) data loaded via 1 hour batches. I don\u2019t need streaming yet nor do I want to implement it. \n\nTools I\u2019ve considered:\nMage*\nDagster\nAirflow\nPandas/Polars*\nDuckDB\nDBT*\nMinio*\n\n*Current favorite for that role\n\n\nMy thought is, if I\u2019m rebuilding this anyway, would it make more sense to move to an ELT architecture with DBT or maintain my ETL architecture with Python/SQL. What would that look like? Load everything raw into parquet files and process from there? Load raw data into intermediate tables on the same server as the data warehouse? If the majority of my data is already structured, should I bother with parquet? I apologize for the deluge of questions, I\u2019m trying to wrap my head around this new world order after spending too long in the comfortable embrace of SSIS. I appreciate your help!", "author_fullname": "t2_95pplng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT for On-Prem ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dtqiu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706031025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, to provide some background, I currently run a very small data team in an industry that\u2019s heavily biased towards on-prem. After facing some limitations with SSIS, we\u2019re looking to move to a more modern ETL solution. My issue is we are currently built to run on-prem ETL while modern architecture seems to focus cloud ELT.&lt;/p&gt;\n\n&lt;p&gt;My current setup is as follows: \n1. Extract data from a variety of structured SQL servers (most on-prem, some accessed through VPN. All have direct SQL access)\n2. Transform that data using SQL into a more usable, de-normalized format\n3. Upload the data into our on-prem data warehouse for analytics. &lt;/p&gt;\n\n&lt;p&gt;A few complicating factors:\n1. Several of our vendors are moving to the cloud and I\u2019ll need to export the data via API (and one vendor through snowflake)\n2. Our largest data table is under 30 million rows, I don\u2019t expect I need anything more complicated than polars/pandas and SQL\n3. In the future, we\u2019d like to be able to ingest and process high frequency (~5-15s) data loaded via 1 hour batches. I don\u2019t need streaming yet nor do I want to implement it. &lt;/p&gt;\n\n&lt;p&gt;Tools I\u2019ve considered:\nMage*\nDagster\nAirflow\nPandas/Polars*\nDuckDB\nDBT*\nMinio*&lt;/p&gt;\n\n&lt;p&gt;*Current favorite for that role&lt;/p&gt;\n\n&lt;p&gt;My thought is, if I\u2019m rebuilding this anyway, would it make more sense to move to an ELT architecture with DBT or maintain my ETL architecture with Python/SQL. What would that look like? Load everything raw into parquet files and process from there? Load raw data into intermediate tables on the same server as the data warehouse? If the majority of my data is already structured, should I bother with parquet? I apologize for the deluge of questions, I\u2019m trying to wrap my head around this new world order after spending too long in the comfortable embrace of SSIS. I appreciate your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dtqiu", "is_robot_indexable": true, "report_reasons": null, "author": "Lord_Lloydd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dtqiu/dbt_for_onprem_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dtqiu/dbt_for_onprem_etl/", "subreddit_subscribers": 155169, "created_utc": 1706031025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had a phone screen yesterday for a data analytics engineer role. \n\nI was asked how do I monitor the data pipelines and ensure its accuracy. My response was, I enjoy working with the end user and am really great about getting constant feedback. I said how in my current role, as a Product Engineer, i spend a lot of time with users and going through user data/feedback to determine the success of a feature. \n\nNow that I'm thinking about it -- they may have been asking me what tools I use.\n\nEarlier, I described a FastAPI poller I built that detected any new data from an AWS EC2 where I dumped everything. Then it took the new data, transformed it in into the \"pretty\" staging structures then updated the appropriate (separate) EC2 tables. In this case, I use pydantic models to ensure that the data is structured correctly. Any issues I can see in the logs. \n\nNow that time has passed I think they were asking about testing (in dbt) and monitoring tools. \n\nIs it worth following-up and clarifying?", "author_fullname": "t2_qn4mfqku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maybe bombed this interview question? Asked about data validation and accuracy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsy6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706028848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a phone screen yesterday for a data analytics engineer role. &lt;/p&gt;\n\n&lt;p&gt;I was asked how do I monitor the data pipelines and ensure its accuracy. My response was, I enjoy working with the end user and am really great about getting constant feedback. I said how in my current role, as a Product Engineer, i spend a lot of time with users and going through user data/feedback to determine the success of a feature. &lt;/p&gt;\n\n&lt;p&gt;Now that I&amp;#39;m thinking about it -- they may have been asking me what tools I use.&lt;/p&gt;\n\n&lt;p&gt;Earlier, I described a FastAPI poller I built that detected any new data from an AWS EC2 where I dumped everything. Then it took the new data, transformed it in into the &amp;quot;pretty&amp;quot; staging structures then updated the appropriate (separate) EC2 tables. In this case, I use pydantic models to ensure that the data is structured correctly. Any issues I can see in the logs. &lt;/p&gt;\n\n&lt;p&gt;Now that time has passed I think they were asking about testing (in dbt) and monitoring tools. &lt;/p&gt;\n\n&lt;p&gt;Is it worth following-up and clarifying?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "19dsy6j", "is_robot_indexable": true, "report_reasons": null, "author": "No_Egg1537", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsy6j/maybe_bombed_this_interview_question_asked_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsy6j/maybe_bombed_this_interview_question_asked_about/", "subreddit_subscribers": 155169, "created_utc": 1706028848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pulling my hear out with this one: \n\nWe've upgraded our cluster from 10.3 LTS, to 13.3 LTS and all the unit tests are not running . For 11.3 and everything in between it's the same.\n\nWe even have a simple one that just checks if a given table has data. \n\nAs in: \nassert df.count() &gt; 0 \n\nAnd for each and every one of them we get: \n\nOSError: [Errno 95] Operation not supported: 'Workspace/Repos/..../tests/__pycache__'\n\nTest called using\nIf __name__ == '__main__':\n    pytest.main([test_file.py])\n\nMade sure these files are 'files' not 'notebooks', tried to delete them, put them back. Nothing seem to work. \n\nAny advice?", "author_fullname": "t2_tfnrz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pytest on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsj3n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706027797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pulling my hear out with this one: &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve upgraded our cluster from 10.3 LTS, to 13.3 LTS and all the unit tests are not running . For 11.3 and everything in between it&amp;#39;s the same.&lt;/p&gt;\n\n&lt;p&gt;We even have a simple one that just checks if a given table has data. &lt;/p&gt;\n\n&lt;p&gt;As in: \nassert df.count() &amp;gt; 0 &lt;/p&gt;\n\n&lt;p&gt;And for each and every one of them we get: &lt;/p&gt;\n\n&lt;p&gt;OSError: [Errno 95] Operation not supported: &amp;#39;Workspace/Repos/..../tests/&lt;strong&gt;pycache&lt;/strong&gt;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;Test called using\nIf &lt;strong&gt;name&lt;/strong&gt; == &amp;#39;&lt;strong&gt;main&lt;/strong&gt;&amp;#39;:\n    pytest.main([test_file.py])&lt;/p&gt;\n\n&lt;p&gt;Made sure these files are &amp;#39;files&amp;#39; not &amp;#39;notebooks&amp;#39;, tried to delete them, put them back. Nothing seem to work. &lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dsj3n", "is_robot_indexable": true, "report_reasons": null, "author": "albowiem", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsj3n/pytest_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsj3n/pytest_on_databricks/", "subreddit_subscribers": 155169, "created_utc": 1706027797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Excited to share my latest blog post on migrating Hive UDFs to BigQuery SQL UDFs! Whether you're a data engineer or a CTO, this guide is crafted to simplify your migration process. Dive into the step-by-step approach and discover how to leverage BigQuery's SQL for effective data processing. #BigQuery #DataMigration #HiveUDFs  \n[https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs](https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs) ", "author_fullname": "t2_i8mbe4p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to migrate Hive custom functions to BigQuery UDFs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dpdzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706019389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excited to share my latest blog post on migrating Hive UDFs to BigQuery SQL UDFs! Whether you&amp;#39;re a data engineer or a CTO, this guide is crafted to simplify your migration process. Dive into the step-by-step approach and discover how to leverage BigQuery&amp;#39;s SQL for effective data processing. #BigQuery #DataMigration #HiveUDFs&lt;br/&gt;\n&lt;a href=\"https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs\"&gt;https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?auto=webp&amp;s=d95614e57ed2669470322a80b70a54a37b9a411f", "width": 1000, "height": 563}, "resolutions": [{"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b18bd61d245eba9d7460d0c701f80cba132a467", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9f75adac548a55174ee82382734822fbe11a9be", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c610787baf08afe19bf6ace421e078af749ba4ec", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc3425c2df86e372e54f81945c729e712d3d76e7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/hQvH-hVT9oFzNmzU4-qY-XSC-wZJTpd23BXUcJS0Tkk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b25e50b9cbf9c1fcfdfbe8584477b3b55de979ea", "width": 960, "height": 540}], "variants": {}, "id": "qCzPN1KDPKfwBHNbz-J6-U8nt2moKeWLqjzSk7X0QEc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19dpdzg", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Collar9129", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dpdzg/how_to_migrate_hive_custom_functions_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dpdzg/how_to_migrate_hive_custom_functions_to_bigquery/", "subreddit_subscribers": 155169, "created_utc": 1706019389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to backfill via the Airflow UI? I know that it's possible via CLI, but I have a requirement to do it over the UI, did some searching couldn't find much info, wonder if it's possible to do via UI. ", "author_fullname": "t2_15rwxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow backfill via UI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19doe1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706016425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to backfill via the Airflow UI? I know that it&amp;#39;s possible via CLI, but I have a requirement to do it over the UI, did some searching couldn&amp;#39;t find much info, wonder if it&amp;#39;s possible to do via UI. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19doe1t", "is_robot_indexable": true, "report_reasons": null, "author": "cjj1120", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19doe1t/airflow_backfill_via_ui/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19doe1t/airflow_backfill_via_ui/", "subreddit_subscribers": 155169, "created_utc": 1706016425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like any other optimization, parquet row group sizing has trade-offs, when do you choose to go for more row groups or less row groups?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet Row Group Sizing, how do you choose your sizing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dnoxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706014188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like any other optimization, parquet row group sizing has trade-offs, when do you choose to go for more row groups or less row groups?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dnoxa", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dnoxa/parquet_row_group_sizing_how_do_you_choose_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dnoxa/parquet_row_group_sizing_how_do_you_choose_your/", "subreddit_subscribers": 155169, "created_utc": 1706014188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started as a Data Analyst after a career change. As with many others, I was quickly identified as a candidate for a JDE role. Ever since (~2 months) I have been mainly working with spatial data in FME and handling pipelines and dashboards.\n\nMy organization have suggested I take some training on spatial data, but I also want to know where else I can apply myself to progress becoming a Data Engineer. Any tips on specific skills, certifications, or experiences that could help me? I'd say I have moderate ability with SQL and basic Python understanding. Should I be putting all my time in data handling / manipulation in Python? Should I be looking at any other ETL tools (we currently use FME exclusively). Should I be considering database managent?? \n\nAny help and advice is really appreciated!", "author_fullname": "t2_3ewzzqbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New player to the game", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19djcsf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705996666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started as a Data Analyst after a career change. As with many others, I was quickly identified as a candidate for a JDE role. Ever since (~2 months) I have been mainly working with spatial data in FME and handling pipelines and dashboards.&lt;/p&gt;\n\n&lt;p&gt;My organization have suggested I take some training on spatial data, but I also want to know where else I can apply myself to progress becoming a Data Engineer. Any tips on specific skills, certifications, or experiences that could help me? I&amp;#39;d say I have moderate ability with SQL and basic Python understanding. Should I be putting all my time in data handling / manipulation in Python? Should I be looking at any other ETL tools (we currently use FME exclusively). Should I be considering database managent?? &lt;/p&gt;\n\n&lt;p&gt;Any help and advice is really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19djcsf", "is_robot_indexable": true, "report_reasons": null, "author": "BrittleTupperwareBox", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19djcsf/new_player_to_the_game/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19djcsf/new_player_to_the_game/", "subreddit_subscribers": 155169, "created_utc": 1705996666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI've graduated my bachelor and decided to put my time in Data Engineering cause it seems more interesting than other fields. I've been looking at what the fundamentals are and if I understand correctly I should focus on getting to an advanced level on Python and SQL first before moving to some suite of tools to build my first solo-project to demonstrate I can actually be of use to a company.\n\nI come to ask you pros/cons on this two approaches for the next months: \n\n* Get a back-end developer job in my vicinity as quick as possible, even if it is not Data Engineering related, just to get some experience on my resume and not look like a complete beginner to future DE recruiters; i would study on a portion of my free time.\n* Not get a job and just spend my days studying DE and making that project to get good quicker than the first option; I don't know how long that would take and if being unemployed for that amount of time would look very bad on my resume. To give more info, I am not in a financial situation where I need a job to keep floating.", "author_fullname": "t2_d3fnsuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting a job asap or go full learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19du2hj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706031855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve graduated my bachelor and decided to put my time in Data Engineering cause it seems more interesting than other fields. I&amp;#39;ve been looking at what the fundamentals are and if I understand correctly I should focus on getting to an advanced level on Python and SQL first before moving to some suite of tools to build my first solo-project to demonstrate I can actually be of use to a company.&lt;/p&gt;\n\n&lt;p&gt;I come to ask you pros/cons on this two approaches for the next months: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Get a back-end developer job in my vicinity as quick as possible, even if it is not Data Engineering related, just to get some experience on my resume and not look like a complete beginner to future DE recruiters; i would study on a portion of my free time.&lt;/li&gt;\n&lt;li&gt;Not get a job and just spend my days studying DE and making that project to get good quicker than the first option; I don&amp;#39;t know how long that would take and if being unemployed for that amount of time would look very bad on my resume. To give more info, I am not in a financial situation where I need a job to keep floating.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19du2hj", "is_robot_indexable": true, "report_reasons": null, "author": "Di4mond4rr3l", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19du2hj/getting_a_job_asap_or_go_full_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19du2hj/getting_a_job_asap_or_go_full_learning/", "subreddit_subscribers": 155169, "created_utc": 1706031855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a manufactoring project I have two sensors:\n\n1. Sensor 1: temperature data sampled at 10Hz continously.\n2. Sensor 2: 3-axis accelerometer data sampled at 6kHz in a window of 10s every 10m. In other words, every 10m I have a windows of 10s containing 10\\*6k=60000 records. Every record has a timestamp, a value for axis x, y, and z. 60000x4 table.\n\nOn sensors 2 data:\n\nThe ideas is to perform, at some stage, a \"data engineering\" phase where the \"raw data\" from sensors 2 mentionted before are processed in order to output some informative and less-dimensional data. For instance, letting the inputs be:\n\n* Window 1 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).\n* Window 2 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).\n* ...\n* Window M: ...\n\nThe output would be:\n\n* MxN table/matrix (windows\\_id, timestamp\\_start\\_window, feature1, feature2, ..., feature N-2).\n\nWhere N is the number of synthetic features created (e.g. mean x, median y, max z, min z, etc..) plus a timestamp (for instance the start of the window) and the windows ID and M is the number of windows.\n\nIf I want to save these two data raw sources (inputs) into a file system or database, and also the synthetic data (outputs), how would you save them in order to be flexible and efficient with later data analysis? The analysis will be based on time-series algorithm in order to dedect patterns and anomaly detections.\n\nNote, the two sensors are an example of different sources with different requirements but the use case is not \"that simple\". I would like to discuss the design of modeling and storing/extraction of these time-series with easiness, scaling, and efficiency in mind.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model and save these two data source.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dt9fn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706029646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a manufactoring project I have two sensors:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Sensor 1: temperature data sampled at 10Hz continously.&lt;/li&gt;\n&lt;li&gt;Sensor 2: 3-axis accelerometer data sampled at 6kHz in a window of 10s every 10m. In other words, every 10m I have a windows of 10s containing 10*6k=60000 records. Every record has a timestamp, a value for axis x, y, and z. 60000x4 table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;On sensors 2 data:&lt;/p&gt;\n\n&lt;p&gt;The ideas is to perform, at some stage, a &amp;quot;data engineering&amp;quot; phase where the &amp;quot;raw data&amp;quot; from sensors 2 mentionted before are processed in order to output some informative and less-dimensional data. For instance, letting the inputs be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Window 1 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).&lt;/li&gt;\n&lt;li&gt;Window 2 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;li&gt;Window M: ...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The output would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MxN table/matrix (windows_id, timestamp_start_window, feature1, feature2, ..., feature N-2).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Where N is the number of synthetic features created (e.g. mean x, median y, max z, min z, etc..) plus a timestamp (for instance the start of the window) and the windows ID and M is the number of windows.&lt;/p&gt;\n\n&lt;p&gt;If I want to save these two data raw sources (inputs) into a file system or database, and also the synthetic data (outputs), how would you save them in order to be flexible and efficient with later data analysis? The analysis will be based on time-series algorithm in order to dedect patterns and anomaly detections.&lt;/p&gt;\n\n&lt;p&gt;Note, the two sensors are an example of different sources with different requirements but the use case is not &amp;quot;that simple&amp;quot;. I would like to discuss the design of modeling and storing/extraction of these time-series with easiness, scaling, and efficiency in mind.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dt9fn", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dt9fn/how_to_model_and_save_these_two_data_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dt9fn/how_to_model_and_save_these_two_data_source/", "subreddit_subscribers": 155169, "created_utc": 1706029646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to move and schedule the data copy operation from the company Sharepoint to the Azure Blob Storage. I am new to the Azure and found two candidate solutions.\n\n* Use Azure Data Factory predefined jobs.\n* Use Azure Functions with Python scripts.\n\nFrom what I gathered Azure Functions would be cheaper, this is not super hard to code so I wonder if there is any benefit to the ADF. Do you have any thoughts/recommendations?", "author_fullname": "t2_8wtp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy Data from Sharepoint to the Azure Blob storage.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsv7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706028645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to move and schedule the data copy operation from the company Sharepoint to the Azure Blob Storage. I am new to the Azure and found two candidate solutions.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Azure Data Factory predefined jobs.&lt;/li&gt;\n&lt;li&gt;Use Azure Functions with Python scripts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;From what I gathered Azure Functions would be cheaper, this is not super hard to code so I wonder if there is any benefit to the ADF. Do you have any thoughts/recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dsv7p", "is_robot_indexable": true, "report_reasons": null, "author": "bartosaq", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsv7p/copy_data_from_sharepoint_to_the_azure_blob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsv7p/copy_data_from_sharepoint_to_the_azure_blob/", "subreddit_subscribers": 155169, "created_utc": 1706028645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys I've recently graduated from college  and have completed chemical engineering and I want to transition to  data domain . I  need your help guys to get the answer to few of my questions \n\n1) What are some specialized areas within data engineering, and how can one decide on a niche to focus on.\n\n2)Are there specific skills or certifications that stand out in the job market?\n\n3)What key elements should be highlighted on a resume to catch the attention of recruiters?", "author_fullname": "t2_ciac8nle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Navigating the Path to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsg0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706027585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys I&amp;#39;ve recently graduated from college  and have completed chemical engineering and I want to transition to  data domain . I  need your help guys to get the answer to few of my questions &lt;/p&gt;\n\n&lt;p&gt;1) What are some specialized areas within data engineering, and how can one decide on a niche to focus on.&lt;/p&gt;\n\n&lt;p&gt;2)Are there specific skills or certifications that stand out in the job market?&lt;/p&gt;\n\n&lt;p&gt;3)What key elements should be highlighted on a resume to catch the attention of recruiters?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dsg0t", "is_robot_indexable": true, "report_reasons": null, "author": "No-Salt5525", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsg0t/navigating_the_path_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsg0t/navigating_the_path_to_data_engineering/", "subreddit_subscribers": 155169, "created_utc": 1706027585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone using Cronitor to monitor their Airflow instance? Would be curious to hear what the value add is to have another tool in the loop.", "author_fullname": "t2_ahqse5d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cronitor &amp; Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dq2rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706021355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone using Cronitor to monitor their Airflow instance? Would be curious to hear what the value add is to have another tool in the loop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dq2rf", "is_robot_indexable": true, "report_reasons": null, "author": "Express-Comb8675", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dq2rf/cronitor_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dq2rf/cronitor_airflow/", "subreddit_subscribers": 155169, "created_utc": 1706021355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello !\n\nI am managing a team of 5 data engineers for the analytics of a retail compagnie and we use dbt and Redshift. The team is 5 years old and we have an accumulation of old databases that were not removed + manual changed that were made through the console and are not under control.  \nI have been searching but didn't find good tools to monitor it (would love to get something that points the differences between dbt and the real state of snowflake).  \nAny recommendations?", "author_fullname": "t2_vj9hoxbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage drift between dbt and Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dojoh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706016908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello !&lt;/p&gt;\n\n&lt;p&gt;I am managing a team of 5 data engineers for the analytics of a retail compagnie and we use dbt and Redshift. The team is 5 years old and we have an accumulation of old databases that were not removed + manual changed that were made through the console and are not under control.&lt;br/&gt;\nI have been searching but didn&amp;#39;t find good tools to monitor it (would love to get something that points the differences between dbt and the real state of snowflake).&lt;br/&gt;\nAny recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dojoh", "is_robot_indexable": true, "report_reasons": null, "author": "New_Detective_1363", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dojoh/how_to_manage_drift_between_dbt_and_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dojoh/how_to_manage_drift_between_dbt_and_redshift/", "subreddit_subscribers": 155169, "created_utc": 1706016908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Approach vs Technology Confusion: Where do Data Products Fit In?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_19dniju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/2iy6O91vClPxLDw_T7LEiMEAyh9accFLwTNCOzO6CR0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706013580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/the-approach-vs-technology-confusion", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?auto=webp&amp;s=0585b15384e7d440e0f3ed2c8e179d4df169f4ee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76b08c4340fd3a338259276522bd3ad0bdcee80b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63e73851597ec711ad2afac61a0b50283819b7cb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=901f376a03f580e47403aae7da1ba4ff760eb7d2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6530906d14bf4317d729f530476e916cdeb5b9ee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d16ae6f68be203f247899fbd67766afc52e8786", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/5AL28hz2n754HdSAWQ-VnyymtXMiAN81JBzeVd3x_Oc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c7108b1090b3d117202b28c4401ce0b2f0a81d4", "width": 1080, "height": 540}], "variants": {}, "id": "WZQNPHnIwS1k8Ldp-BOh-gOuTsAl2SniG9SuBLuR0Ig"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19dniju", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dniju/the_approach_vs_technology_confusion_where_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/the-approach-vs-technology-confusion", "subreddit_subscribers": 155169, "created_utc": 1706013580.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}