{"kind": "Listing", "data": {"after": "t3_19c8o7y", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7w02e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Largest storage upgrade (so far) - 11x 18TB WD HC550's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": false, "name": "t3_19ckhn3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 90, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/W9M2qG0qfZ8twcmmFHnM7PGTLeaSAMgeNpj1J_V8XjU.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705889214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/89mub2aifwdc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?auto=webp&amp;s=44f4ee1a36b5c203effa1bca162883b8d4132a34", "width": 1918, "height": 1368}, "resolutions": [{"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=beb5b847982a71fc6854db954935a4006c9649f9", "width": 108, "height": 77}, {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5dc3a3f06cbdee77bae261cb2fca561f4ca2c482", "width": 216, "height": 154}, {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27c704f4b73b0d889744d84e98ec361c2cd9d3af", "width": 320, "height": 228}, {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cffd76df5552bdad0c10beeac5bc8e1cb324043", "width": 640, "height": 456}, {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=893383a7edfed67e149b8579441f535ead887ae9", "width": 960, "height": 684}, {"url": "https://preview.redd.it/89mub2aifwdc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6013e9a66fe719063a65c28348f70f6de1edbe97", "width": 1080, "height": 770}], "variants": {}, "id": "oWEhJ-0huYGifcQcEpJmjx7W4Mv1xvkWGkeaY-es-Oo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "280TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ckhn3", "is_robot_indexable": true, "report_reasons": null, "author": "harritaco", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19ckhn3/largest_storage_upgrade_so_far_11x_18tb_wd_hc550s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/89mub2aifwdc1.jpeg", "subreddit_subscribers": 727026, "created_utc": 1705889214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_poc45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody know of a resource explaining the differences between these checksum algorithms at a fairly basic level (pros, cons, etc)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_19c1i83", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pgaxQYNaszskmUFYuj8pN7AKq_qQSMuaVFDeUQKq4Ws.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705836721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rvosn91b4sdc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rvosn91b4sdc1.png?auto=webp&amp;s=210dd504440e5659069f03f6ac07b97d7a9e880a", "width": 978, "height": 657}, "resolutions": [{"url": "https://preview.redd.it/rvosn91b4sdc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93631ac6ed38c35554d2812b1ad097e19f3f697a", "width": 108, "height": 72}, {"url": "https://preview.redd.it/rvosn91b4sdc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=da02ebe634c43acf9800f9bbaeadc7d22501ce0a", "width": 216, "height": 145}, {"url": "https://preview.redd.it/rvosn91b4sdc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb03d6e74e89a408d7dbf4a2b6140877ea9f9f22", "width": 320, "height": 214}, {"url": "https://preview.redd.it/rvosn91b4sdc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e5aee88d10b3efc41876da9f802ae6ebaca63c6c", "width": 640, "height": 429}, {"url": "https://preview.redd.it/rvosn91b4sdc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=06e3d410e75b4ccc402b66068b75503f7f444580", "width": 960, "height": 644}], "variants": {}, "id": "aNxWOGfGK9il-LQFREbw-rVJ_dUzpvkQiVEhK9Jd5ho"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c1i83", "is_robot_indexable": true, "report_reasons": null, "author": "danielrosehill", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19c1i83/anybody_know_of_a_resource_explaining_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rvosn91b4sdc1.png", "subreddit_subscribers": 727026, "created_utc": 1705836721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Do you physically destroy them at home and toss the pieces into trash? Go to a place that have drive shredders (can't find many around my location)?\n\nThank you for reading and hopefully answering soon. :)", "author_fullname": "t2_4a27h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do with your personal home dead/broken storage drives (e.g., old HDDs) that you want to get rid of?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c6x2a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705853900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you physically destroy them at home and toss the pieces into trash? Go to a place that have drive shredders (can&amp;#39;t find many around my location)?&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading and hopefully answering soon. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Where's the big floppy disk(ette) flair? :P", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c6x2a", "is_robot_indexable": true, "report_reasons": null, "author": "antdude", "discussion_type": null, "num_comments": 52, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19c6x2a/what_do_you_do_with_your_personal_home_deadbroken/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c6x2a/what_do_you_do_with_your_personal_home_deadbroken/", "subreddit_subscribers": 727026, "created_utc": 1705853900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did a thing! I wrote my own PowerShell hashing script called... POWERHASH! (cue the dramatic music - PowerShell + Hash = POWERHASH). It does more than just hash your files though.\n\nWindows does a crap job at offering built in tools to validate your data, so you have to resort to third party tools. Unfortunately there are limited third party tools out there, and many older ones that are no longer maintained or supported or cost money. So I decided to write my own. I am not a developer or programmer, so pardon the mess, but as far as I can tell the script is fully functional and efficient. I'm open to feedback from testing, however.\n\nThe biggest thing I wanted from a hashing program is to be able to update hashes of changed or added files to a folder without having to rehash the entire folder. I did not find this feature in the handful of programs I evaluated. So I implemented that feature. This way you can hash some files, then run the UPDATE function to update hashes and remove hashes of files removed from the folder. But it can do more than that as well. See below for features. \n\nThe project was first started by using stock PowerShell 5.1 that comes with Windows, because I like to use stock apps whenever I can. But I quickly realized that PS 5.1 is antiquated and the latest PowerShell Core 7 (currently version 7.4.1) is so much better, offers a ton of cmdlets to prevent having to load or program your own functions, and is very efficient with file comparisons. It's free and easy to install too. \n\n__*____*__ **FEATURES** __*____*__\n\nSo what does PowerHash do?\n\n* Generate SHA256 (or MD5) hashes of files in a folder recursively (makes use of `Get-FileHash` cmdlet)\n* Omit folders and files from checksum operation by using exclusion keywords for folders and files separately\n* Update existing hash log file with only files that have changed and/or been deleted from a folder so a full hash of all files does not need to be completed\n* Scrub a folder against an existing POWERHASH log file to check for discrepancies\n* Compare two log files for discrepancies (so you can hash two locations then scrub them based on the log files)\n* Find duplicate files based on matching hashes\n* Run from interactive menu or use command line flags (so you can schedule updates and scrubs with Task Scheduler)\n\n__*____*__ **REQUIREMENTS** __*____*__\n\nPowerShell 7 (Core) is required to run this script, but it's free and easy to install. Instructions here:\n\nhttps://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4\n\nOr just type in cmd prompt or powershell prompt: `winget install --id Microsoft.Powershell --source winget`\n\nThis will install the latest PowerShell version (currently 7.4.1 as of this writing, which is what this program was validated with).\n\n__*____*__ **DOWNLOAD** __*____*__\n\nWhile I recommend you read my diatribe on how to use the program, you can download it from github:\n\nhttps://github.com/HTWingNut/PowerHash\n\nIt's currently considered BETA. You can nab the `powerhash.ps1` over on the right under \"releases\".\n\nI have a video on how to use the program as well here: https://youtu.be/U9usaQQJxLs\n\n__*____*__ **INSTRUCTIONS** __*____*__\n\nThe program runs in both an interactive menu mode through cmd prompt or powershell prompt. It's best to `CD` to the folder where your `powerhash.ps1` folder resides and run from there.\n\nTo run: `pwsh .\\powerhash.ps1`\n\nBy default it will generate SHA256 hashes of files, but you can use the `-MD5` flag to have it generate and manage MD5 hash checksums instead (i.e. `pwsh .\\powerhash.ps1 -md5`)\n\nThis will take you to the interactive menu. I will first take you through the interactive menu, then show you how to run the same commands using command line mode.\n\nIt may be a good idea to have a Windows Explorer window open so you can click/drag folder and file names to the cmd windows to save keystrokes. Just remember to click the command windows after dragging file names/paths over.\n\nYou can get command line help at any time by typing: `pwsh .\\powerhash.ps1 -help` or even a full readme with `-readme` flag. Or use `-readme` flag with specific function you want. Those available readme's are shown in the `-help`.\n\nThe script will generate a MAIN HASH LOG that will contain SHA256 hash values, file name, file size, last modified date of every file hashed. This is the MAIN HASH LOG that will be set to READ ONLY using the naming convention:\n\n    SHA256_[FOLDER NAME]_[DATE TIME STAMP].log\n    Example: User enters folder D:\\DATA, the log file will be 'SHA256_DATA_20240117_190211.log'\n\nLet's call this '[hashlog].log'\n\nYou can rename [hashlog].log whatever you want to after it's made, but the file will be set to READ ONLY. This file should not be hand edited or it could make it not work properly with the script.\n\nAll operations that touch the MAIN HASH LOG will be summarized in the '[hashlog]_history.log' file. This file you can make notes in if desired as its reference only.\n\nThere will be other supplementary log files generated depending on what function you use. The log files that can be generated are:\n\n       HISTORY: \u2018[hashlog]_history.log\u2019     Maintains summary of all actions performed on [hashlog].log\n       UPDATED: \u2018[hashlog]_updated.log\u2019     Details of file changes when using \u2018UPDATE\u2019 fn\n       COMPARE: \u2018[hashlog]_compare.log\u2019     Details of results when comparing two logs \u2018COMPARE\u2019 fn\n         SCRUB: \u2018[hashlog]_scrub.log\u2019       Details of results after running \u2018SCRUB\u2019 fn\n    DUPLICATES: \u2018[hashlog]_duplicates.log\u2019  Details of duplicate files list after running \u2018DUPLICATES\u2019 fn\n    EXCLUSIONS: \u2018[hashlog]_excluded.log\u2019    Lists files Excluded from exclusion keywords set by user\n      PREVIOUS: \u2018[hashlog]_previous.log\u2019    Copy of [hashlog].log as \u201cundo\u201d after running \u2018UPDATE\u2019 fn\n\n\n__*____*__ **MAIN MENU** __*____*__\n\nThe Main Menu will present you with multiple options:\n\n    =POWERHASH SHA256= by HTWingNut v2024.01.17\n    Type q from any menu to return here\n    \n    Choose from the following:\n     [G]enerate New SHA256 Hash Log\n     [U]pdate Hash Log\n     [C]ompare Hash Logs\n     [S]crub Folder with Log\n     [D]uplicate File Check\n     [Q]uit\n    \n    CHOICE:\n\n__*____*__ **GENERATE (OR CREATE)** __*____*__\n\n**BEFORE YOU CAN DO ANYTHING** with any of the other functions you must create a MAIN HASH LOG ([G] in menu or `-create` flag in command line). You point the program to the folder that you want to generate hashes from.\n\nYou can then assign any folder and file exclusion parameters you want. These are in the fomr of keywords or keyphrases and must be entered in a specific format. Folder and File exclusions are independent of each other. Folder means path without the file. File means just the file name and extension.\n\nThe format to use - single quotes around each keyphrase with multiple keyphrases separted by a comma:\n\n`Enter FOLDER Exclusion: '\\LinuxISO','Temp','\\Recycle Bin'`\n\nNo wildcards allowed and IS NOT case sensitive. This example would exclude any file path starting with `LinuxISO`, any folder path containing the word `temp`, and any file path starting with `Recycle Bin`\n\n`Enter FILE Exclusion: '.pdf','Thumbs.db'\n\nThis would exclude any files with a `.pdf` extension (or if file contains `.pdf` in the filename itself) and anything named `Thumbs.db`.\n\n**Command Line**\n\nTo create this log from command line use:\n\n    pwsh .\\powerhash.ps1 -create -path \"D:\\Data\"\n\nif you want to add exclusions:\n\n    pwsh .\\powerhash.ps1 -create -path \"D:\\Data\" -excludefolders \"'\\LinuxISO','Temp','\\Recycle Bin'\" -excludefiles \"'.pdf','Thumbs.db'\"\n\nNote the exclusion lists have to be wrapped completely in double quotes.\n\nThe results will be stored in '[hashlog].log'.\n\nIf exclusions were added it will generate a '[hashlog]_exclusion.log' file which will list the file names of all files excluded from hashing along with the rule that excluded it.\n\nAnd of course everything will be summarized in the '[hashlog]_history.log' file.\n\n\n__*____*__ **UPDATE HASH LOG** __*____*__\n\nThis function will allow you to update an existing hash log with only files that have been changed, added/new to the folder, or deleted from the folder. It will also provide potentially renamed or moved files. You will have to provide the log file to update and the file path where to look for updated files.\n\nFolder and File exclusions can also be added, modified, or removed at this time. It will remove any existing log entries that match the exclusion profiles provided by the user.\n\n**Command Line**\n\nThe command line version of this would look something like this:\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\DATA\"\n\nAnd like the create/generate hash prompt you can exclude files as well.\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\Data\" -excludefolders \"'\\LinuxISO','Temp','\\Recycle Bin'\" -excludefiles \"'.pdf','Thumbs.db'\"\n\nIf you want to clear exclusions from command line you can use the '-excludeclear' command:\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\Data\" -excludeclear\n\nThe main log file '[hashlog].log' will be updated with any changes detected in the folder.\n\nDetails of the update will be stored in '[hashlog]_updated.log'.\n\nAnd of course everything will be summarized in the '[hashlog]_history.log' file.\n\n\n__*____*__ **SCRUB FOLDER** __*____*__\n\nThe SCRUB folder function will take the contents of [hashlog].log file and compare it with the contents of user specified folder location to check if any hashes have changed. It will also provide a lot of additional data including:\n\n* Total Log Entries\n* Total Files in Folder\n* Files Failed to Match Hash\n* Files Missing from Folder\n* New Files in Folder\n* Files with Mismatch Size/Date\n* Files Busy not Hashed\n* Files Excluded by the User\n* Potential Renamed Files\n\nThe user just needs to provide the log file containing hashes and the path to folder that contains the files they want to scrub.\n\nThere will be an option to \"HASH NEW FILES\". Basically this will hash any new files found in the folder that don't exist in the log file. This will be used to determine if a file is potentially moved or renamed. It will not update the main log file. The scrub operation is for reporting only. It will not update the main log file. You will have to run the 'UPDATE' option separately to do that.\n\nScrubbing time will depend on number of files and size of files in the folder. Because all files will need to be hashed to compare against the log file.\n\n**Command Line**\n\n    pwsh .\\powerhash.ps1 -scrub -log \"SHA256_DATA_20240117_190211.log\" -path \"D:\\DATA\"\n\nif you wish to hash new found files to check for moved or renamed files just add `-hashnew` to the end\n\n    pwsh .\\powerhash.ps1 -scrub -log \"SHA256_DATA_20240117_190211.log\" -path \"D:\\DATA\" -hashnew\n\nDetailed results will be stored in '[hashlog]_scrub.log' and summary in '[hashlog]_history.log' file.\n\n\n__*____*__ **DUPLICATE CHECK** __*____*__\n\nThe DUPLICATES function will check for file duplicates based on file hashes in a previously generated main log file. This means files are exactly identical.\n\nUser just needs to provide a main log file to check for duplicates.\n\n**Command Line**\n\nCommand line for checking for duplicates is pretty straight forward.\n\n    pwsh .\\powerhash.ps1 -duplicates -log \"SHA256_DATA_20240117_190211.log\"\n\nDetailed results will be stored in '[hashlog]_duplicates.log' and summary in '[hashlog]_history.log'\n\n\n\n__*____*__ **FINAL THOUGHTS** __*____*__\n\nI appreciate any feedback on this, especially if you notice any bugs. I don't plan on adding additional features in the near future. Although I am considering adding a \"recovery\" mode so that if the program or computer crashes while generating hashes during the \"generate hashes\" stage it will pick up where it left off.\n\nAlso, keep in mind that this is simply a tool to provide information about your files. It is not meant to make decisions for you.\n\nThanks for taking the time to read this and hope some of you find the program useful!", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PowerHash - A PowerShell SHA256/MD5 Hashing Script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cj4bb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1705885960.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705885201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a thing! I wrote my own PowerShell hashing script called... POWERHASH! (cue the dramatic music - PowerShell + Hash = POWERHASH). It does more than just hash your files though.&lt;/p&gt;\n\n&lt;p&gt;Windows does a crap job at offering built in tools to validate your data, so you have to resort to third party tools. Unfortunately there are limited third party tools out there, and many older ones that are no longer maintained or supported or cost money. So I decided to write my own. I am not a developer or programmer, so pardon the mess, but as far as I can tell the script is fully functional and efficient. I&amp;#39;m open to feedback from testing, however.&lt;/p&gt;\n\n&lt;p&gt;The biggest thing I wanted from a hashing program is to be able to update hashes of changed or added files to a folder without having to rehash the entire folder. I did not find this feature in the handful of programs I evaluated. So I implemented that feature. This way you can hash some files, then run the UPDATE function to update hashes and remove hashes of files removed from the folder. But it can do more than that as well. See below for features. &lt;/p&gt;\n\n&lt;p&gt;The project was first started by using stock PowerShell 5.1 that comes with Windows, because I like to use stock apps whenever I can. But I quickly realized that PS 5.1 is antiquated and the latest PowerShell Core 7 (currently version 7.4.1) is so much better, offers a ton of cmdlets to prevent having to load or program your own functions, and is very efficient with file comparisons. It&amp;#39;s free and easy to install too. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;FEATURES&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So what does PowerHash do?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Generate SHA256 (or MD5) hashes of files in a folder recursively (makes use of &lt;code&gt;Get-FileHash&lt;/code&gt; cmdlet)&lt;/li&gt;\n&lt;li&gt;Omit folders and files from checksum operation by using exclusion keywords for folders and files separately&lt;/li&gt;\n&lt;li&gt;Update existing hash log file with only files that have changed and/or been deleted from a folder so a full hash of all files does not need to be completed&lt;/li&gt;\n&lt;li&gt;Scrub a folder against an existing POWERHASH log file to check for discrepancies&lt;/li&gt;\n&lt;li&gt;Compare two log files for discrepancies (so you can hash two locations then scrub them based on the log files)&lt;/li&gt;\n&lt;li&gt;Find duplicate files based on matching hashes&lt;/li&gt;\n&lt;li&gt;Run from interactive menu or use command line flags (so you can schedule updates and scrubs with Task Scheduler)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;REQUIREMENTS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;PowerShell 7 (Core) is required to run this script, but it&amp;#39;s free and easy to install. Instructions here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4\"&gt;https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or just type in cmd prompt or powershell prompt: &lt;code&gt;winget install --id Microsoft.Powershell --source winget&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This will install the latest PowerShell version (currently 7.4.1 as of this writing, which is what this program was validated with).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;DOWNLOAD&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While I recommend you read my diatribe on how to use the program, you can download it from github:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/HTWingNut/PowerHash\"&gt;https://github.com/HTWingNut/PowerHash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s currently considered BETA. You can nab the &lt;code&gt;powerhash.ps1&lt;/code&gt; over on the right under &amp;quot;releases&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I have a video on how to use the program as well here: &lt;a href=\"https://youtu.be/U9usaQQJxLs\"&gt;https://youtu.be/U9usaQQJxLs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;INSTRUCTIONS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The program runs in both an interactive menu mode through cmd prompt or powershell prompt. It&amp;#39;s best to &lt;code&gt;CD&lt;/code&gt; to the folder where your &lt;code&gt;powerhash.ps1&lt;/code&gt; folder resides and run from there.&lt;/p&gt;\n\n&lt;p&gt;To run: &lt;code&gt;pwsh .\\powerhash.ps1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;By default it will generate SHA256 hashes of files, but you can use the &lt;code&gt;-MD5&lt;/code&gt; flag to have it generate and manage MD5 hash checksums instead (i.e. &lt;code&gt;pwsh .\\powerhash.ps1 -md5&lt;/code&gt;)&lt;/p&gt;\n\n&lt;p&gt;This will take you to the interactive menu. I will first take you through the interactive menu, then show you how to run the same commands using command line mode.&lt;/p&gt;\n\n&lt;p&gt;It may be a good idea to have a Windows Explorer window open so you can click/drag folder and file names to the cmd windows to save keystrokes. Just remember to click the command windows after dragging file names/paths over.&lt;/p&gt;\n\n&lt;p&gt;You can get command line help at any time by typing: &lt;code&gt;pwsh .\\powerhash.ps1 -help&lt;/code&gt; or even a full readme with &lt;code&gt;-readme&lt;/code&gt; flag. Or use &lt;code&gt;-readme&lt;/code&gt; flag with specific function you want. Those available readme&amp;#39;s are shown in the &lt;code&gt;-help&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;The script will generate a MAIN HASH LOG that will contain SHA256 hash values, file name, file size, last modified date of every file hashed. This is the MAIN HASH LOG that will be set to READ ONLY using the naming convention:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SHA256_[FOLDER NAME]_[DATE TIME STAMP].log\nExample: User enters folder D:\\DATA, the log file will be &amp;#39;SHA256_DATA_20240117_190211.log&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Let&amp;#39;s call this &amp;#39;[hashlog].log&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;You can rename [hashlog].log whatever you want to after it&amp;#39;s made, but the file will be set to READ ONLY. This file should not be hand edited or it could make it not work properly with the script.&lt;/p&gt;\n\n&lt;p&gt;All operations that touch the MAIN HASH LOG will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file. This file you can make notes in if desired as its reference only.&lt;/p&gt;\n\n&lt;p&gt;There will be other supplementary log files generated depending on what function you use. The log files that can be generated are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;   HISTORY: \u2018[hashlog]_history.log\u2019     Maintains summary of all actions performed on [hashlog].log\n   UPDATED: \u2018[hashlog]_updated.log\u2019     Details of file changes when using \u2018UPDATE\u2019 fn\n   COMPARE: \u2018[hashlog]_compare.log\u2019     Details of results when comparing two logs \u2018COMPARE\u2019 fn\n     SCRUB: \u2018[hashlog]_scrub.log\u2019       Details of results after running \u2018SCRUB\u2019 fn\nDUPLICATES: \u2018[hashlog]_duplicates.log\u2019  Details of duplicate files list after running \u2018DUPLICATES\u2019 fn\nEXCLUSIONS: \u2018[hashlog]_excluded.log\u2019    Lists files Excluded from exclusion keywords set by user\n  PREVIOUS: \u2018[hashlog]_previous.log\u2019    Copy of [hashlog].log as \u201cundo\u201d after running \u2018UPDATE\u2019 fn\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;MAIN MENU&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Main Menu will present you with multiple options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;=POWERHASH SHA256= by HTWingNut v2024.01.17\nType q from any menu to return here\n\nChoose from the following:\n [G]enerate New SHA256 Hash Log\n [U]pdate Hash Log\n [C]ompare Hash Logs\n [S]crub Folder with Log\n [D]uplicate File Check\n [Q]uit\n\nCHOICE:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;GENERATE (OR CREATE)&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BEFORE YOU CAN DO ANYTHING&lt;/strong&gt; with any of the other functions you must create a MAIN HASH LOG ([G] in menu or &lt;code&gt;-create&lt;/code&gt; flag in command line). You point the program to the folder that you want to generate hashes from.&lt;/p&gt;\n\n&lt;p&gt;You can then assign any folder and file exclusion parameters you want. These are in the fomr of keywords or keyphrases and must be entered in a specific format. Folder and File exclusions are independent of each other. Folder means path without the file. File means just the file name and extension.&lt;/p&gt;\n\n&lt;p&gt;The format to use - single quotes around each keyphrase with multiple keyphrases separted by a comma:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Enter FOLDER Exclusion: &amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;No wildcards allowed and IS NOT case sensitive. This example would exclude any file path starting with &lt;code&gt;LinuxISO&lt;/code&gt;, any folder path containing the word &lt;code&gt;temp&lt;/code&gt;, and any file path starting with &lt;code&gt;Recycle Bin&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;`Enter FILE Exclusion: &amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;This would exclude any files with a &lt;code&gt;.pdf&lt;/code&gt; extension (or if file contains &lt;code&gt;.pdf&lt;/code&gt; in the filename itself) and anything named &lt;code&gt;Thumbs.db&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To create this log from command line use:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -create -path &amp;quot;D:\\Data&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if you want to add exclusions:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -create -path &amp;quot;D:\\Data&amp;quot; -excludefolders &amp;quot;&amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&amp;quot; -excludefiles &amp;quot;&amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note the exclusion lists have to be wrapped completely in double quotes.&lt;/p&gt;\n\n&lt;p&gt;The results will be stored in &amp;#39;[hashlog].log&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;If exclusions were added it will generate a &amp;#39;[hashlog]_exclusion.log&amp;#39; file which will list the file names of all files excluded from hashing along with the rule that excluded it.&lt;/p&gt;\n\n&lt;p&gt;And of course everything will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;UPDATE HASH LOG&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This function will allow you to update an existing hash log with only files that have been changed, added/new to the folder, or deleted from the folder. It will also provide potentially renamed or moved files. You will have to provide the log file to update and the file path where to look for updated files.&lt;/p&gt;\n\n&lt;p&gt;Folder and File exclusions can also be added, modified, or removed at this time. It will remove any existing log entries that match the exclusion profiles provided by the user.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The command line version of this would look something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\DATA&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And like the create/generate hash prompt you can exclude files as well.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\Data&amp;quot; -excludefolders &amp;quot;&amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&amp;quot; -excludefiles &amp;quot;&amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If you want to clear exclusions from command line you can use the &amp;#39;-excludeclear&amp;#39; command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\Data&amp;quot; -excludeclear\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The main log file &amp;#39;[hashlog].log&amp;#39; will be updated with any changes detected in the folder.&lt;/p&gt;\n\n&lt;p&gt;Details of the update will be stored in &amp;#39;[hashlog]_updated.log&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;And of course everything will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;SCRUB FOLDER&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The SCRUB folder function will take the contents of [hashlog].log file and compare it with the contents of user specified folder location to check if any hashes have changed. It will also provide a lot of additional data including:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Total Log Entries&lt;/li&gt;\n&lt;li&gt;Total Files in Folder&lt;/li&gt;\n&lt;li&gt;Files Failed to Match Hash&lt;/li&gt;\n&lt;li&gt;Files Missing from Folder&lt;/li&gt;\n&lt;li&gt;New Files in Folder&lt;/li&gt;\n&lt;li&gt;Files with Mismatch Size/Date&lt;/li&gt;\n&lt;li&gt;Files Busy not Hashed&lt;/li&gt;\n&lt;li&gt;Files Excluded by the User&lt;/li&gt;\n&lt;li&gt;Potential Renamed Files&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The user just needs to provide the log file containing hashes and the path to folder that contains the files they want to scrub.&lt;/p&gt;\n\n&lt;p&gt;There will be an option to &amp;quot;HASH NEW FILES&amp;quot;. Basically this will hash any new files found in the folder that don&amp;#39;t exist in the log file. This will be used to determine if a file is potentially moved or renamed. It will not update the main log file. The scrub operation is for reporting only. It will not update the main log file. You will have to run the &amp;#39;UPDATE&amp;#39; option separately to do that.&lt;/p&gt;\n\n&lt;p&gt;Scrubbing time will depend on number of files and size of files in the folder. Because all files will need to be hashed to compare against the log file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -scrub -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot; -path &amp;quot;D:\\DATA&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if you wish to hash new found files to check for moved or renamed files just add &lt;code&gt;-hashnew&lt;/code&gt; to the end&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -scrub -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot; -path &amp;quot;D:\\DATA&amp;quot; -hashnew\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Detailed results will be stored in &amp;#39;[hashlog]_scrub.log&amp;#39; and summary in &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;DUPLICATE CHECK&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The DUPLICATES function will check for file duplicates based on file hashes in a previously generated main log file. This means files are exactly identical.&lt;/p&gt;\n\n&lt;p&gt;User just needs to provide a main log file to check for duplicates.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Command line for checking for duplicates is pretty straight forward.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -duplicates -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Detailed results will be stored in &amp;#39;[hashlog]_duplicates.log&amp;#39; and summary in &amp;#39;[hashlog]_history.log&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;FINAL THOUGHTS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I appreciate any feedback on this, especially if you notice any bugs. I don&amp;#39;t plan on adding additional features in the near future. Although I am considering adding a &amp;quot;recovery&amp;quot; mode so that if the program or computer crashes while generating hashes during the &amp;quot;generate hashes&amp;quot; stage it will pick up where it left off.&lt;/p&gt;\n\n&lt;p&gt;Also, keep in mind that this is simply a tool to provide information about your files. It is not meant to make decisions for you.&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to read this and hope some of you find the program useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2RXw4nGo-oOJQju-H6pVtp_XQu2BzgVzsOsTdbPh-rs.jpg?auto=webp&amp;s=8d6ab09b988f4c7876a59de5a6766331a6dbaeb0", "width": 128, "height": 128}, "resolutions": [{"url": "https://external-preview.redd.it/2RXw4nGo-oOJQju-H6pVtp_XQu2BzgVzsOsTdbPh-rs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1297ffd0375f9ab7beff64e767500d2bfc24493a", "width": 108, "height": 108}], "variants": {}, "id": "RdYD16_h_i3Y4Wv2YmKJ3o6JYUyjmEY5DEi0Ulw_Hac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cj4bb", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19cj4bb/powerhash_a_powershell_sha256md5_hashing_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cj4bb/powerhash_a_powershell_sha256md5_hashing_script/", "subreddit_subscribers": 727026, "created_utc": 1705885201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My wife passed away years ago. She left behind a sizable collection of pre-digital prints and negatives, which is being scanned, and a big collection in Google Photos.\n\nI like that Google Photos shows me my old pictures daily, and it would be nice to see her pics too. It would also be nice for other family members to be able to browse the pictures.\n\nShould I move her Google Photo pictures to my account? Leave them in hers? Should I put them in my Google Drive account? Somewhere else?\n\nBonus question: the scanned pictures are going to have the scanning date as the EXIF date. What's the done thing here? Should I update the EXIF date with the actual picture date? Put the actual date somewhere else? How can I make sure that Google Photos, for example, will show the pictures in correct chronological order?", "author_fullname": "t2_mwhfv38o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with my deceased wife's photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19bvqos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705813870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My wife passed away years ago. She left behind a sizable collection of pre-digital prints and negatives, which is being scanned, and a big collection in Google Photos.&lt;/p&gt;\n\n&lt;p&gt;I like that Google Photos shows me my old pictures daily, and it would be nice to see her pics too. It would also be nice for other family members to be able to browse the pictures.&lt;/p&gt;\n\n&lt;p&gt;Should I move her Google Photo pictures to my account? Leave them in hers? Should I put them in my Google Drive account? Somewhere else?&lt;/p&gt;\n\n&lt;p&gt;Bonus question: the scanned pictures are going to have the scanning date as the EXIF date. What&amp;#39;s the done thing here? Should I update the EXIF date with the actual picture date? Put the actual date somewhere else? How can I make sure that Google Photos, for example, will show the pictures in correct chronological order?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19bvqos", "is_robot_indexable": true, "report_reasons": null, "author": "optimisticlemur", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19bvqos/what_to_do_with_my_deceased_wifes_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19bvqos/what_to_do_with_my_deceased_wifes_photos/", "subreddit_subscribers": 727026, "created_utc": 1705813870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm a photographer for a large construction company. We need a free DAM for the purpose of tagging and key wording photos. I use mac and my co worker uses windows so I need something that works for both. Any suggestions? ", "author_fullname": "t2_6agcep66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best free Digital Asset Manager for tagging photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19chu1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705881532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m a photographer for a large construction company. We need a free DAM for the purpose of tagging and key wording photos. I use mac and my co worker uses windows so I need something that works for both. Any suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19chu1l", "is_robot_indexable": true, "report_reasons": null, "author": "lilfishgod", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19chu1l/best_free_digital_asset_manager_for_tagging_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19chu1l/best_free_digital_asset_manager_for_tagging_photos/", "subreddit_subscribers": 727026, "created_utc": 1705881532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello.\nI've been trying to manage my library for years without success.\nI have photos, backups of these photos, copies with different names, some of them with fucked date metadata...\n\nI wonder if there is a software that finds duplicates, and keep ALL metadata merged, picking the oldest date available for example.\n\nCan you guide me to start organizing this mess?\nI would be happy renaming all files with it's date\n\nIm talking about 500.000 files aprox.\n\nI'm using Windows 10\n\nThank you!", "author_fullname": "t2_b969b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need photo/video management help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c29y2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705840492.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705839687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.\nI&amp;#39;ve been trying to manage my library for years without success.\nI have photos, backups of these photos, copies with different names, some of them with fucked date metadata...&lt;/p&gt;\n\n&lt;p&gt;I wonder if there is a software that finds duplicates, and keep ALL metadata merged, picking the oldest date available for example.&lt;/p&gt;\n\n&lt;p&gt;Can you guide me to start organizing this mess?\nI would be happy renaming all files with it&amp;#39;s date&lt;/p&gt;\n\n&lt;p&gt;Im talking about 500.000 files aprox.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Windows 10&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c29y2", "is_robot_indexable": true, "report_reasons": null, "author": "smaiderman", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19c29y2/i_need_photovideo_management_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c29y2/i_need_photovideo_management_help/", "subreddit_subscribers": 727026, "created_utc": 1705839687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "without really needing to, I was looking at my future expansion possibilities, in my current chassis I have run out of physical space, aside from a 5.25 bay in the front which I have yet to populate. \nI was thinking that in the future I could use a 5.25 to 4x2.5 cage, and was looking at SSD's that could be put in there.\n\nI keep seeing that the cheaper SSD's often have poor write endurence. Such as 700TB's for the crucial BX500 compared to 2800TB for an Ironwolf 125\nBut do I even need to care about this? I do write to my pool every day, but they're not large writes. When I do do large writes, they are usually only a few gigs. \n\nBut I practically never delete anything. I assume this applies to most of us hoarders. So I can't imagine I could ever come close to 700TB's on a 2TB drive.\nI suppose games and whatnot that I keep on there do get updates, but still, 700TB's?\n\n(This is for a TrueNAS box, so ZFS.)\nAm I missing something?", "author_fullname": "t2_thagl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we even need to care about write endurance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cfedw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705875144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;without really needing to, I was looking at my future expansion possibilities, in my current chassis I have run out of physical space, aside from a 5.25 bay in the front which I have yet to populate. \nI was thinking that in the future I could use a 5.25 to 4x2.5 cage, and was looking at SSD&amp;#39;s that could be put in there.&lt;/p&gt;\n\n&lt;p&gt;I keep seeing that the cheaper SSD&amp;#39;s often have poor write endurence. Such as 700TB&amp;#39;s for the crucial BX500 compared to 2800TB for an Ironwolf 125\nBut do I even need to care about this? I do write to my pool every day, but they&amp;#39;re not large writes. When I do do large writes, they are usually only a few gigs. &lt;/p&gt;\n\n&lt;p&gt;But I practically never delete anything. I assume this applies to most of us hoarders. So I can&amp;#39;t imagine I could ever come close to 700TB&amp;#39;s on a 2TB drive.\nI suppose games and whatnot that I keep on there do get updates, but still, 700TB&amp;#39;s?&lt;/p&gt;\n\n&lt;p&gt;(This is for a TrueNAS box, so ZFS.)\nAm I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19cfedw", "is_robot_indexable": true, "report_reasons": null, "author": "IvanezerScrooge", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cfedw/do_we_even_need_to_care_about_write_endurance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cfedw/do_we_even_need_to_care_about_write_endurance/", "subreddit_subscribers": 727026, "created_utc": 1705875144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am fairly new to this and just recently in the past 6 months or so (June 2023) started to organize my life and data to have better control and archiving. I've got 4 kids, a few businesses, wife, and as everyone can imagine - countless data, docs and files that I should save and stop paying cloud providers for.\n\nI'm now about 8 TB deep into my system. I see how this will get out of control quickly, and my issue that I didn't consider is retrieval.\n\n**How does everyone organize or index their data?** \n\nI don't mean like file name conventions, tag or folder systems. I am curious if you build a database to store key information such as:\n\n* Backup Archives\n   * File Name | Archiving Date | File Summary | Etc. \n* Directory MOC\n   * Directory Name | File Types/Rules | Dates and Source Locations | etc... \n\n&amp;#x200B;\n\nI want to avoid the pitfall of when I need to find that one set of PDFs or photos from that one period I think was maybe between August 2016 and March 2017... for example. \n\n&amp;#x200B;\n\nJust curious of the process some bigger hoarders have taken to help me avoid a nightmare. ", "author_fullname": "t2_aikbtreq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Documenting and Indexing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c70r5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705854163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am fairly new to this and just recently in the past 6 months or so (June 2023) started to organize my life and data to have better control and archiving. I&amp;#39;ve got 4 kids, a few businesses, wife, and as everyone can imagine - countless data, docs and files that I should save and stop paying cloud providers for.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now about 8 TB deep into my system. I see how this will get out of control quickly, and my issue that I didn&amp;#39;t consider is retrieval.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does everyone organize or index their data?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mean like file name conventions, tag or folder systems. I am curious if you build a database to store key information such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Backup Archives\n\n&lt;ul&gt;\n&lt;li&gt;File Name | Archiving Date | File Summary | Etc. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Directory MOC\n\n&lt;ul&gt;\n&lt;li&gt;Directory Name | File Types/Rules | Dates and Source Locations | etc... &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to avoid the pitfall of when I need to find that one set of PDFs or photos from that one period I think was maybe between August 2016 and March 2017... for example. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Just curious of the process some bigger hoarders have taken to help me avoid a nightmare. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c70r5", "is_robot_indexable": true, "report_reasons": null, "author": "HisNameWasShagbark", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19c70r5/documenting_and_indexing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c70r5/documenting_and_indexing/", "subreddit_subscribers": 727026, "created_utc": 1705854163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of buying a portable ssd enclosure but where I live they are pretty expensive so I'm wondering could I buy a 10\u20ac one from aliexpress or are they just complete garbage and I should just pay 12\u20ac more for a better know quality one.", "author_fullname": "t2_g46z0sj7u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on portable ssd enclosures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c07oi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705831859.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705831458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of buying a portable ssd enclosure but where I live they are pretty expensive so I&amp;#39;m wondering could I buy a 10\u20ac one from aliexpress or are they just complete garbage and I should just pay 12\u20ac more for a better know quality one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c07oi", "is_robot_indexable": true, "report_reasons": null, "author": "kalameespeeter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19c07oi/advice_on_portable_ssd_enclosures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c07oi/advice_on_portable_ssd_enclosures/", "subreddit_subscribers": 727026, "created_utc": 1705831458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been with CrashPlan for almost a decade, but my storage needs have far outgrown CrashPlan's ice age-speed. What's your experience of Backblaze vs IDrive, specifically on their backup and restore speed in the US (perfect if you've used both services)?\n\nI currently have 4TB data and growing. As a content creator I steadily output sizable data that needs versioning and backup, and I have a budget of around $10/mo.\n\nI'm specifically comparing the speed of these two services, not other options. But if you feel strongly to suggest others, I won't stop you ;) Also, I'm mainly looking for their \"standard\" services, instead of B2, e2 or other object storage solution. Mainly because of the costs, and I admit, I'm not very familiar how the workflow &amp; technology works.\n\nThanks in advance!", "author_fullname": "t2_fcb1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup &amp; Restore Speed on Backblaze vs IDrive, as in US, 2024?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ch4cn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705886691.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705879582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been with CrashPlan for almost a decade, but my storage needs have far outgrown CrashPlan&amp;#39;s ice age-speed. What&amp;#39;s your experience of Backblaze vs IDrive, specifically on their backup and restore speed in the US (perfect if you&amp;#39;ve used both services)?&lt;/p&gt;\n\n&lt;p&gt;I currently have 4TB data and growing. As a content creator I steadily output sizable data that needs versioning and backup, and I have a budget of around $10/mo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifically comparing the speed of these two services, not other options. But if you feel strongly to suggest others, I won&amp;#39;t stop you ;) Also, I&amp;#39;m mainly looking for their &amp;quot;standard&amp;quot; services, instead of B2, e2 or other object storage solution. Mainly because of the costs, and I admit, I&amp;#39;m not very familiar how the workflow &amp;amp; technology works.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ch4cn", "is_robot_indexable": true, "report_reasons": null, "author": "AlienBoy_tw", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ch4cn/backup_restore_speed_on_backblaze_vs_idrive_as_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ch4cn/backup_restore_speed_on_backblaze_vs_idrive_as_in/", "subreddit_subscribers": 727026, "created_utc": 1705879582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "G\u2019day - title may have been weird if so apologies, and I did a search but couldn\u2019t find what I was after\n\nI have a few hundred hard drives - both in servers and also externals \n\nIs there a program \u2018anywhere\u2019 that would allow me to scan these drives , and list the contents, and alert me when there are dupes ?\n\nKinda like a big database ?\n\nBe even more helpful if once the scanning process was done, that \u2018this program\u2019 would add new downloads to the database etc\n\nI keep finding that I have multiple copies of various things which is not only a waste of bandwidth but more importantly hard drive space , and the system I\u2019m currently using is obviously really bad\n\nSo I figure that there has to be a decent way or organization out there right ??", "author_fullname": "t2_dak0xorw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program to scan drives for content to list dupes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccac2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705867433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;G\u2019day - title may have been weird if so apologies, and I did a search but couldn\u2019t find what I was after&lt;/p&gt;\n\n&lt;p&gt;I have a few hundred hard drives - both in servers and also externals &lt;/p&gt;\n\n&lt;p&gt;Is there a program \u2018anywhere\u2019 that would allow me to scan these drives , and list the contents, and alert me when there are dupes ?&lt;/p&gt;\n\n&lt;p&gt;Kinda like a big database ?&lt;/p&gt;\n\n&lt;p&gt;Be even more helpful if once the scanning process was done, that \u2018this program\u2019 would add new downloads to the database etc&lt;/p&gt;\n\n&lt;p&gt;I keep finding that I have multiple copies of various things which is not only a waste of bandwidth but more importantly hard drive space , and the system I\u2019m currently using is obviously really bad&lt;/p&gt;\n\n&lt;p&gt;So I figure that there has to be a decent way or organization out there right ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ccac2", "is_robot_indexable": true, "report_reasons": null, "author": "ectoplasmic-warrior", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccac2/program_to_scan_drives_for_content_to_list_dupes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccac2/program_to_scan_drives_for_content_to_list_dupes/", "subreddit_subscribers": 727026, "created_utc": 1705867433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\n# \n\nI accidentally deleted my documents on my laptop(windows os) and I need them in two days. Any tools for recovery that are easy to use?", "author_fullname": "t2_k1mgo7uwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data recovery softwares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cbtmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705866277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I accidentally deleted my documents on my laptop(windows os) and I need them in two days. Any tools for recovery that are easy to use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cbtmn", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent_Eye1248", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cbtmn/data_recovery_softwares/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cbtmn/data_recovery_softwares/", "subreddit_subscribers": 727026, "created_utc": 1705866277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hard drives, as I understand it, often exhibit warning symptoms before they fail. And SSDs, maybe not so much. But still, I'm certain there must be some software out that to notify you if your drive health is showing anything concerning.\n\nIs there a good light-weight system tray app that can run in the background and run daily checks or something?\n\nI have a Windows desktop machine in my office with 14 GB of storage, and a Windows desktop acting as a server in the living room with 20 GB storage.", "author_fullname": "t2_4uv8kp1km", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best software for monitoring hard drive health status? (Windows)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19caacs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705862425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hard drives, as I understand it, often exhibit warning symptoms before they fail. And SSDs, maybe not so much. But still, I&amp;#39;m certain there must be some software out that to notify you if your drive health is showing anything concerning.&lt;/p&gt;\n\n&lt;p&gt;Is there a good light-weight system tray app that can run in the background and run daily checks or something?&lt;/p&gt;\n\n&lt;p&gt;I have a Windows desktop machine in my office with 14 GB of storage, and a Windows desktop acting as a server in the living room with 20 GB storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19caacs", "is_robot_indexable": true, "report_reasons": null, "author": "raging_pastafarian", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19caacs/best_software_for_monitoring_hard_drive_health/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19caacs/best_software_for_monitoring_hard_drive_health/", "subreddit_subscribers": 727026, "created_utc": 1705862425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Say I export a channel's content using Discord Chat Exporter, and then the channel I exported from gets deleted\n\nWill the export disappear? If yes, how can I keep the logs after its deletion? Is there a way to make a hard save in my pc?", "author_fullname": "t2_ejv9g8yek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Discord Chat Exporter files stay available after a channel gets deleted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19bxsn2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705821498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I export a channel&amp;#39;s content using Discord Chat Exporter, and then the channel I exported from gets deleted&lt;/p&gt;\n\n&lt;p&gt;Will the export disappear? If yes, how can I keep the logs after its deletion? Is there a way to make a hard save in my pc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19bxsn2", "is_robot_indexable": true, "report_reasons": null, "author": "DeenDre", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19bxsn2/will_discord_chat_exporter_files_stay_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19bxsn2/will_discord_chat_exporter_files_stay_available/", "subreddit_subscribers": 727026, "created_utc": 1705821498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The interface says SATA, anyone know why it wouldn't work?\n\n[https://www.ebay.com/itm/386098483254](https://www.ebay.com/itm/386098483254)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_9sz62", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need cheap 4tb hd, this listing says \"Will not work on home desktops\" ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19cno77", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705899233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The interface says SATA, anyone know why it wouldn&amp;#39;t work?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ebay.com/itm/386098483254\"&gt;https://www.ebay.com/itm/386098483254&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?auto=webp&amp;s=51c941d811c9cea3e3208d37c1f59f061f55d9ce", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99445263380ad2ce7da4a3dce295bac447035930", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=652bee7c1abfe93cf12413d45ccd5b3fdce2448b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8b804606141f67ebc1d3196beef1df8fd10bd06", "width": 320, "height": 320}], "variants": {}, "id": "SKyQ6dJCaX_1qvtWHiT1j0UCV74_JDE2yhUT7ihJXMM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cno77", "is_robot_indexable": true, "report_reasons": null, "author": "mxl555", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cno77/need_cheap_4tb_hd_this_listing_says_will_not_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cno77/need_cheap_4tb_hd_this_listing_says_will_not_work/", "subreddit_subscribers": 727026, "created_utc": 1705899233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about a couple thousand posts/reels saved and would like to download the media but the data export only put my saved in a json/ text file. Instead of logging in with a third party app, or downloading manually is there a good way to get the images/videos/posts exported using python or another way from the urls?", "author_fullname": "t2_9so865d8r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tutorial on downloading SAVED Instagram albums/ collections from URLS on data export to json/txt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cix1p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705884612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about a couple thousand posts/reels saved and would like to download the media but the data export only put my saved in a json/ text file. Instead of logging in with a third party app, or downloading manually is there a good way to get the images/videos/posts exported using python or another way from the urls?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cix1p", "is_robot_indexable": true, "report_reasons": null, "author": "undeadartist1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cix1p/any_tutorial_on_downloading_saved_instagram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cix1p/any_tutorial_on_downloading_saved_instagram/", "subreddit_subscribers": 727026, "created_utc": 1705884612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been thinking about building a NAS and got the following for free:\n\n* no-name case (four 5.25\" drive bays + three 3.5\" internal bays), 80mm exhaust fan\n\n* Motherboard: [Asus A88xm-plus](https://www.asus.com/supportonly/a88xmplus/helpdesk_cpu/): micro-ATX, FM2+ socket w\\ 8 SATA6 ports, 64GB max ram. \n\n* RAM: 8GB DDR3 1333MhZ (2x4GB, Kingston KVR)\n\n* CPU: [AMD A8-5600k](https://www.techpowerup.com/cpu-specs/a8-5600k.c1101) (3.6Ghz 4 core, 4 thread.)\n\n* CPU cooler: stock low-profile with 80mm fan\n\n* Boot drive: [Crucial  BX500 240GB](https://www.amazon.com/Crucial-BX500-240GB-2-5-Inch-Internal/dp/B07G3YNLJB?th=1)\n\n* PSU: generic brand, swapped out with Corsair RM650x\n\nThis feels pretty ancient but the onboard 8 SATA ports made me think it could be a good platform to explore building a first NAS.  I'm planning to put TrueNAS Scale on it.\n\n**Question --&gt;**: Is this a worthwhile project, or should I look to invest in more modern hardware?  \n\nI'm mostly looking to use this as a storage device.  I might try putting a media server on it once things are set up properly, but I have no idea how good or capable the A8 CPU is at transcoding (though I'd only be sending things to a 4k tv).  Also, I have no idea if there are better CPUs I could chuck in this thing.  The 5600k's 100w TDP seems like a lot in this case, but...it was free.  Can anyone recommend an upgrade path here?\n\n\nSpeaking of upgrades, here were some new purchases I was considering:\n\n* More ram: I have 4 DIMMS and a 64GB cap, so I'll probably look at picking up some 8gb 1600mhz sticks.  Not sure if I can find unbuffered ECC 16GB sticks compatible with this motherboard. (I can't seem to find and non-ECC DDR3 16GB sticks) \n\n* Drives, obviously.\n\n* Hot-swap caddies for the 5.25\" drive bays Any recommendations?\n\n* quieter exhaust fans.  Any recommendations? Was looking at Noctua 80mm fans but realized I don't know anything about this.\n\n\nThanks!", "author_fullname": "t2_6ix2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Worth it?] Repurposing an 10yo PC for a NAS: a88xm-plus mobo + A8-5600k AMD CPU.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19chu5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705881542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been thinking about building a NAS and got the following for free:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;no-name case (four 5.25&amp;quot; drive bays + three 3.5&amp;quot; internal bays), 80mm exhaust fan&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Motherboard: &lt;a href=\"https://www.asus.com/supportonly/a88xmplus/helpdesk_cpu/\"&gt;Asus A88xm-plus&lt;/a&gt;: micro-ATX, FM2+ socket w\\ 8 SATA6 ports, 64GB max ram. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;RAM: 8GB DDR3 1333MhZ (2x4GB, Kingston KVR)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;CPU: &lt;a href=\"https://www.techpowerup.com/cpu-specs/a8-5600k.c1101\"&gt;AMD A8-5600k&lt;/a&gt; (3.6Ghz 4 core, 4 thread.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;CPU cooler: stock low-profile with 80mm fan&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Boot drive: &lt;a href=\"https://www.amazon.com/Crucial-BX500-240GB-2-5-Inch-Internal/dp/B07G3YNLJB?th=1\"&gt;Crucial  BX500 240GB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;PSU: generic brand, swapped out with Corsair RM650x&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This feels pretty ancient but the onboard 8 SATA ports made me think it could be a good platform to explore building a first NAS.  I&amp;#39;m planning to put TrueNAS Scale on it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question --&amp;gt;&lt;/strong&gt;: Is this a worthwhile project, or should I look to invest in more modern hardware?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m mostly looking to use this as a storage device.  I might try putting a media server on it once things are set up properly, but I have no idea how good or capable the A8 CPU is at transcoding (though I&amp;#39;d only be sending things to a 4k tv).  Also, I have no idea if there are better CPUs I could chuck in this thing.  The 5600k&amp;#39;s 100w TDP seems like a lot in this case, but...it was free.  Can anyone recommend an upgrade path here?&lt;/p&gt;\n\n&lt;p&gt;Speaking of upgrades, here were some new purchases I was considering:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;More ram: I have 4 DIMMS and a 64GB cap, so I&amp;#39;ll probably look at picking up some 8gb 1600mhz sticks.  Not sure if I can find unbuffered ECC 16GB sticks compatible with this motherboard. (I can&amp;#39;t seem to find and non-ECC DDR3 16GB sticks) &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Drives, obviously.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Hot-swap caddies for the 5.25&amp;quot; drive bays Any recommendations?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;quieter exhaust fans.  Any recommendations? Was looking at Noctua 80mm fans but realized I don&amp;#39;t know anything about this.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19chu5z", "is_robot_indexable": true, "report_reasons": null, "author": "fumblesmcdrum", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19chu5z/worth_it_repurposing_an_10yo_pc_for_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19chu5z/worth_it_repurposing_an_10yo_pc_for_a_nas/", "subreddit_subscribers": 727026, "created_utc": 1705881542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all. \n\nWant to start archiving family photos and videos for long term storage using 100gb M-Discs (1.5tb in total so far), and seen the Asus BW-16D1HT internal drive. \n\nAnyone use it?  Any issues?? \n\nAlso seen on eBay 20 disc spindles of 100gb from Japan for \u00a395, yet seen others that are the same price for only 5 in cases. Any reason why there is such a massive difference?? \n\n\nThanks.", "author_fullname": "t2_77qyrr6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone used an Asus BW-16D1HT for M-Disc Archiving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cgltj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705878247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. &lt;/p&gt;\n\n&lt;p&gt;Want to start archiving family photos and videos for long term storage using 100gb M-Discs (1.5tb in total so far), and seen the Asus BW-16D1HT internal drive. &lt;/p&gt;\n\n&lt;p&gt;Anyone use it?  Any issues?? &lt;/p&gt;\n\n&lt;p&gt;Also seen on eBay 20 disc spindles of 100gb from Japan for \u00a395, yet seen others that are the same price for only 5 in cases. Any reason why there is such a massive difference?? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cgltj", "is_robot_indexable": true, "report_reasons": null, "author": "EquivalentTip4103", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cgltj/anyone_used_an_asus_bw16d1ht_for_mdisc_archiving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cgltj/anyone_used_an_asus_bw16d1ht_for_mdisc_archiving/", "subreddit_subscribers": 727026, "created_utc": 1705878247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a few older HDD ranging in sizes from 500GB to 3TB. I'm thinking about using as cold storage backups for my main NAS. Looking for what people doing for the same and what their strategies are. \n\nMy NAS is a 36TB (23tb used) raidz2 pool with automated snapshots and unencrypted volumes for each category of data. (Linux ISO's, homedirs, music, roms, personal photos, etc).  I automated backup of the important/irreplacable stuff to a 3tb disk elsewhere on a rpi4. (warm backup). I do have a 2 bay USB/Sata dock for quickly popping disks in and out, and another single disk sata/usb adapter.\n\nMy initial idea is use pairs of matching sized disks. First run badblocks and validate each disks is healthy. On each of them make a small vfat volume with a README and other details about what the disks contains and how to access it should someone in the distant future who isn't me try to read the disks on a windows box. On the remaining space make a mirrored ZFS pool with both disks. For volumes that can easily fit entirely on one set of disks (like my /home is only 600GB) use zfs send/recv and add encryption on the receiving end. On sets too large to fit, instead create new encrypted volumes and rsync the data over, splitting the volumes up alphabetically and fitting as much as I can. The README will contain information on where to find the decryption keys. Afterwards catalog and name each disk with post-it notes (coldbackup-1a and 1b).  Keep a running copy of what is on each pair of disks and when it was last scrubbed in a central document.  Then store the drives in static bags with silica packets in my basement (cool, steady temperature, and relatively dry). And make calendar events so every 2 to 3 years I go through and scrub all the disks and replace as needed. On super irreplacable stuff (family photos and the like) maybe I even use a 3-way mirror using another sata-usb cable i have.\n\nThoughts? What would you do differently?  Is encrypting backups a bad idea even if I make sure that Key's are recoverable (i may even print out QR codes on paper in addition to KeePass). Is there a benefit to settings copies=2 in a 2way mirror for this use case?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_53838", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do for cold backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cdt01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705871221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a few older HDD ranging in sizes from 500GB to 3TB. I&amp;#39;m thinking about using as cold storage backups for my main NAS. Looking for what people doing for the same and what their strategies are. &lt;/p&gt;\n\n&lt;p&gt;My NAS is a 36TB (23tb used) raidz2 pool with automated snapshots and unencrypted volumes for each category of data. (Linux ISO&amp;#39;s, homedirs, music, roms, personal photos, etc).  I automated backup of the important/irreplacable stuff to a 3tb disk elsewhere on a rpi4. (warm backup). I do have a 2 bay USB/Sata dock for quickly popping disks in and out, and another single disk sata/usb adapter.&lt;/p&gt;\n\n&lt;p&gt;My initial idea is use pairs of matching sized disks. First run badblocks and validate each disks is healthy. On each of them make a small vfat volume with a README and other details about what the disks contains and how to access it should someone in the distant future who isn&amp;#39;t me try to read the disks on a windows box. On the remaining space make a mirrored ZFS pool with both disks. For volumes that can easily fit entirely on one set of disks (like my /home is only 600GB) use zfs send/recv and add encryption on the receiving end. On sets too large to fit, instead create new encrypted volumes and rsync the data over, splitting the volumes up alphabetically and fitting as much as I can. The README will contain information on where to find the decryption keys. Afterwards catalog and name each disk with post-it notes (coldbackup-1a and 1b).  Keep a running copy of what is on each pair of disks and when it was last scrubbed in a central document.  Then store the drives in static bags with silica packets in my basement (cool, steady temperature, and relatively dry). And make calendar events so every 2 to 3 years I go through and scrub all the disks and replace as needed. On super irreplacable stuff (family photos and the like) maybe I even use a 3-way mirror using another sata-usb cable i have.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? What would you do differently?  Is encrypting backups a bad idea even if I make sure that Key&amp;#39;s are recoverable (i may even print out QR codes on paper in addition to KeePass). Is there a benefit to settings copies=2 in a 2way mirror for this use case?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cdt01", "is_robot_indexable": true, "report_reasons": null, "author": "skreak", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cdt01/what_do_you_do_for_cold_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cdt01/what_do_you_do_for_cold_backups/", "subreddit_subscribers": 727026, "created_utc": 1705871221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to move my backups to different location, since in the case of fire or theft I could lose all data.\n\nI was thinking to copy everything on external 2.5\" HDDs and store them far away from my apartment.\n\nI have:\n\n* Maxtor M3 Portable External Hard Drive (**4TB**)\n* Seagate Basic Portable Drive (**5TB**)\n* and plan to buy WD Elements Portable Black 2.5 (**5TB**)  \n\n\nAll drives are new and not used much. I have a house in a countryside and that is where I plan to keep my offsite backup. No one lives there and temperature can go from -20\u00b0C to 40\u00b0C.   \nI was wondering do you have any suggestions on how to store these drives? I guess I need to keep them in shade and keep them dry. Do I need to buy some protective materials to isolate them better? \n\nI would like to keep them in a good health for 10 years and to replace them after. Not sure if these ones are able to live that long (even when not used), since they are the cheapest I could find.  \n", "author_fullname": "t2_26c1b19v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store external HDD as an offsite backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccz2t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705869140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to move my backups to different location, since in the case of fire or theft I could lose all data.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to copy everything on external 2.5&amp;quot; HDDs and store them far away from my apartment.&lt;/p&gt;\n\n&lt;p&gt;I have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Maxtor M3 Portable External Hard Drive (&lt;strong&gt;4TB&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Seagate Basic Portable Drive (&lt;strong&gt;5TB&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;and plan to buy WD Elements Portable Black 2.5 (&lt;strong&gt;5TB&lt;/strong&gt;)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All drives are new and not used much. I have a house in a countryside and that is where I plan to keep my offsite backup. No one lives there and temperature can go from -20\u00b0C to 40\u00b0C.&lt;br/&gt;\nI was wondering do you have any suggestions on how to store these drives? I guess I need to keep them in shade and keep them dry. Do I need to buy some protective materials to isolate them better? &lt;/p&gt;\n\n&lt;p&gt;I would like to keep them in a good health for 10 years and to replace them after. Not sure if these ones are able to live that long (even when not used), since they are the cheapest I could find.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ccz2t", "is_robot_indexable": true, "report_reasons": null, "author": "zp-87", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccz2t/how_to_store_external_hdd_as_an_offsite_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccz2t/how_to_store_external_hdd_as_an_offsite_backup/", "subreddit_subscribers": 727026, "created_utc": 1705869140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know that there is individual that share storage between themselves and I know that there is FileCoin\n\nBut no fren and FIL is actually just a currency to give to classic providers\n\nI search for a way to share my storage P2P on a network with random people and get the same transfer/storage amount in return, no money\n\nOnly problem being availability as I don't have a server, so I can't be online all the time. And my data is metered so I want to share really few data because basically it'll cost me 4 times the data amount I want to save (like backup 1 GB of data means one upload of me, one of the other one, one download per person too, multiply it by wanted backups)", "author_fullname": "t2_5slsu5xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mutual storage sharing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccgq2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Serious answer only", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705867870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that there is individual that share storage between themselves and I know that there is FileCoin&lt;/p&gt;\n\n&lt;p&gt;But no fren and FIL is actually just a currency to give to classic providers&lt;/p&gt;\n\n&lt;p&gt;I search for a way to share my storage P2P on a network with random people and get the same transfer/storage amount in return, no money&lt;/p&gt;\n\n&lt;p&gt;Only problem being availability as I don&amp;#39;t have a server, so I can&amp;#39;t be online all the time. And my data is metered so I want to share really few data because basically it&amp;#39;ll cost me 4 times the data amount I want to save (like backup 1 GB of data means one upload of me, one of the other one, one download per person too, multiply it by wanted backups)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19ccgq2", "is_robot_indexable": true, "report_reasons": null, "author": "xqoe", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccgq2/mutual_storage_sharing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccgq2/mutual_storage_sharing/", "subreddit_subscribers": 727026, "created_utc": 1705867870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to find a good capture card for digitizing tapes.\nHaulage pvr2 is my go to for capture but I understand stand it doesn't work with VirtualDub and it has built in compression.\n\nTried a cheap 5 dollar capture card I bought online and it works with VDub but so far it seems to have Jpg artifacts in the recording, haven't determined if it's the tape or the card but I see compression in VDubs control panel and assume its hardware since I can't turn it off.\n\nI also have a Diamond VC500 that I haven't tried.\nPc is running latest Windows.", "author_fullname": "t2_fk8e08x1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need personal recommendations on capture cards.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cb7gl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705864724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to find a good capture card for digitizing tapes.\nHaulage pvr2 is my go to for capture but I understand stand it doesn&amp;#39;t work with VirtualDub and it has built in compression.&lt;/p&gt;\n\n&lt;p&gt;Tried a cheap 5 dollar capture card I bought online and it works with VDub but so far it seems to have Jpg artifacts in the recording, haven&amp;#39;t determined if it&amp;#39;s the tape or the card but I see compression in VDubs control panel and assume its hardware since I can&amp;#39;t turn it off.&lt;/p&gt;\n\n&lt;p&gt;I also have a Diamond VC500 that I haven&amp;#39;t tried.\nPc is running latest Windows.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cb7gl", "is_robot_indexable": true, "report_reasons": null, "author": "KWalthersArt", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cb7gl/need_personal_recommendations_on_capture_cards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cb7gl/need_personal_recommendations_on_capture_cards/", "subreddit_subscribers": 727026, "created_utc": 1705864724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nI understand why WD Blue and Barracuda HDD shouldn't be used for NAS or 24/7 surveillance, I find why it's not recommended to use WD Red or an Ironwolf hd for basic computing? \\*   \n\n\nspecifically a 4tb SG Ironwolfis $79.99 and SG Barracuda $78.99.  same rpm and same cache size.   \n\n\nI was just looking for a 4tb non-smr drive, Ironwolf is the cheapest option locally.    \n\n\n\\*I'm using it to back pictures, movies, and on occasion installing steam games that are too big to go onto my OS's SSD (just a that need150gb+)", "author_fullname": "t2_4d5wxcdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downsides using IronWolf for basic daily computing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c9h1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705860403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I understand why WD Blue and Barracuda HDD shouldn&amp;#39;t be used for NAS or 24/7 surveillance, I find why it&amp;#39;s not recommended to use WD Red or an Ironwolf hd for basic computing? *   &lt;/p&gt;\n\n&lt;p&gt;specifically a 4tb SG Ironwolfis $79.99 and SG Barracuda $78.99.  same rpm and same cache size.   &lt;/p&gt;\n\n&lt;p&gt;I was just looking for a 4tb non-smr drive, Ironwolf is the cheapest option locally.    &lt;/p&gt;\n\n&lt;p&gt;*I&amp;#39;m using it to back pictures, movies, and on occasion installing steam games that are too big to go onto my OS&amp;#39;s SSD (just a that need150gb+)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c9h1s", "is_robot_indexable": true, "report_reasons": null, "author": "WalterMelon81", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19c9h1s/downsides_using_ironwolf_for_basic_daily_computing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c9h1s/downsides_using_ironwolf_for_basic_daily_computing/", "subreddit_subscribers": 727026, "created_utc": 1705860403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all, sorry if this is kind of a newbie-ish question.\n\nNow, been looking on Google and so far I've only seen a bunch of options:\n\n- Google cloud backups (up to 15GB free, with 100 or so being paid/subscription based)\n- Cloud backup via jailbreak apps, which are not feasible for me\n- Or a PC, HDD, SSD, or any physical storage media.\n\nMy idea for this is to use a PC I have, but I wanted to know if there's a better way, since this is pretty much my first time doing this sort of stuff, and Google mostly led me to somewhat older posts and websites, so things may have changed.\n\nI also wanted to know: if I go by making a backup on PC/external storage, would there be a loss of quality on media files, and if so, is there a way to prevent or minimize it? Some of my files are FLAC audios, 4K images and the like, and I really want to keep them at the best quality possible.", "author_fullname": "t2_37k7e5yu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to/Best way to backup an Android device?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19c8o7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705858409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, sorry if this is kind of a newbie-ish question.&lt;/p&gt;\n\n&lt;p&gt;Now, been looking on Google and so far I&amp;#39;ve only seen a bunch of options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Google cloud backups (up to 15GB free, with 100 or so being paid/subscription based)&lt;/li&gt;\n&lt;li&gt;Cloud backup via jailbreak apps, which are not feasible for me&lt;/li&gt;\n&lt;li&gt;Or a PC, HDD, SSD, or any physical storage media.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My idea for this is to use a PC I have, but I wanted to know if there&amp;#39;s a better way, since this is pretty much my first time doing this sort of stuff, and Google mostly led me to somewhat older posts and websites, so things may have changed.&lt;/p&gt;\n\n&lt;p&gt;I also wanted to know: if I go by making a backup on PC/external storage, would there be a loss of quality on media files, and if so, is there a way to prevent or minimize it? Some of my files are FLAC audios, 4K images and the like, and I really want to keep them at the best quality possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19c8o7y", "is_robot_indexable": true, "report_reasons": null, "author": "Hemlock_Deci", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19c8o7y/how_tobest_way_to_backup_an_android_device/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19c8o7y/how_tobest_way_to_backup_an_android_device/", "subreddit_subscribers": 727026, "created_utc": 1705858409.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}