{"kind": "Listing", "data": {"after": "t3_19ccgq2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is just something that's been on my mind but before I start, I wanted to say that obviously I realize that the vast majority of the users here don't fall into this, but I think it could be an interesting discussion.\n\nWhat one may call 'Tech Literacy' is on the decline as companies push more and more tech that is 'User Friendly' which also means 'Hostile to tinkering, just push the magic button that does the thing and stop asking questions about how it works under the hood'.  This has also leaned itself to piracy where users looking to pirate things increasingly rely on 'A magic pirate streaming website, full of god awful ads that may or my not attempt to mind crypto through your browser, where you just push the button'.  I once did a panel at an anime convention, pretending on fandom level efforts to preserve out of print media, and at the Q&amp;A at the end, a Zoomer raised their hand and asked me 'You kept using this word 'Torrent', what does that mean?'  It had never occurred to me as I had planned this panel that should have explained what a 'torrent' was.  I would have never had to do that at an anime convention 15 years ago.\n\nAnyway, getting to the point, I've noticed the occasional series of 'weird posts' where someone respectably wants to preserve something or manipulate their data, has the right idea, but lacks some core base knowledge that they go about it in an odd way.  When it comes to 'hoarding' media, I think we all agree there are best routes to go, and that is usually 'The highest quality version that is closest to the original source as possible'.  Normally disc remuxes for video, streaming rips where disc releases don't exist, FLAC copies of music from CD, direct rips from where the music is available from if it's not on disc, and so on.  For space reasons, it's also pretty common to prefer first generation transcodes from those, particularly of BD/DVD content.\n\nBut that's where we get into the weird stuff.  A few years ago some YouTube channel that just uploaded video game music is getting a take down (Shocking!) and someone wants to 'hoard' the YouTube channel.  ...That channel was nothing but rips uploaded to YouTube, if you want to preserve the music, you want to find the CDs or FLACs or direct game file rips that were uploaded to YouTube, you don't want to rip the YouTube itself.\n\nJust the other day, in a quickly deleted thread, someone was asking how to rip files from a shitty pirate cartoon streaming website, because that was the only source they could conceive of to have copies of the cartoons that it hosted.  Of course, everything uploaded to that site would have come from a higher quality source that the operates just torrented, pulled from usenet, or otherwise collected.\n\nI even saw a post where someone could not 'understand' handbrake, so instead they would upload videos to YouTube, then use a ripping tool to download the output from YouTube, effectively hacking YouTube into being a cloud video encoder...  That is both dumbfounding but also an awe inspiring solution where someone 'Thought a hammer was the only tool in the world, so they found some wild ways to utilize a hammer'.\n\nNow, obviously 'Any copy is better than no copy', but the cracks are starting to show that less and less people, even when wanting to 'have a copy', have no idea how to go about correctly acquiring a copy in the first place and are just contributing to generational loss of those copies.", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The decline of 'Tech Literacy' having an influence on Data Hoarding.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cx718", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 217, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 217, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705934694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just something that&amp;#39;s been on my mind but before I start, I wanted to say that obviously I realize that the vast majority of the users here don&amp;#39;t fall into this, but I think it could be an interesting discussion.&lt;/p&gt;\n\n&lt;p&gt;What one may call &amp;#39;Tech Literacy&amp;#39; is on the decline as companies push more and more tech that is &amp;#39;User Friendly&amp;#39; which also means &amp;#39;Hostile to tinkering, just push the magic button that does the thing and stop asking questions about how it works under the hood&amp;#39;.  This has also leaned itself to piracy where users looking to pirate things increasingly rely on &amp;#39;A magic pirate streaming website, full of god awful ads that may or my not attempt to mind crypto through your browser, where you just push the button&amp;#39;.  I once did a panel at an anime convention, pretending on fandom level efforts to preserve out of print media, and at the Q&amp;amp;A at the end, a Zoomer raised their hand and asked me &amp;#39;You kept using this word &amp;#39;Torrent&amp;#39;, what does that mean?&amp;#39;  It had never occurred to me as I had planned this panel that should have explained what a &amp;#39;torrent&amp;#39; was.  I would have never had to do that at an anime convention 15 years ago.&lt;/p&gt;\n\n&lt;p&gt;Anyway, getting to the point, I&amp;#39;ve noticed the occasional series of &amp;#39;weird posts&amp;#39; where someone respectably wants to preserve something or manipulate their data, has the right idea, but lacks some core base knowledge that they go about it in an odd way.  When it comes to &amp;#39;hoarding&amp;#39; media, I think we all agree there are best routes to go, and that is usually &amp;#39;The highest quality version that is closest to the original source as possible&amp;#39;.  Normally disc remuxes for video, streaming rips where disc releases don&amp;#39;t exist, FLAC copies of music from CD, direct rips from where the music is available from if it&amp;#39;s not on disc, and so on.  For space reasons, it&amp;#39;s also pretty common to prefer first generation transcodes from those, particularly of BD/DVD content.&lt;/p&gt;\n\n&lt;p&gt;But that&amp;#39;s where we get into the weird stuff.  A few years ago some YouTube channel that just uploaded video game music is getting a take down (Shocking!) and someone wants to &amp;#39;hoard&amp;#39; the YouTube channel.  ...That channel was nothing but rips uploaded to YouTube, if you want to preserve the music, you want to find the CDs or FLACs or direct game file rips that were uploaded to YouTube, you don&amp;#39;t want to rip the YouTube itself.&lt;/p&gt;\n\n&lt;p&gt;Just the other day, in a quickly deleted thread, someone was asking how to rip files from a shitty pirate cartoon streaming website, because that was the only source they could conceive of to have copies of the cartoons that it hosted.  Of course, everything uploaded to that site would have come from a higher quality source that the operates just torrented, pulled from usenet, or otherwise collected.&lt;/p&gt;\n\n&lt;p&gt;I even saw a post where someone could not &amp;#39;understand&amp;#39; handbrake, so instead they would upload videos to YouTube, then use a ripping tool to download the output from YouTube, effectively hacking YouTube into being a cloud video encoder...  That is both dumbfounding but also an awe inspiring solution where someone &amp;#39;Thought a hammer was the only tool in the world, so they found some wild ways to utilize a hammer&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Now, obviously &amp;#39;Any copy is better than no copy&amp;#39;, but the cracks are starting to show that less and less people, even when wanting to &amp;#39;have a copy&amp;#39;, have no idea how to go about correctly acquiring a copy in the first place and are just contributing to generational loss of those copies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19cx718", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 118, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cx718/the_decline_of_tech_literacy_having_an_influence/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cx718/the_decline_of_tech_literacy_having_an_influence/", "subreddit_subscribers": 727172, "created_utc": 1705934694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did a thing! I wrote my own PowerShell hashing script called... POWERHASH! (cue the dramatic music - PowerShell + Hash = POWERHASH). It does more than just hash your files though.\n\nWindows does a crap job at offering built in tools to validate your data, so you have to resort to third party tools. Unfortunately there are limited third party tools out there, and many older ones that are no longer maintained or supported or cost money. So I decided to write my own. I am not a developer or programmer, so pardon the mess, but as far as I can tell the script is fully functional and efficient. I'm open to feedback from testing, however.\n\nThe biggest thing I wanted from a hashing program is to be able to update hashes of changed or added files to a folder without having to rehash the entire folder. I did not find this feature in the handful of programs I evaluated. So I implemented that feature. This way you can hash some files, then run the UPDATE function to update hashes and remove hashes of files removed from the folder. But it can do more than that as well. See below for features. \n\nThe project was first started by using stock PowerShell 5.1 that comes with Windows, because I like to use stock apps whenever I can. But I quickly realized that PS 5.1 is antiquated and the latest PowerShell Core 7 (currently version 7.4.1) is so much better, offers a ton of cmdlets to prevent having to load or program your own functions, and is very efficient with file comparisons. It's free and easy to install too. \n\n__*____*__ **FEATURES** __*____*__\n\nSo what does PowerHash do?\n\n* Generate SHA256 (or MD5) hashes of files in a folder recursively (makes use of `Get-FileHash` cmdlet)\n* Omit folders and files from checksum operation by using exclusion keywords for folders and files separately\n* Update existing hash log file with only files that have changed and/or been deleted from a folder so a full hash of all files does not need to be completed\n* Scrub a folder against an existing POWERHASH log file to check for discrepancies\n* Compare two log files for discrepancies (so you can hash two locations then scrub them based on the log files)\n* Find duplicate files based on matching hashes\n* Run from interactive menu or use command line flags (so you can schedule updates and scrubs with Task Scheduler)\n\n__*____*__ **REQUIREMENTS** __*____*__\n\nPowerShell 7 (Core) is required to run this script, but it's free and easy to install. Instructions here:\n\nhttps://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4\n\nOr just type in cmd prompt or powershell prompt: `winget install --id Microsoft.Powershell --source winget`\n\nThis will install the latest PowerShell version (currently 7.4.1 as of this writing, which is what this program was validated with).\n\n__*____*__ **DOWNLOAD** __*____*__\n\nWhile I recommend you read my diatribe on how to use the program, you can download it from github:\n\nhttps://github.com/HTWingNut/PowerHash\n\nIt's currently considered BETA. You can nab the `powerhash.ps1` over on the right under \"releases\".\n\nI have a video on how to use the program as well here: https://youtu.be/U9usaQQJxLs\n\n__*____*__ **INSTRUCTIONS** __*____*__\n\nThe program runs in both an interactive menu mode through cmd prompt or powershell prompt. It's best to `CD` to the folder where your `powerhash.ps1` folder resides and run from there.\n\nTo run: `pwsh .\\powerhash.ps1`\n\nBy default it will generate SHA256 hashes of files, but you can use the `-MD5` flag to have it generate and manage MD5 hash checksums instead (i.e. `pwsh .\\powerhash.ps1 -md5`)\n\nThis will take you to the interactive menu. I will first take you through the interactive menu, then show you how to run the same commands using command line mode.\n\nIt may be a good idea to have a Windows Explorer window open so you can click/drag folder and file names to the cmd windows to save keystrokes. Just remember to click the command windows after dragging file names/paths over.\n\nYou can get command line help at any time by typing: `pwsh .\\powerhash.ps1 -help` or even a full readme with `-readme` flag. Or use `-readme` flag with specific function you want. Those available readme's are shown in the `-help`.\n\nThe script will generate a MAIN HASH LOG that will contain SHA256 hash values, file name, file size, last modified date of every file hashed. This is the MAIN HASH LOG that will be set to READ ONLY using the naming convention:\n\n    SHA256_[FOLDER NAME]_[DATE TIME STAMP].log\n    Example: User enters folder D:\\DATA, the log file will be 'SHA256_DATA_20240117_190211.log'\n\nLet's call this '[hashlog].log'\n\nYou can rename [hashlog].log whatever you want to after it's made, but the file will be set to READ ONLY. This file should not be hand edited or it could make it not work properly with the script.\n\nAll operations that touch the MAIN HASH LOG will be summarized in the '[hashlog]_history.log' file. This file you can make notes in if desired as its reference only.\n\nThere will be other supplementary log files generated depending on what function you use. The log files that can be generated are:\n\n       HISTORY: \u2018[hashlog]_history.log\u2019     Maintains summary of all actions performed on [hashlog].log\n       UPDATED: \u2018[hashlog]_updated.log\u2019     Details of file changes when using \u2018UPDATE\u2019 fn\n       COMPARE: \u2018[hashlog]_compare.log\u2019     Details of results when comparing two logs \u2018COMPARE\u2019 fn\n         SCRUB: \u2018[hashlog]_scrub.log\u2019       Details of results after running \u2018SCRUB\u2019 fn\n    DUPLICATES: \u2018[hashlog]_duplicates.log\u2019  Details of duplicate files list after running \u2018DUPLICATES\u2019 fn\n    EXCLUSIONS: \u2018[hashlog]_excluded.log\u2019    Lists files Excluded from exclusion keywords set by user\n      PREVIOUS: \u2018[hashlog]_previous.log\u2019    Copy of [hashlog].log as \u201cundo\u201d after running \u2018UPDATE\u2019 fn\n\n\n__*____*__ **MAIN MENU** __*____*__\n\nThe Main Menu will present you with multiple options:\n\n    =POWERHASH SHA256= by HTWingNut v2024.01.17\n    Type q from any menu to return here\n    \n    Choose from the following:\n     [G]enerate New SHA256 Hash Log\n     [U]pdate Hash Log\n     [C]ompare Hash Logs\n     [S]crub Folder with Log\n     [D]uplicate File Check\n     [Q]uit\n    \n    CHOICE:\n\n__*____*__ **GENERATE (OR CREATE)** __*____*__\n\n**BEFORE YOU CAN DO ANYTHING** with any of the other functions you must create a MAIN HASH LOG ([G] in menu or `-create` flag in command line). You point the program to the folder that you want to generate hashes from.\n\nYou can then assign any folder and file exclusion parameters you want. These are in the fomr of keywords or keyphrases and must be entered in a specific format. Folder and File exclusions are independent of each other. Folder means path without the file. File means just the file name and extension.\n\nThe format to use - single quotes around each keyphrase with multiple keyphrases separted by a comma:\n\n`Enter FOLDER Exclusion: '\\LinuxISO','Temp','\\Recycle Bin'`\n\nNo wildcards allowed and IS NOT case sensitive. This example would exclude any file path starting with `LinuxISO`, any folder path containing the word `temp`, and any file path starting with `Recycle Bin`\n\n`Enter FILE Exclusion: '.pdf','Thumbs.db'\n\nThis would exclude any files with a `.pdf` extension (or if file contains `.pdf` in the filename itself) and anything named `Thumbs.db`.\n\n**Command Line**\n\nTo create this log from command line use:\n\n    pwsh .\\powerhash.ps1 -create -path \"D:\\Data\"\n\nif you want to add exclusions:\n\n    pwsh .\\powerhash.ps1 -create -path \"D:\\Data\" -excludefolders \"'\\LinuxISO','Temp','\\Recycle Bin'\" -excludefiles \"'.pdf','Thumbs.db'\"\n\nNote the exclusion lists have to be wrapped completely in double quotes.\n\nThe results will be stored in '[hashlog].log'.\n\nIf exclusions were added it will generate a '[hashlog]_exclusion.log' file which will list the file names of all files excluded from hashing along with the rule that excluded it.\n\nAnd of course everything will be summarized in the '[hashlog]_history.log' file.\n\n\n__*____*__ **UPDATE HASH LOG** __*____*__\n\nThis function will allow you to update an existing hash log with only files that have been changed, added/new to the folder, or deleted from the folder. It will also provide potentially renamed or moved files. You will have to provide the log file to update and the file path where to look for updated files.\n\nFolder and File exclusions can also be added, modified, or removed at this time. It will remove any existing log entries that match the exclusion profiles provided by the user.\n\n**Command Line**\n\nThe command line version of this would look something like this:\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\DATA\"\n\nAnd like the create/generate hash prompt you can exclude files as well.\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\Data\" -excludefolders \"'\\LinuxISO','Temp','\\Recycle Bin'\" -excludefiles \"'.pdf','Thumbs.db'\"\n\nIf you want to clear exclusions from command line you can use the '-excludeclear' command:\n\n    pwsh .\\powerhash.ps1 -update -log \"SHA256_DATA_20240117_190211.log' -path \"D:\\Data\" -excludeclear\n\nThe main log file '[hashlog].log' will be updated with any changes detected in the folder.\n\nDetails of the update will be stored in '[hashlog]_updated.log'.\n\nAnd of course everything will be summarized in the '[hashlog]_history.log' file.\n\n\n__*____*__ **SCRUB FOLDER** __*____*__\n\nThe SCRUB folder function will take the contents of [hashlog].log file and compare it with the contents of user specified folder location to check if any hashes have changed. It will also provide a lot of additional data including:\n\n* Total Log Entries\n* Total Files in Folder\n* Files Failed to Match Hash\n* Files Missing from Folder\n* New Files in Folder\n* Files with Mismatch Size/Date\n* Files Busy not Hashed\n* Files Excluded by the User\n* Potential Renamed Files\n\nThe user just needs to provide the log file containing hashes and the path to folder that contains the files they want to scrub.\n\nThere will be an option to \"HASH NEW FILES\". Basically this will hash any new files found in the folder that don't exist in the log file. This will be used to determine if a file is potentially moved or renamed. It will not update the main log file. The scrub operation is for reporting only. It will not update the main log file. You will have to run the 'UPDATE' option separately to do that.\n\nScrubbing time will depend on number of files and size of files in the folder. Because all files will need to be hashed to compare against the log file.\n\n**Command Line**\n\n    pwsh .\\powerhash.ps1 -scrub -log \"SHA256_DATA_20240117_190211.log\" -path \"D:\\DATA\"\n\nif you wish to hash new found files to check for moved or renamed files just add `-hashnew` to the end\n\n    pwsh .\\powerhash.ps1 -scrub -log \"SHA256_DATA_20240117_190211.log\" -path \"D:\\DATA\" -hashnew\n\nDetailed results will be stored in '[hashlog]_scrub.log' and summary in '[hashlog]_history.log' file.\n\n\n__*____*__ **DUPLICATE CHECK** __*____*__\n\nThe DUPLICATES function will check for file duplicates based on file hashes in a previously generated main log file. This means files are exactly identical.\n\nUser just needs to provide a main log file to check for duplicates.\n\n**Command Line**\n\nCommand line for checking for duplicates is pretty straight forward.\n\n    pwsh .\\powerhash.ps1 -duplicates -log \"SHA256_DATA_20240117_190211.log\"\n\nDetailed results will be stored in '[hashlog]_duplicates.log' and summary in '[hashlog]_history.log'\n\n\n\n__*____*__ **FINAL THOUGHTS** __*____*__\n\nI appreciate any feedback on this, especially if you notice any bugs. I don't plan on adding additional features in the near future. Although I am considering adding a \"recovery\" mode so that if the program or computer crashes while generating hashes during the \"generate hashes\" stage it will pick up where it left off.\n\nAlso, keep in mind that this is simply a tool to provide information about your files. It is not meant to make decisions for you.\n\nThanks for taking the time to read this and hope some of you find the program useful!", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PowerHash - A PowerShell SHA256/MD5 Hashing Script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cj4bb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1705885960.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705885201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a thing! I wrote my own PowerShell hashing script called... POWERHASH! (cue the dramatic music - PowerShell + Hash = POWERHASH). It does more than just hash your files though.&lt;/p&gt;\n\n&lt;p&gt;Windows does a crap job at offering built in tools to validate your data, so you have to resort to third party tools. Unfortunately there are limited third party tools out there, and many older ones that are no longer maintained or supported or cost money. So I decided to write my own. I am not a developer or programmer, so pardon the mess, but as far as I can tell the script is fully functional and efficient. I&amp;#39;m open to feedback from testing, however.&lt;/p&gt;\n\n&lt;p&gt;The biggest thing I wanted from a hashing program is to be able to update hashes of changed or added files to a folder without having to rehash the entire folder. I did not find this feature in the handful of programs I evaluated. So I implemented that feature. This way you can hash some files, then run the UPDATE function to update hashes and remove hashes of files removed from the folder. But it can do more than that as well. See below for features. &lt;/p&gt;\n\n&lt;p&gt;The project was first started by using stock PowerShell 5.1 that comes with Windows, because I like to use stock apps whenever I can. But I quickly realized that PS 5.1 is antiquated and the latest PowerShell Core 7 (currently version 7.4.1) is so much better, offers a ton of cmdlets to prevent having to load or program your own functions, and is very efficient with file comparisons. It&amp;#39;s free and easy to install too. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;FEATURES&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So what does PowerHash do?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Generate SHA256 (or MD5) hashes of files in a folder recursively (makes use of &lt;code&gt;Get-FileHash&lt;/code&gt; cmdlet)&lt;/li&gt;\n&lt;li&gt;Omit folders and files from checksum operation by using exclusion keywords for folders and files separately&lt;/li&gt;\n&lt;li&gt;Update existing hash log file with only files that have changed and/or been deleted from a folder so a full hash of all files does not need to be completed&lt;/li&gt;\n&lt;li&gt;Scrub a folder against an existing POWERHASH log file to check for discrepancies&lt;/li&gt;\n&lt;li&gt;Compare two log files for discrepancies (so you can hash two locations then scrub them based on the log files)&lt;/li&gt;\n&lt;li&gt;Find duplicate files based on matching hashes&lt;/li&gt;\n&lt;li&gt;Run from interactive menu or use command line flags (so you can schedule updates and scrubs with Task Scheduler)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;REQUIREMENTS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;PowerShell 7 (Core) is required to run this script, but it&amp;#39;s free and easy to install. Instructions here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4\"&gt;https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or just type in cmd prompt or powershell prompt: &lt;code&gt;winget install --id Microsoft.Powershell --source winget&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This will install the latest PowerShell version (currently 7.4.1 as of this writing, which is what this program was validated with).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;DOWNLOAD&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While I recommend you read my diatribe on how to use the program, you can download it from github:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/HTWingNut/PowerHash\"&gt;https://github.com/HTWingNut/PowerHash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s currently considered BETA. You can nab the &lt;code&gt;powerhash.ps1&lt;/code&gt; over on the right under &amp;quot;releases&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I have a video on how to use the program as well here: &lt;a href=\"https://youtu.be/U9usaQQJxLs\"&gt;https://youtu.be/U9usaQQJxLs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;INSTRUCTIONS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The program runs in both an interactive menu mode through cmd prompt or powershell prompt. It&amp;#39;s best to &lt;code&gt;CD&lt;/code&gt; to the folder where your &lt;code&gt;powerhash.ps1&lt;/code&gt; folder resides and run from there.&lt;/p&gt;\n\n&lt;p&gt;To run: &lt;code&gt;pwsh .\\powerhash.ps1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;By default it will generate SHA256 hashes of files, but you can use the &lt;code&gt;-MD5&lt;/code&gt; flag to have it generate and manage MD5 hash checksums instead (i.e. &lt;code&gt;pwsh .\\powerhash.ps1 -md5&lt;/code&gt;)&lt;/p&gt;\n\n&lt;p&gt;This will take you to the interactive menu. I will first take you through the interactive menu, then show you how to run the same commands using command line mode.&lt;/p&gt;\n\n&lt;p&gt;It may be a good idea to have a Windows Explorer window open so you can click/drag folder and file names to the cmd windows to save keystrokes. Just remember to click the command windows after dragging file names/paths over.&lt;/p&gt;\n\n&lt;p&gt;You can get command line help at any time by typing: &lt;code&gt;pwsh .\\powerhash.ps1 -help&lt;/code&gt; or even a full readme with &lt;code&gt;-readme&lt;/code&gt; flag. Or use &lt;code&gt;-readme&lt;/code&gt; flag with specific function you want. Those available readme&amp;#39;s are shown in the &lt;code&gt;-help&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;The script will generate a MAIN HASH LOG that will contain SHA256 hash values, file name, file size, last modified date of every file hashed. This is the MAIN HASH LOG that will be set to READ ONLY using the naming convention:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SHA256_[FOLDER NAME]_[DATE TIME STAMP].log\nExample: User enters folder D:\\DATA, the log file will be &amp;#39;SHA256_DATA_20240117_190211.log&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Let&amp;#39;s call this &amp;#39;[hashlog].log&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;You can rename [hashlog].log whatever you want to after it&amp;#39;s made, but the file will be set to READ ONLY. This file should not be hand edited or it could make it not work properly with the script.&lt;/p&gt;\n\n&lt;p&gt;All operations that touch the MAIN HASH LOG will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file. This file you can make notes in if desired as its reference only.&lt;/p&gt;\n\n&lt;p&gt;There will be other supplementary log files generated depending on what function you use. The log files that can be generated are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;   HISTORY: \u2018[hashlog]_history.log\u2019     Maintains summary of all actions performed on [hashlog].log\n   UPDATED: \u2018[hashlog]_updated.log\u2019     Details of file changes when using \u2018UPDATE\u2019 fn\n   COMPARE: \u2018[hashlog]_compare.log\u2019     Details of results when comparing two logs \u2018COMPARE\u2019 fn\n     SCRUB: \u2018[hashlog]_scrub.log\u2019       Details of results after running \u2018SCRUB\u2019 fn\nDUPLICATES: \u2018[hashlog]_duplicates.log\u2019  Details of duplicate files list after running \u2018DUPLICATES\u2019 fn\nEXCLUSIONS: \u2018[hashlog]_excluded.log\u2019    Lists files Excluded from exclusion keywords set by user\n  PREVIOUS: \u2018[hashlog]_previous.log\u2019    Copy of [hashlog].log as \u201cundo\u201d after running \u2018UPDATE\u2019 fn\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;MAIN MENU&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Main Menu will present you with multiple options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;=POWERHASH SHA256= by HTWingNut v2024.01.17\nType q from any menu to return here\n\nChoose from the following:\n [G]enerate New SHA256 Hash Log\n [U]pdate Hash Log\n [C]ompare Hash Logs\n [S]crub Folder with Log\n [D]uplicate File Check\n [Q]uit\n\nCHOICE:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;GENERATE (OR CREATE)&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BEFORE YOU CAN DO ANYTHING&lt;/strong&gt; with any of the other functions you must create a MAIN HASH LOG ([G] in menu or &lt;code&gt;-create&lt;/code&gt; flag in command line). You point the program to the folder that you want to generate hashes from.&lt;/p&gt;\n\n&lt;p&gt;You can then assign any folder and file exclusion parameters you want. These are in the fomr of keywords or keyphrases and must be entered in a specific format. Folder and File exclusions are independent of each other. Folder means path without the file. File means just the file name and extension.&lt;/p&gt;\n\n&lt;p&gt;The format to use - single quotes around each keyphrase with multiple keyphrases separted by a comma:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Enter FOLDER Exclusion: &amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;No wildcards allowed and IS NOT case sensitive. This example would exclude any file path starting with &lt;code&gt;LinuxISO&lt;/code&gt;, any folder path containing the word &lt;code&gt;temp&lt;/code&gt;, and any file path starting with &lt;code&gt;Recycle Bin&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;`Enter FILE Exclusion: &amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;This would exclude any files with a &lt;code&gt;.pdf&lt;/code&gt; extension (or if file contains &lt;code&gt;.pdf&lt;/code&gt; in the filename itself) and anything named &lt;code&gt;Thumbs.db&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To create this log from command line use:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -create -path &amp;quot;D:\\Data&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if you want to add exclusions:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -create -path &amp;quot;D:\\Data&amp;quot; -excludefolders &amp;quot;&amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&amp;quot; -excludefiles &amp;quot;&amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note the exclusion lists have to be wrapped completely in double quotes.&lt;/p&gt;\n\n&lt;p&gt;The results will be stored in &amp;#39;[hashlog].log&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;If exclusions were added it will generate a &amp;#39;[hashlog]_exclusion.log&amp;#39; file which will list the file names of all files excluded from hashing along with the rule that excluded it.&lt;/p&gt;\n\n&lt;p&gt;And of course everything will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;UPDATE HASH LOG&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This function will allow you to update an existing hash log with only files that have been changed, added/new to the folder, or deleted from the folder. It will also provide potentially renamed or moved files. You will have to provide the log file to update and the file path where to look for updated files.&lt;/p&gt;\n\n&lt;p&gt;Folder and File exclusions can also be added, modified, or removed at this time. It will remove any existing log entries that match the exclusion profiles provided by the user.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The command line version of this would look something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\DATA&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And like the create/generate hash prompt you can exclude files as well.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\Data&amp;quot; -excludefolders &amp;quot;&amp;#39;\\LinuxISO&amp;#39;,&amp;#39;Temp&amp;#39;,&amp;#39;\\Recycle Bin&amp;#39;&amp;quot; -excludefiles &amp;quot;&amp;#39;.pdf&amp;#39;,&amp;#39;Thumbs.db&amp;#39;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If you want to clear exclusions from command line you can use the &amp;#39;-excludeclear&amp;#39; command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -update -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;#39; -path &amp;quot;D:\\Data&amp;quot; -excludeclear\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The main log file &amp;#39;[hashlog].log&amp;#39; will be updated with any changes detected in the folder.&lt;/p&gt;\n\n&lt;p&gt;Details of the update will be stored in &amp;#39;[hashlog]_updated.log&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;And of course everything will be summarized in the &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;SCRUB FOLDER&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The SCRUB folder function will take the contents of [hashlog].log file and compare it with the contents of user specified folder location to check if any hashes have changed. It will also provide a lot of additional data including:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Total Log Entries&lt;/li&gt;\n&lt;li&gt;Total Files in Folder&lt;/li&gt;\n&lt;li&gt;Files Failed to Match Hash&lt;/li&gt;\n&lt;li&gt;Files Missing from Folder&lt;/li&gt;\n&lt;li&gt;New Files in Folder&lt;/li&gt;\n&lt;li&gt;Files with Mismatch Size/Date&lt;/li&gt;\n&lt;li&gt;Files Busy not Hashed&lt;/li&gt;\n&lt;li&gt;Files Excluded by the User&lt;/li&gt;\n&lt;li&gt;Potential Renamed Files&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The user just needs to provide the log file containing hashes and the path to folder that contains the files they want to scrub.&lt;/p&gt;\n\n&lt;p&gt;There will be an option to &amp;quot;HASH NEW FILES&amp;quot;. Basically this will hash any new files found in the folder that don&amp;#39;t exist in the log file. This will be used to determine if a file is potentially moved or renamed. It will not update the main log file. The scrub operation is for reporting only. It will not update the main log file. You will have to run the &amp;#39;UPDATE&amp;#39; option separately to do that.&lt;/p&gt;\n\n&lt;p&gt;Scrubbing time will depend on number of files and size of files in the folder. Because all files will need to be hashed to compare against the log file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -scrub -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot; -path &amp;quot;D:\\DATA&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if you wish to hash new found files to check for moved or renamed files just add &lt;code&gt;-hashnew&lt;/code&gt; to the end&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -scrub -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot; -path &amp;quot;D:\\DATA&amp;quot; -hashnew\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Detailed results will be stored in &amp;#39;[hashlog]_scrub.log&amp;#39; and summary in &amp;#39;[hashlog]_history.log&amp;#39; file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;DUPLICATE CHECK&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The DUPLICATES function will check for file duplicates based on file hashes in a previously generated main log file. This means files are exactly identical.&lt;/p&gt;\n\n&lt;p&gt;User just needs to provide a main log file to check for duplicates.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Command Line&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Command line for checking for duplicates is pretty straight forward.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pwsh .\\powerhash.ps1 -duplicates -log &amp;quot;SHA256_DATA_20240117_190211.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Detailed results will be stored in &amp;#39;[hashlog]_duplicates.log&amp;#39; and summary in &amp;#39;[hashlog]_history.log&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt; &lt;strong&gt;FINAL THOUGHTS&lt;/strong&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I appreciate any feedback on this, especially if you notice any bugs. I don&amp;#39;t plan on adding additional features in the near future. Although I am considering adding a &amp;quot;recovery&amp;quot; mode so that if the program or computer crashes while generating hashes during the &amp;quot;generate hashes&amp;quot; stage it will pick up where it left off.&lt;/p&gt;\n\n&lt;p&gt;Also, keep in mind that this is simply a tool to provide information about your files. It is not meant to make decisions for you.&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to read this and hope some of you find the program useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2RXw4nGo-oOJQju-H6pVtp_XQu2BzgVzsOsTdbPh-rs.jpg?auto=webp&amp;s=8d6ab09b988f4c7876a59de5a6766331a6dbaeb0", "width": 128, "height": 128}, "resolutions": [{"url": "https://external-preview.redd.it/2RXw4nGo-oOJQju-H6pVtp_XQu2BzgVzsOsTdbPh-rs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1297ffd0375f9ab7beff64e767500d2bfc24493a", "width": 108, "height": 108}], "variants": {}, "id": "RdYD16_h_i3Y4Wv2YmKJ3o6JYUyjmEY5DEi0Ulw_Hac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cj4bb", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/19cj4bb/powerhash_a_powershell_sha256md5_hashing_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cj4bb/powerhash_a_powershell_sha256md5_hashing_script/", "subreddit_subscribers": 727172, "created_utc": 1705885201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am done with hard drive noise. can't take it anymore. I live in a tiny apartment and I can hear the damn thing purring as the torrents write when I sleep.\n\n&amp;#x200B;\n\nAll I want is a total of 15GB in whatever config.... No need for backup (I'll use the HDD's for cold backup).\n\n&amp;#x200B;\n\n I could get four 4TB NVME's but that's about 1200$, or one of those 16TB Intel SSD's for 500$ but I heard they go bad? \n\nSpeed is not important, it's for a home plex server.", "author_fullname": "t2_9a5802tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the most economical way to acquire 15TB~ in SSD/NVME storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cpzsd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705907688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am done with hard drive noise. can&amp;#39;t take it anymore. I live in a tiny apartment and I can hear the damn thing purring as the torrents write when I sleep.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All I want is a total of 15GB in whatever config.... No need for backup (I&amp;#39;ll use the HDD&amp;#39;s for cold backup).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I could get four 4TB NVME&amp;#39;s but that&amp;#39;s about 1200$, or one of those 16TB Intel SSD&amp;#39;s for 500$ but I heard they go bad? &lt;/p&gt;\n\n&lt;p&gt;Speed is not important, it&amp;#39;s for a home plex server.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cpzsd", "is_robot_indexable": true, "report_reasons": null, "author": "testicularbat", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cpzsd/whats_the_most_economical_way_to_acquire_15tb_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cpzsd/whats_the_most_economical_way_to_acquire_15tb_in/", "subreddit_subscribers": 727172, "created_utc": 1705907688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "without really needing to, I was looking at my future expansion possibilities, in my current chassis I have run out of physical space, aside from a 5.25 bay in the front which I have yet to populate. \nI was thinking that in the future I could use a 5.25 to 4x2.5 cage, and was looking at SSD's that could be put in there.\n\nI keep seeing that the cheaper SSD's often have poor write endurence. Such as 700TB's for the crucial BX500 compared to 2800TB for an Ironwolf 125\nBut do I even need to care about this? I do write to my pool every day, but they're not large writes. When I do do large writes, they are usually only a few gigs. \n\nBut I practically never delete anything. I assume this applies to most of us hoarders. So I can't imagine I could ever come close to 700TB's on a 2TB drive.\nI suppose games and whatnot that I keep on there do get updates, but still, 700TB's?\n\n(This is for a TrueNAS box, so ZFS.)\nAm I missing something?", "author_fullname": "t2_thagl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we even need to care about write endurance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cfedw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705875144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;without really needing to, I was looking at my future expansion possibilities, in my current chassis I have run out of physical space, aside from a 5.25 bay in the front which I have yet to populate. \nI was thinking that in the future I could use a 5.25 to 4x2.5 cage, and was looking at SSD&amp;#39;s that could be put in there.&lt;/p&gt;\n\n&lt;p&gt;I keep seeing that the cheaper SSD&amp;#39;s often have poor write endurence. Such as 700TB&amp;#39;s for the crucial BX500 compared to 2800TB for an Ironwolf 125\nBut do I even need to care about this? I do write to my pool every day, but they&amp;#39;re not large writes. When I do do large writes, they are usually only a few gigs. &lt;/p&gt;\n\n&lt;p&gt;But I practically never delete anything. I assume this applies to most of us hoarders. So I can&amp;#39;t imagine I could ever come close to 700TB&amp;#39;s on a 2TB drive.\nI suppose games and whatnot that I keep on there do get updates, but still, 700TB&amp;#39;s?&lt;/p&gt;\n\n&lt;p&gt;(This is for a TrueNAS box, so ZFS.)\nAm I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19cfedw", "is_robot_indexable": true, "report_reasons": null, "author": "IvanezerScrooge", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cfedw/do_we_even_need_to_care_about_write_endurance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cfedw/do_we_even_need_to_care_about_write_endurance/", "subreddit_subscribers": 727172, "created_utc": 1705875144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The interface says SATA, anyone know why it wouldn't work?\n\n[https://www.ebay.com/itm/386098483254](https://www.ebay.com/itm/386098483254)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_9sz62", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need cheap 4tb hd, this listing says \"Will not work on home desktops\" ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cno77", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705899233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The interface says SATA, anyone know why it wouldn&amp;#39;t work?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ebay.com/itm/386098483254\"&gt;https://www.ebay.com/itm/386098483254&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?auto=webp&amp;s=51c941d811c9cea3e3208d37c1f59f061f55d9ce", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99445263380ad2ce7da4a3dce295bac447035930", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=652bee7c1abfe93cf12413d45ccd5b3fdce2448b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Kl-GUJYsFlWVyhMQvXucH2Q-NyJ2gXnqGREMTOMKYx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8b804606141f67ebc1d3196beef1df8fd10bd06", "width": 320, "height": 320}], "variants": {}, "id": "SKyQ6dJCaX_1qvtWHiT1j0UCV74_JDE2yhUT7ihJXMM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cno77", "is_robot_indexable": true, "report_reasons": null, "author": "mxl555", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cno77/need_cheap_4tb_hd_this_listing_says_will_not_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cno77/need_cheap_4tb_hd_this_listing_says_will_not_work/", "subreddit_subscribers": 727172, "created_utc": 1705899233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been thinking about building a NAS and got the following for free:\n\n* no-name case (four 5.25\" drive bays + three 3.5\" internal bays), 80mm exhaust fan\n\n* Motherboard: [Asus A88xm-plus](https://www.asus.com/supportonly/a88xmplus/helpdesk_cpu/): micro-ATX, FM2+ socket w\\ 8 SATA6 ports, 64GB max ram. \n\n* RAM: 8GB DDR3 1333MhZ (2x4GB, Kingston KVR)\n\n* CPU: [AMD A8-5600k](https://www.techpowerup.com/cpu-specs/a8-5600k.c1101) (3.6Ghz 4 core, 4 thread.)\n\n* CPU cooler: stock low-profile with 80mm fan\n\n* Boot drive: [Crucial  BX500 240GB](https://www.amazon.com/Crucial-BX500-240GB-2-5-Inch-Internal/dp/B07G3YNLJB?th=1)\n\n* PSU: generic brand, swapped out with Corsair RM650x\n\nThis feels pretty ancient but the onboard 8 SATA ports made me think it could be a good platform to explore building a first NAS.  I'm planning to put TrueNAS Scale on it.\n\n**Question --&gt;**: Is this a worthwhile project, or should I look to invest in more modern hardware?  \n\nI'm mostly looking to use this as a storage device.  I might try putting a media server on it once things are set up properly, but I have no idea how good or capable the A8 CPU is at transcoding (though I'd only be sending things to a 4k tv).  Also, I have no idea if there are better CPUs I could chuck in this thing.  The 5600k's 100w TDP seems like a lot in this case, but...it was free.  Can anyone recommend an upgrade path here?\n\n\nSpeaking of upgrades, here were some new purchases I was considering:\n\n* More ram: I have 4 DIMMS and a 64GB cap, so I'll probably look at picking up some 8gb 1600mhz sticks.  Not sure if I can find unbuffered ECC 16GB sticks compatible with this motherboard. (I can't seem to find and non-ECC DDR3 16GB sticks) \n\n* Drives, obviously.\n\n* Hot-swap caddies for the 5.25\" drive bays Any recommendations?\n\n* quieter exhaust fans.  Any recommendations? Was looking at Noctua 80mm fans but realized I don't know anything about this.\n\n\nThanks!", "author_fullname": "t2_6ix2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Worth it?] Repurposing an 10yo PC for a NAS: a88xm-plus mobo + A8-5600k AMD CPU.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19chu5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705881542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been thinking about building a NAS and got the following for free:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;no-name case (four 5.25&amp;quot; drive bays + three 3.5&amp;quot; internal bays), 80mm exhaust fan&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Motherboard: &lt;a href=\"https://www.asus.com/supportonly/a88xmplus/helpdesk_cpu/\"&gt;Asus A88xm-plus&lt;/a&gt;: micro-ATX, FM2+ socket w\\ 8 SATA6 ports, 64GB max ram. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;RAM: 8GB DDR3 1333MhZ (2x4GB, Kingston KVR)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;CPU: &lt;a href=\"https://www.techpowerup.com/cpu-specs/a8-5600k.c1101\"&gt;AMD A8-5600k&lt;/a&gt; (3.6Ghz 4 core, 4 thread.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;CPU cooler: stock low-profile with 80mm fan&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Boot drive: &lt;a href=\"https://www.amazon.com/Crucial-BX500-240GB-2-5-Inch-Internal/dp/B07G3YNLJB?th=1\"&gt;Crucial  BX500 240GB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;PSU: generic brand, swapped out with Corsair RM650x&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This feels pretty ancient but the onboard 8 SATA ports made me think it could be a good platform to explore building a first NAS.  I&amp;#39;m planning to put TrueNAS Scale on it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question --&amp;gt;&lt;/strong&gt;: Is this a worthwhile project, or should I look to invest in more modern hardware?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m mostly looking to use this as a storage device.  I might try putting a media server on it once things are set up properly, but I have no idea how good or capable the A8 CPU is at transcoding (though I&amp;#39;d only be sending things to a 4k tv).  Also, I have no idea if there are better CPUs I could chuck in this thing.  The 5600k&amp;#39;s 100w TDP seems like a lot in this case, but...it was free.  Can anyone recommend an upgrade path here?&lt;/p&gt;\n\n&lt;p&gt;Speaking of upgrades, here were some new purchases I was considering:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;More ram: I have 4 DIMMS and a 64GB cap, so I&amp;#39;ll probably look at picking up some 8gb 1600mhz sticks.  Not sure if I can find unbuffered ECC 16GB sticks compatible with this motherboard. (I can&amp;#39;t seem to find and non-ECC DDR3 16GB sticks) &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Drives, obviously.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Hot-swap caddies for the 5.25&amp;quot; drive bays Any recommendations?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;quieter exhaust fans.  Any recommendations? Was looking at Noctua 80mm fans but realized I don&amp;#39;t know anything about this.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19chu5z", "is_robot_indexable": true, "report_reasons": null, "author": "fumblesmcdrum", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19chu5z/worth_it_repurposing_an_10yo_pc_for_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19chu5z/worth_it_repurposing_an_10yo_pc_for_a_nas/", "subreddit_subscribers": 727172, "created_utc": 1705881542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm a photographer for a large construction company. We need a free DAM for the purpose of tagging and key wording photos. I use mac and my co worker uses windows so I need something that works for both. Any suggestions? ", "author_fullname": "t2_6agcep66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best free Digital Asset Manager for tagging photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19chu1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705881532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m a photographer for a large construction company. We need a free DAM for the purpose of tagging and key wording photos. I use mac and my co worker uses windows so I need something that works for both. Any suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19chu1l", "is_robot_indexable": true, "report_reasons": null, "author": "lilfishgod", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19chu1l/best_free_digital_asset_manager_for_tagging_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19chu1l/best_free_digital_asset_manager_for_tagging_photos/", "subreddit_subscribers": 727172, "created_utc": 1705881532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wanting to build a server PC, and don't need a hot rod NVME, buy would like 500GB maybe? Any suggestions on brands that are cheap but not shifty?", "author_fullname": "t2_8n4ipgeu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most economic way to get under 1TB NVME?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19d30si", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705949307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wanting to build a server PC, and don&amp;#39;t need a hot rod NVME, buy would like 500GB maybe? Any suggestions on brands that are cheap but not shifty?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19d30si", "is_robot_indexable": true, "report_reasons": null, "author": "RandyAutoTechSystem", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19d30si/most_economic_way_to_get_under_1tb_nvme/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19d30si/most_economic_way_to_get_under_1tb_nvme/", "subreddit_subscribers": 727172, "created_utc": 1705949307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two data storage and backup needs and I'm curious if an easy solution exists that takes care of both.  Want a physical solution, not something cloud or subscription based. Is there a single solution that would allow me to do the following:\n\n1) backup photos and videos from two android phones\n\n2) network storage that I can access from my laptop. Something that acts like a hard drive disk (like the c drive) so that I can store various files, photos, music, etc.  my laptop has a relatively small SSD so I want to primarily use it for software and store files on a network storage. I don't have a desk for my laptop so something attached to the router is ideal.", "author_fullname": "t2_2hb66f6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a computer storage and phone back up solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d0hob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705943193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two data storage and backup needs and I&amp;#39;m curious if an easy solution exists that takes care of both.  Want a physical solution, not something cloud or subscription based. Is there a single solution that would allow me to do the following:&lt;/p&gt;\n\n&lt;p&gt;1) backup photos and videos from two android phones&lt;/p&gt;\n\n&lt;p&gt;2) network storage that I can access from my laptop. Something that acts like a hard drive disk (like the c drive) so that I can store various files, photos, music, etc.  my laptop has a relatively small SSD so I want to primarily use it for software and store files on a network storage. I don&amp;#39;t have a desk for my laptop so something attached to the router is ideal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19d0hob", "is_robot_indexable": true, "report_reasons": null, "author": "Kingobadiah", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19d0hob/is_there_a_computer_storage_and_phone_back_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19d0hob/is_there_a_computer_storage_and_phone_back_up/", "subreddit_subscribers": 727172, "created_utc": 1705943193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nAs some of you may know (I feel like I've posted enough here!) I use optical media (specifically the M-Disc) to archive my personal data - mostly videos, audio files, photos and - occasionally - important documents. \n\nMy current system is burning every archive disc two times - one is kept onsite and the other is 'queued' for transfer to my offsite library. The offsite library is located at my inlaws' place in the US (I'm on a different continent ... and yes ... I guess I better stay married to this girl for the rest of my life if I want to keep my backup data \ud83d\ude02).\n\nI do the 'offsiting' process myself - by which I mean I put a binder full of CDs into my checked luggage. But I've thought it might be worth trying to mail over some of my CDs. I have hopes that my father in law will join the 'datahoarding movement' (he seems interested in M-Disc). So perhaps he'd do me the favor of taking a UPS delivery from me a few times a year.\n\nI'm wondering has anybody done something like this? And if so any thoughts on how to best package whatever physical media you're sending - whether it be optical media, HDDs, SDDs, LTO tapes, or really whatever?\n\nTIA", "author_fullname": "t2_poc45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on sending backup media (optical, discs) through the mail / via couriers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ctto5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705923991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;As some of you may know (I feel like I&amp;#39;ve posted enough here!) I use optical media (specifically the M-Disc) to archive my personal data - mostly videos, audio files, photos and - occasionally - important documents. &lt;/p&gt;\n\n&lt;p&gt;My current system is burning every archive disc two times - one is kept onsite and the other is &amp;#39;queued&amp;#39; for transfer to my offsite library. The offsite library is located at my inlaws&amp;#39; place in the US (I&amp;#39;m on a different continent ... and yes ... I guess I better stay married to this girl for the rest of my life if I want to keep my backup data \ud83d\ude02).&lt;/p&gt;\n\n&lt;p&gt;I do the &amp;#39;offsiting&amp;#39; process myself - by which I mean I put a binder full of CDs into my checked luggage. But I&amp;#39;ve thought it might be worth trying to mail over some of my CDs. I have hopes that my father in law will join the &amp;#39;datahoarding movement&amp;#39; (he seems interested in M-Disc). So perhaps he&amp;#39;d do me the favor of taking a UPS delivery from me a few times a year.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering has anybody done something like this? And if so any thoughts on how to best package whatever physical media you&amp;#39;re sending - whether it be optical media, HDDs, SDDs, LTO tapes, or really whatever?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ctto5", "is_robot_indexable": true, "report_reasons": null, "author": "danielrosehill", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ctto5/thoughts_on_sending_backup_media_optical_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ctto5/thoughts_on_sending_backup_media_optical_discs/", "subreddit_subscribers": 727172, "created_utc": 1705923991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm expanding my NAS and looking for some deal on eBay I found this guy who is selling 4 2TB barracuda for less than 100\u20ac, I contacted him and there are no bad sectors and all have about 10.000 hours of functioning time, is it a good deal or I would end up with a pile of crap?", "author_fullname": "t2_eoa4e7zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refurbished HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cs7t4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705917420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m expanding my NAS and looking for some deal on eBay I found this guy who is selling 4 2TB barracuda for less than 100\u20ac, I contacted him and there are no bad sectors and all have about 10.000 hours of functioning time, is it a good deal or I would end up with a pile of crap?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cs7t4", "is_robot_indexable": true, "report_reasons": null, "author": "Loitering14", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cs7t4/refurbished_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cs7t4/refurbished_hdds/", "subreddit_subscribers": 727172, "created_utc": 1705917420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been with CrashPlan for almost a decade, but my storage needs have far outgrown CrashPlan's ice age-speed. What's your experience of Backblaze vs IDrive, specifically on their backup and restore speed in the US (perfect if you've used both services)?\n\nI currently have 4TB data and growing. As a content creator I steadily output sizable data that needs versioning and backup, and I have a budget of around $10/mo.\n\nI'm specifically comparing the speed of these two services, not other options. But if you feel strongly to suggest others, I won't stop you ;) Also, I'm mainly looking for their \"standard\" services, instead of B2, e2 or other object storage solution. Mainly because of the costs, and I admit, I'm not very familiar how the workflow &amp; technology works.\n\nThanks in advance!", "author_fullname": "t2_fcb1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup &amp; Restore Speed on Backblaze vs IDrive, as in US, 2024?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ch4cn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705886691.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705879582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been with CrashPlan for almost a decade, but my storage needs have far outgrown CrashPlan&amp;#39;s ice age-speed. What&amp;#39;s your experience of Backblaze vs IDrive, specifically on their backup and restore speed in the US (perfect if you&amp;#39;ve used both services)?&lt;/p&gt;\n\n&lt;p&gt;I currently have 4TB data and growing. As a content creator I steadily output sizable data that needs versioning and backup, and I have a budget of around $10/mo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifically comparing the speed of these two services, not other options. But if you feel strongly to suggest others, I won&amp;#39;t stop you ;) Also, I&amp;#39;m mainly looking for their &amp;quot;standard&amp;quot; services, instead of B2, e2 or other object storage solution. Mainly because of the costs, and I admit, I&amp;#39;m not very familiar how the workflow &amp;amp; technology works.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ch4cn", "is_robot_indexable": true, "report_reasons": null, "author": "AlienBoy_tw", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ch4cn/backup_restore_speed_on_backblaze_vs_idrive_as_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ch4cn/backup_restore_speed_on_backblaze_vs_idrive_as_in/", "subreddit_subscribers": 727172, "created_utc": 1705879582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have quite a few older HDD ranging in sizes from 500GB to 3TB. I'm thinking about using as cold storage backups for my main NAS. Looking for what people doing for the same and what their strategies are. \n\nMy NAS is a 36TB (23tb used) raidz2 pool with automated snapshots and unencrypted volumes for each category of data. (Linux ISO's, homedirs, music, roms, personal photos, etc).  I automated backup of the important/irreplacable stuff to a 3tb disk elsewhere on a rpi4. (warm backup). I do have a 2 bay USB/Sata dock for quickly popping disks in and out, and another single disk sata/usb adapter.\n\nMy initial idea is use pairs of matching sized disks. First run badblocks and validate each disks is healthy. On each of them make a small vfat volume with a README and other details about what the disks contains and how to access it should someone in the distant future who isn't me try to read the disks on a windows box. On the remaining space make a mirrored ZFS pool with both disks. For volumes that can easily fit entirely on one set of disks (like my /home is only 600GB) use zfs send/recv and add encryption on the receiving end. On sets too large to fit, instead create new encrypted volumes and rsync the data over, splitting the volumes up alphabetically and fitting as much as I can. The README will contain information on where to find the decryption keys. Afterwards catalog and name each disk with post-it notes (coldbackup-1a and 1b).  Keep a running copy of what is on each pair of disks and when it was last scrubbed in a central document.  Then store the drives in static bags with silica packets in my basement (cool, steady temperature, and relatively dry). And make calendar events so every 2 to 3 years I go through and scrub all the disks and replace as needed. On super irreplacable stuff (family photos and the like) maybe I even use a 3-way mirror using another sata-usb cable i have.\n\nThoughts? What would you do differently?  Is encrypting backups a bad idea even if I make sure that Key's are recoverable (i may even print out QR codes on paper in addition to KeePass). Is there a benefit to settings copies=2 in a 2way mirror for this use case?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_53838", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do for cold backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cdt01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705871221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a few older HDD ranging in sizes from 500GB to 3TB. I&amp;#39;m thinking about using as cold storage backups for my main NAS. Looking for what people doing for the same and what their strategies are. &lt;/p&gt;\n\n&lt;p&gt;My NAS is a 36TB (23tb used) raidz2 pool with automated snapshots and unencrypted volumes for each category of data. (Linux ISO&amp;#39;s, homedirs, music, roms, personal photos, etc).  I automated backup of the important/irreplacable stuff to a 3tb disk elsewhere on a rpi4. (warm backup). I do have a 2 bay USB/Sata dock for quickly popping disks in and out, and another single disk sata/usb adapter.&lt;/p&gt;\n\n&lt;p&gt;My initial idea is use pairs of matching sized disks. First run badblocks and validate each disks is healthy. On each of them make a small vfat volume with a README and other details about what the disks contains and how to access it should someone in the distant future who isn&amp;#39;t me try to read the disks on a windows box. On the remaining space make a mirrored ZFS pool with both disks. For volumes that can easily fit entirely on one set of disks (like my /home is only 600GB) use zfs send/recv and add encryption on the receiving end. On sets too large to fit, instead create new encrypted volumes and rsync the data over, splitting the volumes up alphabetically and fitting as much as I can. The README will contain information on where to find the decryption keys. Afterwards catalog and name each disk with post-it notes (coldbackup-1a and 1b).  Keep a running copy of what is on each pair of disks and when it was last scrubbed in a central document.  Then store the drives in static bags with silica packets in my basement (cool, steady temperature, and relatively dry). And make calendar events so every 2 to 3 years I go through and scrub all the disks and replace as needed. On super irreplacable stuff (family photos and the like) maybe I even use a 3-way mirror using another sata-usb cable i have.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? What would you do differently?  Is encrypting backups a bad idea even if I make sure that Key&amp;#39;s are recoverable (i may even print out QR codes on paper in addition to KeePass). Is there a benefit to settings copies=2 in a 2way mirror for this use case?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cdt01", "is_robot_indexable": true, "report_reasons": null, "author": "skreak", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cdt01/what_do_you_do_for_cold_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cdt01/what_do_you_do_for_cold_backups/", "subreddit_subscribers": 727172, "created_utc": 1705871221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "G\u2019day - title may have been weird if so apologies, and I did a search but couldn\u2019t find what I was after\n\nI have a few hundred hard drives - both in servers and also externals \n\nIs there a program \u2018anywhere\u2019 that would allow me to scan these drives , and list the contents, and alert me when there are dupes ?\n\nKinda like a big database ?\n\nBe even more helpful if once the scanning process was done, that \u2018this program\u2019 would add new downloads to the database etc\n\nI keep finding that I have multiple copies of various things which is not only a waste of bandwidth but more importantly hard drive space , and the system I\u2019m currently using is obviously really bad\n\nSo I figure that there has to be a decent way or organization out there right ??", "author_fullname": "t2_dak0xorw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program to scan drives for content to list dupes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccac2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705867433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;G\u2019day - title may have been weird if so apologies, and I did a search but couldn\u2019t find what I was after&lt;/p&gt;\n\n&lt;p&gt;I have a few hundred hard drives - both in servers and also externals &lt;/p&gt;\n\n&lt;p&gt;Is there a program \u2018anywhere\u2019 that would allow me to scan these drives , and list the contents, and alert me when there are dupes ?&lt;/p&gt;\n\n&lt;p&gt;Kinda like a big database ?&lt;/p&gt;\n\n&lt;p&gt;Be even more helpful if once the scanning process was done, that \u2018this program\u2019 would add new downloads to the database etc&lt;/p&gt;\n\n&lt;p&gt;I keep finding that I have multiple copies of various things which is not only a waste of bandwidth but more importantly hard drive space , and the system I\u2019m currently using is obviously really bad&lt;/p&gt;\n\n&lt;p&gt;So I figure that there has to be a decent way or organization out there right ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ccac2", "is_robot_indexable": true, "report_reasons": null, "author": "ectoplasmic-warrior", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccac2/program_to_scan_drives_for_content_to_list_dupes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccac2/program_to_scan_drives_for_content_to_list_dupes/", "subreddit_subscribers": 727172, "created_utc": 1705867433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i have a bunch of computers that i have replaced with newer models. i have them because i have data (documents, emails, etc.) that i want to save. i am looking for a solution where i can warehouse emails. i played around with the thought of using one of my old servers with an old copy of enterprise server which had a copy of exchange. \n\nmy solution thus far has been using a desktop or a server with an os running an email client and importing mailboxes from all the computers and moving forward allowing it to download a copy of emails. ", "author_fullname": "t2_mrh26suq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "email hoarding question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d1mxy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705945942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have a bunch of computers that i have replaced with newer models. i have them because i have data (documents, emails, etc.) that i want to save. i am looking for a solution where i can warehouse emails. i played around with the thought of using one of my old servers with an old copy of enterprise server which had a copy of exchange. &lt;/p&gt;\n\n&lt;p&gt;my solution thus far has been using a desktop or a server with an os running an email client and importing mailboxes from all the computers and moving forward allowing it to download a copy of emails. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19d1mxy", "is_robot_indexable": true, "report_reasons": null, "author": "mel69issa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19d1mxy/email_hoarding_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19d1mxy/email_hoarding_question/", "subreddit_subscribers": 727172, "created_utc": 1705945942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TLDR:  \nI'm thinking of buying these for using for video and picture backup. Videos will be hooked up to a jellyfin (basically like plex) server so my wife can watch them. Do I need to worry about CMR vs SMR for the HDDS?  \nBay (run in raid 5): [https://www.amazon.com/gp/product/B003YFHEAC?th=1](https://www.amazon.com/gp/product/B003YFHEAC?th=1)  \nHDD(SMR): [https://www.westerndigital.com/products/internal-drives/wd-red-sata-hdd?sku=WD60EFAX](https://www.westerndigital.com/products/internal-drives/wd-red-sata-hdd?sku=WD60EFAX)  \nHDD(CMR):[https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD60EFPX](https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD60EFPX)\n\n&amp;#x200B;\n\nLong:\n\nI am currently running an old 4tb drive that I got for my old gaming and I've been using it to back up all of our old videos and pictures. It's hooked up to a Jellyfin server to serve up the videos for watching as we need. The hard drive is starting to fill up though so I am looking at expanding but I'm not sure if CMR vs SMR matters for what I am doing. It sounds like CMR is all around better in every way other than price where it costs more, but I'm not sure if it's worth the extra money where I won't be writing a lot I'll mostly just be reading after writing a single time. ", "author_fullname": "t2_wx8yx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a little direction (CMR vs SMR)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19d0jxy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705943355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR:&lt;br/&gt;\nI&amp;#39;m thinking of buying these for using for video and picture backup. Videos will be hooked up to a jellyfin (basically like plex) server so my wife can watch them. Do I need to worry about CMR vs SMR for the HDDS?&lt;br/&gt;\nBay (run in raid 5): &lt;a href=\"https://www.amazon.com/gp/product/B003YFHEAC?th=1\"&gt;https://www.amazon.com/gp/product/B003YFHEAC?th=1&lt;/a&gt;&lt;br/&gt;\nHDD(SMR): &lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-red-sata-hdd?sku=WD60EFAX\"&gt;https://www.westerndigital.com/products/internal-drives/wd-red-sata-hdd?sku=WD60EFAX&lt;/a&gt;&lt;br/&gt;\nHDD(CMR):&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD60EFPX\"&gt;https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD60EFPX&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Long:&lt;/p&gt;\n\n&lt;p&gt;I am currently running an old 4tb drive that I got for my old gaming and I&amp;#39;ve been using it to back up all of our old videos and pictures. It&amp;#39;s hooked up to a Jellyfin server to serve up the videos for watching as we need. The hard drive is starting to fill up though so I am looking at expanding but I&amp;#39;m not sure if CMR vs SMR matters for what I am doing. It sounds like CMR is all around better in every way other than price where it costs more, but I&amp;#39;m not sure if it&amp;#39;s worth the extra money where I won&amp;#39;t be writing a lot I&amp;#39;ll mostly just be reading after writing a single time. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19d0jxy", "is_robot_indexable": true, "report_reasons": null, "author": "Mouseater", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19d0jxy/looking_for_a_little_direction_cmr_vs_smr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19d0jxy/looking_for_a_little_direction_cmr_vs_smr/", "subreddit_subscribers": 727172, "created_utc": 1705943355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to work out a solution that fits my needs but I am at a loss.\n\nCurrently I was using a Kingston Data Traveler Locker G3+ secure USB drive but I forgot my password and it locked me out - making the drive and data useless.\n\nI need to do the following.\n\n1) Files are stored on a usb drive - which I can then use with my own personal desktop at home and lapotop whilst at work. \n\n2) Work has PCs which I use daily. Eithe generic hot desks or a shared laptop.\n\nI travel on the train and find the patchy connections means I have to work off the USB drive.\n\nSo can anyone suggest a solution where I have local access as well as remote / cloud backup as I don't want all my data on the usb and then either lose that or be locked out of it.\n\nI tend to work in areas that have patchy or unreliable internet hence the use of USB drive. \n\nAlso I may need to rethink how I work - so any suggestions are welcome.", "author_fullname": "t2_4j1b5rvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USb drive synced with online storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cvo50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705930254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to work out a solution that fits my needs but I am at a loss.&lt;/p&gt;\n\n&lt;p&gt;Currently I was using a Kingston Data Traveler Locker G3+ secure USB drive but I forgot my password and it locked me out - making the drive and data useless.&lt;/p&gt;\n\n&lt;p&gt;I need to do the following.&lt;/p&gt;\n\n&lt;p&gt;1) Files are stored on a usb drive - which I can then use with my own personal desktop at home and lapotop whilst at work. &lt;/p&gt;\n\n&lt;p&gt;2) Work has PCs which I use daily. Eithe generic hot desks or a shared laptop.&lt;/p&gt;\n\n&lt;p&gt;I travel on the train and find the patchy connections means I have to work off the USB drive.&lt;/p&gt;\n\n&lt;p&gt;So can anyone suggest a solution where I have local access as well as remote / cloud backup as I don&amp;#39;t want all my data on the usb and then either lose that or be locked out of it.&lt;/p&gt;\n\n&lt;p&gt;I tend to work in areas that have patchy or unreliable internet hence the use of USB drive. &lt;/p&gt;\n\n&lt;p&gt;Also I may need to rethink how I work - so any suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cvo50", "is_robot_indexable": true, "report_reasons": null, "author": "randismo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cvo50/usb_drive_synced_with_online_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cvo50/usb_drive_synced_with_online_storage/", "subreddit_subscribers": 727172, "created_utc": 1705930254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to a reliable backup storage for my pictures and video files but want to stay away from external HD. I was hoping yall could help me. Would it be a good option to install an internal HD, then download all my files into it, then unplug HD until the next batch of files. Ive read internal HD are more reliable which is why im going this route, plus it seems to be true since all the external HDs i have gotten have failed and no longer work in like 5 yrs or so. Any other options out there that dont require spending too much other than the HD itself or having to build something costly? Thanks\n", "author_fullname": "t2_1g2jttvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would adding internal HD to PC just as a backup be a good option for pic/vid files? Plug in when in use, then unplug when not in use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cvnrx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705946170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705930223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to a reliable backup storage for my pictures and video files but want to stay away from external HD. I was hoping yall could help me. Would it be a good option to install an internal HD, then download all my files into it, then unplug HD until the next batch of files. Ive read internal HD are more reliable which is why im going this route, plus it seems to be true since all the external HDs i have gotten have failed and no longer work in like 5 yrs or so. Any other options out there that dont require spending too much other than the HD itself or having to build something costly? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cvnrx", "is_robot_indexable": true, "report_reasons": null, "author": "HOUSTONFOOL", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cvnrx/would_adding_internal_hd_to_pc_just_as_a_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cvnrx/would_adding_internal_hd_to_pc_just_as_a_backup/", "subreddit_subscribers": 727172, "created_utc": 1705930223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_jk456", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AIC J4078-02-04X 78-bay JBOD Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_19cvbzw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Review", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EQfm0WKR8Waf_SGvthseMx6_08S4jCpZoDQnjOCFZMc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705929158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "servethehome.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.servethehome.com/aic-j4078-02-04x-78-bay-jbod-review-toshiba/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?auto=webp&amp;s=cdd790cc40b754af2a5fe4dba70dd5fb5be51618", "width": 1199, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50582764d15cc4c5e9392f32118c723a5b1fc38c", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64a868140780bd244c073d10d0930a90f93a4858", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d07df12cbd94d1ed97c16c530dedb8ca86f30a18", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b316d74eacbfbccc3313da476c006e8dac9cb61", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cfa4f61c027563467c7c67bb9360bc6dc719113b", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/TjdGhuat_OMc-JuEKy3bNHgRZPks7XD_pH5iQEUazsU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=04477b775f187bb7c742241467a5c158d50e5d46", "width": 1080, "height": 720}], "variants": {}, "id": "I8MJRPuarL2Rrxuf7dLuT3oXea5yGAfeoZnP6aMTGKI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19cvbzw", "is_robot_indexable": true, "report_reasons": null, "author": "NISMO1968", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cvbzw/aic_j40780204x_78bay_jbod_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.servethehome.com/aic-j4078-02-04x-78-bay-jbod-review-toshiba/", "subreddit_subscribers": 727172, "created_utc": 1705929158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to find an app or whatever that will let me do the following from my Android phone with the data being on my TrueNAS server.  If simpler, it can just be accessible from within my home network, but ideally I'd like it to work remotely.  The three requirements are;\n\n\\- automatic backup of and scrolling through photos like Google photos on phone  \n\\- access to upload and download other documents such as PDFs on demand on phone  \n\\- while at home be able to access all the photos and documents over a network share in Windows Explorer from a PC\n\nNextCloud seems to be the thing people recommend, but I haven't had much luck getting it to work the way I want so far.  Between the changes between each TN version, differences in Scale and Core, the official app and the TrueCharts app, conflicting documentation and videos, it really seems more complicated than it needs to be.  I got a simple container working but then couldn't figure out how to access the documents via SMB, either internally or through the External Storage app.\n\nI've been playing with Proxmox on a different box and it's VM/CT support seems better than TN, but at this point it is still a learning task, and not something \"home production\".  IE, I'd have preferred to get something known working on TN as I'm more familiar with it, and then separately experiment with how I to install, configure, and refine NC on Proxmox.\n\nI guess I could try a Proxmox solution, but also for simplicity I was hoping the data would be on the same box as the NC server.  I'd also have to have a backup/recovery plan for the Proxmox box, which I don't have now as just a test system.\n\nThoughts?  Advice?\n\n&amp;#x200B;", "author_fullname": "t2_b39gg9g1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive/Photos for TrueNAS Scale - NextCloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19csbxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705917892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to find an app or whatever that will let me do the following from my Android phone with the data being on my TrueNAS server.  If simpler, it can just be accessible from within my home network, but ideally I&amp;#39;d like it to work remotely.  The three requirements are;&lt;/p&gt;\n\n&lt;p&gt;- automatic backup of and scrolling through photos like Google photos on phone&lt;br/&gt;\n- access to upload and download other documents such as PDFs on demand on phone&lt;br/&gt;\n- while at home be able to access all the photos and documents over a network share in Windows Explorer from a PC&lt;/p&gt;\n\n&lt;p&gt;NextCloud seems to be the thing people recommend, but I haven&amp;#39;t had much luck getting it to work the way I want so far.  Between the changes between each TN version, differences in Scale and Core, the official app and the TrueCharts app, conflicting documentation and videos, it really seems more complicated than it needs to be.  I got a simple container working but then couldn&amp;#39;t figure out how to access the documents via SMB, either internally or through the External Storage app.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing with Proxmox on a different box and it&amp;#39;s VM/CT support seems better than TN, but at this point it is still a learning task, and not something &amp;quot;home production&amp;quot;.  IE, I&amp;#39;d have preferred to get something known working on TN as I&amp;#39;m more familiar with it, and then separately experiment with how I to install, configure, and refine NC on Proxmox.&lt;/p&gt;\n\n&lt;p&gt;I guess I could try a Proxmox solution, but also for simplicity I was hoping the data would be on the same box as the NC server.  I&amp;#39;d also have to have a backup/recovery plan for the Proxmox box, which I don&amp;#39;t have now as just a test system.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?  Advice?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19csbxa", "is_robot_indexable": true, "report_reasons": null, "author": "Content-Apple-833", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19csbxa/google_drivephotos_for_truenas_scale_nextcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19csbxa/google_drivephotos_for_truenas_scale_nextcloud/", "subreddit_subscribers": 727172, "created_utc": 1705917892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a small hoarder, currently sitting at 3TB on a 16TB pool (mostly small files &lt; 20MB) and a larger pool in the 40TB (used) area. Using truenas, the larger pool is raidz-2 with 8x12TB drives 1vdev and the smaller 7x4TB drives 1vdev... I need to optimize this (mostly for power consumption and noise) and need to move the data to my backup device (which I haven't kept in sync for about a year... -.-)\n\nSo, what is your way to move large amounts of data to another device, hardware wise? I currently only have 1Gbit NICs, so I'm capped at 100MB/s although the drives could probably give at least a little more. Is there some reliable usb-to-usb solution, or some good WiFi usb NICs someone could advise to me for wireless transport? Or even 2.5Gbit usb NICs that won't break under this load? Are these reliable for larger amounts of continuous transfer?\n\nBonus question, halfway related: if I do setup truenas anew, is there some way to have a single pool being offline while the system is running, just doing regular scrubs and checks on it? Because than I would do this as a secondary backup, where I could internally move files back if something breaks in the future.", "author_fullname": "t2_2k90lmlm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your way of moving data to backup devices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cogp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705901859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a small hoarder, currently sitting at 3TB on a 16TB pool (mostly small files &amp;lt; 20MB) and a larger pool in the 40TB (used) area. Using truenas, the larger pool is raidz-2 with 8x12TB drives 1vdev and the smaller 7x4TB drives 1vdev... I need to optimize this (mostly for power consumption and noise) and need to move the data to my backup device (which I haven&amp;#39;t kept in sync for about a year... -.-)&lt;/p&gt;\n\n&lt;p&gt;So, what is your way to move large amounts of data to another device, hardware wise? I currently only have 1Gbit NICs, so I&amp;#39;m capped at 100MB/s although the drives could probably give at least a little more. Is there some reliable usb-to-usb solution, or some good WiFi usb NICs someone could advise to me for wireless transport? Or even 2.5Gbit usb NICs that won&amp;#39;t break under this load? Are these reliable for larger amounts of continuous transfer?&lt;/p&gt;\n\n&lt;p&gt;Bonus question, halfway related: if I do setup truenas anew, is there some way to have a single pool being offline while the system is running, just doing regular scrubs and checks on it? Because than I would do this as a secondary backup, where I could internally move files back if something breaks in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cogp8", "is_robot_indexable": true, "report_reasons": null, "author": "sebsnake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cogp8/what_is_your_way_of_moving_data_to_backup_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cogp8/what_is_your_way_of_moving_data_to_backup_devices/", "subreddit_subscribers": 727172, "created_utc": 1705901859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all. \n\nWant to start archiving family photos and videos for long term storage using 100gb M-Discs (1.5tb in total so far), and seen the Asus BW-16D1HT internal drive. \n\nAnyone use it?  Any issues?? \n\nAlso seen on eBay 20 disc spindles of 100gb from Japan for \u00a395, yet seen others that are the same price for only 5 in cases. Any reason why there is such a massive difference?? \n\n\nThanks.", "author_fullname": "t2_77qyrr6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone used an Asus BW-16D1HT for M-Disc Archiving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19cgltj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705878247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. &lt;/p&gt;\n\n&lt;p&gt;Want to start archiving family photos and videos for long term storage using 100gb M-Discs (1.5tb in total so far), and seen the Asus BW-16D1HT internal drive. &lt;/p&gt;\n\n&lt;p&gt;Anyone use it?  Any issues?? &lt;/p&gt;\n\n&lt;p&gt;Also seen on eBay 20 disc spindles of 100gb from Japan for \u00a395, yet seen others that are the same price for only 5 in cases. Any reason why there is such a massive difference?? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cgltj", "is_robot_indexable": true, "report_reasons": null, "author": "EquivalentTip4103", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cgltj/anyone_used_an_asus_bw16d1ht_for_mdisc_archiving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cgltj/anyone_used_an_asus_bw16d1ht_for_mdisc_archiving/", "subreddit_subscribers": 727172, "created_utc": 1705878247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/1e7zvrjdcvdc1.png?width=838&amp;format=png&amp;auto=webp&amp;s=242879c900c6e38a5b49ef7f853582d73adf51ca", "author_fullname": "t2_mzifembb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got a Samsung T7 SSD and it came Locked??? New from box i downloaded software and sent me to this screen", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1e7zvrjdcvdc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/1e7zvrjdcvdc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=958a65fa7e8acd8aed50f06a59c8248d55f4b196"}, {"y": 166, "x": 216, "u": "https://preview.redd.it/1e7zvrjdcvdc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=75106923a83814df894f27d898a1865599478869"}, {"y": 247, "x": 320, "u": "https://preview.redd.it/1e7zvrjdcvdc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=727918f10a70e3ff3f56b335b76ca4da7751b1e6"}, {"y": 494, "x": 640, "u": "https://preview.redd.it/1e7zvrjdcvdc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cf23e801fde50c3903ea90f8ab488cd6e064d15"}], "s": {"y": 647, "x": 838, "u": "https://preview.redd.it/1e7zvrjdcvdc1.png?width=838&amp;format=png&amp;auto=webp&amp;s=242879c900c6e38a5b49ef7f853582d73adf51ca"}, "id": "1e7zvrjdcvdc1"}}, "name": "t3_19cfmre", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dMeesNQLyvEZl5SlmuBjPRD9S0vO0ws0lPZAJAIdIYE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705875737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1e7zvrjdcvdc1.png?width=838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=242879c900c6e38a5b49ef7f853582d73adf51ca\"&gt;https://preview.redd.it/1e7zvrjdcvdc1.png?width=838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=242879c900c6e38a5b49ef7f853582d73adf51ca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19cfmre", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Gazelle3369", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19cfmre/just_got_a_samsung_t7_ssd_and_it_came_locked_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19cfmre/just_got_a_samsung_t7_ssd_and_it_came_locked_new/", "subreddit_subscribers": 727172, "created_utc": 1705875737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to move my backups to different location, since in the case of fire or theft I could lose all data.\n\nI was thinking to copy everything on external 2.5\" HDDs and store them far away from my apartment.\n\nI have:\n\n* Maxtor M3 Portable External Hard Drive (**4TB**)\n* Seagate Basic Portable Drive (**5TB**)\n* and plan to buy WD Elements Portable Black 2.5 (**5TB**)  \n\n\nAll drives are new and not used much. I have a house in a countryside and that is where I plan to keep my offsite backup. No one lives there and temperature can go from -20\u00b0C to 40\u00b0C.   \nI was wondering do you have any suggestions on how to store these drives? I guess I need to keep them in shade and keep them dry. Do I need to buy some protective materials to isolate them better? \n\nI would like to keep them in a good health for 10 years and to replace them after. Not sure if these ones are able to live that long (even when not used), since they are the cheapest I could find.  \n", "author_fullname": "t2_26c1b19v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store external HDD as an offsite backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccz2t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705869140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to move my backups to different location, since in the case of fire or theft I could lose all data.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to copy everything on external 2.5&amp;quot; HDDs and store them far away from my apartment.&lt;/p&gt;\n\n&lt;p&gt;I have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Maxtor M3 Portable External Hard Drive (&lt;strong&gt;4TB&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Seagate Basic Portable Drive (&lt;strong&gt;5TB&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;and plan to buy WD Elements Portable Black 2.5 (&lt;strong&gt;5TB&lt;/strong&gt;)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All drives are new and not used much. I have a house in a countryside and that is where I plan to keep my offsite backup. No one lives there and temperature can go from -20\u00b0C to 40\u00b0C.&lt;br/&gt;\nI was wondering do you have any suggestions on how to store these drives? I guess I need to keep them in shade and keep them dry. Do I need to buy some protective materials to isolate them better? &lt;/p&gt;\n\n&lt;p&gt;I would like to keep them in a good health for 10 years and to replace them after. Not sure if these ones are able to live that long (even when not used), since they are the cheapest I could find.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "19ccz2t", "is_robot_indexable": true, "report_reasons": null, "author": "zp-87", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccz2t/how_to_store_external_hdd_as_an_offsite_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccz2t/how_to_store_external_hdd_as_an_offsite_backup/", "subreddit_subscribers": 727172, "created_utc": 1705869140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know that there is individual that share storage between themselves and I know that there is FileCoin\n\nBut no fren and FIL is actually just a currency to give to classic providers\n\nI search for a way to share my storage P2P on a network with random people and get the same transfer/storage amount in return, no money\n\nOnly problem being availability as I don't have a server, so I can't be online all the time. And my data is metered so I want to share really few data because basically it'll cost me 4 times the data amount I want to save (like backup 1 GB of data means one upload of me, one of the other one, one download per person too, multiply it by wanted backups)", "author_fullname": "t2_5slsu5xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mutual storage sharing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ccgq2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Serious answer only", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705867870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that there is individual that share storage between themselves and I know that there is FileCoin&lt;/p&gt;\n\n&lt;p&gt;But no fren and FIL is actually just a currency to give to classic providers&lt;/p&gt;\n\n&lt;p&gt;I search for a way to share my storage P2P on a network with random people and get the same transfer/storage amount in return, no money&lt;/p&gt;\n\n&lt;p&gt;Only problem being availability as I don&amp;#39;t have a server, so I can&amp;#39;t be online all the time. And my data is metered so I want to share really few data because basically it&amp;#39;ll cost me 4 times the data amount I want to save (like backup 1 GB of data means one upload of me, one of the other one, one download per person too, multiply it by wanted backups)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "19ccgq2", "is_robot_indexable": true, "report_reasons": null, "author": "xqoe", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/19ccgq2/mutual_storage_sharing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/19ccgq2/mutual_storage_sharing/", "subreddit_subscribers": 727172, "created_utc": 1705867870.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}