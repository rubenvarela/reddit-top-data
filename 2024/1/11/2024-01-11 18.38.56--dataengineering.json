{"kind": "Listing", "data": {"after": "t3_193wa6n", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guess the data type \u0ca0_\u0ca0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1945s14", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 115, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 115, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6OUWGdi7zMP1eZSi_VoZQEu6EsikAvzSqJz6d5ovrEc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704990687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t9nn0bq88ubc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t9nn0bq88ubc1.png?auto=webp&amp;s=cad6c0561f9df548a29f20fa31ad6eae64e1fc60", "width": 778, "height": 1200}, "resolutions": [{"url": "https://preview.redd.it/t9nn0bq88ubc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6424c32802e9fba636d819f274dc1f1eba931b44", "width": 108, "height": 166}, {"url": "https://preview.redd.it/t9nn0bq88ubc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebf60f950e042c9f9b237b37c8baa29031edaf21", "width": 216, "height": 333}, {"url": "https://preview.redd.it/t9nn0bq88ubc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe80fc967b18438c4a7241d3d6d13d347645d178", "width": 320, "height": 493}, {"url": "https://preview.redd.it/t9nn0bq88ubc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d047d996e9b90ae744ab20efc5bbe8d390b0072d", "width": 640, "height": 987}], "variants": {}, "id": "eGQWKC-q_uhJok7ugfder92YSYVbV5wEkt-NcHXwHw8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1945s14", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1945s14/guess_the_data_type_\u0ca0_\u0ca0/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t9nn0bq88ubc1.png", "subreddit_subscribers": 151930, "created_utc": 1704990687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm hearing more and more about dashboards dying and moving to \"interactive data apps\". I wonder if this is vendor marketing fluff or if this is actually happening. Thoughts?", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will you stop using dashboards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193xq76", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704968069.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704965374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m hearing more and more about dashboards dying and moving to &amp;quot;interactive data apps&amp;quot;. I wonder if this is vendor marketing fluff or if this is actually happening. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193xq76", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193xq76/will_you_stop_using_dashboards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193xq76/will_you_stop_using_dashboards/", "subreddit_subscribers": 151930, "created_utc": 1704965374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some members of my team have raised some points about not having enough focus time, due to: \n\n* general email\n* data issues (via slack/email\n* slack messages (within team or outside)\n* meetings (fairly minimal but still disruptive of the flow)\n* in-person questions - usually from within team (we have semi-open floorplan)\n\na senior data engineer mentioned he rarely gets 90 minutes  of uninterrupted development work per day. Our small team  manages about a dozen third party clients (with unclean data sometimes) plus our own internal processes and . Some weeks, it's not bad, other times there are a string of issues/interruptions. \n\nWhat are some specific techniques y'all have employed to focus?\n\n* snooze slack\n* focus time slack status (do people respect this?)\n* custom slack status\n* block calendar time\n* how to handle in-person chatter/questions? \n   * headphones on means focus?", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage uninterrupted focus time amid job failures, slack messages, emails, meetings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193mypp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704930159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some members of my team have raised some points about not having enough focus time, due to: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;general email&lt;/li&gt;\n&lt;li&gt;data issues (via slack/email&lt;/li&gt;\n&lt;li&gt;slack messages (within team or outside)&lt;/li&gt;\n&lt;li&gt;meetings (fairly minimal but still disruptive of the flow)&lt;/li&gt;\n&lt;li&gt;in-person questions - usually from within team (we have semi-open floorplan)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;a senior data engineer mentioned he rarely gets 90 minutes  of uninterrupted development work per day. Our small team  manages about a dozen third party clients (with unclean data sometimes) plus our own internal processes and . Some weeks, it&amp;#39;s not bad, other times there are a string of issues/interruptions. &lt;/p&gt;\n\n&lt;p&gt;What are some specific techniques y&amp;#39;all have employed to focus?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;snooze slack&lt;/li&gt;\n&lt;li&gt;focus time slack status (do people respect this?)&lt;/li&gt;\n&lt;li&gt;custom slack status&lt;/li&gt;\n&lt;li&gt;block calendar time&lt;/li&gt;\n&lt;li&gt;how to handle in-person chatter/questions? \n\n&lt;ul&gt;\n&lt;li&gt;headphones on means focus?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193mypp", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193mypp/how_do_you_manage_uninterrupted_focus_time_amid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193mypp/how_do_you_manage_uninterrupted_focus_time_amid/", "subreddit_subscribers": 151930, "created_utc": 1704930159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data changed from few thousand records to millions of records. Everything is well organized, tracked and validated. \n\nI\u2019m struggling to learn so many frameworks, and understanding huge data with a lot of attributes for data modeling. \n\nNot able to handle the pressure and feeling of not belonging here, as everyone seems to know what they are doing and deploying lot of changes to production, every week. \n\nI have been working here for 2 months and I got a feedback that I should be faster. I\u2019m not able to progress due to fear of getting fired. People who joined few months before seem to be performing better or meeting expectations.\n\nDid any face these challenges while changing organisation? How did you handle it? Any suggestions?", "author_fullname": "t2_q9ou4icln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moved from a small company, which uses simple cloud services to a big corporation, that uses robust frameworks for ETL, processing and validations.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193vm5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704956576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data changed from few thousand records to millions of records. Everything is well organized, tracked and validated. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m struggling to learn so many frameworks, and understanding huge data with a lot of attributes for data modeling. &lt;/p&gt;\n\n&lt;p&gt;Not able to handle the pressure and feeling of not belonging here, as everyone seems to know what they are doing and deploying lot of changes to production, every week. &lt;/p&gt;\n\n&lt;p&gt;I have been working here for 2 months and I got a feedback that I should be faster. I\u2019m not able to progress due to fear of getting fired. People who joined few months before seem to be performing better or meeting expectations.&lt;/p&gt;\n\n&lt;p&gt;Did any face these challenges while changing organisation? How did you handle it? Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "193vm5s", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Letterhead6262", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193vm5s/moved_from_a_small_company_which_uses_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193vm5s/moved_from_a_small_company_which_uses_simple/", "subreddit_subscribers": 151930, "created_utc": 1704956576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why the Modern Data Stack sucks for data consultancies looking to productize", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_1940jpl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Z80amFoGZF4uKOW3ePYmKPJw815hUsMMEdqHJ_f3Rmg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704976019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/why-the-modern-data-stack-sucks-for-data-consultancies-looking-to-productize/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?auto=webp&amp;s=d881a43f6049b89e4e066fdec179ca867234699a", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d50965a9c8e4dfb95d059efbb0ca8874a7e9fd24", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d51552db0d7f00af72c2917c137535e480b083fb", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=abada4d1af8927e2dee9a58e9931ad829e98a32b", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7129035f695679de5a1082d9bad5e24b3cf5f9ff", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fca419eef28bf35b5f5827f58a0d8841aab8355", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/2TIvmEmBcxrV7XLN389zAFH2ntXlGvYETUM8QQPA9AM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0509de0824804f288d0bc095bfdf0d6bad45829a", "width": 1080, "height": 617}], "variants": {}, "id": "WpZH0isc5AN4ZOJ6PliPBgbg_B4B6TaTgAHr3YBSZII"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1940jpl", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1940jpl/why_the_modern_data_stack_sucks_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/why-the-modern-data-stack-sucks-for-data-consultancies-looking-to-productize/", "subreddit_subscribers": 151930, "created_utc": 1704976019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAt the moment, I organize my workload with a single machine per application instance. I am about to deploy an orchestration server to trigger some automations on a schedule.\n\nI\u2019ve got all my automations on my local machine. These aren\u2019t applications per se, just little scripts, ETL jobs, you name it. Now, I\u2019m trying to think of how I want to deploy workers and automations\u2026 it doesn\u2019t really fit in with my current model, does it? I see no sense in allocating an entire machine to a single automation of this scale.\n\nHow do you guys match machine to &lt;thing&gt; in your job? Do you guys have a high level architecture that you follow? Would you end up creating a machine designated for hosting several automation, or design it like how I do apps?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you host multiple apps on the same machine, even if they\u2019re all containers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193r7as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704941988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;At the moment, I organize my workload with a single machine per application instance. I am about to deploy an orchestration server to trigger some automations on a schedule.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got all my automations on my local machine. These aren\u2019t applications per se, just little scripts, ETL jobs, you name it. Now, I\u2019m trying to think of how I want to deploy workers and automations\u2026 it doesn\u2019t really fit in with my current model, does it? I see no sense in allocating an entire machine to a single automation of this scale.&lt;/p&gt;\n\n&lt;p&gt;How do you guys match machine to &amp;lt;thing&amp;gt; in your job? Do you guys have a high level architecture that you follow? Would you end up creating a machine designated for hosting several automation, or design it like how I do apps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193r7as", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193r7as/do_you_host_multiple_apps_on_the_same_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193r7as/do_you_host_multiple_apps_on_the_same_machine/", "subreddit_subscribers": 151930, "created_utc": 1704941988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "what would be the best language to learn as an accessory, the main reason being, I want to learn a low level language to understand Software Engineering better, that would benefit my DE career as well. ", "author_fullname": "t2_68ja65ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "accessory language to python for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193vaqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704955354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what would be the best language to learn as an accessory, the main reason being, I want to learn a low level language to understand Software Engineering better, that would benefit my DE career as well. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193vaqk", "is_robot_indexable": true, "report_reasons": null, "author": "alphamalet997", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193vaqk/accessory_language_to_python_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193vaqk/accessory_language_to_python_for_de/", "subreddit_subscribers": 151930, "created_utc": 1704955354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working as ETL developer since 4 months(fresher) in a service based company, with tech stak snowflake, IICS. \nFirst of all IICS is the worst tool, they are very slow and very unpredictable. Idk about powercenter, but informstica cloud is so much slow. We have to face constant issue saving a taskflow. , and updating a taskflow. \n\nBut thank god I got lucky,  i got chance to use snowflake that is great software. one day my project leader was absent, and a BRE template was needed. I took the weekend that was able to process join condition also. \nThen I was asked to created a stored procedure for a part of project that will process around csv data from around 50 vendors having 4 file type from each vendor I.e 200 files. And load it into staging layer in 20 channels. That's also not a straight pass, many to one mapping. I have successfully implemented that procedure. \n\nDamn my nightmare begins now, first client changed all the  STTM, then our functional group who  made grave mistake they, followed every instructions blindly without analyzing sample data from client. Then data architect changed all the data model. I improvised the code made up the changes. Forcefully developed code to process STTM that will great data model accordingly. My manager was asking to check thoe file mannually \ud83d\ude35\u200d\ud83d\udcab test those ingestion. Disagreeing with him, I automated testing process. I have to debate over that for 2 hours with testing lead. Why is it necessary. \nAfter that they are changing vendor to channel map constantly (following sample data). Basically vendor to channel map is being done on basis of a column. And they are undecisive of those map they are changing it every few days. Simple they say you have automated it. Do it! But those automated process is not so formalised. I had developed to easy my work not to automate the process. You should have give more time. . It takes lot of concentration to avoid those unforseen \n\nI am constantly asking my manager that we are following wrong approach since I have analysed sample data from that on distinct value of that particular column is mapped into each channel. Which is the trigger for that orocedure. When in product environment it will break down graduall. In real data there may be many code. But my manager, god knows why is not forwarding that input. All are acting as per wish. Then they expect refined data. God knows what they do. \ud83e\udd72\ud83e\udd72\ud83e\udd72", "author_fullname": "t2_1ubfs6x4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rant! How functional group and bad management affect Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1946ghu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704992623.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704992387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working as ETL developer since 4 months(fresher) in a service based company, with tech stak snowflake, IICS. \nFirst of all IICS is the worst tool, they are very slow and very unpredictable. Idk about powercenter, but informstica cloud is so much slow. We have to face constant issue saving a taskflow. , and updating a taskflow. &lt;/p&gt;\n\n&lt;p&gt;But thank god I got lucky,  i got chance to use snowflake that is great software. one day my project leader was absent, and a BRE template was needed. I took the weekend that was able to process join condition also. \nThen I was asked to created a stored procedure for a part of project that will process around csv data from around 50 vendors having 4 file type from each vendor I.e 200 files. And load it into staging layer in 20 channels. That&amp;#39;s also not a straight pass, many to one mapping. I have successfully implemented that procedure. &lt;/p&gt;\n\n&lt;p&gt;Damn my nightmare begins now, first client changed all the  STTM, then our functional group who  made grave mistake they, followed every instructions blindly without analyzing sample data from client. Then data architect changed all the data model. I improvised the code made up the changes. Forcefully developed code to process STTM that will great data model accordingly. My manager was asking to check thoe file mannually \ud83d\ude35\u200d\ud83d\udcab test those ingestion. Disagreeing with him, I automated testing process. I have to debate over that for 2 hours with testing lead. Why is it necessary. \nAfter that they are changing vendor to channel map constantly (following sample data). Basically vendor to channel map is being done on basis of a column. And they are undecisive of those map they are changing it every few days. Simple they say you have automated it. Do it! But those automated process is not so formalised. I had developed to easy my work not to automate the process. You should have give more time. . It takes lot of concentration to avoid those unforseen &lt;/p&gt;\n\n&lt;p&gt;I am constantly asking my manager that we are following wrong approach since I have analysed sample data from that on distinct value of that particular column is mapped into each channel. Which is the trigger for that orocedure. When in product environment it will break down graduall. In real data there may be many code. But my manager, god knows why is not forwarding that input. All are acting as per wish. Then they expect refined data. God knows what they do. \ud83e\udd72\ud83e\udd72\ud83e\udd72&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1946ghu", "is_robot_indexable": true, "report_reasons": null, "author": "asud_w_asud", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1946ghu/rant_how_functional_group_and_bad_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1946ghu/rant_how_functional_group_and_bad_management/", "subreddit_subscribers": 151930, "created_utc": 1704992387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm the author of this guide to logical replication in Postgres. I break down some of the internal components of Postgres to make CDC easier to understand. I hope you find it informative and learn something new!  \n\n\n[https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql](https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql)", "author_fullname": "t2_5h5nqi7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Technical Dive into PostgreSQL's replication mechanisms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1945xlx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704991074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m the author of this guide to logical replication in Postgres. I break down some of the internal components of Postgres to make CDC easier to understand. I hope you find it informative and learn something new!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql\"&gt;https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?auto=webp&amp;s=9515e66cd8f13f2a1dca61f0163a8a75b0174642", "width": 2400, "height": 1256}, "resolutions": [{"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=83b29078bcf067ed68763f8c17a57e9b04fbc606", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e05004a3058e45746325e4574fe92aa275729d7", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af2b3b70ea1c354bc34d603275f59f90434d4785", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a19775e3bd8feb47c0cbf9cf832166940e0059b5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7334e16a2cabb7e597852b417b1a28e04d9f09b", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/EzsTjjvkYq85tYMJM94Rg-FkeSSfaOVklY-rrZCGrC8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b9eb8649d858aa647185b5a7d241d1ca76f527a", "width": 1080, "height": 565}], "variants": {}, "id": "rZw_N468oAzYIoTZy4SyJC-DaQSJvh480u3XU8PeyuI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1945xlx", "is_robot_indexable": true, "report_reasons": null, "author": "Chemical-Treat6596", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1945xlx/a_technical_dive_into_postgresqls_replication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1945xlx/a_technical_dive_into_postgresqls_replication/", "subreddit_subscribers": 151930, "created_utc": 1704991074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Each data platform I can think of seems to handle database security similarly: They require privileged access to your database/warehouse through an \"admin\" account and then add their security layer on top. This means no central access governance and no centralized logs, making it hard to put together a picture of who is accessing what. What are people's solutions to this and how do you manage this?", "author_fullname": "t2_q5jcx79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do people manage data security at scale when each vendor adds their security layer on top?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1945pt7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704990534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Each data platform I can think of seems to handle database security similarly: They require privileged access to your database/warehouse through an &amp;quot;admin&amp;quot; account and then add their security layer on top. This means no central access governance and no centralized logs, making it hard to put together a picture of who is accessing what. What are people&amp;#39;s solutions to this and how do you manage this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1945pt7", "is_robot_indexable": true, "report_reasons": null, "author": "bk1007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1945pt7/how_do_people_manage_data_security_at_scale_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1945pt7/how_do_people_manage_data_security_at_scale_when/", "subreddit_subscribers": 151930, "created_utc": 1704990534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I was looking for a useful and interactive program that could help equip me with the appropriate technical skills I need for my analyst role- I\u2019m moving more towards the data engineering side of my project but there are skill gaps I have that I need to fill. I\u2019m just looking for structure without bootcamp commitment. \n\nI came across Data Engineer Academy and looked through some of their content. Does anyone have any experience with DE Academy? How does it compare with other alternatives you can vouch for?", "author_fullname": "t2_4ehev28e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reviews on Data Engineer Academy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_194304v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1704983384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeracademy.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I was looking for a useful and interactive program that could help equip me with the appropriate technical skills I need for my analyst role- I\u2019m moving more towards the data engineering side of my project but there are skill gaps I have that I need to fill. I\u2019m just looking for structure without bootcamp commitment. &lt;/p&gt;\n\n&lt;p&gt;I came across Data Engineer Academy and looked through some of their content. Does anyone have any experience with DE Academy? How does it compare with other alternatives you can vouch for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dataengineeracademy.com", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "194304v", "is_robot_indexable": true, "report_reasons": null, "author": "rae190", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/194304v/reviews_on_data_engineer_academy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dataengineeracademy.com", "subreddit_subscribers": 151930, "created_utc": 1704983384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "we\u2019re currently working on our data, and we just entered the the phase where we\u2019re transforming our data using dbt. \n\nI\u2019m trying to watch closely what my team (1 person) is doing and tbh i didn\u2019t notice any changes!\n\nI mean, I find it exactly replicating the data from the data source. that\u2019s it. I can\u2019t see the added value. \n\nWe only re-group data into facts and dimensions, remove duplicates and rename the columns\u2026 I thought this phase will hold A LOT more. So, it\u2019s clear that we didn\u2019t understand how to use it properly. \n\nas an example, hubspot is one of our data sources and I found the transformed data not adding any value.. and we only have 7 tables from this source (?) leads, deals, quotations and contacts.\n\nI\u2019d appreciate if you can give examples of data transformation the people usually use and apply (I totally understand that this depends on the data/business/ business logic) but I\u2019m here trying to understand what are we missing and why i find it useless here.", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data transformation; what does it really mean?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1941wzt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704980310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;we\u2019re currently working on our data, and we just entered the the phase where we\u2019re transforming our data using dbt. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to watch closely what my team (1 person) is doing and tbh i didn\u2019t notice any changes!&lt;/p&gt;\n\n&lt;p&gt;I mean, I find it exactly replicating the data from the data source. that\u2019s it. I can\u2019t see the added value. &lt;/p&gt;\n\n&lt;p&gt;We only re-group data into facts and dimensions, remove duplicates and rename the columns\u2026 I thought this phase will hold A LOT more. So, it\u2019s clear that we didn\u2019t understand how to use it properly. &lt;/p&gt;\n\n&lt;p&gt;as an example, hubspot is one of our data sources and I found the transformed data not adding any value.. and we only have 7 tables from this source (?) leads, deals, quotations and contacts.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d appreciate if you can give examples of data transformation the people usually use and apply (I totally understand that this depends on the data/business/ business logic) but I\u2019m here trying to understand what are we missing and why i find it useless here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1941wzt", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1941wzt/data_transformation_what_does_it_really_mean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1941wzt/data_transformation_what_does_it_really_mean/", "subreddit_subscribers": 151930, "created_utc": 1704980310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got an offer from a consulting company as Expert Level Support - Level 3 (There's no Level 4). I originally applied for Big Data Engineer, which they changed later during my interview process. My team manager uses DevOps Manager on his LinkedIn.\n\nI read that some people have tried to negotiate their titles during the job offer stage. Is it a good idea to negotiate it to Staff Data Engineer / Principal Data Engineer or even DevOps Engineer? Which title aligns better with expectations in the industry?\n\nThe thing is, I'm not sure if I would apply to another DE role in the future or try DevOps. Perhaps the vagueness of the current title could work in my favor if I ever want to explore other opportunities, perhaps not. Are there significantly more remote opportunities for DE over DevOps? That could be a factor.", "author_fullname": "t2_5qu7qetq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Negotiating job title - Staff Data Engineer or Expert Level Support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19412ic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704977724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an offer from a consulting company as Expert Level Support - Level 3 (There&amp;#39;s no Level 4). I originally applied for Big Data Engineer, which they changed later during my interview process. My team manager uses DevOps Manager on his LinkedIn.&lt;/p&gt;\n\n&lt;p&gt;I read that some people have tried to negotiate their titles during the job offer stage. Is it a good idea to negotiate it to Staff Data Engineer / Principal Data Engineer or even DevOps Engineer? Which title aligns better with expectations in the industry?&lt;/p&gt;\n\n&lt;p&gt;The thing is, I&amp;#39;m not sure if I would apply to another DE role in the future or try DevOps. Perhaps the vagueness of the current title could work in my favor if I ever want to explore other opportunities, perhaps not. Are there significantly more remote opportunities for DE over DevOps? That could be a factor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19412ic", "is_robot_indexable": true, "report_reasons": null, "author": "aguhon", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19412ic/negotiating_job_title_staff_data_engineer_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19412ic/negotiating_job_title_staff_data_engineer_or/", "subreddit_subscribers": 151930, "created_utc": 1704977724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2026how good is it possible, and let\u2019s say only one person in charge plus a remote support person (freelancer, consultant), in case of vacation, longer absence etc.\n\nHow low can be the monthly costs for productive 300 GB data warehouse ?\n\nIt can be cloud based or on-premise.\n\nDo you have ideas or experiences - in midsize companies (below 250 persons) or small companies (1-25 persons)?", "author_fullname": "t2_h8o6jb9k1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Build a (semi) automatic DWH for small midsize companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193uz6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704954172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2026how good is it possible, and let\u2019s say only one person in charge plus a remote support person (freelancer, consultant), in case of vacation, longer absence etc.&lt;/p&gt;\n\n&lt;p&gt;How low can be the monthly costs for productive 300 GB data warehouse ?&lt;/p&gt;\n\n&lt;p&gt;It can be cloud based or on-premise.&lt;/p&gt;\n\n&lt;p&gt;Do you have ideas or experiences - in midsize companies (below 250 persons) or small companies (1-25 persons)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193uz6t", "is_robot_indexable": true, "report_reasons": null, "author": "volvoboy-85", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193uz6t/build_a_semi_automatic_dwh_for_small_midsize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193uz6t/build_a_semi_automatic_dwh_for_small_midsize/", "subreddit_subscribers": 151930, "created_utc": 1704954172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on the azure stack (sql, adls, adf, databricks) with a team of about 7 total plus some contractors/consultants at time. \n\nIn the past year I have put a lot of effort into creating documentation not only for our environment but also the systems and pipelines we develop and maintain. At the moment these are just word docs on our SharePoint that take a fair bit of time to manage and maintain especially when everyone does it slightly differently. \n\nHow have others approached this? What has worked/not worked? What key information is essential for a pipeline/system? \n\n[View Poll](https://www.reddit.com/poll/193uvbq)", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you document systems and pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193uvbq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704954262.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704953787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on the azure stack (sql, adls, adf, databricks) with a team of about 7 total plus some contractors/consultants at time. &lt;/p&gt;\n\n&lt;p&gt;In the past year I have put a lot of effort into creating documentation not only for our environment but also the systems and pipelines we develop and maintain. At the moment these are just word docs on our SharePoint that take a fair bit of time to manage and maintain especially when everyone does it slightly differently. &lt;/p&gt;\n\n&lt;p&gt;How have others approached this? What has worked/not worked? What key information is essential for a pipeline/system? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/193uvbq\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193uvbq", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1705558587576, "options": [{"text": "It\u2019s a well oiled machine", "id": "26695945"}, {"text": "It\u2019s a mess", "id": "26695946"}, {"text": "We don\u2019t document anything", "id": "26695947"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 25, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193uvbq/how_do_you_document_systems_and_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/193uvbq/how_do_you_document_systems_and_pipelines/", "subreddit_subscribers": 151930, "created_utc": 1704953787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was recently hired as a Lead Data Engineer of a pipeline that will serve ML (Machine Learning) models on a Google Cloud &amp; Snowflake stack. I always learned that  is best to understand the end goal of the data pipeline and if it will serve reporting, analytics, or machine learning. I come from a background of pipelines for mostly reporting and analytics use cases. I will be responsible for architecting, modeling, enhancing, and managing the pipelines.\n\nI am curious as to how a ML use case will fundamentally impact my decisions/approach when it comes to some of following key considerations:\n\n* Data Sources\n* Determining Ingestion Strategy\n* Processing Plan\n* Storage Setup\n* Monitoring and Governance\n* Data Quality and Validation\n* Data Drift Monitoring\n* Consumption Layer\n* Currently ETL (Should I push for the more moden ELT approach)?\n\nThank you in advance for all guidance and insights!", "author_fullname": "t2_8b0w9ipt9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Impact to Key Strategies When Serving a ML Use Case", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193sfou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704945938.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704945690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was recently hired as a Lead Data Engineer of a pipeline that will serve ML (Machine Learning) models on a Google Cloud &amp;amp; Snowflake stack. I always learned that  is best to understand the end goal of the data pipeline and if it will serve reporting, analytics, or machine learning. I come from a background of pipelines for mostly reporting and analytics use cases. I will be responsible for architecting, modeling, enhancing, and managing the pipelines.&lt;/p&gt;\n\n&lt;p&gt;I am curious as to how a ML use case will fundamentally impact my decisions/approach when it comes to some of following key considerations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Sources&lt;/li&gt;\n&lt;li&gt;Determining Ingestion Strategy&lt;/li&gt;\n&lt;li&gt;Processing Plan&lt;/li&gt;\n&lt;li&gt;Storage Setup&lt;/li&gt;\n&lt;li&gt;Monitoring and Governance&lt;/li&gt;\n&lt;li&gt;Data Quality and Validation&lt;/li&gt;\n&lt;li&gt;Data Drift Monitoring&lt;/li&gt;\n&lt;li&gt;Consumption Layer&lt;/li&gt;\n&lt;li&gt;Currently ETL (Should I push for the more moden ELT approach)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you in advance for all guidance and insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193sfou", "is_robot_indexable": true, "report_reasons": null, "author": "JoseyWales10", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193sfou/impact_to_key_strategies_when_serving_a_ml_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193sfou/impact_to_key_strategies_when_serving_a_ml_use/", "subreddit_subscribers": 151930, "created_utc": 1704945690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019ve been working on this machine learning model to detect improper payments. \n\nThe performance isn\u2019t exceptional but it\u2019s acceptable given the data. \n\nThe model is saved and works well. The issue I\u2019m having now is deploying and automating the model to run everyday. \n\nThe process currently is I have a sql automation to export the features needed to input into the model into an excel file. \n\nI use python to open the excel and input the the features into the model and return the probability for each one. \n\nIm thinking the excel file is probably a bit excessive as I could use python to connect to database and gather the features from there then input in the model. Without opening an excel file directly. \n\n\nHowever, after the features are exported. Everything is done manually. What are some ways I could automate running the model daily. \n\n\nI\u2019m thinking of just using a batch file to run the python script. However, I\u2019m not sure if there\u2019s a more efficient way to do this. \n\nAfter that I wanna put this all on a tableau dashboard.\n\nAny suggested work around for this, trainings or whatever?", "author_fullname": "t2_8cjp70ft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Model Deployment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193qo6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704940423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve been working on this machine learning model to detect improper payments. &lt;/p&gt;\n\n&lt;p&gt;The performance isn\u2019t exceptional but it\u2019s acceptable given the data. &lt;/p&gt;\n\n&lt;p&gt;The model is saved and works well. The issue I\u2019m having now is deploying and automating the model to run everyday. &lt;/p&gt;\n\n&lt;p&gt;The process currently is I have a sql automation to export the features needed to input into the model into an excel file. &lt;/p&gt;\n\n&lt;p&gt;I use python to open the excel and input the the features into the model and return the probability for each one. &lt;/p&gt;\n\n&lt;p&gt;Im thinking the excel file is probably a bit excessive as I could use python to connect to database and gather the features from there then input in the model. Without opening an excel file directly. &lt;/p&gt;\n\n&lt;p&gt;However, after the features are exported. Everything is done manually. What are some ways I could automate running the model daily. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking of just using a batch file to run the python script. However, I\u2019m not sure if there\u2019s a more efficient way to do this. &lt;/p&gt;\n\n&lt;p&gt;After that I wanna put this all on a tableau dashboard.&lt;/p&gt;\n\n&lt;p&gt;Any suggested work around for this, trainings or whatever?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "193qo6p", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainVJ", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193qo6p/model_deployment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193qo6p/model_deployment/", "subreddit_subscribers": 151930, "created_utc": 1704940423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically the title, what kind of roles should I look at in FAANG as an ETL dev? My skills are adf, python, sql, warehousing.\n\nPlease also tell me how you find the right job for you?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of jobs in FAANG for an ETL dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_194728m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704993847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title, what kind of roles should I look at in FAANG as an ETL dev? My skills are adf, python, sql, warehousing.&lt;/p&gt;\n\n&lt;p&gt;Please also tell me how you find the right job for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "194728m", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/194728m/what_kind_of_jobs_in_faang_for_an_etl_dev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/194728m/what_kind_of_jobs_in_faang_for_an_etl_dev/", "subreddit_subscribers": 151930, "created_utc": 1704993847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow Redditors,\n\n&amp;#x200B;\n\nI've been working as a data engineer with expertise in Google Cloud Platform (GCP) for a while now. Despite my proficiency in GCP, I've been noticing fewer opportunities in the market and I'm contemplating expanding my skill set to include AWS.\n\n&amp;#x200B;\n\nFor those who have made a similar transition or have experience with both platforms, how familiar is AWS for someone who knows GCP well? What challenges did you face, and how did you overcome them? Are there any specific resources or learning paths you recommend for someone in my position?\n\n&amp;#x200B;\n\nI appreciate any insights or advice you can share. Thanks in advance!", "author_fullname": "t2_hm7bbgnf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering a Shift from GCP to AWS: Data Engineer Seeking Advice on Transitioning and Market Opportunities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19466ch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704991690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Redditors,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a data engineer with expertise in Google Cloud Platform (GCP) for a while now. Despite my proficiency in GCP, I&amp;#39;ve been noticing fewer opportunities in the market and I&amp;#39;m contemplating expanding my skill set to include AWS.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For those who have made a similar transition or have experience with both platforms, how familiar is AWS for someone who knows GCP well? What challenges did you face, and how did you overcome them? Are there any specific resources or learning paths you recommend for someone in my position?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I appreciate any insights or advice you can share. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19466ch", "is_robot_indexable": true, "report_reasons": null, "author": "TheOtherNormalOne", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19466ch/considering_a_shift_from_gcp_to_aws_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19466ch/considering_a_shift_from_gcp_to_aws_data_engineer/", "subreddit_subscribers": 151930, "created_utc": 1704991690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been fighting a back-and-forth battle with one project in particular for awhile now at my job and would love some outside perspective on this.\n\nTHE APP:\n\nWithout going too in the weeds, I'm using Python's concurrent futures library to, in parallel, generate a few hundred thousand pandas dataframes. Each dataframe is time-series data with 360 rows. These dataframes currently get inserted to a MS SQL Server table with no indexes or pkeys (these are built once all inserts are done). The connection driver is mssql+pyodbc.\n\nThis data is heavily aggregated in various ways by the end users. \n\nTHE PROBLEM:\n\nOur SQL Server is SO moody and finnicky when dealing with the parallel inserts. The most common error is a connection timeout, but it could be one of a handful of different errors. I work at a huge corp, and any SQL Server-side changes have to go through a lot of red tape and a DBA who works on a different floor. I've managed to allocate more memory and storage to our db to help the process, and while performance has improved, it's still far from production-ready.\n\nTHE CONSTRAINTS:\n\nOur corp's tech stack is a petty gnarly Frankenstein's monster of on-prem stuff. Our two primary database options are MS SQL Server and Dremio (if you even want to call that a db). Compute is handled by an on-prem Kubernetes cluster. We have S3 storage as well. I don't have the ability to \"just throw it on Cloud BigTable\" or anything like that. \n\n&amp;#x200B;\n\nGOAL:\n\nImprove reliability of this solution without sacrificing the end user's ability to run their aggregations and analytics quickly.\n\n&amp;#x200B;\n\nAs far as I can tell, my only other option within these constraints is to write each dataframe as a parquet to S3 and use Dremio to read it like a DB. I'm just worried that analytics will be way slower under these circumstances.\n\n&amp;#x200B;\n\nWould greatly appreciate any insight here!", "author_fullname": "t2_cdyao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to find ideal tech stack solution within the constraints of my giant corp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1945vbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704990917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been fighting a back-and-forth battle with one project in particular for awhile now at my job and would love some outside perspective on this.&lt;/p&gt;\n\n&lt;p&gt;THE APP:&lt;/p&gt;\n\n&lt;p&gt;Without going too in the weeds, I&amp;#39;m using Python&amp;#39;s concurrent futures library to, in parallel, generate a few hundred thousand pandas dataframes. Each dataframe is time-series data with 360 rows. These dataframes currently get inserted to a MS SQL Server table with no indexes or pkeys (these are built once all inserts are done). The connection driver is mssql+pyodbc.&lt;/p&gt;\n\n&lt;p&gt;This data is heavily aggregated in various ways by the end users. &lt;/p&gt;\n\n&lt;p&gt;THE PROBLEM:&lt;/p&gt;\n\n&lt;p&gt;Our SQL Server is SO moody and finnicky when dealing with the parallel inserts. The most common error is a connection timeout, but it could be one of a handful of different errors. I work at a huge corp, and any SQL Server-side changes have to go through a lot of red tape and a DBA who works on a different floor. I&amp;#39;ve managed to allocate more memory and storage to our db to help the process, and while performance has improved, it&amp;#39;s still far from production-ready.&lt;/p&gt;\n\n&lt;p&gt;THE CONSTRAINTS:&lt;/p&gt;\n\n&lt;p&gt;Our corp&amp;#39;s tech stack is a petty gnarly Frankenstein&amp;#39;s monster of on-prem stuff. Our two primary database options are MS SQL Server and Dremio (if you even want to call that a db). Compute is handled by an on-prem Kubernetes cluster. We have S3 storage as well. I don&amp;#39;t have the ability to &amp;quot;just throw it on Cloud BigTable&amp;quot; or anything like that. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;GOAL:&lt;/p&gt;\n\n&lt;p&gt;Improve reliability of this solution without sacrificing the end user&amp;#39;s ability to run their aggregations and analytics quickly.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As far as I can tell, my only other option within these constraints is to write each dataframe as a parquet to S3 and use Dremio to read it like a DB. I&amp;#39;m just worried that analytics will be way slower under these circumstances.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would greatly appreciate any insight here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1945vbc", "is_robot_indexable": true, "report_reasons": null, "author": "ttothesecond", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1945vbc/struggling_to_find_ideal_tech_stack_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1945vbc/struggling_to_find_ideal_tech_stack_solution/", "subreddit_subscribers": 151930, "created_utc": 1704990917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to auto indent any errors in the yaml file with respect to indentation. did anyone of you implement it or any suggestions would be of great help. I am trying using ruaeml.yaml and regex..but with not much progress yet", "author_fullname": "t2_ajjg3i2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YAML errors auto indent using python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19450qg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704988775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to auto indent any errors in the yaml file with respect to indentation. did anyone of you implement it or any suggestions would be of great help. I am trying using ruaeml.yaml and regex..but with not much progress yet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19450qg", "is_robot_indexable": true, "report_reasons": null, "author": "MediumZealousideal29", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19450qg/yaml_errors_auto_indent_using_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19450qg/yaml_errors_auto_indent_using_python/", "subreddit_subscribers": 151930, "created_utc": 1704988775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are folks using to ingest large tabular files from Java into Parquet and friends?\n\nThe data is ultimately going to be reprocessed/joined with other tables and queried through an engine like Trino.\n\nToday a dataset will come in as various tabular formats, our users clean and map it using the Java backend, and it's stored in PostgreSQL. I love postgres, but we're bumping up against size and cost limits for some of the larger datasets; each dataset gets its own postgres instance while it's actively being used. I'd like to try moving away from it and toward the Iceberg ecosystem with object storage and a caching layer.\n\nOnce the data is already in object storage, or in a relational database, there's lots of ways to convert between formats using Trino or Spark, and tons of benchmarks and guides. But what's the best way to get it in there in the first place? How is big data formed?\n\nI've used ParquetWriter and Avro GenericData.Record a bit in the past. It's quite low-level. I'd like to have a bit of control over how the Parquet files are sorted and partitioned, either during the ingestion process or with some post-processing. I'll bet you can do anything with ParquetWriter and the Iceberg library, but getting it right, so the sorting/partitioning I do in the app, matches what I put in the metadata, would be hard, and the kind of thing I'd normally want a DBMS to handle.\n\nI was wondering about bulk loading into Trino via JDBC. I haven't been able to find performance numbers or guidance on whether this is a sane approach. I'm open to almost anything: using ParquetWriter directly, or DuckDB, or a JDBC connection to Trino or StarRocks, whatever makes sense.", "author_fullname": "t2_4m8rzb2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting to Parquet/Iceberg/object storage from Java", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1941ulb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704980107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are folks using to ingest large tabular files from Java into Parquet and friends?&lt;/p&gt;\n\n&lt;p&gt;The data is ultimately going to be reprocessed/joined with other tables and queried through an engine like Trino.&lt;/p&gt;\n\n&lt;p&gt;Today a dataset will come in as various tabular formats, our users clean and map it using the Java backend, and it&amp;#39;s stored in PostgreSQL. I love postgres, but we&amp;#39;re bumping up against size and cost limits for some of the larger datasets; each dataset gets its own postgres instance while it&amp;#39;s actively being used. I&amp;#39;d like to try moving away from it and toward the Iceberg ecosystem with object storage and a caching layer.&lt;/p&gt;\n\n&lt;p&gt;Once the data is already in object storage, or in a relational database, there&amp;#39;s lots of ways to convert between formats using Trino or Spark, and tons of benchmarks and guides. But what&amp;#39;s the best way to get it in there in the first place? How is big data formed?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used ParquetWriter and Avro GenericData.Record a bit in the past. It&amp;#39;s quite low-level. I&amp;#39;d like to have a bit of control over how the Parquet files are sorted and partitioned, either during the ingestion process or with some post-processing. I&amp;#39;ll bet you can do anything with ParquetWriter and the Iceberg library, but getting it right, so the sorting/partitioning I do in the app, matches what I put in the metadata, would be hard, and the kind of thing I&amp;#39;d normally want a DBMS to handle.&lt;/p&gt;\n\n&lt;p&gt;I was wondering about bulk loading into Trino via JDBC. I haven&amp;#39;t been able to find performance numbers or guidance on whether this is a sane approach. I&amp;#39;m open to almost anything: using ParquetWriter directly, or DuckDB, or a JDBC connection to Trino or StarRocks, whatever makes sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1941ulb", "is_robot_indexable": true, "report_reasons": null, "author": "t0astix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1941ulb/ingesting_to_parqueticebergobject_storage_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1941ulb/ingesting_to_parqueticebergobject_storage_from/", "subreddit_subscribers": 151930, "created_utc": 1704980107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone,\n\nI have been tasked to pull data from Sage Business Account Software for the finance team and create a data warehouse.  \n\n\nI don't have an issue with the other process just I haven't connected Azure Data Factory or Synapse Pipeline with Sage Business Account Software before and there is little to no material online on how to go about it.  \n\n\nI would appreciate it if I could get any support, guidance or link on how to configure ADF/Synapes Pipeline to connect with Sage.  \n\n\nThanks house!  \n", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrate Azure Data Factory with Sage Business Account Software(Cloud)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193yc3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704967841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I have been tasked to pull data from Sage Business Account Software for the finance team and create a data warehouse.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have an issue with the other process just I haven&amp;#39;t connected Azure Data Factory or Synapse Pipeline with Sage Business Account Software before and there is little to no material online on how to go about it.  &lt;/p&gt;\n\n&lt;p&gt;I would appreciate it if I could get any support, guidance or link on how to configure ADF/Synapes Pipeline to connect with Sage.  &lt;/p&gt;\n\n&lt;p&gt;Thanks house!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "193yc3e", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193yc3e/integrate_azure_data_factory_with_sage_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193yc3e/integrate_azure_data_factory_with_sage_business/", "subreddit_subscribers": 151930, "created_utc": 1704967841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was developing some internal documentation about phases in data engineering projects and I would love some feedback.\n\nBy now, the phases I have are:\n\n1. Requirements gathering\n2. Data architecture design\n3. Time planning\n3. Develop and implement\n4. Testing\n5. Deploy on production\n6. Maintenance\n7. Evaluation\n\nDid I forget something important? Obviously, each point is explained in detail in the docs.\n\nThank you all in advance!", "author_fullname": "t2_p4nzh8v1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering project phases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193y54p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704967083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was developing some internal documentation about phases in data engineering projects and I would love some feedback.&lt;/p&gt;\n\n&lt;p&gt;By now, the phases I have are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Requirements gathering&lt;/li&gt;\n&lt;li&gt;Data architecture design&lt;/li&gt;\n&lt;li&gt;Time planning&lt;/li&gt;\n&lt;li&gt;Develop and implement&lt;/li&gt;\n&lt;li&gt;Testing&lt;/li&gt;\n&lt;li&gt;Deploy on production&lt;/li&gt;\n&lt;li&gt;Maintenance&lt;/li&gt;\n&lt;li&gt;Evaluation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Did I forget something important? Obviously, each point is explained in detail in the docs.&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "193y54p", "is_robot_indexable": true, "report_reasons": null, "author": "data_macrolide", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/193y54p/data_engineering_project_phases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193y54p/data_engineering_project_phases/", "subreddit_subscribers": 151930, "created_utc": 1704967083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI\u2019m working as a data engineer from 3 to 4 years in service based startup company. \n\nI am the only one from my company working with a client team which consists of 3 to 4 data engineers. we are helping data scientists to create a data pipeline for their ML model. It is kind of MLOps ,where we industrialize the models and deploy into production. my main concern is I have worked on 3 to 4 industrialization of machine learning models, but I am not sure the impact of that. \n\nThe POC is done by data scientist and it and the model is given to our team, we create a pipeline with data quality, data ingestion, transformations, monitoring, etc. that would be deployed to the production. Application maintenance team take it further.\n\n I am trying to find out what Impact am I making to the team and the company but I am not able to understand from where I\u2019ll get to know what the Impact is. There are few models which has been, which has never been used by the business, but we still do some of the enhancements.\nWhat is the best way to create an impact in our projects, how did you do?", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Impact on data engineering projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_193wa6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704959288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I\u2019m working as a data engineer from 3 to 4 years in service based startup company. &lt;/p&gt;\n\n&lt;p&gt;I am the only one from my company working with a client team which consists of 3 to 4 data engineers. we are helping data scientists to create a data pipeline for their ML model. It is kind of MLOps ,where we industrialize the models and deploy into production. my main concern is I have worked on 3 to 4 industrialization of machine learning models, but I am not sure the impact of that. &lt;/p&gt;\n\n&lt;p&gt;The POC is done by data scientist and it and the model is given to our team, we create a pipeline with data quality, data ingestion, transformations, monitoring, etc. that would be deployed to the production. Application maintenance team take it further.&lt;/p&gt;\n\n&lt;p&gt;I am trying to find out what Impact am I making to the team and the company but I am not able to understand from where I\u2019ll get to know what the Impact is. There are few models which has been, which has never been used by the business, but we still do some of the enhancements.\nWhat is the best way to create an impact in our projects, how did you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "193wa6n", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/193wa6n/impact_on_data_engineering_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/193wa6n/impact_on_data_engineering_projects/", "subreddit_subscribers": 151930, "created_utc": 1704959288.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}