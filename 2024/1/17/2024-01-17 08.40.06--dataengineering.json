{"kind": "Listing", "data": {"after": "t3_198diz2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. \n\nWe ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. \n\nThey've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. \n\nAll of them were looking for jobs for at least 4-6 months. \n\nJust wanted to share a data point on the state of the market last year in 2023. \n\nFunny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company just put out 3 data engineering jobs last year, guess who we got?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198kif0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 159, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 159, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705455890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. &lt;/p&gt;\n\n&lt;p&gt;We ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children&amp;#39;s books. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;ve jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. &lt;/p&gt;\n\n&lt;p&gt;All of them were looking for jobs for at least 4-6 months. &lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a data point on the state of the market last year in 2023. &lt;/p&gt;\n\n&lt;p&gt;Funny thing is that I don&amp;#39;t expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198kif0", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/", "subreddit_subscribers": 153436, "created_utc": 1705455890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Observability for the Semantic Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_197xr9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ppnbfojWVnDY-uQPpl-OPS-IwqTmQ_f9UnJLTy6rExM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705392460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/data-drift/data-drift", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?auto=webp&amp;s=30821265908e8b00ae1b1bce3066cb3b6ac3a205", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f6f5c8474eded858513a20b420867038029e54f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d809875272a979acf827c7c43fff9794e3ebc8f2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bf05bc0e4290bc471544f3bdc55c01f5889a184", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b99e134d159de70cb48f987e34e61c2ea63b3615", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52b25aacea01dfb711235c3da151d500df22fea8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NDjSpV0lexBSsnzsa5Ssm0OLJH8Y7uQqDQR2Db2ikzE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1400901ae833dd6009e520161b25442bf8c7bb81", "width": 1080, "height": 540}], "variants": {}, "id": "7g8Z3kvEUj9VarbdauuCtwALuuUzFlwBYoquOdTH1u8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "197xr9y", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/197xr9y/opensource_observability_for_the_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/data-drift/data-drift", "subreddit_subscribers": 153436, "created_utc": 1705392460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vbapbjo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_197ym19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/197ym19", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/9IDk7XfO1__cGQyCmWI2QjIHiCEVM4sr1ewdeObfVX4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705396045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/aRk6Lk6L5gA?si=9iKT8jej7jKLdy0K", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Vr-eKqVWvTqOk2dSJoNpy5BHuZGe6rMKTPYgDULenqU.jpg?auto=webp&amp;s=3fe7d46d9511238668877cf4e8a6d27ca6e6595d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Vr-eKqVWvTqOk2dSJoNpy5BHuZGe6rMKTPYgDULenqU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d54eb373765389ef33e40cfb02a304d4464c4128", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Vr-eKqVWvTqOk2dSJoNpy5BHuZGe6rMKTPYgDULenqU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22f86c96c9b941e95f4d6d7bfed16c7d02722ab4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Vr-eKqVWvTqOk2dSJoNpy5BHuZGe6rMKTPYgDULenqU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7b74800fc67f19535ab936c0fcaeba2058d8dec", "width": 320, "height": 240}], "variants": {}, "id": "VrUmpf7gHYb0jVrTvpkyd98lRn3v5vdJxtDeGa9WdNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "197ym19", "is_robot_indexable": true, "report_reasons": null, "author": "dnulcon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/197ym19/future_of_big_data_systems_by_spark_creator_matei/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/aRk6Lk6L5gA?si=9iKT8jej7jKLdy0K", "subreddit_subscribers": 153436, "created_utc": 1705396045.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. \nIs there something new in this space ready to replace lakeshouses?", "author_fullname": "t2_9815cpn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is next after Lakehouse architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198lq59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705459248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. \nIs there something new in this space ready to replace lakeshouses?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198lq59", "is_robot_indexable": true, "report_reasons": null, "author": "brokeRichieRich", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198lq59/what_is_next_after_lakehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198lq59/what_is_next_after_lakehouse_architecture/", "subreddit_subscribers": 153436, "created_utc": 1705459248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I nedd help!..So i just got a message from an hiring manager on linkedin who told me to prepare for a 1 hr technical interview(intermediate role) in 2 days after sending my resume. This is my first interview ever and i really dont know how to prepare for this, i looked into the company and i figured they are a data consultant agency which makes the preparation difficult since they will be having a lot of different client using different tools. I really don't want to mess this up, i would appreciate any advice on this..Thank you", "author_fullname": "t2_69eunxvf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1986tw8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705422197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I nedd help!..So i just got a message from an hiring manager on linkedin who told me to prepare for a 1 hr technical interview(intermediate role) in 2 days after sending my resume. This is my first interview ever and i really dont know how to prepare for this, i looked into the company and i figured they are a data consultant agency which makes the preparation difficult since they will be having a lot of different client using different tools. I really don&amp;#39;t want to mess this up, i would appreciate any advice on this..Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1986tw8", "is_robot_indexable": true, "report_reasons": null, "author": "logicdata", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1986tw8/data_engineering_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1986tw8/data_engineering_interview/", "subreddit_subscribers": 153436, "created_utc": 1705422197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,  \n\n\nI have a list of PDFs from which I need to extract table data in automated way. I need one specific table or some important data points from that table. PDFs are from different sources, so the table structures are different from one another. I also need to locate the table in PDF because they appear in different pages every year. I was wondering what would be the most robust way of trying to extract the tables in this case?  \n\n\nThings I have experimented:  \n\n\n1. 3rd party Python packages (pdfplumber, tabula): results were not good enough, these packages couldn't extract tables neatly in consistent manner. They were dividing values/labels into chunks and etc.\n2. openAI gpt-4 chat completions endpoint: very much inconsistent. It is difficult both to locate table in the PDF and extract table or specific data points.\n3. openAI gpt-4 vision API endpoint: I take snapshots of PDF pages and try to extract data using vision endpoint, but because the resolution is not high it makes mistakes.  \n\n\nI need as much Automation as possible for this task. That's why I am even trying to locate the table in PDF in automated way. Do any of you have experience with similar task? Does it even make sense to make an effort on this? If so, what would be the most optimal solution?  \n\n\nSample PDF table which I am trying to extract (let's say I need Total revenue &amp; expense for 2023):  \n\n\nhttps://preview.redd.it/iqa9fdmw1tcc1.png?width=1480&amp;format=png&amp;auto=webp&amp;s=6f18977d4c2a77971b4a62970887c4de3971aac7\n\n&amp;#x200B;", "author_fullname": "t2_aek4m4bd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PDF Table Extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 119, "top_awarded_type": null, "hide_score": false, "media_metadata": {"iqa9fdmw1tcc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 92, "x": 108, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc0daf001a2fed794e5e5945c79d4baf25faffe2"}, {"y": 184, "x": 216, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b478b77908997684c7f80cccdf3fa3365cef9419"}, {"y": 272, "x": 320, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4dd7613d8b7b51e0971504558e0662ce82f6065"}, {"y": 545, "x": 640, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f88240efd330cf4d8b54a0ebd571b87b5643666"}, {"y": 817, "x": 960, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f909ef5566822fdcc5452cde6c671e754727317e"}, {"y": 920, "x": 1080, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b9636c44146861faffcd35c2195da6c982a7413a"}], "s": {"y": 1261, "x": 1480, "u": "https://preview.redd.it/iqa9fdmw1tcc1.png?width=1480&amp;format=png&amp;auto=webp&amp;s=6f18977d4c2a77971b4a62970887c4de3971aac7"}, "id": "iqa9fdmw1tcc1"}}, "name": "t3_19832la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lusk49_z0e9i-eiPX5y6NMpI6qwXd5V3ZbDhHFBrWVU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705412266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,  &lt;/p&gt;\n\n&lt;p&gt;I have a list of PDFs from which I need to extract table data in automated way. I need one specific table or some important data points from that table. PDFs are from different sources, so the table structures are different from one another. I also need to locate the table in PDF because they appear in different pages every year. I was wondering what would be the most robust way of trying to extract the tables in this case?  &lt;/p&gt;\n\n&lt;p&gt;Things I have experimented:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;3rd party Python packages (pdfplumber, tabula): results were not good enough, these packages couldn&amp;#39;t extract tables neatly in consistent manner. They were dividing values/labels into chunks and etc.&lt;/li&gt;\n&lt;li&gt;openAI gpt-4 chat completions endpoint: very much inconsistent. It is difficult both to locate table in the PDF and extract table or specific data points.&lt;/li&gt;\n&lt;li&gt;openAI gpt-4 vision API endpoint: I take snapshots of PDF pages and try to extract data using vision endpoint, but because the resolution is not high it makes mistakes.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I need as much Automation as possible for this task. That&amp;#39;s why I am even trying to locate the table in PDF in automated way. Do any of you have experience with similar task? Does it even make sense to make an effort on this? If so, what would be the most optimal solution?  &lt;/p&gt;\n\n&lt;p&gt;Sample PDF table which I am trying to extract (let&amp;#39;s say I need Total revenue &amp;amp; expense for 2023):  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/iqa9fdmw1tcc1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f18977d4c2a77971b4a62970887c4de3971aac7\"&gt;https://preview.redd.it/iqa9fdmw1tcc1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f18977d4c2a77971b4a62970887c4de3971aac7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19832la", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Cod_9001", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19832la/pdf_table_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19832la/pdf_table_extraction/", "subreddit_subscribers": 153436, "created_utc": 1705412266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I could use some thoughts and advice.\n\nI am at a company with 200 employees and just a few data people. Will add more data people soon. We use Snowflake, planning on using DBT as well.\n\nCurrently we are querying a combination of event tables and transactional database tables and have no comprehensive strategy for data modeling. Lots of random views created by various engineers.\n\n* What data modeling approaches have you tried and found most useful? What strikes the right balance of being easiest to maintain and keeping a single source of truth?\n* What is your opinion and experience with the Unified Star Schema approach?\n* Data Vault?\n* Pure Inmon?\n* if you use Kimball star schema, how do you keep the dim and fact tables in a useful multi fact warehouse instead of a bunch of disparate data marts?\n* Do you have any success stories you can share with me? I want to know what it's ke once data is successfully modeled and cleaned.\n\nSorry if this isn't the most coherent question. I don't know what I don't know.", "author_fullname": "t2_jn9782e69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small data team beginning data modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198fqk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705443688.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705443483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I could use some thoughts and advice.&lt;/p&gt;\n\n&lt;p&gt;I am at a company with 200 employees and just a few data people. Will add more data people soon. We use Snowflake, planning on using DBT as well.&lt;/p&gt;\n\n&lt;p&gt;Currently we are querying a combination of event tables and transactional database tables and have no comprehensive strategy for data modeling. Lots of random views created by various engineers.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What data modeling approaches have you tried and found most useful? What strikes the right balance of being easiest to maintain and keeping a single source of truth?&lt;/li&gt;\n&lt;li&gt;What is your opinion and experience with the Unified Star Schema approach?&lt;/li&gt;\n&lt;li&gt;Data Vault?&lt;/li&gt;\n&lt;li&gt;Pure Inmon?&lt;/li&gt;\n&lt;li&gt;if you use Kimball star schema, how do you keep the dim and fact tables in a useful multi fact warehouse instead of a bunch of disparate data marts?&lt;/li&gt;\n&lt;li&gt;Do you have any success stories you can share with me? I want to know what it&amp;#39;s ke once data is successfully modeled and cleaned.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sorry if this isn&amp;#39;t the most coherent question. I don&amp;#39;t know what I don&amp;#39;t know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198fqk7", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Coast_4859", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198fqk7/small_data_team_beginning_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198fqk7/small_data_team_beginning_data_modeling/", "subreddit_subscribers": 153436, "created_utc": 1705443483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m fairly new to both dagster and dbt cloud but do have some experience with both. \n\nI\u2019ve successfully setup a pipeline in dagster that grabs raw data from an API, loads it to cloud storage and copies the raw data from parquet format into snowflake. \n\nI separately have dbt setup to take that raw data and build some data models. \n\nNow I simply want to trigger the dbt cloud job to run once the data pippeline job runs via dagster but I can\u2019t for the life of me figure out the syntax. I\u2019m able to get everything to run in one \u201crun everything\u201d job but this defeats the purpose of having my dbt cloud assets be dependent on the completion of the data pipeline job. Please help???\n\nHas anyone done this before using dbt cloud (not dbt core)?", "author_fullname": "t2_63dasqqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate dbt cloud using Dagster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198o11q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705466168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m fairly new to both dagster and dbt cloud but do have some experience with both. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve successfully setup a pipeline in dagster that grabs raw data from an API, loads it to cloud storage and copies the raw data from parquet format into snowflake. &lt;/p&gt;\n\n&lt;p&gt;I separately have dbt setup to take that raw data and build some data models. &lt;/p&gt;\n\n&lt;p&gt;Now I simply want to trigger the dbt cloud job to run once the data pippeline job runs via dagster but I can\u2019t for the life of me figure out the syntax. I\u2019m able to get everything to run in one \u201crun everything\u201d job but this defeats the purpose of having my dbt cloud assets be dependent on the completion of the data pipeline job. Please help???&lt;/p&gt;\n\n&lt;p&gt;Has anyone done this before using dbt cloud (not dbt core)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198o11q", "is_robot_indexable": true, "report_reasons": null, "author": "skiyogagolfbeer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198o11q/automate_dbt_cloud_using_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198o11q/automate_dbt_cloud_using_dagster/", "subreddit_subscribers": 153436, "created_utc": 1705466168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why starting as a DE might be better than DS, considering maybe going to MLE in the future? I like the DE area and I think maybe strong skills like a DE combined with enough DS could be the key and could it be that DE will be more valued in the future as DS is today?", "author_fullname": "t2_g1c5aj9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choicing beetween DE vs DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19892yb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705427590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why starting as a DE might be better than DS, considering maybe going to MLE in the future? I like the DE area and I think maybe strong skills like a DE combined with enough DS could be the key and could it be that DE will be more valued in the future as DS is today?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19892yb", "is_robot_indexable": true, "report_reasons": null, "author": "M4loka", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19892yb/choicing_beetween_de_vs_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19892yb/choicing_beetween_de_vs_ds/", "subreddit_subscribers": 153436, "created_utc": 1705427590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Network route visualization using pyvista and osmnx](https://preview.redd.it/q0n4jfveqtcc1.png?width=1357&amp;format=png&amp;auto=webp&amp;s=ba5fa1ca2de8fece3098c795d168d5dd5a9d1343)\n\n[Network route visualization using pyvista and osmnx](https://spatial-dev.guru/2024/01/14/network-route-visualization-using-pyvista-and-osmnx/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Network route visualization using pyvista and osmnx", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "media_metadata": {"q0n4jfveqtcc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9abbeb4746ef553f1aef68c8c6debc7c972047b5"}, {"y": 95, "x": 216, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fee2f9de3d2bf3506a1f1acd797a974539a902c"}, {"y": 141, "x": 320, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=52512a48fd5d17b5464324dcdd8fe28842081c9b"}, {"y": 282, "x": 640, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe980a0d3257d5ecaceb8667554af6ce943f7941"}, {"y": 424, "x": 960, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d0b400930a5340127b66e0944670d4f5caae820"}, {"y": 477, "x": 1080, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efaac704c1849865719df7d44b9bb71d709a2666"}], "s": {"y": 600, "x": 1357, "u": "https://preview.redd.it/q0n4jfveqtcc1.png?width=1357&amp;format=png&amp;auto=webp&amp;s=ba5fa1ca2de8fece3098c795d168d5dd5a9d1343"}, "id": "q0n4jfveqtcc1"}}, "name": "t3_19863jk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Y73gmViO0WJP6xXJ9NSWHmIwnM0qZX4ONqMBp6_YRD4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1705420399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q0n4jfveqtcc1.png?width=1357&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba5fa1ca2de8fece3098c795d168d5dd5a9d1343\"&gt;Network route visualization using pyvista and osmnx&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2024/01/14/network-route-visualization-using-pyvista-and-osmnx/\"&gt;Network route visualization using pyvista and osmnx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?auto=webp&amp;s=f76c672e3729075241c2e408d77f800fcf8b6b8b", "width": 1024, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d051fa28286a27adc787c0150ae1e637a461f73d", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40376ac06ae921e17d378596528457dd28ec20c0", "width": 216, "height": 95}, {"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7642985a92596a4c08573bbf334eebe38fcbed85", "width": 320, "height": 141}, {"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a61849bc98d13f7eb57c77c6c5a66e2330d4d0a4", "width": 640, "height": 283}, {"url": "https://external-preview.redd.it/1DeYdjXpDynOJSnSKE2FbMb-scF7xNYcGJESNseGTvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=504776bc5e093fa6617177dcd145005d72f2021f", "width": 960, "height": 424}], "variants": {}, "id": "a-ClTSK0hq4XURzgrNMjNTafXZEzBvgRcq4XTj7ZiPM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19863jk", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19863jk/network_route_visualization_using_pyvista_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19863jk/network_route_visualization_using_pyvista_and/", "subreddit_subscribers": 153436, "created_utc": 1705420399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - danielbeach/fine-tune-openLLaMA: This repo shows how to fine-tune openLLaMA (7b) model on a GPU. (Made for Data Engineers)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1985ama", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GpsXNKoSOQDRIr2irjcJ7xFw-BPJpjO1hI8A8XzQz4c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705418388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/danielbeach/fine-tune-openLLaMA", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?auto=webp&amp;s=937c252f359ef5edccf2fc46ed45985f9084c9ff", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47e8c68f1131503ad73f4ba855e3ebab451ead03", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0f9663c49277299020049492ef7656310be406", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c1a73a1b4aec5105ccbce27299e8f8cef5a4caa", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3a64f4cf634a56fb6ca1e7558ad75ad719cca9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b99e4dc4d4af13ba3477497dac450d1057239844", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qt6Fcxy2wJJxMagDknWEgSkIjvfp3aNWTmdGI14USjY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31c1808058a039ff7d6d8aa62ec94f76892d275c", "width": 1080, "height": 540}], "variants": {}, "id": "b_cOMlzdHKMlrJyYceBFysCEgiK2ma4vc3EwQHqdwo8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1985ama", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1985ama/github_danielbeachfinetuneopenllama_this_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/danielbeach/fine-tune-openLLaMA", "subreddit_subscribers": 153436, "created_utc": 1705418388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I've been using databricks for some time now. I looked into DLT when it first came out and didn't like it. It had a lot of missing features/limitations. \n\nI've since moved to a new org that wants to implement databricks. The sales reps have already turned them on to DLT and I feel pretty uneasy about it.\n\nWhen loading data (files in cloud storage) to an append only bronze table, why use DLT? Why not just use autoloader? \n\nI was watching a demo wherein a SCD2 was able to be created with a simple declaration (from a table stream). You had to tell it the id and the sequence, but what if you had multiple IDs that define uniqueness? And multiple dates to use to figure out the order? Does it support that? \n\n\nAnyway, how far has DLT come? I'd like to keep a config that I can loop through to spin up jobs and run in parallel. Is that even compatible with DLT? I really don't want to be manually creating \"pipelines\" for every single table..", "author_fullname": "t2_43fb03vm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new org thinks databricks DLT can do everything", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198p5hw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705469889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;ve been using databricks for some time now. I looked into DLT when it first came out and didn&amp;#39;t like it. It had a lot of missing features/limitations. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve since moved to a new org that wants to implement databricks. The sales reps have already turned them on to DLT and I feel pretty uneasy about it.&lt;/p&gt;\n\n&lt;p&gt;When loading data (files in cloud storage) to an append only bronze table, why use DLT? Why not just use autoloader? &lt;/p&gt;\n\n&lt;p&gt;I was watching a demo wherein a SCD2 was able to be created with a simple declaration (from a table stream). You had to tell it the id and the sequence, but what if you had multiple IDs that define uniqueness? And multiple dates to use to figure out the order? Does it support that? &lt;/p&gt;\n\n&lt;p&gt;Anyway, how far has DLT come? I&amp;#39;d like to keep a config that I can loop through to spin up jobs and run in parallel. Is that even compatible with DLT? I really don&amp;#39;t want to be manually creating &amp;quot;pipelines&amp;quot; for every single table..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198p5hw", "is_robot_indexable": true, "report_reasons": null, "author": "idiotlog", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198p5hw/my_new_org_thinks_databricks_dlt_can_do_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198p5hw/my_new_org_thinks_databricks_dlt_can_do_everything/", "subreddit_subscribers": 153436, "created_utc": 1705469889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi my brother finished a few years ago his bachlor degree and I want to give him as a gift for his birthday a course that would be relevant for his career and help him in his profession \n\nThx for the helpers", "author_fullname": "t2_6csylq0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relevant Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198cvhr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705436657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi my brother finished a few years ago his bachlor degree and I want to give him as a gift for his birthday a course that would be relevant for his career and help him in his profession &lt;/p&gt;\n\n&lt;p&gt;Thx for the helpers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198cvhr", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Issue4573", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198cvhr/relevant_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198cvhr/relevant_course/", "subreddit_subscribers": 153436, "created_utc": 1705436657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are working at integrating our CRM &amp; ERP systems. However, is this necessary if both were connected to the data warehouse? Is this an either/or situation, or is it valuable to have ERP &amp; CRM integrated whilst having both integrated into the data warehouse?", "author_fullname": "t2_u2vp7ejv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRM-ERP and the Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1987ax9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705423370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are working at integrating our CRM &amp;amp; ERP systems. However, is this necessary if both were connected to the data warehouse? Is this an either/or situation, or is it valuable to have ERP &amp;amp; CRM integrated whilst having both integrated into the data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1987ax9", "is_robot_indexable": true, "report_reasons": null, "author": "-CaptainCapuchin-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1987ax9/crmerp_and_the_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1987ax9/crmerp_and_the_data_warehouse/", "subreddit_subscribers": 153436, "created_utc": 1705423370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a CS undergrad with some exp into ML and LLM applications but i am looking to do more research and projects into the DE side, so thats why i am asking if you guys have any open problems in literature or in general (Problems the ML/AI community as a whole are seeing)   with data structuring, organizing and ingestion for training/fine-tuning or measuring ML models.\n\n&amp;#x200B;\n\nI was thinking about a few problems in this area and would like to hear your feedback on these and suggestions of other topics\n\n1. Orchestrating LLM data for model distilation (i.e LLMs creating data for training smaller models to substitute the larger ones)\n2. general synthetic data generation and filtering for training ML models\n\n&amp;#x200B;\n\nAny more ideas?", "author_fullname": "t2_7xe340s7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for interesting data structuring/ acquisition/ ingestion problems in AI/ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19842tn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705415174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a CS undergrad with some exp into ML and LLM applications but i am looking to do more research and projects into the DE side, so thats why i am asking if you guys have any open problems in literature or in general (Problems the ML/AI community as a whole are seeing)   with data structuring, organizing and ingestion for training/fine-tuning or measuring ML models.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was thinking about a few problems in this area and would like to hear your feedback on these and suggestions of other topics&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Orchestrating LLM data for model distilation (i.e LLMs creating data for training smaller models to substitute the larger ones)&lt;/li&gt;\n&lt;li&gt;general synthetic data generation and filtering for training ML models&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any more ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19842tn", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPineapples7791", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19842tn/suggestions_for_interesting_data_structuring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19842tn/suggestions_for_interesting_data_structuring/", "subreddit_subscribers": 153436, "created_utc": 1705415174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've shared a comprehensive blog on migrating Hive\u2019s UDFs, UDTFs, and UDAFs to BigQuery. It\u2019s a deep dive into the practical strategies and best practices for this crucial migration step.\n\n[www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery](http://www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery)\n\nLet's discuss the challenges and solutions. #DataMigration #BigQuery", "author_fullname": "t2_i8mbe4p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring the Transition from Hive to BigQuery: A Detailed Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19810ce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705405420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve shared a comprehensive blog on migrating Hive\u2019s UDFs, UDTFs, and UDAFs to BigQuery. It\u2019s a deep dive into the practical strategies and best practices for this crucial migration step.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery\"&gt;www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s discuss the challenges and solutions. #DataMigration #BigQuery&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?auto=webp&amp;s=ee70d5b6efe1f371b6a470cee2ffe421742dbab9", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30336970d98641055f591b03be284091a99b5a3f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=35282ec6094788aa9284219f8da9d9657b12992a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0078f97e4629ab1393dd8af05e94654dd32d2838", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65b9cee265fce9311e30b903f1c52acd4629da02", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e5b3888799284285083e259d67638b5f1684ee2", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/XC4tDNNKXNvnyPiftnNbSLL2V5y4CaP2aI-bBRa-I3Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79b7c13eee1e9deb1a2a0942fed80c14c6a2a0a5", "width": 1080, "height": 607}], "variants": {}, "id": "t7qoVmAqnxSF_1Q1M-XdTgVvqZ8z-6Cnn0OiXhaEZII"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19810ce", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Collar9129", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19810ce/exploring_the_transition_from_hive_to_bigquery_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19810ce/exploring_the_transition_from_hive_to_bigquery_a/", "subreddit_subscribers": 153436, "created_utc": 1705405420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been searching online and there are many different examples. I am looking at one way which is the worker pod template with config in the airflow. However, there are no samples of config variables or files to set up scheduler or workers? I am looking at running the scheduler on OpenShift and run pods there.", "author_fullname": "t2_j39ngtt1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone able to help point me in the right direction with setting Airflow and KubernetesExecutor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1980oqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705404213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been searching online and there are many different examples. I am looking at one way which is the worker pod template with config in the airflow. However, there are no samples of config variables or files to set up scheduler or workers? I am looking at running the scheduler on OpenShift and run pods there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1980oqn", "is_robot_indexable": true, "report_reasons": null, "author": "Unsure-9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1980oqn/anyone_able_to_help_point_me_in_the_right/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1980oqn/anyone_able_to_help_point_me_in_the_right/", "subreddit_subscribers": 153436, "created_utc": 1705404213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I hope this question is all right for here. I tried to search the sub, but I'm not really sure what to search for to begin with, so here it goes.\n\n&amp;#x200B;\n\nI'll have a database full of posts that have a description, tags, and the body text. I want to create a suggestion functionality where the user clicks a button, select their preferences and receives a number of suggestions based on what they chose or the other posts they liked, etc. \n\nI don't know what can be used for that, what solutions etc, that's why I need your help!\n\nI know elasticsearch have this kind of functionality, as I've seen it being used at my work, but it was for a gigantic pool of items, so maybe it's overkill? Or maybe there's a better solution focused only on suggestions?\n\n&amp;#x200B;\n\nThanks a lot!", "author_fullname": "t2_49azf3fn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie question: What solutions should I use to suggest my users posts based on their preferences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_197zq93", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705400615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this question is all right for here. I tried to search the sub, but I&amp;#39;m not really sure what to search for to begin with, so here it goes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have a database full of posts that have a description, tags, and the body text. I want to create a suggestion functionality where the user clicks a button, select their preferences and receives a number of suggestions based on what they chose or the other posts they liked, etc. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what can be used for that, what solutions etc, that&amp;#39;s why I need your help!&lt;/p&gt;\n\n&lt;p&gt;I know elasticsearch have this kind of functionality, as I&amp;#39;ve seen it being used at my work, but it was for a gigantic pool of items, so maybe it&amp;#39;s overkill? Or maybe there&amp;#39;s a better solution focused only on suggestions?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "197zq93", "is_robot_indexable": true, "report_reasons": null, "author": "izotAcario", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/197zq93/newbie_question_what_solutions_should_i_use_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/197zq93/newbie_question_what_solutions_should_i_use_to/", "subreddit_subscribers": 153436, "created_utc": 1705400615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,   \n\n\none of the dlt contributors wrote up how they created a dlt + dbt pipeline that they run in cloud functions to get some pricing info on real estate data, that is otherwise not available to the general public.  \n\n\nYou can read it on the dbt blog, sharing it [here](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: Free tier serverless GCP dlt + dbt Portugese real estate price analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_197zo50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705400402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,   &lt;/p&gt;\n\n&lt;p&gt;one of the dlt contributors wrote up how they created a dlt + dbt pipeline that they run in cloud functions to get some pricing info on real estate data, that is otherwise not available to the general public.  &lt;/p&gt;\n\n&lt;p&gt;You can read it on the dbt blog, sharing it &lt;a href=\"https://docs.getdbt.com/blog/serverless-dlt-dbt-stack\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "197zo50", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/197zo50/personal_project_free_tier_serverless_gcp_dlt_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/197zo50/personal_project_free_tier_serverless_gcp_dlt_dbt/", "subreddit_subscribers": 153436, "created_utc": 1705400402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone,\n\nIf I put this in the body of my Lookup (variable names changed from real names):\n\n{  \n\u00a0 \"Id\": \"Blah\",  \n\u00a0 \"value\": 0,  \n\u00a0 \"peak\": 1000,  \n\u00a0 \"filters\": {  \n\"StatMin\": \"1983-09-05\",  \n\"StatMax\": \"2024-04-05\",  \n\"ModMin\": \"1969-10-15\",  \n\"ModMax\": \"2023-10-15\"  \n\u00a0 }  \n}\n\nThe Lookup activity runs perfectly.\n\nHowever, if I try to insert a variable:  \n{  \n\u00a0 \"Id\": \"Blah\",  \n\u00a0 \"value\": 0,  \n\u00a0 \"peak\": 1000,  \n\u00a0 \"filters\": {  \n\"StatMin\": \"1983-09-05\",  \n\"StatMax\": \"2024-04-05\",  \n\"ModMin\": \"1969-10-15\",  \n\"ModMax\":  \"@variables('Date')\"  \n\u00a0 }  \n}\n\nI get a pipeline error. What am I doing wrong to format the variable entry properly?\n\nError below:  \nErrorCode=HttpRequestFailedWithClientError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Http request failed with client error, status code 400 BadRequest, please check your activity settings. If you configured a baseUrl that includes path, please make sure it ends with '/'.\n\nRequest URL: \\[REMOVED\\],Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,'", "author_fullname": "t2_tcne0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to put a variable in Azure Data Factory (ADF) API Lookup Activity.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_198qrrp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705475734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt;\n\n&lt;p&gt;If I put this in the body of my Lookup (variable names changed from real names):&lt;/p&gt;\n\n&lt;p&gt;{&lt;br/&gt;\n\u00a0 &amp;quot;Id&amp;quot;: &amp;quot;Blah&amp;quot;,&lt;br/&gt;\n\u00a0 &amp;quot;value&amp;quot;: 0,&lt;br/&gt;\n\u00a0 &amp;quot;peak&amp;quot;: 1000,&lt;br/&gt;\n\u00a0 &amp;quot;filters&amp;quot;: {&lt;br/&gt;\n&amp;quot;StatMin&amp;quot;: &amp;quot;1983-09-05&amp;quot;,&lt;br/&gt;\n&amp;quot;StatMax&amp;quot;: &amp;quot;2024-04-05&amp;quot;,&lt;br/&gt;\n&amp;quot;ModMin&amp;quot;: &amp;quot;1969-10-15&amp;quot;,&lt;br/&gt;\n&amp;quot;ModMax&amp;quot;: &amp;quot;2023-10-15&amp;quot;&lt;br/&gt;\n\u00a0 }&lt;br/&gt;\n}&lt;/p&gt;\n\n&lt;p&gt;The Lookup activity runs perfectly.&lt;/p&gt;\n\n&lt;p&gt;However, if I try to insert a variable:&lt;br/&gt;\n{&lt;br/&gt;\n\u00a0 &amp;quot;Id&amp;quot;: &amp;quot;Blah&amp;quot;,&lt;br/&gt;\n\u00a0 &amp;quot;value&amp;quot;: 0,&lt;br/&gt;\n\u00a0 &amp;quot;peak&amp;quot;: 1000,&lt;br/&gt;\n\u00a0 &amp;quot;filters&amp;quot;: {&lt;br/&gt;\n&amp;quot;StatMin&amp;quot;: &amp;quot;1983-09-05&amp;quot;,&lt;br/&gt;\n&amp;quot;StatMax&amp;quot;: &amp;quot;2024-04-05&amp;quot;,&lt;br/&gt;\n&amp;quot;ModMin&amp;quot;: &amp;quot;1969-10-15&amp;quot;,&lt;br/&gt;\n&amp;quot;ModMax&amp;quot;:  &amp;quot;@variables(&amp;#39;Date&amp;#39;)&amp;quot;&lt;br/&gt;\n\u00a0 }&lt;br/&gt;\n}&lt;/p&gt;\n\n&lt;p&gt;I get a pipeline error. What am I doing wrong to format the variable entry properly?&lt;/p&gt;\n\n&lt;p&gt;Error below:&lt;br/&gt;\nErrorCode=HttpRequestFailedWithClientError,&amp;#39;Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Http request failed with client error, status code 400 BadRequest, please check your activity settings. If you configured a baseUrl that includes path, please make sure it ends with &amp;#39;/&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Request URL: [REMOVED],Source=Microsoft.DataTransfer.ClientLibrary,&amp;#39;&amp;#39;Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,&amp;#39;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198qrrp", "is_robot_indexable": true, "report_reasons": null, "author": "Karsticles", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198qrrp/trying_to_put_a_variable_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198qrrp/trying_to_put_a_variable_in_azure_data_factory/", "subreddit_subscribers": 153436, "created_utc": 1705475734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nHow would you approach a project, where a DWH is to be migrated from on-prem, teradata, to a private cloud, spark based solution?\n\nI only know spark as a processing engine, and from what I've been reading, no one actually uses it for it's data storage functionalities ('databases', 'tables', and so on). \n\nSo I don't know too much about data architecture, as I've always been just implementing stuff that someone designed (mostly ETL pipelines with spark/airflow). And spark was used to get data from point A to point B, with transformations in between.\n\nDoes any of you work with spark as a processing engine for DWH? How does it work? Can DWH even be in a form of parquet files lying in buckets? How would the data architecture be imposed then? Would dbt help here?\n\nAs you can see, I'm new to the architecture side of data engineering, so please forgive me if some of the questions don't make sense. I'd gladly check out any recommended resources. \n\nThanks", "author_fullname": "t2_gejetxj65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark in pair with DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198pgon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705470969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;How would you approach a project, where a DWH is to be migrated from on-prem, teradata, to a private cloud, spark based solution?&lt;/p&gt;\n\n&lt;p&gt;I only know spark as a processing engine, and from what I&amp;#39;ve been reading, no one actually uses it for it&amp;#39;s data storage functionalities (&amp;#39;databases&amp;#39;, &amp;#39;tables&amp;#39;, and so on). &lt;/p&gt;\n\n&lt;p&gt;So I don&amp;#39;t know too much about data architecture, as I&amp;#39;ve always been just implementing stuff that someone designed (mostly ETL pipelines with spark/airflow). And spark was used to get data from point A to point B, with transformations in between.&lt;/p&gt;\n\n&lt;p&gt;Does any of you work with spark as a processing engine for DWH? How does it work? Can DWH even be in a form of parquet files lying in buckets? How would the data architecture be imposed then? Would dbt help here?&lt;/p&gt;\n\n&lt;p&gt;As you can see, I&amp;#39;m new to the architecture side of data engineering, so please forgive me if some of the questions don&amp;#39;t make sense. I&amp;#39;d gladly check out any recommended resources. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198pgon", "is_robot_indexable": true, "report_reasons": null, "author": "Visual-Exercise8031", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198pgon/spark_in_pair_with_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198pgon/spark_in_pair_with_dwh/", "subreddit_subscribers": 153436, "created_utc": 1705470969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Customers run MinIO wherever they need fast, resilient, scalable object storage. MinIO includes several types of replication to make sure that every application is working with the most recent data regardless of where it runs. We\u2019ve gone into great detail about the various replication options available and their best practices in previous posts about [Batch Replication](https://blog.min.io/announcing-minio-batch-framework-batch-replication/), [Site Replication](https://blog.min.io/minio-replication-best-practices/) and [Bucket Replication](https://blog.min.io/active-active-replication/).\u00a0\n\n[https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=replication\\_up\\_to\\_date](https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=replication_up_to_date)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I know replication is up to date?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198oz5d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705469308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Customers run MinIO wherever they need fast, resilient, scalable object storage. MinIO includes several types of replication to make sure that every application is working with the most recent data regardless of where it runs. We\u2019ve gone into great detail about the various replication options available and their best practices in previous posts about &lt;a href=\"https://blog.min.io/announcing-minio-batch-framework-batch-replication/\"&gt;Batch Replication&lt;/a&gt;, &lt;a href=\"https://blog.min.io/minio-replication-best-practices/\"&gt;Site Replication&lt;/a&gt; and &lt;a href=\"https://blog.min.io/active-active-replication/\"&gt;Bucket Replication&lt;/a&gt;.\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=replication_up_to_date\"&gt;https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=replication_up_to_date&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?auto=webp&amp;s=f4724fd8a3c5d0ab52643082abf3c212e88e84a2", "width": 1200, "height": 637}, "resolutions": [{"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f3fe5b1cf76f3492b446267368de262fb427a82", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51e82dadc2ad24789fea119cfc7a4d3830015844", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a84c227f2b9af5e885f9e2811a3d6f9038cc5260", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d24e7b846f1d43a36249fab1b29d546235db036b", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f734414266ea3b6c7120b85f0a55b971e16cfa31", "width": 960, "height": 509}, {"url": "https://external-preview.redd.it/EdiUbUE45qIB7SuzLWpSA6xzdqe1UemnAiYWocQiOVU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b90789748e2f4ec2d094213d499190be7c865dda", "width": 1080, "height": 573}], "variants": {}, "id": "vdalyfwTqzF7eYv7fQnGFdEkm9lbgzpSPrYXuZ-DgGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "198oz5d", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198oz5d/how_do_i_know_replication_is_up_to_date/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198oz5d/how_do_i_know_replication_is_up_to_date/", "subreddit_subscribers": 153436, "created_utc": 1705469308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently in my freshman year of college and am thinking of going into data engineering, and had a few questions before I start my sophmore year (hopefully in data engineering).\n\nWhat are the best majors or minors to pair with data engineering degree. Is it a good idea to to a business major alongside it?\n\nAlso if you have any tips whatsoever on what I should be working on over the summer, or if there's anything I can learn to hopefully get me an internship in my sophmore year or to just get ahead of the competition.", "author_fullname": "t2_64rysrha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for summer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198idro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705450149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently in my freshman year of college and am thinking of going into data engineering, and had a few questions before I start my sophmore year (hopefully in data engineering).&lt;/p&gt;\n\n&lt;p&gt;What are the best majors or minors to pair with data engineering degree. Is it a good idea to to a business major alongside it?&lt;/p&gt;\n\n&lt;p&gt;Also if you have any tips whatsoever on what I should be working on over the summer, or if there&amp;#39;s anything I can learn to hopefully get me an internship in my sophmore year or to just get ahead of the competition.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198idro", "is_robot_indexable": true, "report_reasons": null, "author": "Karma-4U", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198idro/tips_for_summer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198idro/tips_for_summer/", "subreddit_subscribers": 153436, "created_utc": 1705450149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Never used NoSQL before and been a long time BigQuery user. Can you help me think through if BQ or BigTable is the solution I need?\n\nI have to batch ETL a star schema database into our GCP environment every day. Records will populate a database for analysts to query (exact use cases unclear) and populate a web hosted map (the data contain lat/long). The data are case files of user problems that will have at least 3 entries (create complaint -inspect problem-resolution) and an unbounded upper limit of inspections (impossible to know how many are needed (10 is routine)). Let\u2019s call this progressing workflow the \u201cstatus\u201d. Each case file can have one or many problems (maybe a user has 5 issues they need addressed). The problems may be addressed asynchronously. I need to load in the dataset with each entry as case files are added or progress toward resolution. There are about 250K case files and that number could grow a good bit. Depending on table design, the multiple status and problem per case file could lead to a huge number of rows. One bit of good news is that there are a small number of columns to deal with. Thanks for reading this far :)\n\nI can see a few options for where to load data (suggest others if you like!). Writes happen only once per day and this should be much more of a read heavy design. \n1) have a fully denormalized BQ table, deal with redundant data in all columns except the one or two that contain the progressing updates to the case file. \n2) Least favorite just because it sounds non-performant(?): append new info to the status field each day (DML the existing field to have all previous data and then the new data as a record or structure). \n3) Most intriguing for me: use BigTable and have all the static info as a column family. Have the status field update as a time stamped k/v pair in a second column family. \n\nWould populating a map daily (so a read of every case file\u2019s lat/long) be performant from BigTable? Single case file queries (from another component of the web app) would be easy in BigTable I\u2019d think. Is it cheaper to just have the fully denormalized BQ table for the queries? Any thoughts? I can provide more info as questions arise if needed. Thanks!!!", "author_fullname": "t2_kii3tm7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which GCP database to use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198gxzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705446483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Never used NoSQL before and been a long time BigQuery user. Can you help me think through if BQ or BigTable is the solution I need?&lt;/p&gt;\n\n&lt;p&gt;I have to batch ETL a star schema database into our GCP environment every day. Records will populate a database for analysts to query (exact use cases unclear) and populate a web hosted map (the data contain lat/long). The data are case files of user problems that will have at least 3 entries (create complaint -inspect problem-resolution) and an unbounded upper limit of inspections (impossible to know how many are needed (10 is routine)). Let\u2019s call this progressing workflow the \u201cstatus\u201d. Each case file can have one or many problems (maybe a user has 5 issues they need addressed). The problems may be addressed asynchronously. I need to load in the dataset with each entry as case files are added or progress toward resolution. There are about 250K case files and that number could grow a good bit. Depending on table design, the multiple status and problem per case file could lead to a huge number of rows. One bit of good news is that there are a small number of columns to deal with. Thanks for reading this far :)&lt;/p&gt;\n\n&lt;p&gt;I can see a few options for where to load data (suggest others if you like!). Writes happen only once per day and this should be much more of a read heavy design. \n1) have a fully denormalized BQ table, deal with redundant data in all columns except the one or two that contain the progressing updates to the case file. \n2) Least favorite just because it sounds non-performant(?): append new info to the status field each day (DML the existing field to have all previous data and then the new data as a record or structure). \n3) Most intriguing for me: use BigTable and have all the static info as a column family. Have the status field update as a time stamped k/v pair in a second column family. &lt;/p&gt;\n\n&lt;p&gt;Would populating a map daily (so a read of every case file\u2019s lat/long) be performant from BigTable? Single case file queries (from another component of the web app) would be easy in BigTable I\u2019d think. Is it cheaper to just have the fully denormalized BQ table for the queries? Any thoughts? I can provide more info as questions arise if needed. Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198gxzg", "is_robot_indexable": true, "report_reasons": null, "author": "Ornery_Vanilla1902", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198gxzg/which_gcp_database_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198gxzg/which_gcp_database_to_use/", "subreddit_subscribers": 153436, "created_utc": 1705446483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did a project with Python, Airflow, Docker, and Microsoft Azure last month, and I wanted to get some suggestions for it. I created a dataset for video games released between 2000 -2022 using RAWG API for, filtered by PlayStation, Xbox, and PC games. This is my first project with Airflow / Docker and wanted to know if its considered a professional data engineering project to showcase in my portfolio.   Any suggestions on how to improve the GitHub repo to better display what I did would be much appreciated!\n\n[https://github.com/asadgun006/Video-Game-Warehouse](https://github.com/asadgun006/Video-Game-Warehouse)\n\nPs. I did not know that there was already a dataset available on Kaggle before I made this. However, the code for that project seems relatively complex for using the RAWG API for extracting the game details. I was able to do this with the free number of API calls RAWG gives you.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8z6u9wxo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinion on a project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198diz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705438224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a project with Python, Airflow, Docker, and Microsoft Azure last month, and I wanted to get some suggestions for it. I created a dataset for video games released between 2000 -2022 using RAWG API for, filtered by PlayStation, Xbox, and PC games. This is my first project with Airflow / Docker and wanted to know if its considered a professional data engineering project to showcase in my portfolio.   Any suggestions on how to improve the GitHub repo to better display what I did would be much appreciated!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/asadgun006/Video-Game-Warehouse\"&gt;https://github.com/asadgun006/Video-Game-Warehouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ps. I did not know that there was already a dataset available on Kaggle before I made this. However, the code for that project seems relatively complex for using the RAWG API for extracting the game details. I was able to do this with the free number of API calls RAWG gives you.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?auto=webp&amp;s=4f53b858bd4b3ba92870d2ec14f83e3c58e3228f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0731e08662b2182432db6e5c2abd46cbbd9b289", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5434525429dac8fe6daeaa52b1d32b7bfba09f39", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af65078a9f950f04a5b1775309b523380cb3fe9d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dcca6cad11c7ab3f986d7892b3dc21c654af3f49", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dfc2804522e0ef028d8978e0be99e3574d7bdad", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DoDJWOuaTBZZ-43Vkg7D-hwKh7vP3sYojtA6jhBFjvo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5195205b41917b150f661a2a98bbc1ec59bfe6ae", "width": 1080, "height": 540}], "variants": {}, "id": "06C66iH-QVeWIVwidXpFnYBcpscgIAoNrGyRyP51HnQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "198diz2", "is_robot_indexable": true, "report_reasons": null, "author": "AlwaysAsad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198diz2/opinion_on_a_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198diz2/opinion_on_a_project/", "subreddit_subscribers": 153436, "created_utc": 1705438224.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}