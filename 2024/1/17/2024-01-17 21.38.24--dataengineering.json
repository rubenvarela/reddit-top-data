{"kind": "Listing", "data": {"after": "t3_198zyr4", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. \n\nWe ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. \n\nThey've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. \n\nAll of them were looking for jobs for at least 4-6 months. \n\nJust wanted to share a data point on the state of the market last year in 2023. \n\nFunny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company just put out 3 data engineering jobs last year, guess who we got?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198kif0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 359, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 359, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705455890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. &lt;/p&gt;\n\n&lt;p&gt;We ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children&amp;#39;s books. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;ve jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. &lt;/p&gt;\n\n&lt;p&gt;All of them were looking for jobs for at least 4-6 months. &lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a data point on the state of the market last year in 2023. &lt;/p&gt;\n\n&lt;p&gt;Funny thing is that I don&amp;#39;t expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198kif0", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 84, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/", "subreddit_subscribers": 153606, "created_utc": 1705455890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. \nIs there something new in this space ready to replace lakeshouses?", "author_fullname": "t2_9815cpn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is next after Lakehouse architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198lq59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705459248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. \nIs there something new in this space ready to replace lakeshouses?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198lq59", "is_robot_indexable": true, "report_reasons": null, "author": "brokeRichieRich", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198lq59/what_is_next_after_lakehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198lq59/what_is_next_after_lakehouse_architecture/", "subreddit_subscribers": 153606, "created_utc": 1705459248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Ultimate Guide to Unit Testing With dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_198w13u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kn_7noFCcJ0D3GC3fMXCYuMoZKNZe0YzGczP5o_Lm9E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705496494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datagibberish.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datagibberish.com/p/unit-testing-with-dbt", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dMVSnzYeOz4m-v9_lFGHhqBVERA9g-3W0dnAdpsBiQs.jpg?auto=webp&amp;s=64087a3f1c05693b324e04792b5fe96ff216a457", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/dMVSnzYeOz4m-v9_lFGHhqBVERA9g-3W0dnAdpsBiQs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9174980cce3f2892735c33b75d4870bcd8ee80f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/dMVSnzYeOz4m-v9_lFGHhqBVERA9g-3W0dnAdpsBiQs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c099adcd7a125bb2db8ac947f05ddbe3f0d6cb4", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/dMVSnzYeOz4m-v9_lFGHhqBVERA9g-3W0dnAdpsBiQs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f357a9fdd3ad5f6d9af7cad962d97242a563e81", "width": 320, "height": 320}], "variants": {}, "id": "OawLUvz2FSM3rDsbu-rhVg-PnA7GkLGIR6DvBSR00eE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "198w13u", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/198w13u/the_ultimate_guide_to_unit_testing_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datagibberish.com/p/unit-testing-with-dbt", "subreddit_subscribers": 153606, "created_utc": 1705496494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_evm58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hoping fellow DE can give me some feedback on my resume.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1992zlr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GUloKlOt58hPWw9uQa9XsAYI4lv530Z046qTziME1KU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705514287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/utxbz18gh1dc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/utxbz18gh1dc1.png?auto=webp&amp;s=6e53052d7786d5fa629a1612de73d00d4708034c", "width": 772, "height": 928}, "resolutions": [{"url": "https://preview.redd.it/utxbz18gh1dc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33badf8fd6365ccd3af6ca5ecab1f688b274f211", "width": 108, "height": 129}, {"url": "https://preview.redd.it/utxbz18gh1dc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb8e996a2125dd20c89fca1322eb9010f86513d2", "width": 216, "height": 259}, {"url": "https://preview.redd.it/utxbz18gh1dc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6f4e673a161f5d4991bd8acc68a0e9735c371d8", "width": 320, "height": 384}, {"url": "https://preview.redd.it/utxbz18gh1dc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b1c2b36082501e5de6bd43e71319de43fc0a7df", "width": 640, "height": 769}], "variants": {}, "id": "o1z7VMsHfR4YCBavQ4MEahYGIEtDAidvNR4BRBV4bd4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1992zlr", "is_robot_indexable": true, "report_reasons": null, "author": "yason2", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1992zlr/hoping_fellow_de_can_give_me_some_feedback_on_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/utxbz18gh1dc1.png", "subreddit_subscribers": 153606, "created_utc": 1705514287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I've been using databricks for some time now. I looked into DLT when it first came out and didn't like it. It had a lot of missing features/limitations. \n\nI've since moved to a new org that wants to implement databricks. The sales reps have already turned them on to DLT and I feel pretty uneasy about it.\n\nWhen loading data (files in cloud storage) to an append only bronze table, why use DLT? Why not just use autoloader? \n\nI was watching a demo wherein a SCD2 was able to be created with a simple declaration (from a table stream). You had to tell it the id and the sequence, but what if you had multiple IDs that define uniqueness? And multiple dates to use to figure out the order? Does it support that? \n\n\nAnyway, how far has DLT come? I'd like to keep a config that I can loop through to spin up jobs and run in parallel. Is that even compatible with DLT? I really don't want to be manually creating \"pipelines\" for every single table..", "author_fullname": "t2_43fb03vm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new org thinks databricks DLT can do everything", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198p5hw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705469889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;ve been using databricks for some time now. I looked into DLT when it first came out and didn&amp;#39;t like it. It had a lot of missing features/limitations. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve since moved to a new org that wants to implement databricks. The sales reps have already turned them on to DLT and I feel pretty uneasy about it.&lt;/p&gt;\n\n&lt;p&gt;When loading data (files in cloud storage) to an append only bronze table, why use DLT? Why not just use autoloader? &lt;/p&gt;\n\n&lt;p&gt;I was watching a demo wherein a SCD2 was able to be created with a simple declaration (from a table stream). You had to tell it the id and the sequence, but what if you had multiple IDs that define uniqueness? And multiple dates to use to figure out the order? Does it support that? &lt;/p&gt;\n\n&lt;p&gt;Anyway, how far has DLT come? I&amp;#39;d like to keep a config that I can loop through to spin up jobs and run in parallel. Is that even compatible with DLT? I really don&amp;#39;t want to be manually creating &amp;quot;pipelines&amp;quot; for every single table..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198p5hw", "is_robot_indexable": true, "report_reasons": null, "author": "idiotlog", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198p5hw/my_new_org_thinks_databricks_dlt_can_do_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198p5hw/my_new_org_thinks_databricks_dlt_can_do_everything/", "subreddit_subscribers": 153606, "created_utc": 1705469889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I could use some thoughts and advice.\n\nI am at a company with 200 employees and just a few data people. Will add more data people soon. We use Snowflake, planning on using DBT as well.\n\nCurrently we are querying a combination of event tables and transactional database tables and have no comprehensive strategy for data modeling. Lots of random views created by various engineers.\n\n* What data modeling approaches have you tried and found most useful? What strikes the right balance of being easiest to maintain and keeping a single source of truth?\n* What is your opinion and experience with the Unified Star Schema approach?\n* Data Vault?\n* Pure Inmon?\n* if you use Kimball star schema, how do you keep the dim and fact tables in a useful multi fact warehouse instead of a bunch of disparate data marts?\n* Do you have any success stories you can share with me? I want to know what it's ke once data is successfully modeled and cleaned.\n\nSorry if this isn't the most coherent question. I don't know what I don't know.", "author_fullname": "t2_jn9782e69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small data team beginning data modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198fqk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705443688.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705443483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I could use some thoughts and advice.&lt;/p&gt;\n\n&lt;p&gt;I am at a company with 200 employees and just a few data people. Will add more data people soon. We use Snowflake, planning on using DBT as well.&lt;/p&gt;\n\n&lt;p&gt;Currently we are querying a combination of event tables and transactional database tables and have no comprehensive strategy for data modeling. Lots of random views created by various engineers.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What data modeling approaches have you tried and found most useful? What strikes the right balance of being easiest to maintain and keeping a single source of truth?&lt;/li&gt;\n&lt;li&gt;What is your opinion and experience with the Unified Star Schema approach?&lt;/li&gt;\n&lt;li&gt;Data Vault?&lt;/li&gt;\n&lt;li&gt;Pure Inmon?&lt;/li&gt;\n&lt;li&gt;if you use Kimball star schema, how do you keep the dim and fact tables in a useful multi fact warehouse instead of a bunch of disparate data marts?&lt;/li&gt;\n&lt;li&gt;Do you have any success stories you can share with me? I want to know what it&amp;#39;s ke once data is successfully modeled and cleaned.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sorry if this isn&amp;#39;t the most coherent question. I don&amp;#39;t know what I don&amp;#39;t know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198fqk7", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Coast_4859", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198fqk7/small_data_team_beginning_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198fqk7/small_data_team_beginning_data_modeling/", "subreddit_subscribers": 153606, "created_utc": 1705443483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intro to SQL Indexes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_1991uni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2DRFBEjOvA1bk39vxVMwPZ3SCS2QfqVEESm-rljHqYQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705511648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/intro-to-sql-indexes", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?auto=webp&amp;s=db0dd46d348e43b93993efd89a3ab3188a2f750b", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb2ea7d2253e0b13f31c2b3a501bdb079c99f02a", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=774a9c7b5b1e9f2a6f0c3ad95da0fc6d8a27e60d", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59a28b06415dd585266957e277e86b821c702bb2", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe6f8284bd45732fa8e0bf3c2c0a5161ba9cfde4", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/jv05JgvI3EHf1zXyxBXB7grYHCq0OAKya3QUKdXwQt0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fadc5fdf695a74ba6f949375c70332ad149ab17d", "width": 960, "height": 562}], "variants": {}, "id": "Xv37p1IZSW1jS6YQK2dPjfcAGpVMaRZFFbZVqXsuRIE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1991uni", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1991uni/intro_to_sql_indexes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/intro-to-sql-indexes", "subreddit_subscribers": 153606, "created_utc": 1705511648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\n&amp;#x200B;\n\nI'm currently learning Dagster (open source). At the moment I'm stucked on setting up the snowflake IO Manager. I want to load all my tables which are in my database\\_test DB stored in my schema\\_test schema.\n\n&amp;#x200B;\n\nFollowing the docs ( [Using Dagster with Snowflake | Dagster Docs](https://docs.dagster.io/integrations/snowflake/using-snowflake-with-dagster) ) didn't help at all.\n\n&amp;#x200B;\n\nWhat's the easiest code example to connect to my Snowflake DB?\n\n&amp;#x200B;\n\nThanks in advance!", "author_fullname": "t2_19ci35mg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster - Snowflake IO Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198wapm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705497600.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705497344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently learning Dagster (open source). At the moment I&amp;#39;m stucked on setting up the snowflake IO Manager. I want to load all my tables which are in my database_test DB stored in my schema_test schema.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Following the docs ( &lt;a href=\"https://docs.dagster.io/integrations/snowflake/using-snowflake-with-dagster\"&gt;Using Dagster with Snowflake | Dagster Docs&lt;/a&gt; ) didn&amp;#39;t help at all.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the easiest code example to connect to my Snowflake DB?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?auto=webp&amp;s=d9eec3bfbfc3fd565643229f8a84c399bc1fe73b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=499be57a5b257c149c9417e04fb72b227c3fc6bc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41c03559a31ecfba73ee7ff44bd6293216bcd5f6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1f3696c3a4f7025280b0a98110eadfe130536c4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a12aeb324dd791c5dbd277a3131545efcfa5b3fe", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bceae394da1ca70512dad8f5ec6ce1e142472e86", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29800a80de6287c32b91b05133f7c8e4a2d57eb2", "width": 1080, "height": 567}], "variants": {}, "id": "gUU0nwKC8lE8qWjp6y6pjjOolTxbCTTaSrkKGe3Kq6A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198wapm", "is_robot_indexable": true, "report_reasons": null, "author": "JEY1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198wapm/dagster_snowflake_io_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198wapm/dagster_snowflake_io_manager/", "subreddit_subscribers": 153606, "created_utc": 1705497344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m fairly new to both dagster and dbt cloud but do have some experience with both. \n\nI\u2019ve successfully setup a pipeline in dagster that grabs raw data from an API, loads it to cloud storage and copies the raw data from parquet format into snowflake. \n\nI separately have dbt setup to take that raw data and build some data models. \n\nNow I simply want to trigger the dbt cloud job to run once the data pippeline job runs via dagster but I can\u2019t for the life of me figure out the syntax. I\u2019m able to get everything to run in one \u201crun everything\u201d job but this defeats the purpose of having my dbt cloud assets be dependent on the completion of the data pipeline job. Please help???\n\nHas anyone done this before using dbt cloud (not dbt core)?", "author_fullname": "t2_63dasqqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate dbt cloud using Dagster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198o11q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705466168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m fairly new to both dagster and dbt cloud but do have some experience with both. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve successfully setup a pipeline in dagster that grabs raw data from an API, loads it to cloud storage and copies the raw data from parquet format into snowflake. &lt;/p&gt;\n\n&lt;p&gt;I separately have dbt setup to take that raw data and build some data models. &lt;/p&gt;\n\n&lt;p&gt;Now I simply want to trigger the dbt cloud job to run once the data pippeline job runs via dagster but I can\u2019t for the life of me figure out the syntax. I\u2019m able to get everything to run in one \u201crun everything\u201d job but this defeats the purpose of having my dbt cloud assets be dependent on the completion of the data pipeline job. Please help???&lt;/p&gt;\n\n&lt;p&gt;Has anyone done this before using dbt cloud (not dbt core)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "198o11q", "is_robot_indexable": true, "report_reasons": null, "author": "skiyogagolfbeer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198o11q/automate_dbt_cloud_using_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198o11q/automate_dbt_cloud_using_dagster/", "subreddit_subscribers": 153606, "created_utc": 1705466168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I'm building a data-focused software app, so while I don't require the same scale that traditional data engineering requires (due to lack of actual users so far lol), I'd still like some DE input.\n\nThe app is surrounding League of Legends, and the part of the software I'm concerned with right now is getting the data from the Riot API into our database.\n\nI have to process one player at a time, because the API call is on a per-player/per-match basis\n\nThe steps are essentially:\n\n1. Get the match IDs that a player has played\n2. Loop through these match IDs to make an API call, that returns the actual in-game data for that particular match\n3. Append all this data into a pandas dataframe\n   1. I know pandas is looked down upon as a transformation tool, but the total matches for each player is (at max) 1000 rows and \\~200 columns, which always fits within memory\n4. CAVEAT - due to the game itself changing over time, they are continually adding new columns/stats that are tracked in matches, so there are a changing # of columns returned by the API\n5. Because of the caveat, I take the intersection of the columns we have in the database already, and those returned by the API. I \"manually\" add the columns we don't have in the database `ALTER TABLE ADD COLUMN`\n6. I then just run a `df.to_sql()` to get all the matches into the database\n\nI realize that this isn't the best approach, because to have a table that is continually changing can lead to troubles.\n\nI would ideally like to store ALL data, but I really only need a subset of the data for the actual product that I'm trying to create.\n\n# The main question I have is, how should I handle extra fields being added to the API response?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database design for my software app - how to handle changing fields from API response?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1995ho8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705520216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m building a data-focused software app, so while I don&amp;#39;t require the same scale that traditional data engineering requires (due to lack of actual users so far lol), I&amp;#39;d still like some DE input.&lt;/p&gt;\n\n&lt;p&gt;The app is surrounding League of Legends, and the part of the software I&amp;#39;m concerned with right now is getting the data from the Riot API into our database.&lt;/p&gt;\n\n&lt;p&gt;I have to process one player at a time, because the API call is on a per-player/per-match basis&lt;/p&gt;\n\n&lt;p&gt;The steps are essentially:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get the match IDs that a player has played&lt;/li&gt;\n&lt;li&gt;Loop through these match IDs to make an API call, that returns the actual in-game data for that particular match&lt;/li&gt;\n&lt;li&gt;Append all this data into a pandas dataframe\n\n&lt;ol&gt;\n&lt;li&gt;I know pandas is looked down upon as a transformation tool, but the total matches for each player is (at max) 1000 rows and ~200 columns, which always fits within memory&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;CAVEAT - due to the game itself changing over time, they are continually adding new columns/stats that are tracked in matches, so there are a changing # of columns returned by the API&lt;/li&gt;\n&lt;li&gt;Because of the caveat, I take the intersection of the columns we have in the database already, and those returned by the API. I &amp;quot;manually&amp;quot; add the columns we don&amp;#39;t have in the database &lt;code&gt;ALTER TABLE ADD COLUMN&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;I then just run a &lt;code&gt;df.to_sql()&lt;/code&gt; to get all the matches into the database&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I realize that this isn&amp;#39;t the best approach, because to have a table that is continually changing can lead to troubles.&lt;/p&gt;\n\n&lt;p&gt;I would ideally like to store ALL data, but I really only need a subset of the data for the actual product that I&amp;#39;m trying to create.&lt;/p&gt;\n\n&lt;h1&gt;The main question I have is, how should I handle extra fields being added to the API response?&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1995ho8", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1995ho8/database_design_for_my_software_app_how_to_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1995ho8/database_design_for_my_software_app_how_to_handle/", "subreddit_subscribers": 153606, "created_utc": 1705520216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello experienced devs,\nI am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. \n\nI am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I\u2019m going to ask for. Since I\u2019m a contractor, I cannot connect/ask for guidance in my workplace\n\nCouple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. \n\nOne day I found out that what I was doing was not very different from our in-house data analyst\u2019s role. \n\nThen, I learned Azure services and got a job as a contractor. Now I\u2019m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.\n\ntl;dr: a noob DE want a mentor to help navigate through the data carrier.", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got some time to mentor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1991m63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705511140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello experienced devs,\nI am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. &lt;/p&gt;\n\n&lt;p&gt;I am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I\u2019m going to ask for. Since I\u2019m a contractor, I cannot connect/ask for guidance in my workplace&lt;/p&gt;\n\n&lt;p&gt;Couple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. &lt;/p&gt;\n\n&lt;p&gt;One day I found out that what I was doing was not very different from our in-house data analyst\u2019s role. &lt;/p&gt;\n\n&lt;p&gt;Then, I learned Azure services and got a job as a contractor. Now I\u2019m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.&lt;/p&gt;\n\n&lt;p&gt;tl;dr: a noob DE want a mentor to help navigate through the data carrier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1991m63", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1991m63/got_some_time_to_mentor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1991m63/got_some_time_to_mentor/", "subreddit_subscribers": 153606, "created_utc": 1705511140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cqao8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL for Google Sheets with DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_198sy24", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QonQ2q6Ct33jQlxSJKFOz1dervwrpJTqlYAOSuX9XZA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705484917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arecadata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arecadata.com/sql-for-google-sheets-with-duckdb/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?auto=webp&amp;s=79cf82dfea254c1f96569009765549b4cac35c7e", "width": 1665, "height": 904}, "resolutions": [{"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f23bb3403b5a5a4d943532485c9e42b8c375d8ad", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf8729134fcbbcadea38a781c94cdc9b480cc4c7", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cf52379a8af91a21f1355d748fb655f02d61bdc", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb026b5956879b04f7fc428f1a9a568984da0d58", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=51c253aa237b7b3a301cd606688bfec449f994a6", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/7ZRQvuU8PKFok9cDWlvTgXaB0oac1_t3sOGjdgWBP8c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=944dcf6fbf757fd57cf8e86abaf10425ca135714", "width": 1080, "height": 586}], "variants": {}, "id": "o9IGQNlzMMFSGi3oonRSmbtuC-Rh9y1ZsRkdqm26PEA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "198sy24", "is_robot_indexable": true, "report_reasons": null, "author": "dan_the_lion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198sy24/sql_for_google_sheets_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arecadata.com/sql-for-google-sheets-with-duckdb/", "subreddit_subscribers": 153606, "created_utc": 1705484917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mglx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm looking for Data engineering/data science internships but haven't had much luck, how's my resume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1997dcp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/N4R-j-Us4nZ4HzgEwlXl2019up_Cs79pj-vULvFtOiE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705524671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pwohodvec2dc1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pwohodvec2dc1.png?auto=webp&amp;s=52e754709e6aa49361c6a69871553a1d7fd3b03b", "width": 1276, "height": 1654}, "resolutions": [{"url": "https://preview.redd.it/pwohodvec2dc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae04707e2a7043e38c7fa786d4c761c68b0089c4", "width": 108, "height": 139}, {"url": "https://preview.redd.it/pwohodvec2dc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd2cd647d7b9d0292401f6abf207e2016032707d", "width": 216, "height": 279}, {"url": "https://preview.redd.it/pwohodvec2dc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=77dd9a0e67cadcc787df7022fa82dbf642626134", "width": 320, "height": 414}, {"url": "https://preview.redd.it/pwohodvec2dc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6db163f72f8acdd771e629769714b141fcb230d5", "width": 640, "height": 829}, {"url": "https://preview.redd.it/pwohodvec2dc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ebd39a2279b9fee621e67e4375514e37c444d67", "width": 960, "height": 1244}, {"url": "https://preview.redd.it/pwohodvec2dc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbb96454ef7f08aa011c20d959c7389db1e3e8ce", "width": 1080, "height": 1399}], "variants": {}, "id": "iasUsnss_FeBlju5O22qH_JCfLoeJRrz-oGcjICAVsQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1997dcp", "is_robot_indexable": true, "report_reasons": null, "author": "Infinitrix02", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1997dcp/im_looking_for_data_engineeringdata_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pwohodvec2dc1.png", "subreddit_subscribers": 153606, "created_utc": 1705524671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE position at a major financial services company, and after the initial interview, they chose another candidate. Two months later, the job was reposted, and somewhat impulsively, I reapplied. Now, I've been contacted for another interview. I'm uncertain about proceeding because I fear the initial panel's opinion might not have changed. Would it be advisable to call and cancel the interview?", "author_fullname": "t2_i4gjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reconsidering an Interview Opportunity After Initial Rejection: Seeking Advice on Whether to Proceed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1994v41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705518720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE position at a major financial services company, and after the initial interview, they chose another candidate. Two months later, the job was reposted, and somewhat impulsively, I reapplied. Now, I&amp;#39;ve been contacted for another interview. I&amp;#39;m uncertain about proceeding because I fear the initial panel&amp;#39;s opinion might not have changed. Would it be advisable to call and cancel the interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1994v41", "is_robot_indexable": true, "report_reasons": null, "author": "Greckol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1994v41/reconsidering_an_interview_opportunity_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1994v41/reconsidering_an_interview_opportunity_after/", "subreddit_subscribers": 153606, "created_utc": 1705518720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "   \nI've read a lot about data pipelines, I'd like to learn how to build them but I'm not sure what tools I should use. What do I need to know/learn in order to complete this task successfully?    \nAll I find is mostly generalized information nothing concrete and specific, could someone with experience help?   \nThanks.", "author_fullname": "t2_jrkpwx27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline for begunners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1994mwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705518174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read a lot about data pipelines, I&amp;#39;d like to learn how to build them but I&amp;#39;m not sure what tools I should use. What do I need to know/learn in order to complete this task successfully?&lt;br/&gt;\nAll I find is mostly generalized information nothing concrete and specific, could someone with experience help?&lt;br/&gt;\nThanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1994mwj", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane-Research9306", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1994mwj/data_pipeline_for_begunners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1994mwj/data_pipeline_for_begunners/", "subreddit_subscribers": 153606, "created_utc": 1705518174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \nI hope that people don\u2019t take my post negatively, I\u2019m just one of many confused tech professionals struggling with career path to grow my skillset. \n\nI come from software development/master data management background with around 8 years of experience gained through multiple roles. I\u2019ll keep it short, I didn\u2019t start my career as conventional Java developer, rather, gained mixed-up experience working on IBM Master Data Management and Spring Boot (transactional and non transactional/reactive) on very high level.\n\nWorking on IBM MDM, I have gained knowledge about financial and telecommunication domains, which included data lineage, data profiling, data remediation and so on. On the other hand, working on Spring boot on backend as well as middle layer, gained skills on Java (again not in conventional way; i.e core java - spring jpa/spring bean factory and all!)\n\nSo far I have been trying to keep up with software development and trying to learn more stacks, but I\u2019m finding it not leading me towards a proper career path. Recently, I have started exploring data science/engineering and it is looking much more promising as I can grow my career towards data architect roles. However, I am not able to find a proper career path or guide for me to learn. Please advise. Thanks in advance!", "author_fullname": "t2_83ifwpwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1994kd4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705518021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, \nI hope that people don\u2019t take my post negatively, I\u2019m just one of many confused tech professionals struggling with career path to grow my skillset. &lt;/p&gt;\n\n&lt;p&gt;I come from software development/master data management background with around 8 years of experience gained through multiple roles. I\u2019ll keep it short, I didn\u2019t start my career as conventional Java developer, rather, gained mixed-up experience working on IBM Master Data Management and Spring Boot (transactional and non transactional/reactive) on very high level.&lt;/p&gt;\n\n&lt;p&gt;Working on IBM MDM, I have gained knowledge about financial and telecommunication domains, which included data lineage, data profiling, data remediation and so on. On the other hand, working on Spring boot on backend as well as middle layer, gained skills on Java (again not in conventional way; i.e core java - spring jpa/spring bean factory and all!)&lt;/p&gt;\n\n&lt;p&gt;So far I have been trying to keep up with software development and trying to learn more stacks, but I\u2019m finding it not leading me towards a proper career path. Recently, I have started exploring data science/engineering and it is looking much more promising as I can grow my career towards data architect roles. However, I am not able to find a proper career path or guide for me to learn. Please advise. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1994kd4", "is_robot_indexable": true, "report_reasons": null, "author": "Chemistry-Organic", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1994kd4/career_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1994kd4/career_guide/", "subreddit_subscribers": 153606, "created_utc": 1705518021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you tried to programmatically export access details (name, role etc.) from SQL server (cloud or on-Prem) or oracle DBs? \n\nI am hoping to build a report on access and data rights across various data stores, primarily Azure SQL and others. \n\nAppreciate your perspective if you have tried something similar.", "author_fullname": "t2_dswp11x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question on the database access management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1993m10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705515737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you tried to programmatically export access details (name, role etc.) from SQL server (cloud or on-Prem) or oracle DBs? &lt;/p&gt;\n\n&lt;p&gt;I am hoping to build a report on access and data rights across various data stores, primarily Azure SQL and others. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your perspective if you have tried something similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1993m10", "is_robot_indexable": true, "report_reasons": null, "author": "abskiing403", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1993m10/quick_question_on_the_database_access_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1993m10/quick_question_on_the_database_access_management/", "subreddit_subscribers": 153606, "created_utc": 1705515737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All!\n\nToday, I'm launching [a project that I have been working on](https://www.linkedin.com/posts/briana-okyere_introducing-the-astronomer-champions-program-activity-7153422011896074240-qimp?utm_source=share&amp;utm_medium=member_desktop) for the last 6 months, and I want to share it with all of you.\n\nThe Astronomer Champions Program for Apache Airflow aims to **recognize outstanding data practitioners worldwide** who have **demonstrated excellence in leveraging the full capabilities of Apache Airflow in diverse capacities**. Today, I'm celebrating our Inaugural Cohort, and if you are passionate about Airflow, please [apply](https://docs.google.com/forms/d/e/1FAIpQLScSKVzfRf3wppjUbzx0dUzFLwoP66ZfQ6rYLjk9ZASzpKA2Dw/viewform) to our next cohort.\n\n[**Learn more about the program here**](https://www.astronomer.io/blog/introducing-the-astronomer-champions-program-for-apache-airflow/)**, and feel free to respond with any questions!**", "author_fullname": "t2_l5gu8nhgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Champions Program for Apache Airflow- Invite to Apply", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1991so2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705511520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All!&lt;/p&gt;\n\n&lt;p&gt;Today, I&amp;#39;m launching &lt;a href=\"https://www.linkedin.com/posts/briana-okyere_introducing-the-astronomer-champions-program-activity-7153422011896074240-qimp?utm_source=share&amp;amp;utm_medium=member_desktop\"&gt;a project that I have been working on&lt;/a&gt; for the last 6 months, and I want to share it with all of you.&lt;/p&gt;\n\n&lt;p&gt;The Astronomer Champions Program for Apache Airflow aims to &lt;strong&gt;recognize outstanding data practitioners worldwide&lt;/strong&gt; who have &lt;strong&gt;demonstrated excellence in leveraging the full capabilities of Apache Airflow in diverse capacities&lt;/strong&gt;. Today, I&amp;#39;m celebrating our Inaugural Cohort, and if you are passionate about Airflow, please &lt;a href=\"https://docs.google.com/forms/d/e/1FAIpQLScSKVzfRf3wppjUbzx0dUzFLwoP66ZfQ6rYLjk9ZASzpKA2Dw/viewform\"&gt;apply&lt;/a&gt; to our next cohort.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.astronomer.io/blog/introducing-the-astronomer-champions-program-for-apache-airflow/\"&gt;&lt;strong&gt;Learn more about the program here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, and feel free to respond with any questions!&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xzWehCXnt8MMYbfp5pL-1OxB_6AgZ7eophBngnzVD60.jpg?auto=webp&amp;s=2d909d5a7d65908ef8fcfd7949a9b73664913bbe", "width": 800, "height": 450}, "resolutions": [{"url": "https://external-preview.redd.it/xzWehCXnt8MMYbfp5pL-1OxB_6AgZ7eophBngnzVD60.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e0e68f887e066789358abba6b4426e3b4fe124a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/xzWehCXnt8MMYbfp5pL-1OxB_6AgZ7eophBngnzVD60.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f61ce6fc49f2b1d4585439a5738a62f4939e4a8e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/xzWehCXnt8MMYbfp5pL-1OxB_6AgZ7eophBngnzVD60.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d116fced6575e8124b63ea2f80a67d1954f3ee92", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/xzWehCXnt8MMYbfp5pL-1OxB_6AgZ7eophBngnzVD60.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac1df5a47081fff76bdfe8de826b9f1b9b741c3a", "width": 640, "height": 360}], "variants": {}, "id": "yH5v3ow6vBGicYhsqdR2lMV56YgJD_KEIWRCxL-rZac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1991so2", "is_robot_indexable": true, "report_reasons": null, "author": "BrianaGraceOkyere", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1991so2/champions_program_for_apache_airflow_invite_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1991so2/champions_program_for_apache_airflow_invite_to/", "subreddit_subscribers": 153606, "created_utc": 1705511520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Reddit, i turn to you once again for a DE issue. TIA.\n\n&amp;#x200B;\n\n**Currently:**\n\n\"RDS MySQL &gt; DMS &gt; Redshift\"\n\nDMS is configured for a \\`full load, ongoing replication\\` (lags around 6-12 hours everyday ig)\n\n&amp;#x200B;\n\n**Problem:**\n\nExpense (unutilized instance after initial full load) and want to move away CDC to a different tool, as a POC\n\n&amp;#x200B;\n\n**POC** **TODO**: (*no choice here*)\n\nMySQL &gt; Kafka &gt; *Redshift sync connector (necessarily)*\n\n&amp;#x200B;\n\nAssuming the POC *has* to be done..\n\n*Questions:*\n\n1. **Duplicates**: There's mention of \"atleast once delivery\" into sync'd table, and a dummy topic set up on confluent cloud confirmed presence of \\~10k dupes per 1mil rows. Support could not answer pattern on when/why it occurs. How should one tackle this? Not sure if ksqdDB/single message transform can help here.\n2. **CDC:** There is a latest\\_updated\\_ts to help determine updates, how can the sync connector handle upserting? [docs](https://docs.confluent.io/kafka-connectors/aws-redshift/current/sink_config_options.html#redshift-sink-config-options) mention insert OR update (\"*this will only apply when modifying a record; you can\u2019t use this mode to insert a new record if the record doesn\u2019t already exist*\"). Solution to run two parallel ones?\n3. What other tests should one do? I've done basic data sanity checks so far, and plan to set up a kafka local to stream to redshift to test a dummy run.\n\nApologies if there's basic issues in my writings. TIA, once again!", "author_fullname": "t2_vvcegq1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift sync connector to replace DMS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199105x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705511098.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705509739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Reddit, i turn to you once again for a DE issue. TIA.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Currently:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;RDS MySQL &amp;gt; DMS &amp;gt; Redshift&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;DMS is configured for a `full load, ongoing replication` (lags around 6-12 hours everyday ig)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Expense (unutilized instance after initial full load) and want to move away CDC to a different tool, as a POC&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;POC&lt;/strong&gt; &lt;strong&gt;TODO&lt;/strong&gt;: (&lt;em&gt;no choice here&lt;/em&gt;)&lt;/p&gt;\n\n&lt;p&gt;MySQL &amp;gt; Kafka &amp;gt; &lt;em&gt;Redshift sync connector (necessarily)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Assuming the POC &lt;em&gt;has&lt;/em&gt; to be done..&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Questions:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Duplicates&lt;/strong&gt;: There&amp;#39;s mention of &amp;quot;atleast once delivery&amp;quot; into sync&amp;#39;d table, and a dummy topic set up on confluent cloud confirmed presence of ~10k dupes per 1mil rows. Support could not answer pattern on when/why it occurs. How should one tackle this? Not sure if ksqdDB/single message transform can help here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CDC:&lt;/strong&gt; There is a latest_updated_ts to help determine updates, how can the sync connector handle upserting? &lt;a href=\"https://docs.confluent.io/kafka-connectors/aws-redshift/current/sink_config_options.html#redshift-sink-config-options\"&gt;docs&lt;/a&gt; mention insert OR update (&amp;quot;&lt;em&gt;this will only apply when modifying a record; you can\u2019t use this mode to insert a new record if the record doesn\u2019t already exist&lt;/em&gt;&amp;quot;). Solution to run two parallel ones?&lt;/li&gt;\n&lt;li&gt;What other tests should one do? I&amp;#39;ve done basic data sanity checks so far, and plan to set up a kafka local to stream to redshift to test a dummy run.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Apologies if there&amp;#39;s basic issues in my writings. TIA, once again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199105x", "is_robot_indexable": true, "report_reasons": null, "author": "LocksmithConnect6201", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199105x/redshift_sync_connector_to_replace_dms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199105x/redshift_sync_connector_to_replace_dms/", "subreddit_subscribers": 153606, "created_utc": 1705509739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jjbftfcy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accelerate Your Data Journey: Practical Guide to S3-to-RDS ETL Using Lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 65, "top_awarded_type": null, "hide_score": false, "name": "t3_198upu8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J7g7kkt1GV3YQ4ar8AkMRKEF_h6ipyCPAbOjuogqqsg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705491971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@naveenkumarmurugan/accelerate-your-data-journey-practical-guide-to-s3-to-rds-etl-using-lambda-daf14010a68f", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?auto=webp&amp;s=4cccdace1f06100e8b15d7f71dbd25041348c066", "width": 1200, "height": 560}, "resolutions": [{"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb61276e7771e05fbb066256f5ebb80401253729", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=018d40d1544e69e68116c4446c506ed3fce9e65c", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=58fb13ba4bc48ff170c66d82805481e4293a9826", "width": 320, "height": 149}, {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f5b5e858a201d5a6d09f6f3add94ce64703f448", "width": 640, "height": 298}, {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8d2b20c5b8556b5af4fcade3bbe29227f8528b3", "width": 960, "height": 448}, {"url": "https://external-preview.redd.it/gJxK7HTbOShTpW3iKMHsfmPV0m8lFI7WDOoyF03XEl8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2c630ddb170f561cb6e302b4227065f25b1e18bb", "width": 1080, "height": 504}], "variants": {}, "id": "ewiNCqTi5aC4JriP7acXZF3BPP20WAetGZKr2BsRSLg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "198upu8", "is_robot_indexable": true, "report_reasons": null, "author": "BigNo3623", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198upu8/accelerate_your_data_journey_practical_guide_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@naveenkumarmurugan/accelerate-your-data-journey-practical-guide-to-s3-to-rds-etl-using-lambda-daf14010a68f", "subreddit_subscribers": 153606, "created_utc": 1705491971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jjbftfcy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to use Client-side AWS KMS encryption and How?: A step by step Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": false, "name": "t3_198uhpg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yeM_HQbmU8B7P0K5rySXckKX__36z3I7Y2ATXmWUVzI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705491134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yzvszoppkzcc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yzvszoppkzcc1.png?auto=webp&amp;s=369274cb48b154e34781caf9e11fa7eaa227d35d", "width": 2203, "height": 1039}, "resolutions": [{"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de736b19444400c2e4a6036b85f282351cfa4c69", "width": 108, "height": 50}, {"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fc8aa0f22157431c64ce08509fff734e32d4ce3", "width": 216, "height": 101}, {"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=05d129a9de643917cd902df03a1d75c9c475cd8b", "width": 320, "height": 150}, {"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07c0bdc61d316e34d2d64a0fac7ba6d7a2be51c1", "width": 640, "height": 301}, {"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4196ba19dbafc5a5cf65f5d23bff885820c4bf7c", "width": 960, "height": 452}, {"url": "https://preview.redd.it/yzvszoppkzcc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e547e855418bdd043cdc2a88c82419f8f2b9a1a", "width": 1080, "height": 509}], "variants": {}, "id": "rXfkmCXHpeSvTPCKimUXQCm_lZFbE8yKHXsqW1-afeU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "198uhpg", "is_robot_indexable": true, "report_reasons": null, "author": "BigNo3623", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198uhpg/when_to_use_clientside_aws_kms_encryption_and_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yzvszoppkzcc1.png", "subreddit_subscribers": 153606, "created_utc": 1705491134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nHow would you approach a project, where a DWH is to be migrated from on-prem, teradata, to a private cloud, spark based solution?\n\nI only know spark as a processing engine, and from what I've been reading, no one actually uses it for it's data storage functionalities ('databases', 'tables', and so on). \n\nSo I don't know too much about data architecture, as I've always been just implementing stuff that someone designed (mostly ETL pipelines with spark/airflow). And spark was used to get data from point A to point B, with transformations in between.\n\nDoes any of you work with spark as a processing engine for DWH? How does it work? Can DWH even be in a form of parquet files lying in buckets? How would the data architecture be imposed then? Would dbt help here?\n\nAs you can see, I'm new to the architecture side of data engineering, so please forgive me if some of the questions don't make sense. I'd gladly check out any recommended resources. \n\nThanks", "author_fullname": "t2_gejetxj65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark in pair with DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198pgon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705470969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;How would you approach a project, where a DWH is to be migrated from on-prem, teradata, to a private cloud, spark based solution?&lt;/p&gt;\n\n&lt;p&gt;I only know spark as a processing engine, and from what I&amp;#39;ve been reading, no one actually uses it for it&amp;#39;s data storage functionalities (&amp;#39;databases&amp;#39;, &amp;#39;tables&amp;#39;, and so on). &lt;/p&gt;\n\n&lt;p&gt;So I don&amp;#39;t know too much about data architecture, as I&amp;#39;ve always been just implementing stuff that someone designed (mostly ETL pipelines with spark/airflow). And spark was used to get data from point A to point B, with transformations in between.&lt;/p&gt;\n\n&lt;p&gt;Does any of you work with spark as a processing engine for DWH? How does it work? Can DWH even be in a form of parquet files lying in buckets? How would the data architecture be imposed then? Would dbt help here?&lt;/p&gt;\n\n&lt;p&gt;As you can see, I&amp;#39;m new to the architecture side of data engineering, so please forgive me if some of the questions don&amp;#39;t make sense. I&amp;#39;d gladly check out any recommended resources. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198pgon", "is_robot_indexable": true, "report_reasons": null, "author": "Visual-Exercise8031", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198pgon/spark_in_pair_with_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198pgon/spark_in_pair_with_dwh/", "subreddit_subscribers": 153606, "created_utc": 1705470969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,  \n\n\nHelp! i'm searching for the most awesome dataset(s) you have ever seen\ud83d\udcaa\ud83d\ude80\n\nI'm trying to make a demo with DBT &amp; Snowflake, I want to use the demo prove that two tools can solve \"real life\" problems \ud83d\udee0\ufe0f\u2699\ufe0f\u2744\ufe0f\n\nTherefore I have these requirements:\n\n* Minimum one table with minimum **1tb** data\n* Minimum **20** tables\n* High variety in d**ata types**\n* As many **file types** as possible; CSV, JSON, Parquet etc\n\n&amp;#x200B;\n\nDo any of you have experience with public data sets that meet these requirements?\n\n&amp;#x200B;", "author_fullname": "t2_oeiyrmpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demo Dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_199779a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705524271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,  &lt;/p&gt;\n\n&lt;p&gt;Help! i&amp;#39;m searching for the most awesome dataset(s) you have ever seen\ud83d\udcaa\ud83d\ude80&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to make a demo with DBT &amp;amp; Snowflake, I want to use the demo prove that two tools can solve &amp;quot;real life&amp;quot; problems \ud83d\udee0\ufe0f\u2699\ufe0f\u2744\ufe0f&lt;/p&gt;\n\n&lt;p&gt;Therefore I have these requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Minimum one table with minimum &lt;strong&gt;1tb&lt;/strong&gt; data&lt;/li&gt;\n&lt;li&gt;Minimum &lt;strong&gt;20&lt;/strong&gt; tables&lt;/li&gt;\n&lt;li&gt;High variety in d&lt;strong&gt;ata types&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;As many &lt;strong&gt;file types&lt;/strong&gt; as possible; CSV, JSON, Parquet etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do any of you have experience with public data sets that meet these requirements?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199779a", "is_robot_indexable": true, "report_reasons": null, "author": "formaldehyden", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199779a/demo_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199779a/demo_dataset/", "subreddit_subscribers": 153606, "created_utc": 1705524271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the technical requirements for an organization to justify using a SaaS offering? \n\nContext: We are evaluating a fully managed SaaS platform where data is egressed from user data sources (any cloud DB/DW/Object storage i.e. RDS, Redshift, S3). They are SOC2 certified with ISO27k in-process. Everything is encrypted end-to-end/in-transit/at-rest. They offer a Bring-Your-Own-Cloud approach but do not have things like VPC-peering or PrivateLink equivalents.", "author_fullname": "t2_of3lsecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Egress Concerns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1993z9w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705516614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the technical requirements for an organization to justify using a SaaS offering? &lt;/p&gt;\n\n&lt;p&gt;Context: We are evaluating a fully managed SaaS platform where data is egressed from user data sources (any cloud DB/DW/Object storage i.e. RDS, Redshift, S3). They are SOC2 certified with ISO27k in-process. Everything is encrypted end-to-end/in-transit/at-rest. They offer a Bring-Your-Own-Cloud approach but do not have things like VPC-peering or PrivateLink equivalents.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1993z9w", "is_robot_indexable": true, "report_reasons": null, "author": "Quirky-Repair6791", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1993z9w/seeking_advice_egress_concerns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1993z9w/seeking_advice_egress_concerns/", "subreddit_subscribers": 153606, "created_utc": 1705516614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "API Orchestration Solutions\n\nHi,\n\nI am looking for an API Orchestrator solution.\n\nRequirements:\n\n1. Given a list of API endpoints represented in a configuration of sequence and parallel execution, I want the orchestrator to call the APIs in the serial/parallel order as described in the configuration. The first API in the list will accept the input for the sequence, and the last API will produce the output.\n2. I am looking for an OpenSource library-based solution. I am not interested in a fully hosted solution. Happy to consider Azure solutions since I use Azure. \n3. I want to provide my customers with a domain-specific language (DSL) that they can use to define their orchestration configuration. The system will accept the configuration, create the Orchestration, and expose the API. \n4. I want to provide a way in the DSL for Customers to specify the mapping between the input/output data types to chain the APIs in the configuration.\n5. I want the call to the API Orchestration to be synchronous (not an asynchronous / polling model). Given a request, I want the API Orchestrator to execute the APIs as specified in the configuration and return the response synchronously in a few milliseconds to less than a couple of seconds. The APIs being orchestrated will ensure they return responses in the order of milliseconds.", "author_fullname": "t2_8xmun4y0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API Orchestrator Solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_198zyr4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705507304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;API Orchestration Solutions&lt;/p&gt;\n\n&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am looking for an API Orchestrator solution.&lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Given a list of API endpoints represented in a configuration of sequence and parallel execution, I want the orchestrator to call the APIs in the serial/parallel order as described in the configuration. The first API in the list will accept the input for the sequence, and the last API will produce the output.&lt;/li&gt;\n&lt;li&gt;I am looking for an OpenSource library-based solution. I am not interested in a fully hosted solution. Happy to consider Azure solutions since I use Azure. &lt;/li&gt;\n&lt;li&gt;I want to provide my customers with a domain-specific language (DSL) that they can use to define their orchestration configuration. The system will accept the configuration, create the Orchestration, and expose the API. &lt;/li&gt;\n&lt;li&gt;I want to provide a way in the DSL for Customers to specify the mapping between the input/output data types to chain the APIs in the configuration.&lt;/li&gt;\n&lt;li&gt;I want the call to the API Orchestration to be synchronous (not an asynchronous / polling model). Given a request, I want the API Orchestrator to execute the APIs as specified in the configuration and return the response synchronously in a few milliseconds to less than a couple of seconds. The APIs being orchestrated will ensure they return responses in the order of milliseconds.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "198zyr4", "is_robot_indexable": true, "report_reasons": null, "author": "RedHawk004", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/198zyr4/api_orchestrator_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/198zyr4/api_orchestrator_solutions/", "subreddit_subscribers": 153606, "created_utc": 1705507304.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}