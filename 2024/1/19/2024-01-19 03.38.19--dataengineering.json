{"kind": "Listing", "data": {"after": "t3_199j43i", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y'all do while waiting for something to run?", "author_fullname": "t2_a2lh9x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you spend your time waiting for things to run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199lx3r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705568611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y&amp;#39;all do while waiting for something to run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199lx3r", "is_robot_indexable": true, "report_reasons": null, "author": "an27725", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199lx3r/how_do_you_spend_your_time_waiting_for_things_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199lx3r/how_do_you_spend_your_time_waiting_for_things_to/", "subreddit_subscribers": 153921, "created_utc": 1705568611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 15+ year exp in DW engg and 8+ yrs in cloud aws with big data tools  including spark and data bricks.  I also have few AWS certifications and cloudera certifications but they are not current now.\n\nI recently got laid off in November and have been looking for opportunities.  seems like now a days all the buzz is ML and AI which i am lacking.  Question to fellow Engineers is it better to learn additional tools/platforms like Snowflake + aws professional level certs or jump into AI/ML learning which i got to say is not natural to me i am finding it difficult to learn.  Any advice\n\n&amp;#x200B;", "author_fullname": "t2_j52nsle3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "laid off and looking for next best option", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199n5mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705573849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 15+ year exp in DW engg and 8+ yrs in cloud aws with big data tools  including spark and data bricks.  I also have few AWS certifications and cloudera certifications but they are not current now.&lt;/p&gt;\n\n&lt;p&gt;I recently got laid off in November and have been looking for opportunities.  seems like now a days all the buzz is ML and AI which i am lacking.  Question to fellow Engineers is it better to learn additional tools/platforms like Snowflake + aws professional level certs or jump into AI/ML learning which i got to say is not natural to me i am finding it difficult to learn.  Any advice&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199n5mr", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Fee-4006", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199n5mr/laid_off_and_looking_for_next_best_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199n5mr/laid_off_and_looking_for_next_best_option/", "subreddit_subscribers": 153921, "created_utc": 1705573849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been trying to figure out how to get other teams in my company to use our data platform more. So far, I've made some dashboards and told folks about them, but it's not really doing the trick.\n\nI'm wondering what's worked for you. Have you had any luck with things like office hours, better documentation, or just really awesome dashboards? I'm all ears for any tips or tricks you've got.", "author_fullname": "t2_14xg7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do You Get Teams to Actually Use Your Data Platform?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199wtkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705602137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying to figure out how to get other teams in my company to use our data platform more. So far, I&amp;#39;ve made some dashboards and told folks about them, but it&amp;#39;s not really doing the trick.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what&amp;#39;s worked for you. Have you had any luck with things like office hours, better documentation, or just really awesome dashboards? I&amp;#39;m all ears for any tips or tricks you&amp;#39;ve got.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199wtkc", "is_robot_indexable": true, "report_reasons": null, "author": "joekarlsson", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199wtkc/how_do_you_get_teams_to_actually_use_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199wtkc/how_do_you_get_teams_to_actually_use_your_data/", "subreddit_subscribers": 153921, "created_utc": 1705602137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \nBack again with another post. I recently had a bad interview experience where lack of cloud experience went against me. The fact that I had recently passed the google cloud professional data engineer exam didn't seem to have an impact, the recruiter said they needed someone with on hands experience on cloud. I am just tired of getting constantly rejected, it seems like every recruiter has a check list they are just trying to check everything off from. What can i do? Open to criticism.", "author_fullname": "t2_73llqzmo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Addressing lack of cloud experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199t56b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705592915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, \nBack again with another post. I recently had a bad interview experience where lack of cloud experience went against me. The fact that I had recently passed the google cloud professional data engineer exam didn&amp;#39;t seem to have an impact, the recruiter said they needed someone with on hands experience on cloud. I am just tired of getting constantly rejected, it seems like every recruiter has a check list they are just trying to check everything off from. What can i do? Open to criticism.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "199t56b", "is_robot_indexable": true, "report_reasons": null, "author": "afnan_shahid92", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/199t56b/addressing_lack_of_cloud_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199t56b/addressing_lack_of_cloud_experience/", "subreddit_subscribers": 153921, "created_utc": 1705592915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to using DBT, I would love to know if there are any open source tools in the market that you guys use to schedule these jobs. If not, do you guys just go about building python scripts for the orchestration?", "author_fullname": "t2_h5ll08of", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys orchestrate DBT transforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19a1pf2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705614052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to using DBT, I would love to know if there are any open source tools in the market that you guys use to schedule these jobs. If not, do you guys just go about building python scripts for the orchestration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19a1pf2", "is_robot_indexable": true, "report_reasons": null, "author": "Lucky-Front7675", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a1pf2/how_do_you_guys_orchestrate_dbt_transforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a1pf2/how_do_you_guys_orchestrate_dbt_transforms/", "subreddit_subscribers": 153921, "created_utc": 1705614052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an upcoming interview where one of the steps is to create a mock data model what should I be reading up on in preparation. And what are the key things they will wlbe looking out for and be considering when doing such an exercise?\n\nFor context I have decent amount of data experience just lacking formal data Modeling experience any tips would be appreciated thanks in advance", "author_fullname": "t2_n2nlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling Interview scenario questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199nzng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705577110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an upcoming interview where one of the steps is to create a mock data model what should I be reading up on in preparation. And what are the key things they will wlbe looking out for and be considering when doing such an exercise?&lt;/p&gt;\n\n&lt;p&gt;For context I have decent amount of data experience just lacking formal data Modeling experience any tips would be appreciated thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "199nzng", "is_robot_indexable": true, "report_reasons": null, "author": "yanicklloyd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199nzng/data_modeling_interview_scenario_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199nzng/data_modeling_interview_scenario_questions/", "subreddit_subscribers": 153921, "created_utc": 1705577110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a mysql database that I read the changes (CDC) with Kafka. I need to join 3 tables/topics and send a notification to a page on the front when I have a new conversion. How to make this ETL and this endpoint be consumed?\n\nMy env is in kubernetes", "author_fullname": "t2_9s0c496k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real time infra", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19a0ybk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705612217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mysql database that I read the changes (CDC) with Kafka. I need to join 3 tables/topics and send a notification to a page on the front when I have a new conversion. How to make this ETL and this endpoint be consumed?&lt;/p&gt;\n\n&lt;p&gt;My env is in kubernetes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19a0ybk", "is_robot_indexable": true, "report_reasons": null, "author": "Dwoler", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a0ybk/real_time_infra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a0ybk/real_time_infra/", "subreddit_subscribers": 153921, "created_utc": 1705612217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I read data from Event Hub and write it to a Delta tables after applying a few transformations. \n\nThe transformations involve multiple 'withColumn' and two 'explode' functions (one for processing the body message received in the EventHub event and another one for internal transformations).\n\nI use 'foreach' batches to load the data.\n\n1. If I write directly to delta table, the results are poor, with delays ranging from minutes to hours.\n\n2. If I follow Medallion architecture,\n\n* EventHub to Bronze Delta table. (copy the event as it as)\n* Bronze to Silver Delta table (final table: all the transformation are happening in this layer)\n\nyields significantly better performance. The delay is almost negligible, around 3-4 seconds, despite the additional processing layer.\n\nI'm curious about the specific reasons behind the enhanced performance in the second approach, despite the data passing through two layers of processing.\n\n**Please note:** I use same cluster for both the processes.\n\n[1](https://preview.redd.it/ahll6yvu19dc1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=ef5131d2729b4e28b822970ba486ca861c0bd000)\n\n&amp;#x200B;\n\n[2](https://preview.redd.it/1mhgfg7w19dc1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=94f05f97d26b81fd09dbfffb1dfd086a6fd6fdf9)\n\n&amp;#x200B;", "author_fullname": "t2_ptil4bof4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Databricks Spark streaming.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1mhgfg7w19dc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60c9c3942f6bb9588a912d0a8f200f5f62fcb844"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=878ee859d7898f89c7258401ee915c87ba36b179"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=99e84f76d22e3c8dbc794cc3ac5d4445aa94ca3e"}, {"y": 309, "x": 640, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2079c011ee51b6e883dc99c1d6f615cff21b91d8"}, {"y": 463, "x": 960, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b95fb0280012b771079969381b4eb585ac807f6"}], "s": {"y": 494, "x": 1023, "u": "https://preview.redd.it/1mhgfg7w19dc1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=94f05f97d26b81fd09dbfffb1dfd086a6fd6fdf9"}, "id": "1mhgfg7w19dc1"}, "ahll6yvu19dc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a5f131f9ac3ca2cf334f5a0df772e38e1bb7681"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5b432bfeffaf4d20468d41da1611606ec65c353"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=567754e824abf2190ba48f09ce906e2883510662"}, {"y": 309, "x": 640, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50ece67e5805d4bac3f262c5de27cc635aab50d6"}, {"y": 463, "x": 960, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=184c99827d08c866abb776a09127de973c8fe2b3"}], "s": {"y": 494, "x": 1023, "u": "https://preview.redd.it/ahll6yvu19dc1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=ef5131d2729b4e28b822970ba486ca861c0bd000"}, "id": "ahll6yvu19dc1"}}, "name": "t3_199yd9j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i1ikPJBmt8LmObT11X3GjawayIpOIpNCP1uTwDFlT8o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705605941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read data from Event Hub and write it to a Delta tables after applying a few transformations. &lt;/p&gt;\n\n&lt;p&gt;The transformations involve multiple &amp;#39;withColumn&amp;#39; and two &amp;#39;explode&amp;#39; functions (one for processing the body message received in the EventHub event and another one for internal transformations).&lt;/p&gt;\n\n&lt;p&gt;I use &amp;#39;foreach&amp;#39; batches to load the data.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;If I write directly to delta table, the results are poor, with delays ranging from minutes to hours.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If I follow Medallion architecture,&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;EventHub to Bronze Delta table. (copy the event as it as)&lt;/li&gt;\n&lt;li&gt;Bronze to Silver Delta table (final table: all the transformation are happening in this layer)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;yields significantly better performance. The delay is almost negligible, around 3-4 seconds, despite the additional processing layer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious about the specific reasons behind the enhanced performance in the second approach, despite the data passing through two layers of processing.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; I use same cluster for both the processes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ahll6yvu19dc1.png?width=1023&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef5131d2729b4e28b822970ba486ca861c0bd000\"&gt;1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1mhgfg7w19dc1.png?width=1023&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94f05f97d26b81fd09dbfffb1dfd086a6fd6fdf9\"&gt;2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199yd9j", "is_robot_indexable": true, "report_reasons": null, "author": "HousingStriking3770", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199yd9j/need_help_with_databricks_spark_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199yd9j/need_help_with_databricks_spark_streaming/", "subreddit_subscribers": 153921, "created_utc": 1705605941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those who have undergone multiple interviews in their careers, how frequently were you asked questions related to Dynamic Programming and Graphs? I'm interested in understanding the prevalence of these topics in the interview process. Feel free to share your insights and experience.\nThanks!", "author_fullname": "t2_rphy3d35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importance of Dynamic Programming and Graphs in DE Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199wj94", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705601439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who have undergone multiple interviews in their careers, how frequently were you asked questions related to Dynamic Programming and Graphs? I&amp;#39;m interested in understanding the prevalence of these topics in the interview process. Feel free to share your insights and experience.\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "199wj94", "is_robot_indexable": true, "report_reasons": null, "author": "PunctuallyExcellent", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199wj94/importance_of_dynamic_programming_and_graphs_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199wj94/importance_of_dynamic_programming_and_graphs_in/", "subreddit_subscribers": 153921, "created_utc": 1705601439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI recently started a new position at a consulting company that creates own web-based software products for companies in the energy infrastructure sector in Germany. Because each project was treated isolated regarding its data management, there are many silos of data in Excel, Postgres databases, or Python data frames, some in the Azure Cloud and some on on-premise servers.\n\nI started my position in one of those project teams but convinced the company to start fresh with its data management and build a centralized data warehouse / data lake environment from scratch to be used by every project team.\n\nNow I am struggling to find a decision on what our future stack should look like and if I could start with one tool that may not cover much but incrementally add more tools as soon they are necessary. So a little more context on the status quo:\n\n* Most transformation and general ETL process of data happens in manually triggered Python scripts that may or may not utilize SQL scripts as well\n* There are 6 colleagues who create data products but never cared much about data engineering, which is why I would consider them data analysts / data scientists. All of them are more familiar with Python than SQL, but also can handle both\n* There are 5 developers who cared more for the backend development, but for whom data engineering was rather an addon in the process not worth of thinking more than necessary of. The focus was always how to prepare the data for that specific software feature\n* Data Analytics is not yet a big thing. It is more like data products are created and the pushed to the software it was made for. Updates of these data products are mostly triggered manually. However, clients more and more want a better understanding of the construction process of their infrastructure and projects and asked more frequently for dashboards and reports\n* Because much of the data has a spatial context we use Postgres with its PostGIS extension to handle this data format. It is also where my expertise as a spatial data analyst lays and I have some years of experience working with Postgres and PostGIS. Thus, I am more versatile in SQL than Python but can work with both as well\n* We don't handle big data (yet) and also don't have live data feeds of sensors or such to handle. However, we have data from many sources and also need to create data products that are then used in different software systems, internal and external, and, eventually, also in BI tools.\n\n&amp;#x200B;\n\nMy current plan would include the following tools and technologies to build a data lake and ELT infrastructure from scratch:\n\n* Start rebuilding the old data infrastructure in a new on-prem **Postgres database** using **dbt** to model the data (not dbt cloud).\n* Even though the colleagues are more experienced with Python, I still use dbt for creating the basic models that are then used in **pandas** scripts for further data manipulation by the colleagues and then reintegrated in the data model of dbt\n* After this is done use **Airflow** to schedule and orchestra the data manipulation process.\n* Providing our clients with a centralized **content management system** to upload and store their Excel sheets that follow our data model requirements which we then integrate into the data lake using scheduled jobs in Airflow\n* Use **Power BI** (due to our licences with Microsoft products) to provide a BI tool that leverages data for analysis purposes\n* Create **Fast API** endpoints that look to our data products created in the data lake and used by software developers\n* When we see that we need to scale we may have arguments for the c-level based on the experience we made with the above stack and its benefits it created to migrate to cloud and introduce services like Snowflake and Fivetran\n\n&amp;#x200B;\n\nI would be really happy to see what you may think about my approach and if you have any critics about it and any suggestions for improvement.", "author_fullname": "t2_mwoc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendation for Tech Stack for creating ELT process from scratch for energy infrastructure consulting firm.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199pi8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705582434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently started a new position at a consulting company that creates own web-based software products for companies in the energy infrastructure sector in Germany. Because each project was treated isolated regarding its data management, there are many silos of data in Excel, Postgres databases, or Python data frames, some in the Azure Cloud and some on on-premise servers.&lt;/p&gt;\n\n&lt;p&gt;I started my position in one of those project teams but convinced the company to start fresh with its data management and build a centralized data warehouse / data lake environment from scratch to be used by every project team.&lt;/p&gt;\n\n&lt;p&gt;Now I am struggling to find a decision on what our future stack should look like and if I could start with one tool that may not cover much but incrementally add more tools as soon they are necessary. So a little more context on the status quo:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Most transformation and general ETL process of data happens in manually triggered Python scripts that may or may not utilize SQL scripts as well&lt;/li&gt;\n&lt;li&gt;There are 6 colleagues who create data products but never cared much about data engineering, which is why I would consider them data analysts / data scientists. All of them are more familiar with Python than SQL, but also can handle both&lt;/li&gt;\n&lt;li&gt;There are 5 developers who cared more for the backend development, but for whom data engineering was rather an addon in the process not worth of thinking more than necessary of. The focus was always how to prepare the data for that specific software feature&lt;/li&gt;\n&lt;li&gt;Data Analytics is not yet a big thing. It is more like data products are created and the pushed to the software it was made for. Updates of these data products are mostly triggered manually. However, clients more and more want a better understanding of the construction process of their infrastructure and projects and asked more frequently for dashboards and reports&lt;/li&gt;\n&lt;li&gt;Because much of the data has a spatial context we use Postgres with its PostGIS extension to handle this data format. It is also where my expertise as a spatial data analyst lays and I have some years of experience working with Postgres and PostGIS. Thus, I am more versatile in SQL than Python but can work with both as well&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t handle big data (yet) and also don&amp;#39;t have live data feeds of sensors or such to handle. However, we have data from many sources and also need to create data products that are then used in different software systems, internal and external, and, eventually, also in BI tools.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My current plan would include the following tools and technologies to build a data lake and ELT infrastructure from scratch:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Start rebuilding the old data infrastructure in a new on-prem &lt;strong&gt;Postgres database&lt;/strong&gt; using &lt;strong&gt;dbt&lt;/strong&gt; to model the data (not dbt cloud).&lt;/li&gt;\n&lt;li&gt;Even though the colleagues are more experienced with Python, I still use dbt for creating the basic models that are then used in &lt;strong&gt;pandas&lt;/strong&gt; scripts for further data manipulation by the colleagues and then reintegrated in the data model of dbt&lt;/li&gt;\n&lt;li&gt;After this is done use &lt;strong&gt;Airflow&lt;/strong&gt; to schedule and orchestra the data manipulation process.&lt;/li&gt;\n&lt;li&gt;Providing our clients with a centralized &lt;strong&gt;content management system&lt;/strong&gt; to upload and store their Excel sheets that follow our data model requirements which we then integrate into the data lake using scheduled jobs in Airflow&lt;/li&gt;\n&lt;li&gt;Use &lt;strong&gt;Power BI&lt;/strong&gt; (due to our licences with Microsoft products) to provide a BI tool that leverages data for analysis purposes&lt;/li&gt;\n&lt;li&gt;Create &lt;strong&gt;Fast API&lt;/strong&gt; endpoints that look to our data products created in the data lake and used by software developers&lt;/li&gt;\n&lt;li&gt;When we see that we need to scale we may have arguments for the c-level based on the experience we made with the above stack and its benefits it created to migrate to cloud and introduce services like Snowflake and Fivetran&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would be really happy to see what you may think about my approach and if you have any critics about it and any suggestions for improvement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199pi8s", "is_robot_indexable": true, "report_reasons": null, "author": "rick854", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199pi8s/recommendation_for_tech_stack_for_creating_elt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199pi8s/recommendation_for_tech_stack_for_creating_elt/", "subreddit_subscribers": 153921, "created_utc": 1705582434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software developer in a developing country. I've been working remotely for American companies for the past 8.5 years as a software engineer, but kind of in a niche industry that over the years I've grown to really dislike - so I've been thinking of switching industries for a while now. My main work line is in PHP/Javascript and I've been slowly but steadily moving further to the back-end, trying to get more work in the CI/CD pipelines, data modeling, database management and such.\n\nWithin the companies I've worked for, I've become known for being one of only people around who understands data/databases/search in a deeper way and I got a great deal of exposure to both MySQL, Elastic stack and Redis. I do have an Elastic Engineer certification and a fair amount of knowledge of Bash/Python, Linux, DevOps, CI, containers and such, but no real on-the-job experience with distributed computing/data tools.\n\nEarly last year I got laid off and started applying for DE jobs, but in the era of ATS I didn't land a single interview in this area and ended up taking another job in the same industry (a good one, at least).\n\nI've been around the block and I'm very confident in my ability to learn quickly on the job but I'm now 40 and I have two kids and a career, which means my time for on-the-side projects is really limited and I can't take a dip in salary to start over as a junior (though mostly because of location, my salary isn't really that high, I'm currently at 90k).\n\nIf you were in my shoes, how would you go about trying to land a DE job? Do you think routing through DevOps/SRE could be a good alternative?", "author_fullname": "t2_m3tn2vhhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career shift planning advice (Software Eng to DevOps/SRE to Data Eng?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199oy12", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705580531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software developer in a developing country. I&amp;#39;ve been working remotely for American companies for the past 8.5 years as a software engineer, but kind of in a niche industry that over the years I&amp;#39;ve grown to really dislike - so I&amp;#39;ve been thinking of switching industries for a while now. My main work line is in PHP/Javascript and I&amp;#39;ve been slowly but steadily moving further to the back-end, trying to get more work in the CI/CD pipelines, data modeling, database management and such.&lt;/p&gt;\n\n&lt;p&gt;Within the companies I&amp;#39;ve worked for, I&amp;#39;ve become known for being one of only people around who understands data/databases/search in a deeper way and I got a great deal of exposure to both MySQL, Elastic stack and Redis. I do have an Elastic Engineer certification and a fair amount of knowledge of Bash/Python, Linux, DevOps, CI, containers and such, but no real on-the-job experience with distributed computing/data tools.&lt;/p&gt;\n\n&lt;p&gt;Early last year I got laid off and started applying for DE jobs, but in the era of ATS I didn&amp;#39;t land a single interview in this area and ended up taking another job in the same industry (a good one, at least).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been around the block and I&amp;#39;m very confident in my ability to learn quickly on the job but I&amp;#39;m now 40 and I have two kids and a career, which means my time for on-the-side projects is really limited and I can&amp;#39;t take a dip in salary to start over as a junior (though mostly because of location, my salary isn&amp;#39;t really that high, I&amp;#39;m currently at 90k).&lt;/p&gt;\n\n&lt;p&gt;If you were in my shoes, how would you go about trying to land a DE job? Do you think routing through DevOps/SRE could be a good alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199oy12", "is_robot_indexable": true, "report_reasons": null, "author": "Time_Simple_3250", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199oy12/career_shift_planning_advice_software_eng_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199oy12/career_shift_planning_advice_software_eng_to/", "subreddit_subscribers": 153921, "created_utc": 1705580531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel in the next few years solutions will come out to do analysis on more unstructured data (no not just log files), I mean video, audio, text and doing sentiment analysis on it to extract data and do data quality check to train a LLM\n\nAnyone doing this in their current data engineering job? Any tech stacks to do this that you recommend?", "author_fullname": "t2_5q1b2lj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unstructured data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199hke6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705552263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel in the next few years solutions will come out to do analysis on more unstructured data (no not just log files), I mean video, audio, text and doing sentiment analysis on it to extract data and do data quality check to train a LLM&lt;/p&gt;\n\n&lt;p&gt;Anyone doing this in their current data engineering job? Any tech stacks to do this that you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "199hke6", "is_robot_indexable": true, "report_reasons": null, "author": "Guilty-Commission435", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199hke6/unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199hke6/unstructured_data/", "subreddit_subscribers": 153921, "created_utc": 1705552263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAt our company, we have chosen to use dbt Cloud (most of our developers prefer the cloud IDE was the decision point).\n\nHowever, in terms of provisioning dbt Cloud infrastructure and RBAC, we have come across a few challenges in regards to RBAC primarily. We are wanting to use a service user with the Project Creator permission set, in order to automate the deployment of our infrastructure (which is primarily dbt Cloud projects). However, this service user can also promote other users to any existing dbt Cloud group, and this group could be the group for Account Admins. This is currently a limitation for us, as we cannot allow a circumstance where a service user can promote any user to Account Admin.\n\n  \nI am hence seeking to check with those of you who have automated their dbt Cloud infra deployment, and what their RBAC setup looked like?\n\n&amp;#x200B;\n\nThanks.  \n", "author_fullname": "t2_7erz2gzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Cloud RBAC - Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19a4k8g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705621242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;At our company, we have chosen to use dbt Cloud (most of our developers prefer the cloud IDE was the decision point).&lt;/p&gt;\n\n&lt;p&gt;However, in terms of provisioning dbt Cloud infrastructure and RBAC, we have come across a few challenges in regards to RBAC primarily. We are wanting to use a service user with the Project Creator permission set, in order to automate the deployment of our infrastructure (which is primarily dbt Cloud projects). However, this service user can also promote other users to any existing dbt Cloud group, and this group could be the group for Account Admins. This is currently a limitation for us, as we cannot allow a circumstance where a service user can promote any user to Account Admin.&lt;/p&gt;\n\n&lt;p&gt;I am hence seeking to check with those of you who have automated their dbt Cloud infra deployment, and what their RBAC setup looked like?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19a4k8g", "is_robot_indexable": true, "report_reasons": null, "author": "TinkermanN7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a4k8g/dbt_cloud_rbac_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a4k8g/dbt_cloud_rbac_best_practices/", "subreddit_subscribers": 153921, "created_utc": 1705621242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have two years of experience and I am looking to switch from my current job. The skills required by the company are databricks, Azure data factory, sql and Pyspark. Any tips on what to do and what not to would be appreciated.", "author_fullname": "t2_e8k9c3l7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have my job interview for the role of Azure data engineer. I'm really nervous. Please, any tip would be helpful.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199wu5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705602175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two years of experience and I am looking to switch from my current job. The skills required by the company are databricks, Azure data factory, sql and Pyspark. Any tips on what to do and what not to would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "199wu5k", "is_robot_indexable": true, "report_reasons": null, "author": "SignalCrew739", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199wu5k/i_have_my_job_interview_for_the_role_of_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199wu5k/i_have_my_job_interview_for_the_role_of_azure/", "subreddit_subscribers": 153921, "created_utc": 1705602175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm (ab)using Airflow to ETL some data, and pretty quickly figured out that DAGs with dynamically generated tasks are a big no-no. The data I'm moving is being generated on the fly, and is spread across several databases with the only constant being a universal run ID. It seems intuitive to me to collect run IDs over a time window, then sequentially aggregate and load those runs. \n\nThe way I've been doing this is to have an ETL DAG and a scheduled DAG. The scheduled DAG grabs the run IDs, then externally triggers the ETL DAG for each run. However, this just feels like a bad work around to dynamically generating tasks. Is this bad practice? It makes my DAG list quite... busy to say the least, and trouble shooting the scheduler DAG is very difficult since it's effectively running through runs in one single task.\n\nAm I thinking about this problem wrong? Is there some standard way of approaching this type of transfer that I'm not aware of?", "author_fullname": "t2_qid87sfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Externally Triggering DAGs - Good or Bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199w676", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705600549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m (ab)using Airflow to ETL some data, and pretty quickly figured out that DAGs with dynamically generated tasks are a big no-no. The data I&amp;#39;m moving is being generated on the fly, and is spread across several databases with the only constant being a universal run ID. It seems intuitive to me to collect run IDs over a time window, then sequentially aggregate and load those runs. &lt;/p&gt;\n\n&lt;p&gt;The way I&amp;#39;ve been doing this is to have an ETL DAG and a scheduled DAG. The scheduled DAG grabs the run IDs, then externally triggers the ETL DAG for each run. However, this just feels like a bad work around to dynamically generating tasks. Is this bad practice? It makes my DAG list quite... busy to say the least, and trouble shooting the scheduler DAG is very difficult since it&amp;#39;s effectively running through runs in one single task.&lt;/p&gt;\n\n&lt;p&gt;Am I thinking about this problem wrong? Is there some standard way of approaching this type of transfer that I&amp;#39;m not aware of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199w676", "is_robot_indexable": true, "report_reasons": null, "author": "Alwaysragestillplay", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199w676/externally_triggering_dags_good_or_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199w676/externally_triggering_dags_good_or_bad/", "subreddit_subscribers": 153921, "created_utc": 1705600549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are doing another iteration on our dataplatform. This time we will overhaul our ingestion framework from full-loads only to incremental. The deltas are stored as received in a data lake (ADLS Gen2).\n\nWe are using dbt as transformation tool and Azure SQL database as compute / storage.\n\nMy question is how would you get the incrementals staged ready to be referenced by the dbt models?\nHow to set the desired column data types?\nWhere would you perform schema checking?\n\nAny advice is welcome \ud83e\udd17", "author_fullname": "t2_5zyyjiwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODS with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199n6gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705573940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are doing another iteration on our dataplatform. This time we will overhaul our ingestion framework from full-loads only to incremental. The deltas are stored as received in a data lake (ADLS Gen2).&lt;/p&gt;\n\n&lt;p&gt;We are using dbt as transformation tool and Azure SQL database as compute / storage.&lt;/p&gt;\n\n&lt;p&gt;My question is how would you get the incrementals staged ready to be referenced by the dbt models?\nHow to set the desired column data types?\nWhere would you perform schema checking?&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome \ud83e\udd17&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199n6gu", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed_Cantaloupe_9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199n6gu/ods_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199n6gu/ods_with_dbt/", "subreddit_subscribers": 153921, "created_utc": 1705573940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I work as a data engineer at a large entertainment company. I recently received a 12-month contract offer from a FAANG company, offering almost double my current salary. I\u2019m wondering if it\u2019s a smart idea to quit a fulltime position for a FAANG title on my resume? I\u2019m more inclined towards saying no, but I also wanted to know your opinions.", "author_fullname": "t2_2jpbt4sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worth quitting full time DE job for FAANG Contract? (not Amazon)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19a8h1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705632231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I work as a data engineer at a large entertainment company. I recently received a 12-month contract offer from a FAANG company, offering almost double my current salary. I\u2019m wondering if it\u2019s a smart idea to quit a fulltime position for a FAANG title on my resume? I\u2019m more inclined towards saying no, but I also wanted to know your opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19a8h1q", "is_robot_indexable": true, "report_reasons": null, "author": "mnronyasa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a8h1q/worth_quitting_full_time_de_job_for_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a8h1q/worth_quitting_full_time_de_job_for_faang/", "subreddit_subscribers": 153921, "created_utc": 1705632231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data enthusiasts! \ud83d\udc4b\n\n I recently joined a government organization as a data analyst on the business side, and a significant part of my role involves writing Python scripts for ETL processes. I'd love to get your insights and advice on how I can level up my data engineering pipeline, as I'm keen on enhancing efficiency and exploring new possibilities. \n\n**Current Setup:**\n\n* Our primary data resides in an Oracle environment (Production Repo)\n* I've developed several Python scripts to extract, transform, and load data from the production repo to our SQL Server data warehouse.\n* The ETL scripts are scheduled to run daily using .bat scripts on Windows Task Scheduler, and I diligently maintain logs for transparency and troubleshooting.\n* To handle data from external sources (like Vendor portals), I've implemented Selenium scripts, also scheduled for daily execution. These scripts fetch data from the portals and load it into our SQL Server data warehouse.\n* For real-time data needs, I've created Power BI dashboard reports connected to our data warehouse via the company gateway. These reports refresh daily, providing up-to-date insights.\n\n**Current Development Environment:**\n\n* Operating on a Windows virtual machine.\n\n**Seeking Advice:** \n\nIn addition to my current setup, I initially aimed to enhance my data engineering pipeline further by:\n\n* Installing Windows Subsystem for Linux (WSL) on my Windows server.\n* Scheduling scripts using cron tabs from a Docker container.\n* Exploring the use of PySpark for data transformations to optimize script run time.\n\nHowever, I've hit a roadblock \u2013 the IT team has informed me that I can't install WSL on the Windows machine due to virtualization restrictions. Now, I'm contemplating my next steps and would love your insights? ", "author_fullname": "t2_1hygkaab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice to Enhance Data Engineering Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19a7tnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705630287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data enthusiasts! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I recently joined a government organization as a data analyst on the business side, and a significant part of my role involves writing Python scripts for ETL processes. I&amp;#39;d love to get your insights and advice on how I can level up my data engineering pipeline, as I&amp;#39;m keen on enhancing efficiency and exploring new possibilities. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Our primary data resides in an Oracle environment (Production Repo)&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve developed several Python scripts to extract, transform, and load data from the production repo to our SQL Server data warehouse.&lt;/li&gt;\n&lt;li&gt;The ETL scripts are scheduled to run daily using .bat scripts on Windows Task Scheduler, and I diligently maintain logs for transparency and troubleshooting.&lt;/li&gt;\n&lt;li&gt;To handle data from external sources (like Vendor portals), I&amp;#39;ve implemented Selenium scripts, also scheduled for daily execution. These scripts fetch data from the portals and load it into our SQL Server data warehouse.&lt;/li&gt;\n&lt;li&gt;For real-time data needs, I&amp;#39;ve created Power BI dashboard reports connected to our data warehouse via the company gateway. These reports refresh daily, providing up-to-date insights.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Development Environment:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Operating on a Windows virtual machine.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Seeking Advice:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;In addition to my current setup, I initially aimed to enhance my data engineering pipeline further by:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installing Windows Subsystem for Linux (WSL) on my Windows server.&lt;/li&gt;\n&lt;li&gt;Scheduling scripts using cron tabs from a Docker container.&lt;/li&gt;\n&lt;li&gt;Exploring the use of PySpark for data transformations to optimize script run time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, I&amp;#39;ve hit a roadblock \u2013 the IT team has informed me that I can&amp;#39;t install WSL on the Windows machine due to virtualization restrictions. Now, I&amp;#39;m contemplating my next steps and would love your insights? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19a7tnt", "is_robot_indexable": true, "report_reasons": null, "author": "tragediest", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a7tnt/seeking_advice_to_enhance_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a7tnt/seeking_advice_to_enhance_data_engineering/", "subreddit_subscribers": 153921, "created_utc": 1705630287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on choosing and building a healthcare focused data stack with ETL, data processing, analytics, applications + reporting and so on with a bunch of protected information (PHI). I'm curious if anyone has experience or strong opinions on which cloud stack is the most friendly for healthcare applications?\n\nAt this point, I'm primarily considering AWS vs Azure. I like AWS because I have experience with it and also seems more dev friendly. But I've also heard from colleagues about using Azure in healthcare settings. I am not exactly sure if that's mostly a trend or if there are real reasons why one is better than the other. Would love any thoughts or guidance from more experienced folks here!", "author_fullname": "t2_b2u1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers in healthcare? I'm trying to weigh the pros and cons of different cloud stacks and am looking for guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19a5mog", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705624081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on choosing and building a healthcare focused data stack with ETL, data processing, analytics, applications + reporting and so on with a bunch of protected information (PHI). I&amp;#39;m curious if anyone has experience or strong opinions on which cloud stack is the most friendly for healthcare applications?&lt;/p&gt;\n\n&lt;p&gt;At this point, I&amp;#39;m primarily considering AWS vs Azure. I like AWS because I have experience with it and also seems more dev friendly. But I&amp;#39;ve also heard from colleagues about using Azure in healthcare settings. I am not exactly sure if that&amp;#39;s mostly a trend or if there are real reasons why one is better than the other. Would love any thoughts or guidance from more experienced folks here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19a5mog", "is_robot_indexable": true, "report_reasons": null, "author": "maiden_fan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19a5mog/any_data_engineers_in_healthcare_im_trying_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19a5mog/any_data_engineers_in_healthcare_im_trying_to/", "subreddit_subscribers": 153921, "created_utc": 1705624081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I completed this project a couple months ago, and trying to get into the habit of sharing my personal projects. Letterboard is a Streamlit dashboard that uses data scraped from your Letterboxd diary. I used BS4 for web scraping and Polars for data frame aggregations.  I graduated last May and have been job searching for Data Analyst/Engineer jobs since, and would like to know what you all think of this project and where I could probably make this project better. Also I plan on doing more projects utilizing Modern Data Stack tools.\n\nGithub: [https://github.com/afoshiok/Letterboxd-EDA](https://github.com/afoshiok/Letterboxd-EDA)\n\nSite: [https://letterboard.up.railway.app/](https://letterboard.up.railway.app/) (Recommend using this over Streamlit link)\n\n&amp;#x200B;", "author_fullname": "t2_1eg2brwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Letterboard - Generate a dashboard from Letterboxd data (Personal Project)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199u4mt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705595465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed this project a couple months ago, and trying to get into the habit of sharing my personal projects. Letterboard is a Streamlit dashboard that uses data scraped from your Letterboxd diary. I used BS4 for web scraping and Polars for data frame aggregations.  I graduated last May and have been job searching for Data Analyst/Engineer jobs since, and would like to know what you all think of this project and where I could probably make this project better. Also I plan on doing more projects utilizing Modern Data Stack tools.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/afoshiok/Letterboxd-EDA\"&gt;https://github.com/afoshiok/Letterboxd-EDA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Site: &lt;a href=\"https://letterboard.up.railway.app/\"&gt;https://letterboard.up.railway.app/&lt;/a&gt; (Recommend using this over Streamlit link)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?auto=webp&amp;s=89c0f97550d3ba762edd22a9af9906f95a03a87f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0fde9771e782c9d67a82619a4e2996fb6b03f3f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=45ffff65fc38f836e534820fd146278b1ca8b627", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8a93d7fb68cf84f79a99f1064f748a37d6a9220", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f683ad2e10b43ea82b2186820aa0b6ec00b81281", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=28c481fdd0bdb23c713d6fe62587e9a076ec333f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/KuUaMVRsxyE7AwXQ0qB49rcAN8bFDLlJNjanSAw3NFo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98ced41fba2fce853874c2cdf39c3c31463b6a94", "width": 1080, "height": 540}], "variants": {}, "id": "T76JVv14EkzqGzK_0FxjKmol6VWp_W9fFEpnOUHRTBY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "199u4mt", "is_robot_indexable": true, "report_reasons": null, "author": "FavourOshio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199u4mt/letterboard_generate_a_dashboard_from_letterboxd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199u4mt/letterboard_generate_a_dashboard_from_letterboxd/", "subreddit_subscribers": 153921, "created_utc": 1705595465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was working on Informatica Cloud to move some blob storage of Azure on a S3 bucket. The problem is that when a create mapping, I don't know how to parameterized the fact that the blob storage of Azure create each day a new file with a new name. So in my mapping in Informatica the blob to get remain static. Anyone faced this problem and knows how to get the job dinamically? Thanks.", "author_fullname": "t2_27xod4p1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica cloud azure storage issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199rrya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705589225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was working on Informatica Cloud to move some blob storage of Azure on a S3 bucket. The problem is that when a create mapping, I don&amp;#39;t know how to parameterized the fact that the blob storage of Azure create each day a new file with a new name. So in my mapping in Informatica the blob to get remain static. Anyone faced this problem and knows how to get the job dinamically? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199rrya", "is_robot_indexable": true, "report_reasons": null, "author": "DNSoundRM", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199rrya/informatica_cloud_azure_storage_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199rrya/informatica_cloud_azure_storage_issue/", "subreddit_subscribers": 153921, "created_utc": 1705589225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to run the new Prefect setup with a worker, a work pool, a flow, and prefect server. The flow has dependancies on specific versions of requests, pandas, and selenium. I backed up the flow in a remote gut repository and set it up so that workers shall pull it from this repository before every flow run. So, this way, flows stay up to date with the main branch of remote storage.\n\nRight now, it seems like the only way to get a worker to run this flow is to build the worker in an environment that contains all the flows library dependencies. Such as, creating a `venv`, `pip install -r requirements.txt`, `source ./venv/bin/activate`, then `prefect worker start` with environment variables set for the PREFECT_API_URL and PREFECT_API_KEY. Then the worker will connect to the server, see it\u2019s assigned work pool has work, pull the updated flow code from its repo, and execute the flow.\n\nAm I misunderstanding something though? This suggests that every worker must be built with foreknowledge of the flows it will run and the dependancies of that flow. Let\u2019s say you want to add a new flow with new dependencies, seems like you\u2019d need to rebuild the worker to contain those dependencies. If the two flows have conflicting dependencies, you\u2019d need an entire new worker and work pool. This means workers and work pools are essentially organized by clustering compatible tasks based on their dependancies. That specific architecture also seems to limit the usefulness of workers and work pools, as they now must follow this specific organization pattern.\n\nIs there any way at all to associate requirements with a flow, or a python `venv` with a flow, so that the worker just [builds and] activates the necessary `venv` prior to each flow run?\n\nIf not, what am I missing? How are we supposed to set up flows that have package dependancies without imposing additional complexity on how the platform much be used and planned?", "author_fullname": "t2_l924mfj1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect Users: Is it possible to have Flow dependent dynamic runtime environments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199qn92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705586033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to run the new Prefect setup with a worker, a work pool, a flow, and prefect server. The flow has dependancies on specific versions of requests, pandas, and selenium. I backed up the flow in a remote gut repository and set it up so that workers shall pull it from this repository before every flow run. So, this way, flows stay up to date with the main branch of remote storage.&lt;/p&gt;\n\n&lt;p&gt;Right now, it seems like the only way to get a worker to run this flow is to build the worker in an environment that contains all the flows library dependencies. Such as, creating a &lt;code&gt;venv&lt;/code&gt;, &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;, &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;, then &lt;code&gt;prefect worker start&lt;/code&gt; with environment variables set for the PREFECT_API_URL and PREFECT_API_KEY. Then the worker will connect to the server, see it\u2019s assigned work pool has work, pull the updated flow code from its repo, and execute the flow.&lt;/p&gt;\n\n&lt;p&gt;Am I misunderstanding something though? This suggests that every worker must be built with foreknowledge of the flows it will run and the dependancies of that flow. Let\u2019s say you want to add a new flow with new dependencies, seems like you\u2019d need to rebuild the worker to contain those dependencies. If the two flows have conflicting dependencies, you\u2019d need an entire new worker and work pool. This means workers and work pools are essentially organized by clustering compatible tasks based on their dependancies. That specific architecture also seems to limit the usefulness of workers and work pools, as they now must follow this specific organization pattern.&lt;/p&gt;\n\n&lt;p&gt;Is there any way at all to associate requirements with a flow, or a python &lt;code&gt;venv&lt;/code&gt; with a flow, so that the worker just [builds and] activates the necessary &lt;code&gt;venv&lt;/code&gt; prior to each flow run?&lt;/p&gt;\n\n&lt;p&gt;If not, what am I missing? How are we supposed to set up flows that have package dependancies without imposing additional complexity on how the platform much be used and planned?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "199qn92", "is_robot_indexable": true, "report_reasons": null, "author": "Duck-Delta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199qn92/prefect_users_is_it_possible_to_have_flow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199qn92/prefect_users_is_it_possible_to_have_flow/", "subreddit_subscribers": 153921, "created_utc": 1705586033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting Webhooks From Stripe \u2013 The Better Way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_199py4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RxmwO9TWmB9o8UrvueHi_STjUzydioKgXtL_lAdRPIo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705583887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memphis.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memphis.dev/blog/ingesting-webhooks-from-stripe-the-better-way/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?auto=webp&amp;s=857583b54d32d9869c42686585ed649aaedaf0b7", "width": 1600, "height": 901}, "resolutions": [{"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e37ac62b056da1a5a47aa92d0c7151b531de99f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a727bc5438a66e6251305ef2e265b29830c820e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=346a7a04965f8e2d584ba28d5bf61f1e05ed0c4f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=455d9d1a356873cd1a83a331d028132559fc620e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3209f71df27910f4969a4840e3aca1dd2fa3b4e", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rsj3kc_yP5o-LFOBT82E46xJCSlzK5wjvm0czenBdp4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f3bf526ac4844568667cc5d6b9b9cb33168d213", "width": 1080, "height": 608}], "variants": {}, "id": "gtv1gP4SY3ASsYpjxuOzkyxAxRv81vKbbUyaHtwsHRo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199py4s", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199py4s/ingesting_webhooks_from_stripe_the_better_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memphis.dev/blog/ingesting-webhooks-from-stripe-the-better-way/", "subreddit_subscribers": 153921, "created_utc": 1705583887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vbapbjo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_199mah5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/MUnGNI0I2dg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/199mah5", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ixXoDNIuC3k5S2HZ0V2XNKha68B_BGU12enb9j7dZpE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705570252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/MUnGNI0I2dg?si=0sl1yfQ3LcAKpmOG", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?auto=webp&amp;s=469512af6df797365536f82946f5e0d106c506a3", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=935e4ed71c79c9444e2fed14d4a1be9548385b1f", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f56f6284da6a9826514326614264e122aa4dda7", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/aEypdhNIicRoJ749XLe2gLW_Dah1Yel3p9AnuNykCgQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c6767a696cb8e6620741e42a92396aae68364bc", "width": 320, "height": 240}], "variants": {}, "id": "0eBu9j_LeMMEqxEAAh76GzxnlCUcl-M6kk6-9o8qUGg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199mah5", "is_robot_indexable": true, "report_reasons": null, "author": "dnulcon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199mah5/lstms_according_to_their_inventor_j\u00fcrgen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/MUnGNI0I2dg?si=0sl1yfQ3LcAKpmOG", "subreddit_subscribers": 153921, "created_utc": 1705570252.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "LSTMs according to their inventor J\u00fcrgen Schmidhuber", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/MUnGNI0I2dg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"LSTMs according to their inventor J\u00fcrgen Schmidhuber\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/MUnGNI0I2dg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just created a custom GPT to explore various MMP DBs. \n\nIf you have access to the OpenAI GPT Marketplace, please give it a try at https://chat.openai.com/g/g-yrhRXkSOH-mpp-database-consultant. \n\nI would appreciate any feedback on what is working and what is missing.\n\nThank you!", "author_fullname": "t2_3z4w4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MPP Database Consultant GPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_199j43i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1705557433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just created a custom GPT to explore various MMP DBs. &lt;/p&gt;\n\n&lt;p&gt;If you have access to the OpenAI GPT Marketplace, please give it a try at &lt;a href=\"https://chat.openai.com/g/g-yrhRXkSOH-mpp-database-consultant\"&gt;https://chat.openai.com/g/g-yrhRXkSOH-mpp-database-consultant&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate any feedback on what is working and what is missing.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_0d2v6aqibcxdg487iCYbUALEPotmALam0QOsw8aFl4.jpg?auto=webp&amp;s=fc4ff1643f00d16cbee18c5355e308038628fefd", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/_0d2v6aqibcxdg487iCYbUALEPotmALam0QOsw8aFl4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0967c759cc7f7295cf3bb3f85f872bde8e419878", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_0d2v6aqibcxdg487iCYbUALEPotmALam0QOsw8aFl4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=98283101455b4af4693c655f98bb021c964e0119", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_0d2v6aqibcxdg487iCYbUALEPotmALam0QOsw8aFl4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db3e0df875ce96d2371235d730d219dcb3890e0b", "width": 320, "height": 320}], "variants": {}, "id": "UyedLX-UX5HbkaLTYTa--FpSFs-LTIzS4CmgPXvE6dc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "199j43i", "is_robot_indexable": true, "report_reasons": null, "author": "drighten", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/199j43i/mpp_database_consultant_gpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/199j43i/mpp_database_consultant_gpt/", "subreddit_subscribers": 153921, "created_utc": 1705557433.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}