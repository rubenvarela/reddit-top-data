{"kind": "Listing", "data": {"after": "t3_19fad2j", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\ud83e\udd79", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Well guys, this is the end", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_19f9dba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 187, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 187, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/7x5jtUnlshJchLuRn0n02f2A2K7NMjO-ScduvAb2N14.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706189421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83e\udd79&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mnp1r3m49lec1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?auto=webp&amp;s=4f0dca7df6df211fcf921df37278764902c3d9f4", "width": 1170, "height": 1318}, "resolutions": [{"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=39bd1186f7d50b389a5540192cef70656b4ae2c0", "width": 108, "height": 121}, {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f9e81dee3649cc7f0e533734d85ba9c45a9dda9", "width": 216, "height": 243}, {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=87f62fd0e9de9e68c23979b4db66ab7713735478", "width": 320, "height": 360}, {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97004a2f9973caf14f5953169a67ab4bd8e41ac3", "width": 640, "height": 720}, {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8497f97a4766e79f4f690d63baed2e2760044cdd", "width": 960, "height": 1081}, {"url": "https://preview.redd.it/mnp1r3m49lec1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2801009c75ef87cbde970b742be3c1b4dda0919d", "width": 1080, "height": 1216}], "variants": {}, "id": "N8HCCDotPVMPPG6HXT5IifdiSVD2E2JftVCjBSy_CXs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19f9dba", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19f9dba/well_guys_this_is_the_end/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mnp1r3m49lec1.jpeg", "subreddit_subscribers": 155840, "created_utc": 1706189421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been applying to jobs in FAANG since November and I finally got a call back from Apple. Actually I got callbacks from 3 different teams  in the span of 2 weeks. \n\nSo I've got my first interview on Monday and Im confident in my technical skills. I was wondering if anyone here who has worked for, or interviewed at, Apple had any insights or advice that could be useful. \n\n2 of the roles are more standard data engineering positions and 1 is a devops role (SRE).\n\nEdit: I have 3 YOE so these are all mid level positions", "author_fullname": "t2_cyr5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewing at Apple", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fkptt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706251301.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706219104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been applying to jobs in FAANG since November and I finally got a call back from Apple. Actually I got callbacks from 3 different teams  in the span of 2 weeks. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve got my first interview on Monday and Im confident in my technical skills. I was wondering if anyone here who has worked for, or interviewed at, Apple had any insights or advice that could be useful. &lt;/p&gt;\n\n&lt;p&gt;2 of the roles are more standard data engineering positions and 1 is a devops role (SRE).&lt;/p&gt;\n\n&lt;p&gt;Edit: I have 3 YOE so these are all mid level positions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "19fkptt", "is_robot_indexable": true, "report_reasons": null, "author": "bigYman", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fkptt/interviewing_at_apple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fkptt/interviewing_at_apple/", "subreddit_subscribers": 155840, "created_utc": 1706219104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, hope you're all doing well. i got asked this question amd not sure how to respond. So thought of spurcing some ideas here.", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are high in demand skills related to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19faswq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706193527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, hope you&amp;#39;re all doing well. i got asked this question amd not sure how to respond. So thought of spurcing some ideas here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19faswq", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19faswq/what_are_high_in_demand_skills_related_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19faswq/what_are_high_in_demand_skills_related_to_data/", "subreddit_subscribers": 155840, "created_utc": 1706193527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m basically a ETL/dwh dev working in SQL, from here, how can I start working in real time streaming and spark roles?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you get into streaming jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19f7eju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706182784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m basically a ETL/dwh dev working in SQL, from here, how can I start working in real time streaming and spark roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19f7eju", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19f7eju/how_did_you_get_into_streaming_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19f7eju/how_did_you_get_into_streaming_jobs/", "subreddit_subscribers": 155840, "created_utc": 1706182784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Filtering, grouping etc by foreign key is extremely common, which really suggests to me that they make more sense as dimensions for a particular object rather than 'facts', which generally have unique, high-info (a la shanon) data - that don't refer to other objects or tables.\n\nIs there any good reason that in star/snowflake data warehouses foreign keys get put into fact tables instead? It seems illogical - but I'm probably just missing some crucial intuition here.", "author_fullname": "t2_ofoc42j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are foreign keys \"facts\" rather than \"dims\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19f9s1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706190628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Filtering, grouping etc by foreign key is extremely common, which really suggests to me that they make more sense as dimensions for a particular object rather than &amp;#39;facts&amp;#39;, which generally have unique, high-info (a la shanon) data - that don&amp;#39;t refer to other objects or tables.&lt;/p&gt;\n\n&lt;p&gt;Is there any good reason that in star/snowflake data warehouses foreign keys get put into fact tables instead? It seems illogical - but I&amp;#39;m probably just missing some crucial intuition here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19f9s1r", "is_robot_indexable": true, "report_reasons": null, "author": "PangeanPrawn", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19f9s1r/why_are_foreign_keys_facts_rather_than_dims/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19f9s1r/why_are_foreign_keys_facts_rather_than_dims/", "subreddit_subscribers": 155840, "created_utc": 1706190628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI was just hired on for my first data engineer role at a public health insurance company.  They want to start using Python for data engineering and I was wondering if anyone had any resources on best practices, or was available to talk, about how to set up a modern environment where multiple individuals can collaborate on the same Python project remotely.  They are using Azure DevOps for source control and Control-M to schedule jobs.  The specifics of what to think about beyond this, I don't know.  I've used github for solo projects here and there, but don't have much experience collaborating on python projects with multiple people.  I know there are things like Docker for package management and CI/CD, but how all these things fit together I don't know.\n\nThanks for your insights!", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Python Collaboration Between Multiple Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fld6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706220852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I was just hired on for my first data engineer role at a public health insurance company.  They want to start using Python for data engineering and I was wondering if anyone had any resources on best practices, or was available to talk, about how to set up a modern environment where multiple individuals can collaborate on the same Python project remotely.  They are using Azure DevOps for source control and Control-M to schedule jobs.  The specifics of what to think about beyond this, I don&amp;#39;t know.  I&amp;#39;ve used github for solo projects here and there, but don&amp;#39;t have much experience collaborating on python projects with multiple people.  I know there are things like Docker for package management and CI/CD, but how all these things fit together I don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19fld6g", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fld6g/best_practices_for_python_collaboration_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fld6g/best_practices_for_python_collaboration_between/", "subreddit_subscribers": 155840, "created_utc": 1706220852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone else working in this area notice that HL7 standards are never ever ever ever adhered to in a meaningful way? Like forget dealing with date formats. I'm talking medications buried in the text section of a non standard component, in a ccda progress note... when there is clearly a medication section that this would fit nicely in...\n\n\n\nIt's quite stressful knowing that this sort of neglegence can impact extremely vulnerable people.", "author_fullname": "t2_13jzsi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Health Informatics/Interop Woes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fig4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706213187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone else working in this area notice that HL7 standards are never ever ever ever adhered to in a meaningful way? Like forget dealing with date formats. I&amp;#39;m talking medications buried in the text section of a non standard component, in a ccda progress note... when there is clearly a medication section that this would fit nicely in...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s quite stressful knowing that this sort of neglegence can impact extremely vulnerable people.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19fig4i", "is_robot_indexable": true, "report_reasons": null, "author": "Zacho40", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fig4i/health_informaticsinterop_woes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fig4i/health_informaticsinterop_woes/", "subreddit_subscribers": 155840, "created_utc": 1706213187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_41cp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance review of a search engine running on AWS Lambda and S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 58, "top_awarded_type": null, "hide_score": false, "name": "t3_19fcu2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4zTQrOGZwRRAj2dq7yghfMVSjNAvEPNFbQ0B5RRJcN0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706199005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "quickwit.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://quickwit.io/blog/quickwit-lambda-search-performance", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?auto=webp&amp;s=bc0ed551e6931c5712b968489b8c85211b35f199", "width": 1302, "height": 547}, "resolutions": [{"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a654e062c3b50749f752dd663b99bac48bfb9bc9", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea204fea097a8afd9e1bdee615ab2557bcff53a6", "width": 216, "height": 90}, {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a191e3a0648c8f29c4e8c59fe0c6fe6973910111", "width": 320, "height": 134}, {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba2481ac850ee9e129eff79ba6ce67045785b6ca", "width": 640, "height": 268}, {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84fd65e7976eba16ce9bac3fc115262a24bc6175", "width": 960, "height": 403}, {"url": "https://external-preview.redd.it/PE6dARdtpRXRyA_GiDFwgiqlZ8HutBx1EqtWToV9eJ0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1332fcb410cfdae1c623a0324054cef52950ea62", "width": 1080, "height": 453}], "variants": {}, "id": "UjXXZHZCwqwuRgevQS4jdklq31nmjDdEqf9rBYLyQNU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19fcu2s", "is_robot_indexable": true, "report_reasons": null, "author": "massus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fcu2s/performance_review_of_a_search_engine_running_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://quickwit.io/blog/quickwit-lambda-search-performance", "subreddit_subscribers": 155840, "created_utc": 1706199005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found myself dumped into a pretty unique and scary position. I have the fortunate opportunity of being guy #2 in a $300M+ HaaS (selling an IoT device, then charging monthly subscription) company's newly-created business intelligence/analytics division.\n\nIt's an AWS shop. I have experience building pipelines so am familiar with tools like Kinesis, Dynamo, Redshift, Lambdas, Step Funcitons, etc, and I feel like I have a good grasp on IAM with a rough grasp of networking/security. Know Python and ofc SQL.\n\nHowever, I'd consider myself a junior-type guy - most of my experience is me hacking away in the console as an IC in my prior role where I was the only \"data guy\". So, despite some exposure to IaC, git/repos, CICD,  I'm really studying this stuff hard as I plan to use each in this role since there will be a larger team forming around me.\n\nPillars that are already in place:\n- DynamoDB for all crud operations in our app\n- Redshift for data warehouse\n- Spectrum/external link to Stripe data via Lake formation resource share\n- PowerBI for viz\n- CDK for IaC (org standard, no implementation in this division yet)\n- DevSecOps team. Will be able to lean on them in the beginning for bootstrapping CDK stuff \n\nThe division is intended to be a type of CoE for reporting across every functional area of the business (marketing, sales, user activity, warehouse, etc)\n\nQuestion 1: Is it really essential to have a prod &amp; dev environment (separate AWS accounts) for deployments? Since theres no real \"app\", this sort of feels like overkill for DE, especially since many resources are stateful, like Redshift. Feel free to correct my understanding of this topic\n\nQuestion 2: What resources for DE best-practices would you recommend? Would love to get it right the first time around\n\nQuestion 3: Have any advice concerning messaging/ communication of strategy to the business? I will be acting as the lead, so \"selling\" the vision for the division will be part of this role. This comes more-natrually to me than writing code, but I do love to code. Part of this process will also be me defining my role. Like I said - pretty unique position I find myself in\n\nThanks in advance for your help", "author_fullname": "t2_ea32mpd0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building an Analytics Division From Scratch - Seeking Wisdom :)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fdjlw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706201320.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706200868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found myself dumped into a pretty unique and scary position. I have the fortunate opportunity of being guy #2 in a $300M+ HaaS (selling an IoT device, then charging monthly subscription) company&amp;#39;s newly-created business intelligence/analytics division.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an AWS shop. I have experience building pipelines so am familiar with tools like Kinesis, Dynamo, Redshift, Lambdas, Step Funcitons, etc, and I feel like I have a good grasp on IAM with a rough grasp of networking/security. Know Python and ofc SQL.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;d consider myself a junior-type guy - most of my experience is me hacking away in the console as an IC in my prior role where I was the only &amp;quot;data guy&amp;quot;. So, despite some exposure to IaC, git/repos, CICD,  I&amp;#39;m really studying this stuff hard as I plan to use each in this role since there will be a larger team forming around me.&lt;/p&gt;\n\n&lt;p&gt;Pillars that are already in place:\n- DynamoDB for all crud operations in our app\n- Redshift for data warehouse\n- Spectrum/external link to Stripe data via Lake formation resource share\n- PowerBI for viz\n- CDK for IaC (org standard, no implementation in this division yet)\n- DevSecOps team. Will be able to lean on them in the beginning for bootstrapping CDK stuff &lt;/p&gt;\n\n&lt;p&gt;The division is intended to be a type of CoE for reporting across every functional area of the business (marketing, sales, user activity, warehouse, etc)&lt;/p&gt;\n\n&lt;p&gt;Question 1: Is it really essential to have a prod &amp;amp; dev environment (separate AWS accounts) for deployments? Since theres no real &amp;quot;app&amp;quot;, this sort of feels like overkill for DE, especially since many resources are stateful, like Redshift. Feel free to correct my understanding of this topic&lt;/p&gt;\n\n&lt;p&gt;Question 2: What resources for DE best-practices would you recommend? Would love to get it right the first time around&lt;/p&gt;\n\n&lt;p&gt;Question 3: Have any advice concerning messaging/ communication of strategy to the business? I will be acting as the lead, so &amp;quot;selling&amp;quot; the vision for the division will be part of this role. This comes more-natrually to me than writing code, but I do love to code. Part of this process will also be me defining my role. Like I said - pretty unique position I find myself in&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19fdjlw", "is_robot_indexable": true, "report_reasons": null, "author": "__mifflinPaper", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fdjlw/building_an_analytics_division_from_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fdjlw/building_an_analytics_division_from_scratch/", "subreddit_subscribers": 155840, "created_utc": 1706200868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I have a degree in mechanical engineering and have 2 years under my belt as a Business Intelligence developer on a ETL team. We mostly use Informatica and SQL server. I have got a certification with Azure DE associate but no projects at work.\n\nI\u2019d like to transition to a role with more modern/cloud tools. My end goal is to be a ML engineer. I am considering doing GT or UIUC masters in CS. But for now, what are some tips to make you more competitive for roles with modern tooling when you only have experience with Informatica/on-prem architecture? Any tips on resume or things I can do to stand out?", "author_fullname": "t2_d9d6chv71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transition from GUI DE to more modern stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fgexg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706208038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I have a degree in mechanical engineering and have 2 years under my belt as a Business Intelligence developer on a ETL team. We mostly use Informatica and SQL server. I have got a certification with Azure DE associate but no projects at work.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d like to transition to a role with more modern/cloud tools. My end goal is to be a ML engineer. I am considering doing GT or UIUC masters in CS. But for now, what are some tips to make you more competitive for roles with modern tooling when you only have experience with Informatica/on-prem architecture? Any tips on resume or things I can do to stand out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19fgexg", "is_robot_indexable": true, "report_reasons": null, "author": "putt_stuff98", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fgexg/how_to_transition_from_gui_de_to_more_modern_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fgexg/how_to_transition_from_gui_de_to_more_modern_stack/", "subreddit_subscribers": 155840, "created_utc": 1706208038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're moving our workloads to the public cloud and honestly I cannot think of a single advantage of using CDP over EMR. Has anyone had any experience with this? Does anyone have any good reasons to stick with the Cloudera offering? Also Cloudera sound like they're way behind on everything when it comes to the Cloud. Databricks at least promises proprietary optimizations to Spark but Cloudera offers nothing that EMR or Databricks already do better.", "author_fullname": "t2_8ny7vy28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would anyone pick Cloudera Data Platform over Amazon EMR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fbd0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706195053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re moving our workloads to the public cloud and honestly I cannot think of a single advantage of using CDP over EMR. Has anyone had any experience with this? Does anyone have any good reasons to stick with the Cloudera offering? Also Cloudera sound like they&amp;#39;re way behind on everything when it comes to the Cloud. Databricks at least promises proprietary optimizations to Spark but Cloudera offers nothing that EMR or Databricks already do better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19fbd0n", "is_robot_indexable": true, "report_reasons": null, "author": "oontkima", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fbd0n/why_would_anyone_pick_cloudera_data_platform_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fbd0n/why_would_anyone_pick_cloudera_data_platform_over/", "subreddit_subscribers": 155840, "created_utc": 1706195053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 1 yoe as an data analyst with mostly working on SQL. I want to switch jobs but most of the companies also require data visualisation tools knowledge as well. I know SQL, PySpark and Python and little bit of ADF and Databricks but have only worked with SQL.\nI observed companies want following tools knowledge- SQL, python, MS Excel, Tableau/Power BI mostly.\nMy question is from where can I learn these tools to be prepared for interview and where can I get sample projects regarding the same to showcase on my resume?", "author_fullname": "t2_4od3nio3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1abbn48", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706249463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 1 yoe as an data analyst with mostly working on SQL. I want to switch jobs but most of the companies also require data visualisation tools knowledge as well. I know SQL, PySpark and Python and little bit of ADF and Databricks but have only worked with SQL.\nI observed companies want following tools knowledge- SQL, python, MS Excel, Tableau/Power BI mostly.\nMy question is from where can I learn these tools to be prepared for interview and where can I get sample projects regarding the same to showcase on my resume?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1abbn48", "is_robot_indexable": true, "report_reasons": null, "author": "k_amit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abbn48/learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abbn48/learning_resources/", "subreddit_subscribers": 155840, "created_utc": 1706249463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a Data Analyst with focus on Power BI and llittle experience of SSIS. I recently quit my job and started looking for another job in Power BI development.\nI got two recruitment calls and both of them are willing to interview me but for SSIS development.\n\nShould I go ahead and give the interview for SSIS or wait for better opportunity in Azure Data Engineering?", "author_fullname": "t2_159iah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is SSIS still big in Industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aba1zn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706244066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a Data Analyst with focus on Power BI and llittle experience of SSIS. I recently quit my job and started looking for another job in Power BI development.\nI got two recruitment calls and both of them are willing to interview me but for SSIS development.&lt;/p&gt;\n\n&lt;p&gt;Should I go ahead and give the interview for SSIS or wait for better opportunity in Azure Data Engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aba1zn", "is_robot_indexable": true, "report_reasons": null, "author": "internet_baba", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aba1zn/is_ssis_still_big_in_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aba1zn/is_ssis_still_big_in_industry/", "subreddit_subscribers": 155840, "created_utc": 1706244066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nJust got an interview invite to help out a team that uses Amazon ECS for container orchestration and Databricks. \n\nMy guess is the ECS is used to help distinguish various dev environments but doesn\u2019t Databricks do that already? \n\nWhere does Amazon ECS come into play here? Anyone know?", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ECS and Databricks to design, develop and maintain pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_19fncyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/H2RsWKJg-HaCOdnjAtcR9wicy2uCjlUMPfm8dOM5txE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706226111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got an interview invite to help out a team that uses Amazon ECS for container orchestration and Databricks. &lt;/p&gt;\n\n&lt;p&gt;My guess is the ECS is used to help distinguish various dev environments but doesn\u2019t Databricks do that already? &lt;/p&gt;\n\n&lt;p&gt;Where does Amazon ECS come into play here? Anyone know?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dk1wkrh8aoec1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?auto=webp&amp;s=3bc2c7f800f32bdfccd99de16dae584846ee5e1e", "width": 960, "height": 1325}, "resolutions": [{"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3012da3283decf1526e2c3144e6bd3eba1bb3b1e", "width": 108, "height": 149}, {"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43e511de55a6213311f97156d2d31637381f6aad", "width": 216, "height": 298}, {"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=431ae21cdb979d75fa075bfdd4bbdab4e40f5d47", "width": 320, "height": 441}, {"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0314935ab856dd2156dccc0234c54dc758a20c45", "width": 640, "height": 883}, {"url": "https://preview.redd.it/dk1wkrh8aoec1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5bb8a10fb1a198e0e7bb046ed55d12a3a8b2902", "width": 960, "height": 1325}], "variants": {}, "id": "IpJ3wT72IUTAMAleZGkqCW6gpwFplZ0r8hU94mAJSiw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "19fncyj", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fncyj/ecs_and_databricks_to_design_develop_and_maintain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dk1wkrh8aoec1.jpeg", "subreddit_subscribers": 155840, "created_utc": 1706226111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nhey all.. I understand Databricks is the best choice when it comes to these Deltalake jargon.. But bear with me, in this case I'll only focus on Synapse.\n\nCurrent situation, using Synapse Pipeline for Orchestration and Ingestion from source (copy activity), then data transformation using Spark Pool with pyspark.. Result is in delta table format.. Now questions:\n\n1. I noticed in dbricks we have SQL Endpoint that able to connect PBI to adls delta table.. Similar things non-existent in Synapse? Do we really need to push data to Dedicated pool to be able to use these data in PBI? If so, what's the point to have uncosumable delta table in Synapse?\n\n2. Have heated args with colleague about whether compute in Spark Pool (coding + sql) vs Dedicated Pool (pure SQL StoredProc styles).. I'm not a big fan of Datalake with dwh-table styles, as datalake should be an open table format which can help user with so many possiblities such as ML/AI, programming-styles development, sql-styles development, etc.. Not only limiting people with sql. Then we can build dwh on top of Datalake (like gold layer or whatever those jargons are).. Proof me if I was wrong? \n\nfyi we have 3 big ERP as sources, such as: Oracle &amp; SAP.. Multiple DBs and Hundreds of tables as sources..", "author_fullname": "t2_tro7fcb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deltalake using Azure Synapse Analytics/Workspace is good?? Please advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19faisc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706192755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey all.. I understand Databricks is the best choice when it comes to these Deltalake jargon.. But bear with me, in this case I&amp;#39;ll only focus on Synapse.&lt;/p&gt;\n\n&lt;p&gt;Current situation, using Synapse Pipeline for Orchestration and Ingestion from source (copy activity), then data transformation using Spark Pool with pyspark.. Result is in delta table format.. Now questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I noticed in dbricks we have SQL Endpoint that able to connect PBI to adls delta table.. Similar things non-existent in Synapse? Do we really need to push data to Dedicated pool to be able to use these data in PBI? If so, what&amp;#39;s the point to have uncosumable delta table in Synapse?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have heated args with colleague about whether compute in Spark Pool (coding + sql) vs Dedicated Pool (pure SQL StoredProc styles).. I&amp;#39;m not a big fan of Datalake with dwh-table styles, as datalake should be an open table format which can help user with so many possiblities such as ML/AI, programming-styles development, sql-styles development, etc.. Not only limiting people with sql. Then we can build dwh on top of Datalake (like gold layer or whatever those jargons are).. Proof me if I was wrong? &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;fyi we have 3 big ERP as sources, such as: Oracle &amp;amp; SAP.. Multiple DBs and Hundreds of tables as sources..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19faisc", "is_robot_indexable": true, "report_reasons": null, "author": "BumbleBeeBumbleBoo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19faisc/deltalake_using_azure_synapse_analyticsworkspace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19faisc/deltalake_using_azure_synapse_analyticsworkspace/", "subreddit_subscribers": 155840, "created_utc": 1706192755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have worked with multiple B2C and B2B companies of different maturity for several years.   \nIn this blog post, I summarize and compare what I have learned about the three types of data models most companies use for product events.\n\nIf you are planing to build your own product event model in the data warehouse, I recommend to read it and not fall into the same mistakes as some of my previous companies did.\n\n[https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse](https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse)\n\n&amp;#x200B;", "author_fullname": "t2_gnytqihqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 ways to model product events in the data warehouse.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19f6git", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706179038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have worked with multiple B2C and B2B companies of different maturity for several years.&lt;br/&gt;\nIn this blog post, I summarize and compare what I have learned about the three types of data models most companies use for product events.&lt;/p&gt;\n\n&lt;p&gt;If you are planing to build your own product event model in the data warehouse, I recommend to read it and not fall into the same mistakes as some of my previous companies did.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse\"&gt;https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?auto=webp&amp;s=a6405f1ddd715d75ba917aaa6a3e953997e6c0e6", "width": 1776, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35d45dc739331bae0e83a52f2e94dd7dadd6eb99", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea404cd6448b2d564aec3a9050a5e2c11082ebaf", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f0d1523901bdcc75523ee9b32fed5ff18c918f6", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c803f1d3e226dd17602bc86180030c12b777b22", "width": 640, "height": 288}, {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e3f07a61fdbee5140f04604e1da5e127f328b26", "width": 960, "height": 432}, {"url": "https://external-preview.redd.it/m9okk261-0QiOIOno3CM3AvzRfFFQyI3oINfL8MMUhs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d1a20828f77e021e13d53f6fcd04868bb8f306c", "width": 1080, "height": 486}], "variants": {}, "id": "ER1-rrLJA1pJrZ4LgIKIuK3grEHKYh-c05izYQu72iw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19f6git", "is_robot_indexable": true, "report_reasons": null, "author": "MitzuIstvan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19f6git/3_ways_to_model_product_events_in_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19f6git/3_ways_to_model_product_events_in_the_data/", "subreddit_subscribers": 155840, "created_utc": 1706179038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI\u2019m curious, what\u2019s the right way to let someone access private cloud resources from their home? I have a typical public / private subnet setup with a security group on the public that lets only office IP access anything,\n\nShould I let them vpn into the VPC network, or have them vpn into a place with a permitted static IP?\n\nI\u2019m using AWS. What\u2019s considered the easiest? What\u2019s considered the cheapest? Any reliable open source solutions?\n\nThanks!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to let users access private cloud resources from their home?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abamz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706245987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious, what\u2019s the right way to let someone access private cloud resources from their home? I have a typical public / private subnet setup with a security group on the public that lets only office IP access anything,&lt;/p&gt;\n\n&lt;p&gt;Should I let them vpn into the VPC network, or have them vpn into a place with a permitted static IP?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m using AWS. What\u2019s considered the easiest? What\u2019s considered the cheapest? Any reliable open source solutions?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1abamz6", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abamz6/best_way_to_let_users_access_private_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abamz6/best_way_to_let_users_access_private_cloud/", "subreddit_subscribers": 155840, "created_utc": 1706245987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just got a second interview for an ML &amp; Data Engineering Manager at a huge pharma company. I\u2019ve been a Product Manager, Analytics Engineer, data scientist, but not a data engineer. \n\nWhat makes a good data engineering and ML manager from your experience? \n\nI know it\u2019s a broad question and can vary, but any general advice to know would be great. I am not sure if my skills in managing teams well would directly apply to this role.", "author_fullname": "t2_lh8s5ycdc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good ML &amp; Data Engineering Manager?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fjowq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706216330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got a second interview for an ML &amp;amp; Data Engineering Manager at a huge pharma company. I\u2019ve been a Product Manager, Analytics Engineer, data scientist, but not a data engineer. &lt;/p&gt;\n\n&lt;p&gt;What makes a good data engineering and ML manager from your experience? &lt;/p&gt;\n\n&lt;p&gt;I know it\u2019s a broad question and can vary, but any general advice to know would be great. I am not sure if my skills in managing teams well would directly apply to this role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19fjowq", "is_robot_indexable": true, "report_reasons": null, "author": "imjusthereforPMstuff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fjowq/what_makes_a_good_ml_data_engineering_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fjowq/what_makes_a_good_ml_data_engineering_manager/", "subreddit_subscribers": 155840, "created_utc": 1706216330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those of you that are happy with your current job, please can you give me 5 of your main reasons why? \n\n\nI\u2019m in my first DE job, and it has progressively worsened since I\u2019ve joined. I just want to see what it\u2019s like for others in their roles. \n\nThanks in advance", "author_fullname": "t2_ecnqt6o1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19ffjqd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706205808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you that are happy with your current job, please can you give me 5 of your main reasons why? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m in my first DE job, and it has progressively worsened since I\u2019ve joined. I just want to see what it\u2019s like for others in their roles. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19ffjqd", "is_robot_indexable": true, "report_reasons": null, "author": "silentwardrbe", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19ffjqd/need_some_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19ffjqd/need_some_advice/", "subreddit_subscribers": 155840, "created_utc": 1706205808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd love your feedback on a different approach to Retrieval Augmented Generation (RAG) for LLMs that uses function calling to retrieve the most relevant data from APIs. It aims to solve the problem of connecting LLMs with your data so that the LLM can pull in the context it needs to provide high quality answers to user questions. \n\nThe [Github repository](https://github.com/DataSQRL/apiRAG) contains the code and some examples. Here is a video that shows how it works:\n\n[https://www.youtube.com/watch?v=Mx-slh6h42c](https://www.youtube.com/watch?v=Mx-slh6h42c)\n\nWe developed apiRAG because we found that existing RAG approaches (text search, vector based, FLARE, etc) don't work well with structured and semi-structured data.\n\napiRAG can efficiently augment from structured and semi-structured data by translating user questions to relevant API requests and then presenting the result data to the user. It supports textual presentation (with Markdown for tables and such) and visual presentation by charting data when appropriate. Check out the IoT chatbot that can answer questions about collected sensor data or the credit card example that gives users a customized spending analysis (shown in the video).\n\nUnlike RAG approaches that use text or vector search, apiRAG uses the LLM to determine what information should be retrieved and augmented into the context via function calling. \n\nThat means, it often does a better job at identifying relevant information by pushing down filters, like when you ask \"Who appears in the third episode of season 2\" in our \"Rick and Morty\" example.", "author_fullname": "t2_ihfn9f9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "apiRAG: Retrieval Augmented Generation for LLMs from APIs with Function Calling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fenqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706203653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d love your feedback on a different approach to Retrieval Augmented Generation (RAG) for LLMs that uses function calling to retrieve the most relevant data from APIs. It aims to solve the problem of connecting LLMs with your data so that the LLM can pull in the context it needs to provide high quality answers to user questions. &lt;/p&gt;\n\n&lt;p&gt;The &lt;a href=\"https://github.com/DataSQRL/apiRAG\"&gt;Github repository&lt;/a&gt; contains the code and some examples. Here is a video that shows how it works:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=Mx-slh6h42c\"&gt;https://www.youtube.com/watch?v=Mx-slh6h42c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We developed apiRAG because we found that existing RAG approaches (text search, vector based, FLARE, etc) don&amp;#39;t work well with structured and semi-structured data.&lt;/p&gt;\n\n&lt;p&gt;apiRAG can efficiently augment from structured and semi-structured data by translating user questions to relevant API requests and then presenting the result data to the user. It supports textual presentation (with Markdown for tables and such) and visual presentation by charting data when appropriate. Check out the IoT chatbot that can answer questions about collected sensor data or the credit card example that gives users a customized spending analysis (shown in the video).&lt;/p&gt;\n\n&lt;p&gt;Unlike RAG approaches that use text or vector search, apiRAG uses the LLM to determine what information should be retrieved and augmented into the context via function calling. &lt;/p&gt;\n\n&lt;p&gt;That means, it often does a better job at identifying relevant information by pushing down filters, like when you ask &amp;quot;Who appears in the third episode of season 2&amp;quot; in our &amp;quot;Rick and Morty&amp;quot; example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?auto=webp&amp;s=615faf8a112ece822dd323802445a183a09169d6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b16c4bcde81d8967ecabf3f87e1e556811a9bc03", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53f6226920f4c12fc2cbb7be4308129f21ca98a8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0afa704e5c38fbb469012b9e0d17305387374730", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=833011a8acb2ab6b20912e82054e644b7a1ad5c1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=66498436f53477a496f16b3797a73b9e5c79d14b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SB1YUp3KDqsZiv4ycM-cGqahhUdMOHQEGbnyDG-k4pI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73c72cac2a35f9e30b6316cf33537bc060e1231c", "width": 1080, "height": 540}], "variants": {}, "id": "baT8p7sg8PtZqgSqU2UJJ0SP44rUZB6onLHp9cUY148"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "19fenqt", "is_robot_indexable": true, "report_reasons": null, "author": "matthiasBcom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fenqt/apirag_retrieval_augmented_generation_for_llms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fenqt/apirag_retrieval_augmented_generation_for_llms/", "subreddit_subscribers": 155840, "created_utc": 1706203653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've a question for those who use DBT on a daily basis. So i was working on defining some models in DBT and we ran into this problem. The naming convention that we are currently using to name our table is useful and intuitive for us (ML engineering team) but not for the DS teams that use final tables (e.g. table name **sales\\_customer\\_info\\_customer\\_country\\_daily** should be something like **customer\\_distribution**).  \nTherefore, my question is, how can i set up the YAML file for that specific table so that it uses an alias when building the final table keeping the current name unchanged for our dbt project?   \nI already tried browsing the DBT documentation but haven't found anything useful ", "author_fullname": "t2_6glf220s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT final table aliases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fdkvi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706200961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve a question for those who use DBT on a daily basis. So i was working on defining some models in DBT and we ran into this problem. The naming convention that we are currently using to name our table is useful and intuitive for us (ML engineering team) but not for the DS teams that use final tables (e.g. table name &lt;strong&gt;sales_customer_info_customer_country_daily&lt;/strong&gt; should be something like &lt;strong&gt;customer_distribution&lt;/strong&gt;).&lt;br/&gt;\nTherefore, my question is, how can i set up the YAML file for that specific table so that it uses an alias when building the final table keeping the current name unchanged for our dbt project?&lt;br/&gt;\nI already tried browsing the DBT documentation but haven&amp;#39;t found anything useful &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19fdkvi", "is_robot_indexable": true, "report_reasons": null, "author": "stefano250396", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fdkvi/dbt_final_table_aliases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fdkvi/dbt_final_table_aliases/", "subreddit_subscribers": 155840, "created_utc": 1706200961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, I lead a product management team. One of our products is around specific features oriented at data engineers.  \n\n\nI'd like to get up to speed hands-on with the core workflows and problems that data engineers face. Particularly around data transformations.  \n\n\nDo you have any good hands-on training suggestions that could give a good flavor - working with Databricks, Snowflake, Azure Synapse / Fabric, dbt to provide an immersive experience for a determined PM with some coding and SQL skills? perhaps some course that has an overview of multiple platforms?", "author_fullname": "t2_ccory", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proposed short courses for starting on the data engineering for product people", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fbtgh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706196283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, I lead a product management team. One of our products is around specific features oriented at data engineers.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to get up to speed hands-on with the core workflows and problems that data engineers face. Particularly around data transformations.  &lt;/p&gt;\n\n&lt;p&gt;Do you have any good hands-on training suggestions that could give a good flavor - working with Databricks, Snowflake, Azure Synapse / Fabric, dbt to provide an immersive experience for a determined PM with some coding and SQL skills? perhaps some course that has an overview of multiple platforms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19fbtgh", "is_robot_indexable": true, "report_reasons": null, "author": "ivenger", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fbtgh/proposed_short_courses_for_starting_on_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fbtgh/proposed_short_courses_for_starting_on_the_data/", "subreddit_subscribers": 155840, "created_utc": 1706196283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have some delta tables set up in Databricks and the question came up about whether we could do some querying against them without spinning up a Databricks compute cluster.\n\nI was trying to use some python libraries to get this done, namely:  \n\\- delta-rs: used this to open the delta table and convert to a pyarrow table  \n\\- duckdb: used this to perform queries against this\n\nThe problem I ran into is that I couldn't get it into a pyarrow table without loading the entire thing into memory, which is a problem for very large tables.  Is there a way around this?  My attempts have been unsuccessful to this point.\n\nNote:  I'm not a data engineer, so I'm kind of winging it here.  We're also trying to stick to Python for this, as it aligns with the technology stack we are using.", "author_fullname": "t2_763buppi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "delta table querying question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fauzo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706193679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have some delta tables set up in Databricks and the question came up about whether we could do some querying against them without spinning up a Databricks compute cluster.&lt;/p&gt;\n\n&lt;p&gt;I was trying to use some python libraries to get this done, namely:&lt;br/&gt;\n- delta-rs: used this to open the delta table and convert to a pyarrow table&lt;br/&gt;\n- duckdb: used this to perform queries against this&lt;/p&gt;\n\n&lt;p&gt;The problem I ran into is that I couldn&amp;#39;t get it into a pyarrow table without loading the entire thing into memory, which is a problem for very large tables.  Is there a way around this?  My attempts have been unsuccessful to this point.&lt;/p&gt;\n\n&lt;p&gt;Note:  I&amp;#39;m not a data engineer, so I&amp;#39;m kind of winging it here.  We&amp;#39;re also trying to stick to Python for this, as it aligns with the technology stack we are using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19fauzo", "is_robot_indexable": true, "report_reasons": null, "author": "jaltiere", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fauzo/delta_table_querying_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fauzo/delta_table_querying_question/", "subreddit_subscribers": 155840, "created_utc": 1706193679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a project where i'm using apache beam's  `ReadFromJdbc` for reading a set of data in a certain time range from a database in one sub-network and then processing the data and finally writing the processed dataset to a table in another database in a separate sub-network using `WriteToJdbc`.  I'm using Apache Beam's Python SDK to develop the scripts. These pipelines i build will be hosted in Google's cloud data flow. \n\n&amp;#x200B;\n\nI need some help from you guys/gals clarifying few questions i have regarding this project which i wasn't able to find answers to by my own. My questions are,\n\n1. If the cloud data-flow  pipeline fails during a writing phase is there a method to implement a retry mechanism? I ask this cause if the data is being written parallel to the target tables i can't use the  maximum id written to the database to start from there. \n2. When reading from the database how do i make sure the reading process doesn't cause any issues to the other operations that takes place in the database. (I'm reading data from a database that is being used by another application, how to make sure read, write operations to this database isn't effected by my pipeline.)\n3. I need to read data from several tables and then write them to several tables in the target database, can i do this in the same pipeline like the below code snippets given at the end of the post? If not and i have to build separate pipelines for each read and write operations is there a way i can ensure a cloud data flow pipeline runs its job right after the completion of the first  pipeline's job is completed. (The reason i want the pipeline to run one after the another is to avoid the high resource burden it might cause on the source database when the data is being read at the same time).\n\n&amp;#x200B;\n\nMy code snippets for possible solutions to read from  and write to multiple tables are given below. Let me know if this is possible. \n\n    def run(source_table, source_connection_url, source_username, source_password, source_query, target_table, target_connection_url, target_username, target_password):\n        options = PipelineOptions(\n            runner='DataflowRunner',\n            project='',\n            staging_location='',\n            temp_location='',\n            region='',\n            template_location='',\n        )\n        with beam.Pipeline(options=options) as pipeline:\n            pcollection_tablex = (\n                pipeline\n                | f'Read from source database tablex' &gt;&gt; ReadFromJdbc(\n                    table_name=source_table,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=source_connection_url,\n                    username=source_username,\n                    password=source_password,\n                    query=source_query,\n                    fetch_size=5000\n                )\n            )\n            (\n                pcollection\n                | 'Write to target database tablex' &gt;&gt; WriteToJdbc(\n                    table_name=target_table,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=target_connection_url,\n                    username=target_username,\n                    password=target_password\n                )\n            )\n            pcollection_tabley = (\n                pipeline\n                | f'Read from source database tabley' &gt;&gt; ReadFromJdbc(\n                    table_name=source_table_y,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=source_connection_url_y,\n                    username=source_username_y,\n                    password=source_password_y,\n                    query=source_query_y,\n                    fetch_size=5000\n                )\n            )\n            (\n                pcollection_tabley\n                | 'Write to target database tabley' &gt;&gt; WriteToJdbc(\n                    table_name=target_table_y,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=target_connection_url_y,\n                    username=target_username_y,\n                    password=target_password_y\n                )\n            )\n    \n    \n    \n    \n    \n    if __name__ == '__main__':\n        run()\n\n&amp;#x200B;\n\n    def run(source_table_x, source_connection_url_x, source_username_x, source_password_x, source_query_x, target_table_x, target_connection_url_x, target_username_x, target_password_x,\n            source_table_y, source_connection_url_y, source_username_y, source_password_y, source_query_y, target_table_y, target_connection_url_y, target_username_y, target_password_y):\n        options = PipelineOptions(\n            runner='DataflowRunner',\n            project='',\n            staging_location='',\n            temp_location='',\n            region='',\n            template_location='',\n        )\n    \n        with beam.Pipeline(options=options) as pipeline:\n            pcollection_tablex = (\n                pipeline\n                | f'Read from source database tablex' &gt;&gt; ReadFromJdbc(\n                    table_name=source_table_x,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=source_connection_url_x,\n                    username=source_username_x,\n                    password=source_password_x,\n                    query=source_query_x,\n                    fetch_size=5000\n                )\n            )\n            (\n                pcollection_tablex\n                | 'Write to target database tablex' &gt;&gt; WriteToJdbc(\n                    table_name=target_table_x,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=target_connection_url_x,\n                    username=target_username_x,\n                    password=target_password_x\n                )\n            )\n    \n        with beam.Pipeline(options=options) as pipeline:\n            pcollection_tabley = (\n                pipeline\n                | f'Read from source database tabley' &gt;&gt; ReadFromJdbc(\n                    table_name=source_table_y,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=source_connection_url_y,\n                    username=source_username_y,\n                    password=source_password_y,\n                    query=source_query_y,\n                    fetch_size=5000\n                )\n            )\n            (\n                pcollection_tabley\n                | 'Write to target database tabley' &gt;&gt; WriteToJdbc(\n                    table_name=target_table_y,\n                    driver_class_name='com.mysql.cj.jdbc.Driver',\n                    jdbc_url=target_connection_url_y,\n                    username=target_username_y,\n                    password=target_password_y\n                )\n            )\n\n&amp;#x200B;", "author_fullname": "t2_cqi53m2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Understanding Cloud Dataflow Retry Mechanism and Sequential Execution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19f3vul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706167747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project where i&amp;#39;m using apache beam&amp;#39;s  &lt;code&gt;ReadFromJdbc&lt;/code&gt; for reading a set of data in a certain time range from a database in one sub-network and then processing the data and finally writing the processed dataset to a table in another database in a separate sub-network using &lt;code&gt;WriteToJdbc&lt;/code&gt;.  I&amp;#39;m using Apache Beam&amp;#39;s Python SDK to develop the scripts. These pipelines i build will be hosted in Google&amp;#39;s cloud data flow. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need some help from you guys/gals clarifying few questions i have regarding this project which i wasn&amp;#39;t able to find answers to by my own. My questions are,&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If the cloud data-flow  pipeline fails during a writing phase is there a method to implement a retry mechanism? I ask this cause if the data is being written parallel to the target tables i can&amp;#39;t use the  maximum id written to the database to start from there. &lt;/li&gt;\n&lt;li&gt;When reading from the database how do i make sure the reading process doesn&amp;#39;t cause any issues to the other operations that takes place in the database. (I&amp;#39;m reading data from a database that is being used by another application, how to make sure read, write operations to this database isn&amp;#39;t effected by my pipeline.)&lt;/li&gt;\n&lt;li&gt;I need to read data from several tables and then write them to several tables in the target database, can i do this in the same pipeline like the below code snippets given at the end of the post? If not and i have to build separate pipelines for each read and write operations is there a way i can ensure a cloud data flow pipeline runs its job right after the completion of the first  pipeline&amp;#39;s job is completed. (The reason i want the pipeline to run one after the another is to avoid the high resource burden it might cause on the source database when the data is being read at the same time).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My code snippets for possible solutions to read from  and write to multiple tables are given below. Let me know if this is possible. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def run(source_table, source_connection_url, source_username, source_password, source_query, target_table, target_connection_url, target_username, target_password):\n    options = PipelineOptions(\n        runner=&amp;#39;DataflowRunner&amp;#39;,\n        project=&amp;#39;&amp;#39;,\n        staging_location=&amp;#39;&amp;#39;,\n        temp_location=&amp;#39;&amp;#39;,\n        region=&amp;#39;&amp;#39;,\n        template_location=&amp;#39;&amp;#39;,\n    )\n    with beam.Pipeline(options=options) as pipeline:\n        pcollection_tablex = (\n            pipeline\n            | f&amp;#39;Read from source database tablex&amp;#39; &amp;gt;&amp;gt; ReadFromJdbc(\n                table_name=source_table,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=source_connection_url,\n                username=source_username,\n                password=source_password,\n                query=source_query,\n                fetch_size=5000\n            )\n        )\n        (\n            pcollection\n            | &amp;#39;Write to target database tablex&amp;#39; &amp;gt;&amp;gt; WriteToJdbc(\n                table_name=target_table,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=target_connection_url,\n                username=target_username,\n                password=target_password\n            )\n        )\n        pcollection_tabley = (\n            pipeline\n            | f&amp;#39;Read from source database tabley&amp;#39; &amp;gt;&amp;gt; ReadFromJdbc(\n                table_name=source_table_y,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=source_connection_url_y,\n                username=source_username_y,\n                password=source_password_y,\n                query=source_query_y,\n                fetch_size=5000\n            )\n        )\n        (\n            pcollection_tabley\n            | &amp;#39;Write to target database tabley&amp;#39; &amp;gt;&amp;gt; WriteToJdbc(\n                table_name=target_table_y,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=target_connection_url_y,\n                username=target_username_y,\n                password=target_password_y\n            )\n        )\n\n\n\n\n\nif __name__ == &amp;#39;__main__&amp;#39;:\n    run()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def run(source_table_x, source_connection_url_x, source_username_x, source_password_x, source_query_x, target_table_x, target_connection_url_x, target_username_x, target_password_x,\n        source_table_y, source_connection_url_y, source_username_y, source_password_y, source_query_y, target_table_y, target_connection_url_y, target_username_y, target_password_y):\n    options = PipelineOptions(\n        runner=&amp;#39;DataflowRunner&amp;#39;,\n        project=&amp;#39;&amp;#39;,\n        staging_location=&amp;#39;&amp;#39;,\n        temp_location=&amp;#39;&amp;#39;,\n        region=&amp;#39;&amp;#39;,\n        template_location=&amp;#39;&amp;#39;,\n    )\n\n    with beam.Pipeline(options=options) as pipeline:\n        pcollection_tablex = (\n            pipeline\n            | f&amp;#39;Read from source database tablex&amp;#39; &amp;gt;&amp;gt; ReadFromJdbc(\n                table_name=source_table_x,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=source_connection_url_x,\n                username=source_username_x,\n                password=source_password_x,\n                query=source_query_x,\n                fetch_size=5000\n            )\n        )\n        (\n            pcollection_tablex\n            | &amp;#39;Write to target database tablex&amp;#39; &amp;gt;&amp;gt; WriteToJdbc(\n                table_name=target_table_x,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=target_connection_url_x,\n                username=target_username_x,\n                password=target_password_x\n            )\n        )\n\n    with beam.Pipeline(options=options) as pipeline:\n        pcollection_tabley = (\n            pipeline\n            | f&amp;#39;Read from source database tabley&amp;#39; &amp;gt;&amp;gt; ReadFromJdbc(\n                table_name=source_table_y,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=source_connection_url_y,\n                username=source_username_y,\n                password=source_password_y,\n                query=source_query_y,\n                fetch_size=5000\n            )\n        )\n        (\n            pcollection_tabley\n            | &amp;#39;Write to target database tabley&amp;#39; &amp;gt;&amp;gt; WriteToJdbc(\n                table_name=target_table_y,\n                driver_class_name=&amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n                jdbc_url=target_connection_url_y,\n                username=target_username_y,\n                password=target_password_y\n            )\n        )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19f3vul", "is_robot_indexable": true, "report_reasons": null, "author": "Plus_Flight8909", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19f3vul/need_help_understanding_cloud_dataflow_retry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19f3vul/need_help_understanding_cloud_dataflow_retry/", "subreddit_subscribers": 155840, "created_utc": 1706167747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is your preferred way in modeling your warehouse and why? Any tips to better execute common modeling situations?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preferred Approach to Warehouse Modeling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fad2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706192307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is your preferred way in modeling your warehouse and why? Any tips to better execute common modeling situations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19fad2j", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19fad2j/preferred_approach_to_warehouse_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19fad2j/preferred_approach_to_warehouse_modeling/", "subreddit_subscribers": 155840, "created_utc": 1706192307.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}