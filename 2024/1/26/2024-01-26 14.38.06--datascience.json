{"kind": "Listing", "data": {"after": null, "dist": 4, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have worked on several forecasting projects in the past few months, and I decided to write a blog to share my learnings and insights with data analysts and junior data scientists. After writing the blog, I submitted it to TDS. They rejected it, stating that \n\n    'the overall flow of the post was too disjointed and the approach to the topic was somewhat too high-level and not actionable/concrete enough.' \n\nI don't blame them for this feedback, and I've done some editing to make the article smoother. Has the article improved? Anything I should add to the article? I hope to turn this around and win back on TDS. Any advise will be helpful.  \n\n\nI've post it here: [https://acho.io/blogs/why-i-perfer-tree-models](https://acho.io/blogs/why-i-perfer-tree-models)\n\n&amp;#x200B;", "author_fullname": "t2_qs6vh93qj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got rejected by Toward Datascience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fdpwm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706201329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked on several forecasting projects in the past few months, and I decided to write a blog to share my learnings and insights with data analysts and junior data scientists. After writing the blog, I submitted it to TDS. They rejected it, stating that &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;#39;the overall flow of the post was too disjointed and the approach to the topic was somewhat too high-level and not actionable/concrete enough.&amp;#39; \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I don&amp;#39;t blame them for this feedback, and I&amp;#39;ve done some editing to make the article smoother. Has the article improved? Anything I should add to the article? I hope to turn this around and win back on TDS. Any advise will be helpful.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve post it here: &lt;a href=\"https://acho.io/blogs/why-i-perfer-tree-models\"&gt;https://acho.io/blogs/why-i-perfer-tree-models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19fdpwm", "is_robot_indexable": true, "report_reasons": null, "author": "zachzachaaaa", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19fdpwm/i_got_rejected_by_toward_datascience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19fdpwm/i_got_rejected_by_toward_datascience/", "subreddit_subscribers": 1278100, "created_utc": 1706201329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How did you accomplish it?\n\nI have \\~7 years of experience as a Data Scientist -- I've worked my way up that ladder to a medium-high-level, but I've come to the realization that what I actually care about is the \"data librarian\" kind of stuff that gets ascribed to analytics engineers. Stuff like, what's the canonical way of doing this transformation or filtering this data, what's the definitive source of this information, where should you be pulling from, how do we prevent unexpected values from breaking pipelines or visualizations (or generating incorrect financial numbers). The kind of thing where you centralize and review changes to how data is structured, transformed and stored.... hopefully along the way you also reduce the DB and development costs associated with the team.\n\nI've been interviewing for DS roles since being laid off last year, but I think what I actually want to do is address the kinds of questions I mentioned above. Anybody made this kind of leap? What did it take to accomplish it?", "author_fullname": "t2_2yhkq8zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone jumped from DS to analytics engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fegcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706203152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How did you accomplish it?&lt;/p&gt;\n\n&lt;p&gt;I have ~7 years of experience as a Data Scientist -- I&amp;#39;ve worked my way up that ladder to a medium-high-level, but I&amp;#39;ve come to the realization that what I actually care about is the &amp;quot;data librarian&amp;quot; kind of stuff that gets ascribed to analytics engineers. Stuff like, what&amp;#39;s the canonical way of doing this transformation or filtering this data, what&amp;#39;s the definitive source of this information, where should you be pulling from, how do we prevent unexpected values from breaking pipelines or visualizations (or generating incorrect financial numbers). The kind of thing where you centralize and review changes to how data is structured, transformed and stored.... hopefully along the way you also reduce the DB and development costs associated with the team.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been interviewing for DS roles since being laid off last year, but I think what I actually want to do is address the kinds of questions I mentioned above. Anybody made this kind of leap? What did it take to accomplish it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19fegcf", "is_robot_indexable": true, "report_reasons": null, "author": "finite_user_names", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19fegcf/has_anyone_jumped_from_ds_to_analytics_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19fegcf/has_anyone_jumped_from_ds_to_analytics_engineer/", "subreddit_subscribers": 1278100, "created_utc": 1706203152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I read an article in medium (yeah, I know this is starting bad) about data visualization for Large Data that went from using pyspark on the data warehouse to a plotly-dash dashboard, with a pretty sneaky toPandas transformation on the middle. This step was which I thought it was actually killing the idea of visualization on big data, but it is actually practical? Or everyone just filter the data to get a light representation?", "author_fullname": "t2_t4026fbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How common is Data Visualization near real-time for Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19fatcn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706193559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read an article in medium (yeah, I know this is starting bad) about data visualization for Large Data that went from using pyspark on the data warehouse to a plotly-dash dashboard, with a pretty sneaky toPandas transformation on the middle. This step was which I thought it was actually killing the idea of visualization on big data, but it is actually practical? Or everyone just filter the data to get a light representation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "19fatcn", "is_robot_indexable": true, "report_reasons": null, "author": "chris_813", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/19fatcn/how_common_is_data_visualization_near_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/19fatcn/how_common_is_data_visualization_near_realtime/", "subreddit_subscribers": 1278100, "created_utc": 1706193559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m working on tackling daily and weekly swings in some of our key metrics. These metrics are based on similar data coming through our logistics system that basically a waterfall. A change towards the beginning of the process will likely affect a change towards the middle and end unless materials fall out if the process somewhere. We have seen that these metrics do often live together but we don\u2019t currently have a clean way to see causation (or at least correlation) when x metric moves. So when we see a shift in metric A we\u2019re all running around pulling raw data and looking at old dashboards trying to come up with a story.\n\nBy building a table with daily granularity and breaking out each KPI by day I\u2019d like to write a model to determine feature importance (as a coefficient) all other metrics have on each other metric. Basically get multiple coefficient scores for each feature (metric) that represents how they impact each other metric. The methods I\u2019m thinking of testing are below, but before I start writing any code I figured I\u2019d ask here to make sure I\u2019m thinking about this the right way. Would also greatly appreciate input from anyone that has built a well-performing feature importance model. I\u2019ll be reading up this weekend as well. \n\nCART \nDecisionTree\nLiner regression\nXGBoost", "author_fullname": "t2_h9bp3xei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Importance for KPIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1abinn6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706276636.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on tackling daily and weekly swings in some of our key metrics. These metrics are based on similar data coming through our logistics system that basically a waterfall. A change towards the beginning of the process will likely affect a change towards the middle and end unless materials fall out if the process somewhere. We have seen that these metrics do often live together but we don\u2019t currently have a clean way to see causation (or at least correlation) when x metric moves. So when we see a shift in metric A we\u2019re all running around pulling raw data and looking at old dashboards trying to come up with a story.&lt;/p&gt;\n\n&lt;p&gt;By building a table with daily granularity and breaking out each KPI by day I\u2019d like to write a model to determine feature importance (as a coefficient) all other metrics have on each other metric. Basically get multiple coefficient scores for each feature (metric) that represents how they impact each other metric. The methods I\u2019m thinking of testing are below, but before I start writing any code I figured I\u2019d ask here to make sure I\u2019m thinking about this the right way. Would also greatly appreciate input from anyone that has built a well-performing feature importance model. I\u2019ll be reading up this weekend as well. &lt;/p&gt;\n\n&lt;p&gt;CART \nDecisionTree\nLiner regression\nXGBoost&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1abinn6", "is_robot_indexable": true, "report_reasons": null, "author": "yrmidon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1abinn6/feature_importance_for_kpis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1abinn6/feature_importance_for_kpis/", "subreddit_subscribers": 1278100, "created_utc": 1706276636.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}