{"kind": "Listing", "data": {"after": "t3_18xp1k3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dashboards, views, tables, pipelines, entire data marts. Why does 90% of the work I do never get used?   \n\nI used to be one of the best BA's in my entire company so I am very good at requirements gathering and understanding what the business is trying to accomplish. Most of the work that I get comes from the CEO/VP level (global corporation not startup so real CEO and real VP 's) so a lot of people seem like they are very invested in solving these problems and my work always gets rave reviews.....but once things go into prod they basically never get touched.  \n\nSix months ago I just.... stopped doing QA.. I have been relying on the \"scream test\", I mark tickets resolved and immediately move to prod and only do QA if someone screams that something is wrong. I have yet to hear back on anything.", "author_fullname": "t2_o1c691xd6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does nothing ever get used?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xj97r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704288050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dashboards, views, tables, pipelines, entire data marts. Why does 90% of the work I do never get used?   &lt;/p&gt;\n\n&lt;p&gt;I used to be one of the best BA&amp;#39;s in my entire company so I am very good at requirements gathering and understanding what the business is trying to accomplish. Most of the work that I get comes from the CEO/VP level (global corporation not startup so real CEO and real VP &amp;#39;s) so a lot of people seem like they are very invested in solving these problems and my work always gets rave reviews.....but once things go into prod they basically never get touched.  &lt;/p&gt;\n\n&lt;p&gt;Six months ago I just.... stopped doing QA.. I have been relying on the &amp;quot;scream test&amp;quot;, I mark tickets resolved and immediately move to prod and only do QA if someone screams that something is wrong. I have yet to hear back on anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xj97r", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive-One6226", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xj97r/why_does_nothing_ever_get_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xj97r/why_does_nothing_ever_get_used/", "subreddit_subscribers": 150308, "created_utc": 1704288050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted this on /rust and I thought /dataengineering might find it interesting! \n\nI saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)\n\nRunning the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.\n\n\n|Implementation|Time|Code/Gist Link|\n|:-|:-|:-|\n|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|\n|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|\n|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|\n|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\\_average\\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|\n\nThanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing a One Billion Row Challenge in with Rust and Python with Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x2214", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704244329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704233622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted this on /rust and I thought /dataengineering might find it interesting! &lt;/p&gt;\n\n&lt;p&gt;I saw this &lt;a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\"&gt;Blog Post&lt;/a&gt; on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;Code/Gist here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Implementation&lt;/th&gt;\n&lt;th align=\"left\"&gt;Time&lt;/th&gt;\n&lt;th align=\"left\"&gt;Code/Gist Link&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;39s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust STD Libray&lt;/td&gt;\n&lt;td align=\"left\"&gt;19s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/coriolinus/1brc\"&gt;Coriolinus Solution&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Python + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.41 sec&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py\"&gt;https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Java &lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;royvanrijn&lt;/a&gt;&amp;#39;s Solution&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.366sec on the (8 core, 32 GB RAM)&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Thanks to @&lt;a href=\"https://www.reddit.com/user/coriolinus/\"&gt;coriolinus&lt;/a&gt; and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @&lt;a href=\"https://www.reddit.com/user/ritchie46/\"&gt;ritchie46&lt;/a&gt; for the Polars recommendations and the great library!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x2214", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "subreddit_subscribers": 150308, "created_utc": 1704233622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, here's an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I've seen recently.\n\n\"Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.\"\n\nSo.. I'm about to graduate from a Master's in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.\n\nYes the master's teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master's. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master's was a Bachelor's or an \"Intro to Data Literacy\" course.\n\nWhere do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master's to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?\n\nGee. I feel like uni has absolutely not made me job ready in any way.", "author_fullname": "t2_3wfadcr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "None of what I learned is a job requirement. I am essentially skill-less.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xb4ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704258392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, here&amp;#39;s an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I&amp;#39;ve seen recently.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So.. I&amp;#39;m about to graduate from a Master&amp;#39;s in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.&lt;/p&gt;\n\n&lt;p&gt;Yes the master&amp;#39;s teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master&amp;#39;s. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master&amp;#39;s was a Bachelor&amp;#39;s or an &amp;quot;Intro to Data Literacy&amp;quot; course.&lt;/p&gt;\n\n&lt;p&gt;Where do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master&amp;#39;s to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?&lt;/p&gt;\n\n&lt;p&gt;Gee. I feel like uni has absolutely not made me job ready in any way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18xb4ug", "is_robot_indexable": true, "report_reasons": null, "author": "Zomdou", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/", "subreddit_subscribers": 150308, "created_utc": 1704258392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?", "author_fullname": "t2_3wbyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your reverse ETL tool of choice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x6y0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704246120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x6y0i", "is_robot_indexable": true, "report_reasons": null, "author": "axlee", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "subreddit_subscribers": 150308, "created_utc": 1704246120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "u/gunnarmorling [launched a fun challenge](https://www.morling.dev/blog/one-billion-row-challenge/) this week: how fast can you aggregate and summarise a billion rows of data?\n\nI'm not a Java coder (which is what the challenge is set in) but thought it'd be fun to do it in SQL with DuckDB nonetheless.\n\nLoading the CSV in is simple enough:\n\n    CREATE OR REPLACE TABLE measurements AS\n            SELECT * FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';') LIMIT 2048;\n\nas are the calculations:\n\n    SELECT station_name, \n               MIN(measurement),\n               AVG(measurement),\n               MAX(measurement)\n        FROM measurements \n        GROUP BY station_name\n\nThe funky bit comes in trying to reproduce the specified output format:\n\n    SELECT '{' || \n                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||\n                '}' AS \"1BRC\"\n        FROM src;\n\nThe final script looks like this, and takes about 26 seconds to run:\n\n    \u276f /usr/bin/time -p duckdb -no-stdin -init 1brc.opt2.sql\n    -- Loading resources from 1brc.opt2.sql\n    \n    WITH src AS (SELECT station_name,\n                        MIN(measurement) AS min_measurement,\n                        CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean_measurement,\n                        MAX(measurement) AS max_measurement\n                FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';')\n                GROUP BY station_name)\n        SELECT '{' ||\n                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||\n                '}' AS \"1BRC\"\n        FROM src;\n    100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f\n    1BRC{Abha=-33.0/18.0/69.2, Abidjan=-24.4/26.0/75.4, Ab\u00e9ch\u00e9=-21.1/29.4/77.1, Accra=-25.1/26.4/79.0, [\u2026]Zanzibar City=-23.9/26.0/77.2, Z\u00fcrich=-39.0/9.3/56.0, \u00dcr\u00fcmqi=-39.6/7.4/58.1, \u0130zmir=-32.8/17.9/67.9}Run Time (s): real 25.539 user 203.968621 sys 2.572107\n    \n    .quit\n    real 25.58\n    user 203.98\n    sys 2.57\n\n**\ud83d\udc49 Full writeup:** [**1\ufe0f\u20e3\ud83d\udc1d\ud83c\udfce\ufe0f\ud83e\udd86 (1BRC in SQL with DuckDB)**](https://rmoff.net/2024/01/03/1%EF%B8%8F%E2%83%A3%EF%B8%8F-1brc-in-sql-with-duckdb/)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One Billion Row Challenge\u2014using SQL and DuckDB 1\ufe0f\u20e3\ud83d\udc1d\ud83c\udfce\ufe0f\ud83e\udd86", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xldbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704294064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"/u/gunnarmorling\"&gt;u/gunnarmorling&lt;/a&gt; &lt;a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\"&gt;launched a fun challenge&lt;/a&gt; this week: how fast can you aggregate and summarise a billion rows of data?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not a Java coder (which is what the challenge is set in) but thought it&amp;#39;d be fun to do it in SQL with DuckDB nonetheless.&lt;/p&gt;\n\n&lt;p&gt;Loading the CSV in is simple enough:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE TABLE measurements AS\n        SELECT * FROM READ_CSV(&amp;#39;measurements.txt&amp;#39;, header=false, columns= {&amp;#39;station_name&amp;#39;:&amp;#39;VARCHAR&amp;#39;,&amp;#39;measurement&amp;#39;:&amp;#39;double&amp;#39;}, delim=&amp;#39;;&amp;#39;) LIMIT 2048;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;as are the calculations:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT station_name, \n           MIN(measurement),\n           AVG(measurement),\n           MAX(measurement)\n    FROM measurements \n    GROUP BY station_name\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The funky bit comes in trying to reproduce the specified output format:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT &amp;#39;{&amp;#39; || \n            ARRAY_TO_STRING(LIST_SORT(LIST(station_name || &amp;#39;=&amp;#39; || CONCAT_WS(&amp;#39;/&amp;#39;,min_measurement, mean_measurement, max_measurement))),&amp;#39;, &amp;#39;) ||\n            &amp;#39;}&amp;#39; AS &amp;quot;1BRC&amp;quot;\n    FROM src;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The final script looks like this, and takes about 26 seconds to run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u276f /usr/bin/time -p duckdb -no-stdin -init 1brc.opt2.sql\n-- Loading resources from 1brc.opt2.sql\n\nWITH src AS (SELECT station_name,\n                    MIN(measurement) AS min_measurement,\n                    CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean_measurement,\n                    MAX(measurement) AS max_measurement\n            FROM READ_CSV(&amp;#39;measurements.txt&amp;#39;, header=false, columns= {&amp;#39;station_name&amp;#39;:&amp;#39;VARCHAR&amp;#39;,&amp;#39;measurement&amp;#39;:&amp;#39;double&amp;#39;}, delim=&amp;#39;;&amp;#39;)\n            GROUP BY station_name)\n    SELECT &amp;#39;{&amp;#39; ||\n            ARRAY_TO_STRING(LIST_SORT(LIST(station_name || &amp;#39;=&amp;#39; || CONCAT_WS(&amp;#39;/&amp;#39;,min_measurement, mean_measurement, max_measurement))),&amp;#39;, &amp;#39;) ||\n            &amp;#39;}&amp;#39; AS &amp;quot;1BRC&amp;quot;\n    FROM src;\n100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f\n1BRC{Abha=-33.0/18.0/69.2, Abidjan=-24.4/26.0/75.4, Ab\u00e9ch\u00e9=-21.1/29.4/77.1, Accra=-25.1/26.4/79.0, [\u2026]Zanzibar City=-23.9/26.0/77.2, Z\u00fcrich=-39.0/9.3/56.0, \u00dcr\u00fcmqi=-39.6/7.4/58.1, \u0130zmir=-32.8/17.9/67.9}Run Time (s): real 25.539 user 203.968621 sys 2.572107\n\n.quit\nreal 25.58\nuser 203.98\nsys 2.57\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udc49 Full writeup:&lt;/strong&gt; &lt;a href=\"https://rmoff.net/2024/01/03/1%EF%B8%8F%E2%83%A3%EF%B8%8F-1brc-in-sql-with-duckdb/\"&gt;&lt;strong&gt;1\ufe0f\u20e3\ud83d\udc1d\ud83c\udfce\ufe0f\ud83e\udd86 (1BRC in SQL with DuckDB)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18xldbk", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xldbk/one_billion_row_challengeusing_sql_and_duckdb_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xldbk/one_billion_row_challengeusing_sql_and_duckdb_1/", "subreddit_subscribers": 150308, "created_utc": 1704294064.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I`m used to work with SP whenever I am etl`ing from database to database. I only use python when I need to work with csv, an pai, Json.\n\nBut I think I rear about using python and learning python (for begginers mainly) too much.\n\nDoes It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?", "author_fullname": "t2_8lo1pjes", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python over Stored Procedure in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x3ygn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704238335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&lt;code&gt;m used to work with SP whenever I am etl&lt;/code&gt;ing from database to database. I only use python when I need to work with csv, an pai, Json.&lt;/p&gt;\n\n&lt;p&gt;But I think I rear about using python and learning python (for begginers mainly) too much.&lt;/p&gt;\n\n&lt;p&gt;Does It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x3ygn", "is_robot_indexable": true, "report_reasons": null, "author": "DesperateBus362", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "subreddit_subscribers": 150308, "created_utc": 1704238335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jayt2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fastest Way to Read Excel in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": false, "name": "t3_18xiu4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8cMcPCXb08vMLYWq7uJMHWwdunpUaTJeef8XeBBPZWo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704286755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hakibenita.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hakibenita.com/fast-excel-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jo5Vlc6pD3jMr_Xsd_mbM-Z3lkixwcIFaKHnHff-JSw.jpg?auto=webp&amp;s=5e9cbf64a9c1de5b880abad10a0c8bf86005df0c", "width": 515, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/jo5Vlc6pD3jMr_Xsd_mbM-Z3lkixwcIFaKHnHff-JSw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db947785956ab738a898a135b5b851c3b72aa735", "width": 108, "height": 83}, {"url": "https://external-preview.redd.it/jo5Vlc6pD3jMr_Xsd_mbM-Z3lkixwcIFaKHnHff-JSw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f89fc2b250aa711a063abfd0f4280774cc9765c8", "width": 216, "height": 167}, {"url": "https://external-preview.redd.it/jo5Vlc6pD3jMr_Xsd_mbM-Z3lkixwcIFaKHnHff-JSw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a515eadbaa4eb634f51d3b992e10cbde7c510058", "width": 320, "height": 248}], "variants": {}, "id": "pJyQaK84V_TZQ12VFWSnepvOTiE1dmH9Jz8B6l7kRbI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18xiu4l", "is_robot_indexable": true, "report_reasons": null, "author": "be_haki", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xiu4l/fastest_way_to_read_excel_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hakibenita.com/fast-excel-python", "subreddit_subscribers": 150308, "created_utc": 1704286755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table of aggregated data in Redshift which has +7 billion rows. I can\u2019t aggregate it any further. \n\nThey want me to run it through some python time series library and then through a data science model (which the data scientist will create) but pandas can only handle 5GB of data from my understanding. \n\nWhat are my options? The company is on a tight budget so Spark is not something they\u2019re willing to pay for.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to work with Billions of rows of Time Series Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x908p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704251928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table of aggregated data in Redshift which has +7 billion rows. I can\u2019t aggregate it any further. &lt;/p&gt;\n\n&lt;p&gt;They want me to run it through some python time series library and then through a data science model (which the data scientist will create) but pandas can only handle 5GB of data from my understanding. &lt;/p&gt;\n\n&lt;p&gt;What are my options? The company is on a tight budget so Spark is not something they\u2019re willing to pay for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18x908p", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x908p/how_to_work_with_billions_of_rows_of_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x908p/how_to_work_with_billions_of_rows_of_time_series/", "subreddit_subscribers": 150308, "created_utc": 1704251928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working as a data engineer. At my current company the projects are mostly Ops type so there isn't much to learn. I am trying to switch from a year now but cant find any company in my country sometimes because there aren't much opportunities and sometimes because my knowledge and skills are not up to par and I fail the interviews.\n\nNow I want to dedicate a year and make myself at least a better data engineer so that after a year I can call myself a data engineer. My experience currently is 2-3 years. So to make myself justify that experience what skills, projects, topics, tools should I focus on. \n\nPlease help me. ", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn on my own along with a job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xmcm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704296632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a data engineer. At my current company the projects are mostly Ops type so there isn&amp;#39;t much to learn. I am trying to switch from a year now but cant find any company in my country sometimes because there aren&amp;#39;t much opportunities and sometimes because my knowledge and skills are not up to par and I fail the interviews.&lt;/p&gt;\n\n&lt;p&gt;Now I want to dedicate a year and make myself at least a better data engineer so that after a year I can call myself a data engineer. My experience currently is 2-3 years. So to make myself justify that experience what skills, projects, topics, tools should I focus on. &lt;/p&gt;\n\n&lt;p&gt;Please help me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18xmcm7", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xmcm7/how_to_learn_on_my_own_along_with_a_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xmcm7/how_to_learn_on_my_own_along_with_a_job/", "subreddit_subscribers": 150308, "created_utc": 1704296632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm working in a product business, and we have about 200 members in our customer community. Most of them are Data Engineers. The main reason for having this community is to have a place for questions, ideas, and feedback about the product. Currently, I'm not satisfied with how active our community is.\n\nSo, I'm asking, what kind of things should I publish/create/do to activate our community?   \nHave you ended up in some good communities that I can benchmark? ", "author_fullname": "t2_lm2q4av4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to maintain an active Data Engineer Community?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xblnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704259898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m working in a product business, and we have about 200 members in our customer community. Most of them are Data Engineers. The main reason for having this community is to have a place for questions, ideas, and feedback about the product. Currently, I&amp;#39;m not satisfied with how active our community is.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m asking, what kind of things should I publish/create/do to activate our community?&lt;br/&gt;\nHave you ended up in some good communities that I can benchmark? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xblnf", "is_robot_indexable": true, "report_reasons": null, "author": "Nikke47", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xblnf/how_to_maintain_an_active_data_engineer_community/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xblnf/how_to_maintain_an_active_data_engineer_community/", "subreddit_subscribers": 150308, "created_utc": 1704259898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Previously, I was a mechanical engineer where I was working in a factory setting. However, I did a bootcamp and landed a data engineering role about 8 months ago and I\u2019m excited but struggling. I\u2019m new to the corporate world (politics) and software engineering and want someone to talk to/ have conversations with stay on top of software trends, become more confident in my role, and overall feel good about this career. If interested, please let me know and we can discuss rates.", "author_fullname": "t2_e1kommwl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need a mentor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xqr9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704307983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Previously, I was a mechanical engineer where I was working in a factory setting. However, I did a bootcamp and landed a data engineering role about 8 months ago and I\u2019m excited but struggling. I\u2019m new to the corporate world (politics) and software engineering and want someone to talk to/ have conversations with stay on top of software trends, become more confident in my role, and overall feel good about this career. If interested, please let me know and we can discuss rates.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xqr9b", "is_robot_indexable": true, "report_reasons": null, "author": "geekyabs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xqr9b/i_need_a_mentor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xqr9b/i_need_a_mentor/", "subreddit_subscribers": 150308, "created_utc": 1704307983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my case it always happens to be \"scrum bi/weekly\" but I never felt it as perfect match due to continuous fire fighting, last minute unpredictable decision changes etc. However I never had the chance to test alternatives. What about you?", "author_fullname": "t2_utk7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which management framework (scrum, lean, kanban, etc) does your company/team use for Data Engineering and more generally which framework do you think is the best for Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xlejx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704294146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my case it always happens to be &amp;quot;scrum bi/weekly&amp;quot; but I never felt it as perfect match due to continuous fire fighting, last minute unpredictable decision changes etc. However I never had the chance to test alternatives. What about you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xlejx", "is_robot_indexable": true, "report_reasons": null, "author": "df016", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xlejx/which_management_framework_scrum_lean_kanban_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xlejx/which_management_framework_scrum_lean_kanban_etc/", "subreddit_subscribers": 150308, "created_utc": 1704294146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, a Fractional Head of Growth here.  \nHas anyone implemented **Customer Level Unit economics**? (B2B or B2C)\n\nBasically have an event-driven data architecture that allows you to assign cost or revenue to every action a customer might take. The goal is to identify the most profitable customers and the actions that drive profitability.\n\nIt helps answer questions like:\n\n1. Bottom 10% of loss-making customers\n2. Profitability of customers in 20s vs 30s vs 40s\n3. Profitability of customers who signed up through Google vs Meta\n\nIf yes, how? Is there any resource I can refer to? I am looking to implement this at 2 companies (B2B and B2C)\n\nThanks!", "author_fullname": "t2_nvnticro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Customer Level Unit economics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xemop", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704270891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, a Fractional Head of Growth here.&lt;br/&gt;\nHas anyone implemented &lt;strong&gt;Customer Level Unit economics&lt;/strong&gt;? (B2B or B2C)&lt;/p&gt;\n\n&lt;p&gt;Basically have an event-driven data architecture that allows you to assign cost or revenue to every action a customer might take. The goal is to identify the most profitable customers and the actions that drive profitability.&lt;/p&gt;\n\n&lt;p&gt;It helps answer questions like:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Bottom 10% of loss-making customers&lt;/li&gt;\n&lt;li&gt;Profitability of customers in 20s vs 30s vs 40s&lt;/li&gt;\n&lt;li&gt;Profitability of customers who signed up through Google vs Meta&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If yes, how? Is there any resource I can refer to? I am looking to implement this at 2 companies (B2B and B2C)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xemop", "is_robot_indexable": true, "report_reasons": null, "author": "Scary-Swing2852", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xemop/customer_level_unit_economics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xemop/customer_level_unit_economics/", "subreddit_subscribers": 150308, "created_utc": 1704270891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm build a test modern Data Stack and intent to use Airbyte Self hosted version. I was going to host into a t2.medium EC2 but saw some people hosting a lot of docker containers inside AWS LightSail. \n\nDoes LightSail is cheaper than a EC2 in that situation? \n\nAlso, I want to use Prefect, Postgres and DBT. A friend told me that I could to all of this in one LightSail instance. What you guys think? ", "author_fullname": "t2_oefriy2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It is better to host Airbyte in EC2 or AWS lightSail?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xoeqv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704302373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m build a test modern Data Stack and intent to use Airbyte Self hosted version. I was going to host into a t2.medium EC2 but saw some people hosting a lot of docker containers inside AWS LightSail. &lt;/p&gt;\n\n&lt;p&gt;Does LightSail is cheaper than a EC2 in that situation? &lt;/p&gt;\n\n&lt;p&gt;Also, I want to use Prefect, Postgres and DBT. A friend told me that I could to all of this in one LightSail instance. What you guys think? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xoeqv", "is_robot_indexable": true, "report_reasons": null, "author": "EconomySuch7621", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xoeqv/it_is_better_to_host_airbyte_in_ec2_or_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xoeqv/it_is_better_to_host_airbyte_in_ec2_or_aws/", "subreddit_subscribers": 150308, "created_utc": 1704302373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was reading *\"Fundamentals of Data Engineering\"* by Joe Reis and Matt Housley and a question came to my mind.\n\nIn the book, it says that for a company at the early stage of its data journey (when you are just starting to worry about data) the best approach is to stay away from code. Is that true for real life?\n\nHonestly, I find no trouble in setting up a virtual machine on the cloud, writing some Python scripts and automate them using whichever task scheduler the operating system has. Or even setting up scripts in AWS Lambda or GCP Cloud functions. Is, for example, Apache NiFi better suited for these cases?\n\nI have been working as a data engineer for 2 years now and all I see at companies starting with data is custom Python code. Which I think makes sense.\n\nI would love to read your experiences.\n\nThank you and happy new year!", "author_fullname": "t2_p4nzh8v1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Companies at early stage of the data journey: no code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xqdvt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704307101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was reading &lt;em&gt;&amp;quot;Fundamentals of Data Engineering&amp;quot;&lt;/em&gt; by Joe Reis and Matt Housley and a question came to my mind.&lt;/p&gt;\n\n&lt;p&gt;In the book, it says that for a company at the early stage of its data journey (when you are just starting to worry about data) the best approach is to stay away from code. Is that true for real life?&lt;/p&gt;\n\n&lt;p&gt;Honestly, I find no trouble in setting up a virtual machine on the cloud, writing some Python scripts and automate them using whichever task scheduler the operating system has. Or even setting up scripts in AWS Lambda or GCP Cloud functions. Is, for example, Apache NiFi better suited for these cases?&lt;/p&gt;\n\n&lt;p&gt;I have been working as a data engineer for 2 years now and all I see at companies starting with data is custom Python code. Which I think makes sense.&lt;/p&gt;\n\n&lt;p&gt;I would love to read your experiences.&lt;/p&gt;\n\n&lt;p&gt;Thank you and happy new year!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xqdvt", "is_robot_indexable": true, "report_reasons": null, "author": "data_macrolide", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xqdvt/companies_at_early_stage_of_the_data_journey_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xqdvt/companies_at_early_stage_of_the_data_journey_no/", "subreddit_subscribers": 150308, "created_utc": 1704307101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are migrating relational data in our oracle database to Neo4J graph database. The flow is like this, first we extract the data from oracle into CSV file, then load data from CSV to Neo4J. Currently this is not in production and I run the above tasks in my own system.\n\n\nNow as the data is getting larger storing it as CSV is taking a lot of time, hence we want to eliminate that stage and directly load data from SQL to Neo4j without any middle layer like CSV.\n\n\nMy first question is how can i do that?\nThrough some searches on the net, I found out that Airflow provides operators for Neo4j which I can use. But my doubt is how can I setup a DAG that extracts data from oracle and loads it to Neo4j. Where will the data be stored inbetween moving it?", "author_fullname": "t2_ht9x5dmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help - Sql to Neo4J using Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xqclw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704307011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are migrating relational data in our oracle database to Neo4J graph database. The flow is like this, first we extract the data from oracle into CSV file, then load data from CSV to Neo4J. Currently this is not in production and I run the above tasks in my own system.&lt;/p&gt;\n\n&lt;p&gt;Now as the data is getting larger storing it as CSV is taking a lot of time, hence we want to eliminate that stage and directly load data from SQL to Neo4j without any middle layer like CSV.&lt;/p&gt;\n\n&lt;p&gt;My first question is how can i do that?\nThrough some searches on the net, I found out that Airflow provides operators for Neo4j which I can use. But my doubt is how can I setup a DAG that extracts data from oracle and loads it to Neo4j. Where will the data be stored inbetween moving it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xqclw", "is_robot_indexable": true, "report_reasons": null, "author": "kaachejl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xqclw/help_sql_to_neo4j_using_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xqclw/help_sql_to_neo4j_using_airflow/", "subreddit_subscribers": 150308, "created_utc": 1704307011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does this sound as a dwh / orchestration setup for an IoT startup with &lt;100 tables and pipelines to manage?\n\nI have a dbt / dagster project in a monorepo, containing all our db models, pipeline code and a Dockerfile. \n\nFor local development, the Dockerfile is used to setup a dev container running dagster daemon + webserver. For staging / production, changes are pulled into staging / production branches and a GitHub action is triggered to run CI tests + build an image which is pushed to Artifact Registry, and then onto a cloud hosted VM (also running dagster daemon + webserver).\n\nI would be using BigQuery as a dwh, and I was thinking each branch of the repo would have its own dataset in BigQuery. I think I could get this to work using environment variables in the dbt config files.\n\nI'm aware that it probably makes more sense to deploy staging / production on kubernetes, but I don't want to overcomplicate things as this is already breaking a lot of new ground for me\n\nIs there anything I've overlooked?", "author_fullname": "t2_i7dbhuu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT + Dagster setup for an IoT startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xmwva", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704298080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does this sound as a dwh / orchestration setup for an IoT startup with &amp;lt;100 tables and pipelines to manage?&lt;/p&gt;\n\n&lt;p&gt;I have a dbt / dagster project in a monorepo, containing all our db models, pipeline code and a Dockerfile. &lt;/p&gt;\n\n&lt;p&gt;For local development, the Dockerfile is used to setup a dev container running dagster daemon + webserver. For staging / production, changes are pulled into staging / production branches and a GitHub action is triggered to run CI tests + build an image which is pushed to Artifact Registry, and then onto a cloud hosted VM (also running dagster daemon + webserver).&lt;/p&gt;\n\n&lt;p&gt;I would be using BigQuery as a dwh, and I was thinking each branch of the repo would have its own dataset in BigQuery. I think I could get this to work using environment variables in the dbt config files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that it probably makes more sense to deploy staging / production on kubernetes, but I don&amp;#39;t want to overcomplicate things as this is already breaking a lot of new ground for me&lt;/p&gt;\n\n&lt;p&gt;Is there anything I&amp;#39;ve overlooked?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xmwva", "is_robot_indexable": true, "report_reasons": null, "author": "hennyblub", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xmwva/dbt_dagster_setup_for_an_iot_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xmwva/dbt_dagster_setup_for_an_iot_startup/", "subreddit_subscribers": 150308, "created_utc": 1704298080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the past few months, I have written about a number of different technologies ([Ray Data](https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/), [Ray Train](https://blog.min.io/distributed-training-with-ray-train-and-minio/), and [MLflow](https://blog.min.io/mlflow-tracking-and-minio/)). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my [Ray Train post](https://blog.min.io/distributed-training-with-ray-train-and-minio/) that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found [here](https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io).\n\n[https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=distributed\\_training\\_experiment\\_tracking\\_ray\\_train\\_mlflow+](https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Training and Experiment Tracking with Ray Train, MLflow, and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x0tav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704230704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past few months, I have written about a number of different technologies (&lt;a href=\"https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/\"&gt;Ray Data&lt;/a&gt;, &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train&lt;/a&gt;, and &lt;a href=\"https://blog.min.io/mlflow-tracking-and-minio/\"&gt;MLflow&lt;/a&gt;). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train post&lt;/a&gt; that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found &lt;a href=\"https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+\"&gt;https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?auto=webp&amp;s=60a9a5bb13ad52dc6f983d22a24a53b3b835a119", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db2aa38b11de95fa42f9dbf716d73c3b2a64e4f3", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c18b4e8bc4062930b8490b5721bf20e5d91b96a", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e065467bf6807e1f7014eb05a7e8aef79c5b4e88", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74d1b5b114a455946e7c372fa0cc71907ebdd6dd", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d0ea6ae4d3f00a6c97c690b38471318183ee2c8", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=011a2c8ca8070e4782e8cf151170e64f2db7fd85", "width": 1080, "height": 323}], "variants": {}, "id": "ycJVbgf3bDDq06hpn_WuGtC2gZGgwo5DkVBTrJOe5j8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18x0tav", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "subreddit_subscribers": 150308, "created_utc": 1704230704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I deploy my code in \"cloud function\", it extracts data from a project and with that data I create a table in another project. It also automates it with a \"cloud schedule\" to run every Saturday. But \"cloud programming\" gives me \"failed\"\n\nCloud funtcion:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ykgz0im7daac1.png?width=333&amp;format=png&amp;auto=webp&amp;s=03ebde55d433c35dc5afac38ce52c694a80709a7\n\n&amp;#x200B;\n\nhttps://preview.redd.it/coos5hn8daac1.png?width=385&amp;format=png&amp;auto=webp&amp;s=c1650497e7c4645d8b3f7b254c5e05bdedb241c2\n\nhttps://preview.redd.it/h95jigk9daac1.png?width=597&amp;format=png&amp;auto=webp&amp;s=1cfc4cefd28a759e0b725e7a95359db7b1dc62fd\n\n \n\nI share an example of my code in python:\n\nfrom google.cloud import bigquery  \nfrom google.oauth2 import service\\_account  \nimport time  \nimport decimal  \nimport logging\n\nproject\\_extract = 'one'  \nproject\\_load = 'two'\n\ncredentials\\_extract = {  \n\"type\": \"service\\_account\",  \n\"project\\_id\": \"one\",  \n\"private\\_key\\_id\": \"\",  \n\"private\\_key\": \"\",etc}\n\ncredentials\\_extract = service\\_account.Credentials.from\\_service\\_account\\_info(credentials\\_extract)  \nclient\\_extract = bigquery.Client(project=project\\_extract, credentials=credentials\\_extract)\n\ncredentials\\_load = {  \n\"type\": \"service\\_account\",  \n\"project\\_id\": \"two\",  \n\"private\\_key\\_id\": \"\",  \n\"private\\_key\": \"\",etc}\n\ncredentials\\_load = service\\_account.Credentials.from\\_service\\_account\\_info(credentials\\_load)  \nclient\\_load = bigquery.Client(project=project\\_load , credentials=credentials\\_load)\n\ndef decimal\\_to\\_float(obj):  \nif isinstance(obj, decimal.Decimal):  \nreturn float(obj)  \nelse:  \nreturn obj  \ndef process\\_and\\_load\\_table(client\\_extract, client\\_load, query, schema, table\\_name):  \ntry:  \n\\# Ejecutar la consulta  \nquery\\_job = client\\_extract.query(query)  \nresults = query\\_job.result()  \nnum\\_filas\\_ini = results.total\\_rows  \nlogging.info(\"N\u00famero de filas antes de la consulta: \" + str(num\\_filas\\_ini))  \n\\# Construir lista de filas para la carga  \nlistawyids = \\[  \n{  \nfield.name: decimal\\_to\\_float(getattr(row, field.name))  \nfor field in results.schema  \n} for row in results  \n\\]  \n\\# Eliminar la tabla existente si existe  \nclient\\_load.delete\\_table(table\\_name, not\\_found\\_ok=True)  \ntime.sleep(120) # Esperar para asegurar que la eliminaci\u00f3n se haya completado  \n\\# Crear la nueva tabla  \ntables = bigquery.Table(table\\_name, schema=schema)  \ntables = client\\_load.create\\_table(tables)  \ntime.sleep(120) # Esperar para asegurar que la creaci\u00f3n se haya completado  \n\\# Insertar filas en lotes  \ninsert\\_rows\\_in\\_batches(client\\_load, tables, listawyids, 500)  \nexcept Exception as e:  \nlogging.error(\"Error \u00a0&gt;&gt; %s\", e)\n\ndef insert\\_rows\\_in\\_batches(client, table, rows, batch\\_size):  \nstart = 0  \nwhile start &lt; len(rows):  \nend = start + batch\\_size  \nbatch = rows\\[start:end\\]  \nerrors = client.insert\\_rows(table, batch)  \nif errors:  \nprint(f\"Se encontraron errores al insertar filas: {errors}\")  \nelse:  \nprint(f\"Las filas {start} a {end} han sido a\u00f1adidas correctamente.\")  \nstart = end\n\ndef process\\_all\\_tables(request):  \n\\# table competition  \nschema\\_competition = \\[  \nbigquery.SchemaField(\"column\", \"INT64\", mode=\"NULLABLE\"),  \netc  \n\\]\n\ntable\\_competition = f\"{project\\_load}.data\\_wyscout.pruebafede\"  \nquery\\_competition = \"\"\"  \nMy query  \n\"\"\"\n\n\\# Procesar y cargar tabla de competici\u00f3n  \nprocess\\_and\\_load\\_table(client\\_extract, client\\_load, query\\_competition, schema\\_competition, table\\_competition)  \nif \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":  \nprocess\\_all\\_tables(None)", "author_fullname": "t2_498lsodi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Function OK , but Cloud Scheduler Failed GET 504", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": true, "media_metadata": {"ykgz0im7daac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/ykgz0im7daac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=14abf0d57c5b0d1673f34469acdf2e31ad81ae7a"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/ykgz0im7daac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aef679fa28dd476e366f1570560d2c3848afd7b9"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/ykgz0im7daac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bb9defd6ac01380bafc52919c936df0a21de812"}], "s": {"y": 193, "x": 333, "u": "https://preview.redd.it/ykgz0im7daac1.png?width=333&amp;format=png&amp;auto=webp&amp;s=03ebde55d433c35dc5afac38ce52c694a80709a7"}, "id": "ykgz0im7daac1"}, "h95jigk9daac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 7, "x": 108, "u": "https://preview.redd.it/h95jigk9daac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19437eec85a2226841d7f9876fd2e5385f7e74b1"}, {"y": 15, "x": 216, "u": "https://preview.redd.it/h95jigk9daac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c4e69d1bb151ea3d0a8661514e678dd6dd48227"}, {"y": 23, "x": 320, "u": "https://preview.redd.it/h95jigk9daac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cb907a81ecb25e5e14e2c93520af4ffcf99ea15"}], "s": {"y": 43, "x": 597, "u": "https://preview.redd.it/h95jigk9daac1.png?width=597&amp;format=png&amp;auto=webp&amp;s=1cfc4cefd28a759e0b725e7a95359db7b1dc62fd"}, "id": "h95jigk9daac1"}, "coos5hn8daac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/coos5hn8daac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ffb3f7d090d707bb0226a8ef684655e29a80ae1b"}, {"y": 117, "x": 216, "u": "https://preview.redd.it/coos5hn8daac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=573c185e34316702564ff8b076a1ebd3c5fb82f1"}, {"y": 173, "x": 320, "u": "https://preview.redd.it/coos5hn8daac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9054e7055ee6ee136512b5f3a658a08ddd2e378a"}], "s": {"y": 209, "x": 385, "u": "https://preview.redd.it/coos5hn8daac1.png?width=385&amp;format=png&amp;auto=webp&amp;s=c1650497e7c4645d8b3f7b254c5e05bdedb241c2"}, "id": "coos5hn8daac1"}}, "name": "t3_18xtn32", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fhtMtqm8BALhs_rBsiNNjKD1-RwBvzPDuh32duOTMJc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704314875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deploy my code in &amp;quot;cloud function&amp;quot;, it extracts data from a project and with that data I create a table in another project. It also automates it with a &amp;quot;cloud schedule&amp;quot; to run every Saturday. But &amp;quot;cloud programming&amp;quot; gives me &amp;quot;failed&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Cloud funtcion:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ykgz0im7daac1.png?width=333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03ebde55d433c35dc5afac38ce52c694a80709a7\"&gt;https://preview.redd.it/ykgz0im7daac1.png?width=333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03ebde55d433c35dc5afac38ce52c694a80709a7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/coos5hn8daac1.png?width=385&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1650497e7c4645d8b3f7b254c5e05bdedb241c2\"&gt;https://preview.redd.it/coos5hn8daac1.png?width=385&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1650497e7c4645d8b3f7b254c5e05bdedb241c2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h95jigk9daac1.png?width=597&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cfc4cefd28a759e0b725e7a95359db7b1dc62fd\"&gt;https://preview.redd.it/h95jigk9daac1.png?width=597&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1cfc4cefd28a759e0b725e7a95359db7b1dc62fd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I share an example of my code in python:&lt;/p&gt;\n\n&lt;p&gt;from google.cloud import bigquery&lt;br/&gt;\nfrom google.oauth2 import service_account&lt;br/&gt;\nimport time&lt;br/&gt;\nimport decimal&lt;br/&gt;\nimport logging&lt;/p&gt;\n\n&lt;p&gt;project_extract = &amp;#39;one&amp;#39;&lt;br/&gt;\nproject_load = &amp;#39;two&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;credentials_extract = {&lt;br/&gt;\n&amp;quot;type&amp;quot;: &amp;quot;service_account&amp;quot;,&lt;br/&gt;\n&amp;quot;project_id&amp;quot;: &amp;quot;one&amp;quot;,&lt;br/&gt;\n&amp;quot;private_key_id&amp;quot;: &amp;quot;&amp;quot;,&lt;br/&gt;\n&amp;quot;private_key&amp;quot;: &amp;quot;&amp;quot;,etc}&lt;/p&gt;\n\n&lt;p&gt;credentials_extract = service_account.Credentials.from_service_account_info(credentials_extract)&lt;br/&gt;\nclient_extract = bigquery.Client(project=project_extract, credentials=credentials_extract)&lt;/p&gt;\n\n&lt;p&gt;credentials_load = {&lt;br/&gt;\n&amp;quot;type&amp;quot;: &amp;quot;service_account&amp;quot;,&lt;br/&gt;\n&amp;quot;project_id&amp;quot;: &amp;quot;two&amp;quot;,&lt;br/&gt;\n&amp;quot;private_key_id&amp;quot;: &amp;quot;&amp;quot;,&lt;br/&gt;\n&amp;quot;private_key&amp;quot;: &amp;quot;&amp;quot;,etc}&lt;/p&gt;\n\n&lt;p&gt;credentials_load = service_account.Credentials.from_service_account_info(credentials_load)&lt;br/&gt;\nclient_load = bigquery.Client(project=project_load , credentials=credentials_load)&lt;/p&gt;\n\n&lt;p&gt;def decimal_to_float(obj):&lt;br/&gt;\nif isinstance(obj, decimal.Decimal):&lt;br/&gt;\nreturn float(obj)&lt;br/&gt;\nelse:&lt;br/&gt;\nreturn obj&lt;br/&gt;\ndef process_and_load_table(client_extract, client_load, query, schema, table_name):&lt;br/&gt;\ntry:&lt;br/&gt;\n# Ejecutar la consulta&lt;br/&gt;\nquery_job = client_extract.query(query)&lt;br/&gt;\nresults = query_job.result()&lt;br/&gt;\nnum_filas_ini = results.total_rows&lt;br/&gt;\nlogging.info(&amp;quot;N\u00famero de filas antes de la consulta: &amp;quot; + str(num_filas_ini))&lt;br/&gt;\n# Construir lista de filas para la carga&lt;br/&gt;\nlistawyids = [&lt;br/&gt;\n{&lt;br/&gt;\nfield.name: decimal_to_float(getattr(row, field.name))&lt;br/&gt;\nfor field in results.schema&lt;br/&gt;\n} for row in results&lt;br/&gt;\n]&lt;br/&gt;\n# Eliminar la tabla existente si existe&lt;br/&gt;\nclient_load.delete_table(table_name, not_found_ok=True)&lt;br/&gt;\ntime.sleep(120) # Esperar para asegurar que la eliminaci\u00f3n se haya completado&lt;br/&gt;\n# Crear la nueva tabla&lt;br/&gt;\ntables = bigquery.Table(table_name, schema=schema)&lt;br/&gt;\ntables = client_load.create_table(tables)&lt;br/&gt;\ntime.sleep(120) # Esperar para asegurar que la creaci\u00f3n se haya completado&lt;br/&gt;\n# Insertar filas en lotes&lt;br/&gt;\ninsert_rows_in_batches(client_load, tables, listawyids, 500)&lt;br/&gt;\nexcept Exception as e:&lt;br/&gt;\nlogging.error(&amp;quot;Error \u00a0&amp;gt;&amp;gt; %s&amp;quot;, e)&lt;/p&gt;\n\n&lt;p&gt;def insert_rows_in_batches(client, table, rows, batch_size):&lt;br/&gt;\nstart = 0&lt;br/&gt;\nwhile start &amp;lt; len(rows):&lt;br/&gt;\nend = start + batch_size&lt;br/&gt;\nbatch = rows[start:end]&lt;br/&gt;\nerrors = client.insert_rows(table, batch)&lt;br/&gt;\nif errors:&lt;br/&gt;\nprint(f&amp;quot;Se encontraron errores al insertar filas: {errors}&amp;quot;)&lt;br/&gt;\nelse:&lt;br/&gt;\nprint(f&amp;quot;Las filas {start} a {end} han sido a\u00f1adidas correctamente.&amp;quot;)&lt;br/&gt;\nstart = end&lt;/p&gt;\n\n&lt;p&gt;def process_all_tables(request):&lt;br/&gt;\n# table competition&lt;br/&gt;\nschema_competition = [&lt;br/&gt;\nbigquery.SchemaField(&amp;quot;column&amp;quot;, &amp;quot;INT64&amp;quot;, mode=&amp;quot;NULLABLE&amp;quot;),&lt;br/&gt;\netc&lt;br/&gt;\n]&lt;/p&gt;\n\n&lt;p&gt;table_competition = f&amp;quot;{project_load}.data_wyscout.pruebafede&amp;quot;&lt;br/&gt;\nquery_competition = &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt;\nMy query&lt;br/&gt;\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;# Procesar y cargar tabla de competici\u00f3n&lt;br/&gt;\nprocess_and_load_table(client_extract, client_load, query_competition, schema_competition, table_competition)&lt;br/&gt;\nif __name__ == &amp;quot;__main__&amp;quot;:&lt;br/&gt;\nprocess_all_tables(None)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xtn32", "is_robot_indexable": true, "report_reasons": null, "author": "CurrentBottle9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xtn32/cloud_function_ok_but_cloud_scheduler_failed_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xtn32/cloud_function_ok_but_cloud_scheduler_failed_get/", "subreddit_subscribers": 150308, "created_utc": 1704314875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Genuinely curious. Assuming you were interviewing for a role you thought would be a good fit, what % salary increase would you want/need to make the jump?   \nA few days back I posted about taking a new role and a handful of comments were made around salary increase (what % is worth it or not).   \nIn your current role what % increase do you feel is \"worth it\" to you when entertaining taking a new role?\n\n[View Poll](https://www.reddit.com/poll/18xsni9)", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What salary % increase would it take for you to leave your current role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18xsni9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704312452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Genuinely curious. Assuming you were interviewing for a role you thought would be a good fit, what % salary increase would you want/need to make the jump?&lt;br/&gt;\nA few days back I posted about taking a new role and a handful of comments were made around salary increase (what % is worth it or not).&lt;br/&gt;\nIn your current role what % increase do you feel is &amp;quot;worth it&amp;quot; to you when entertaining taking a new role?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18xsni9\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xsni9", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1704571652770, "options": [{"text": "0% - I just want something different", "id": "26609865"}, {"text": "Between 0 and 10%", "id": "26609866"}, {"text": "Between 10 and 15%", "id": "26609867"}, {"text": "Between 15 and 25%", "id": "26609868"}, {"text": "Between 25 and 50%", "id": "26609869"}, {"text": "Over 50%", "id": "26609870"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 43, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xsni9/what_salary_increase_would_it_take_for_you_to/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18xsni9/what_salary_increase_would_it_take_for_you_to/", "subreddit_subscribers": 150308, "created_utc": 1704312452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello friends ! \n\nI have been working as an internal IT for the past  years and my day to day job is mainly setting up applications ,maintenance ,  patching, UATs etc. The above gives me exposure to SQL but nothing advanced. I'm also using Alteryx for reporting and ETLs\n\nI want to start planning a path where it will lead me getting a DE job but I am a bit overwhelmed on where to start. I was thinking maybe a bootcamp but I just started out learning Python and a lot of the camps require basic to intermediate knowledge. \n\nAnything that will help me organise my confused brain will be very appreciated ! \n\nThank you.", "author_fullname": "t2_1tbmnh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help starting out!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18xsaov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704311602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends ! &lt;/p&gt;\n\n&lt;p&gt;I have been working as an internal IT for the past  years and my day to day job is mainly setting up applications ,maintenance ,  patching, UATs etc. The above gives me exposure to SQL but nothing advanced. I&amp;#39;m also using Alteryx for reporting and ETLs&lt;/p&gt;\n\n&lt;p&gt;I want to start planning a path where it will lead me getting a DE job but I am a bit overwhelmed on where to start. I was thinking maybe a bootcamp but I just started out learning Python and a lot of the camps require basic to intermediate knowledge. &lt;/p&gt;\n\n&lt;p&gt;Anything that will help me organise my confused brain will be very appreciated ! &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xsaov", "is_robot_indexable": true, "report_reasons": null, "author": "insomniaxcm__", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xsaov/need_help_starting_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xsaov/need_help_starting_out/", "subreddit_subscribers": 150308, "created_utc": 1704311602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like the title says, I'm looking to get another language under my belt besides Python. Those working in Data Engineering, do you think C++ or Java would be more \"bang for my buck\"?\n\nI've been using Python for almost a decade and would like to pick up another programming language. Took two Scala courses a while back, but other than working in Spark (Which I don't do for work) I found few other uses for it. I took Java MOOC, but didn't keep up using it cause I got confused a bit with the framework that we were using. I have done some Rust, but I think they hype around it is dying down and I honestly think it will go on to be a good system programming language, but not really a good one for every day use. Haven't used C++ since high school, but willing to re-learn/play around with it.", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seasoned Professionals - What should I learn after Python. C++ or Java", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xrpfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704310201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I&amp;#39;m looking to get another language under my belt besides Python. Those working in Data Engineering, do you think C++ or Java would be more &amp;quot;bang for my buck&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Python for almost a decade and would like to pick up another programming language. Took two Scala courses a while back, but other than working in Spark (Which I don&amp;#39;t do for work) I found few other uses for it. I took Java MOOC, but didn&amp;#39;t keep up using it cause I got confused a bit with the framework that we were using. I have done some Rust, but I think they hype around it is dying down and I honestly think it will go on to be a good system programming language, but not really a good one for every day use. Haven&amp;#39;t used C++ since high school, but willing to re-learn/play around with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xrpfl", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xrpfl/seasoned_professionals_what_should_i_learn_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xrpfl/seasoned_professionals_what_should_i_learn_after/", "subreddit_subscribers": 150308, "created_utc": 1704310201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Guys, I had an interview at a company which gave me an assessment. The assessment was pretty long and I had to share my solution to them. I haven't really worked with geospatial data and used a technique that left a query running for three days and it has impacted their database performance. \n\nAnyway the relevant person talked to me via email and he was very polite and professional. I owned my mistake and gave a couple of resolutions too.\n\nNow I have a presentation coming up at the company. I would be presenting my solution. What do you guys think I should be prepared of? Is it a straight up rejection in their minds. Or what kind of questions should I prepare myself for. \n\nI would appreciate any help. I am really frustrated at this moment.", "author_fullname": "t2_gccs4t3qm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does my bad query leaves a bad impact?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xr0wg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704308607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Guys, I had an interview at a company which gave me an assessment. The assessment was pretty long and I had to share my solution to them. I haven&amp;#39;t really worked with geospatial data and used a technique that left a query running for three days and it has impacted their database performance. &lt;/p&gt;\n\n&lt;p&gt;Anyway the relevant person talked to me via email and he was very polite and professional. I owned my mistake and gave a couple of resolutions too.&lt;/p&gt;\n\n&lt;p&gt;Now I have a presentation coming up at the company. I would be presenting my solution. What do you guys think I should be prepared of? Is it a straight up rejection in their minds. Or what kind of questions should I prepare myself for. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate any help. I am really frustrated at this moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18xr0wg", "is_robot_indexable": true, "report_reasons": null, "author": "milostough", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xr0wg/does_my_bad_query_leaves_a_bad_impact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xr0wg/does_my_bad_query_leaves_a_bad_impact/", "subreddit_subscribers": 150308, "created_utc": 1704308607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are developing a tool that can scan and optimize your snowflake environment for you. The best part? It's completely free in our closed beta! All it takes is 5 minutes of your time and you'll receive a fancy report that could save you a lot of money. \n\n**Interested? Please ping me for details.** \n\n&amp;#x200B;\n\nP.S.\n\nThe report might include some Memes :)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/syg585a9h9ac1.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=fa36b9cd0fe8e81e510f949104a3489ae399eb35", "author_fullname": "t2_el2n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testers needed - Snowflake optimization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"syg585a9h9ac1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/syg585a9h9ac1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c8e207250b8edd6020ba0183f1e4e35f2d50618"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/syg585a9h9ac1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b67a809949a17e9b8e1930279783e8a97114d141"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/syg585a9h9ac1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ebcb33605dca0ccfba4b292eadd18436c3c69fb"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/syg585a9h9ac1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e89d57ea34ab80b1c752320b6df1b009c39efb53"}], "s": {"y": 500, "x": 750, "u": "https://preview.redd.it/syg585a9h9ac1.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=fa36b9cd0fe8e81e510f949104a3489ae399eb35"}, "id": "syg585a9h9ac1"}}, "name": "t3_18xp1se", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0nN9M1Wj9Fdg8A9TTX1rGOCjOIFPROGL_X4T3-QlYPI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704303878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are developing a tool that can scan and optimize your snowflake environment for you. The best part? It&amp;#39;s completely free in our closed beta! All it takes is 5 minutes of your time and you&amp;#39;ll receive a fancy report that could save you a lot of money. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Interested? Please ping me for details.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S.&lt;/p&gt;\n\n&lt;p&gt;The report might include some Memes :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/syg585a9h9ac1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa36b9cd0fe8e81e510f949104a3489ae399eb35\"&gt;https://preview.redd.it/syg585a9h9ac1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa36b9cd0fe8e81e510f949104a3489ae399eb35&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xp1se", "is_robot_indexable": true, "report_reasons": null, "author": "st-yoni", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xp1se/testers_needed_snowflake_optimization_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xp1se/testers_needed_snowflake_optimization_tool/", "subreddit_subscribers": 150308, "created_utc": 1704303878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3ixkfqzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran API Client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18xp1k3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/tfZNSgYXHJOXi8ksgtZhKlXRooEdmXR8ttJcL5qlpd0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704303864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/kharigardner/pyfivetran", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?auto=webp&amp;s=330b3740332322ead890f763f0bbfdf62c1ef0e6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65559700c3e23204da3ca44ceee80aa11aabf2ec", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb84eeacfd470f878fda057f2e133f14a6efd728", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ad1ed3010241b6171dc00dc4614f7c5f8884736", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e1fc05b63c5a16c6947d9a691ca2e34609c148ed", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=444432f6050890ed196d0badca71e32c409825a1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/vjslPODcT8zeo6qct2ScFH_h8GD57TDpzo2cLU9sFqw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4c82e7da57824fd27bbb5835976cf97bd1a8ee7", "width": 1080, "height": 540}], "variants": {}, "id": "5tF1Vi-G4Y4NJ6B9wGwiT8KfCDmVMC1rVvEvXiYe5bU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18xp1k3", "is_robot_indexable": true, "report_reasons": null, "author": "kharigardner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18xp1k3/fivetran_api_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/kharigardner/pyfivetran", "subreddit_subscribers": 150308, "created_utc": 1704303864.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}