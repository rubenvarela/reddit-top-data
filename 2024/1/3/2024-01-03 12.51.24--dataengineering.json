{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted this on /rust and I thought /dataengineering might find it interesting! \n\nI saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)\n\nRunning the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.\n\n\n|Implementation|Time|Code/Gist Link|\n|:-|:-|:-|\n|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|\n|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|\n|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|\n|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\\_average\\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|\n\nThanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing a One Billion Row Challenge in with Rust and Python with Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x2214", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704244329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704233622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted this on /rust and I thought /dataengineering might find it interesting! &lt;/p&gt;\n\n&lt;p&gt;I saw this &lt;a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\"&gt;Blog Post&lt;/a&gt; on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;Code/Gist here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Implementation&lt;/th&gt;\n&lt;th align=\"left\"&gt;Time&lt;/th&gt;\n&lt;th align=\"left\"&gt;Code/Gist Link&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;39s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust STD Libray&lt;/td&gt;\n&lt;td align=\"left\"&gt;19s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/coriolinus/1brc\"&gt;Coriolinus Solution&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Python + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.41 sec&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py\"&gt;https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Java &lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;royvanrijn&lt;/a&gt;&amp;#39;s Solution&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.366sec on the (8 core, 32 GB RAM)&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Thanks to @&lt;a href=\"https://www.reddit.com/user/coriolinus/\"&gt;coriolinus&lt;/a&gt; and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @&lt;a href=\"https://www.reddit.com/user/ritchie46/\"&gt;ritchie46&lt;/a&gt; for the Polars recommendations and the great library!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x2214", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "subreddit_subscribers": 150204, "created_utc": 1704233622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, here's an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I've seen recently.\n\n\"Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.\"\n\nSo.. I'm about to graduate from a Master's in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.\n\nYes the master's teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master's. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master's was a Bachelor's or an \"Intro to Data Literacy\" course.\n\nWhere do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master's to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?\n\nGee. I feel like uni has absolutely not made me job ready in any way.", "author_fullname": "t2_3wfadcr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "None of what I learned is a job requirement. I am essentially skill-less.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xb4ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704258392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, here&amp;#39;s an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I&amp;#39;ve seen recently.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So.. I&amp;#39;m about to graduate from a Master&amp;#39;s in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.&lt;/p&gt;\n\n&lt;p&gt;Yes the master&amp;#39;s teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master&amp;#39;s. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master&amp;#39;s was a Bachelor&amp;#39;s or an &amp;quot;Intro to Data Literacy&amp;quot; course.&lt;/p&gt;\n\n&lt;p&gt;Where do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master&amp;#39;s to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?&lt;/p&gt;\n\n&lt;p&gt;Gee. I feel like uni has absolutely not made me job ready in any way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18xb4ug", "is_robot_indexable": true, "report_reasons": null, "author": "Zomdou", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/", "subreddit_subscribers": 150204, "created_utc": 1704258392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it's counterpart, airflow. Is this a common thing, or is my PC not good enough for this?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Airbyte so buggy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18woe6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704198787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it&amp;#39;s counterpart, airflow. Is this a common thing, or is my PC not good enough for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18woe6i", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "subreddit_subscribers": 150204, "created_utc": 1704198787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I`m used to work with SP whenever I am etl`ing from database to database. I only use python when I need to work with csv, an pai, Json.\n\nBut I think I rear about using python and learning python (for begginers mainly) too much.\n\nDoes It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?", "author_fullname": "t2_8lo1pjes", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python over Stored Procedure in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x3ygn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704238335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&lt;code&gt;m used to work with SP whenever I am etl&lt;/code&gt;ing from database to database. I only use python when I need to work with csv, an pai, Json.&lt;/p&gt;\n\n&lt;p&gt;But I think I rear about using python and learning python (for begginers mainly) too much.&lt;/p&gt;\n\n&lt;p&gt;Does It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x3ygn", "is_robot_indexable": true, "report_reasons": null, "author": "DesperateBus362", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "subreddit_subscribers": 150204, "created_utc": 1704238335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?", "author_fullname": "t2_3wbyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your reverse ETL tool of choice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x6y0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704246120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x6y0i", "is_robot_indexable": true, "report_reasons": null, "author": "axlee", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "subreddit_subscribers": 150204, "created_utc": 1704246120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it's public sector to emphasise that both the people and processes there are a bit old school.\n\nSo, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.\n\nThe person who developed this workflow 10 years or so ago, which hasn't changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!", "author_fullname": "t2_y2r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Data Warehousing Methodology Viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wpk8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704202495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it&amp;#39;s public sector to emphasise that both the people and processes there are a bit old school.&lt;/p&gt;\n\n&lt;p&gt;So, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.&lt;/p&gt;\n\n&lt;p&gt;The person who developed this workflow 10 years or so ago, which hasn&amp;#39;t changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp;amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wpk8k", "is_robot_indexable": true, "report_reasons": null, "author": "Vextus420", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "subreddit_subscribers": 150204, "created_utc": 1704202495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm working in a product business, and we have about 200 members in our customer community. Most of them are Data Engineers. The main reason for having this community is to have a place for questions, ideas, and feedback about the product. Currently, I'm not satisfied with how active our community is.\n\nSo, I'm asking, what kind of things should I publish/create/do to activate our community?   \nHave you ended up in some good communities that I can benchmark? ", "author_fullname": "t2_lm2q4av4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to maintain an active Data Engineer Community?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xblnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704259898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m working in a product business, and we have about 200 members in our customer community. Most of them are Data Engineers. The main reason for having this community is to have a place for questions, ideas, and feedback about the product. Currently, I&amp;#39;m not satisfied with how active our community is.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m asking, what kind of things should I publish/create/do to activate our community?&lt;br/&gt;\nHave you ended up in some good communities that I can benchmark? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xblnf", "is_robot_indexable": true, "report_reasons": null, "author": "Nikke47", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xblnf/how_to_maintain_an_active_data_engineer_community/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xblnf/how_to_maintain_an_active_data_engineer_community/", "subreddit_subscribers": 150204, "created_utc": 1704259898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table of aggregated data in Redshift which has +7 billion rows. I can\u2019t aggregate it any further. \n\nThey want me to run it through some python time series library and then through a data science model (which the data scientist will create) but pandas can only handle 5GB of data from my understanding. \n\nWhat are my options? The company is on a tight budget so Spark is not something they\u2019re willing to pay for.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to work with Billions of rows of Time Series Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x908p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704251928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table of aggregated data in Redshift which has +7 billion rows. I can\u2019t aggregate it any further. &lt;/p&gt;\n\n&lt;p&gt;They want me to run it through some python time series library and then through a data science model (which the data scientist will create) but pandas can only handle 5GB of data from my understanding. &lt;/p&gt;\n\n&lt;p&gt;What are my options? The company is on a tight budget so Spark is not something they\u2019re willing to pay for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18x908p", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x908p/how_to_work_with_billions_of_rows_of_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x908p/how_to_work_with_billions_of_rows_of_time_series/", "subreddit_subscribers": 150204, "created_utc": 1704251928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some of the high level meeting discussed these buzz words.\n\nSpark\nAzure\nIceberg\nStarrocks\n\nIn understand infrastructure is Azure and computational is done by Spark.\n\nWhat does StarRocks and Iceberg do in this setup. Can some one please explain how all these concepts fit in architecture or pipeline", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to DE concepts. My company analytics team was working on setting up DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wyvdt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704226049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some of the high level meeting discussed these buzz words.&lt;/p&gt;\n\n&lt;p&gt;Spark\nAzure\nIceberg\nStarrocks&lt;/p&gt;\n\n&lt;p&gt;In understand infrastructure is Azure and computational is done by Spark.&lt;/p&gt;\n\n&lt;p&gt;What does StarRocks and Iceberg do in this setup. Can some one please explain how all these concepts fit in architecture or pipeline&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wyvdt", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wyvdt/new_to_de_concepts_my_company_analytics_team_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wyvdt/new_to_de_concepts_my_company_analytics_team_was/", "subreddit_subscribers": 150204, "created_utc": 1704226049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everybody, HNY 2024 \ud83c\udf89\n\nI am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.\n\nCurrently I am focusing more on optimising the existing jobs in the current org.\n\nI use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. \n\nHowever could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.\n \nI have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. \n\nI would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.\n\nThanks !", "author_fullname": "t2_188qz428", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestions for Optimising Spark Jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wrfv0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704207813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everybody, HNY 2024 \ud83c\udf89&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.&lt;/p&gt;\n\n&lt;p&gt;Currently I am focusing more on optimising the existing jobs in the current org.&lt;/p&gt;\n\n&lt;p&gt;I use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. &lt;/p&gt;\n\n&lt;p&gt;However could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.&lt;/p&gt;\n\n&lt;p&gt;I have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. &lt;/p&gt;\n\n&lt;p&gt;I would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wrfv0", "is_robot_indexable": true, "report_reasons": null, "author": "swarup_i_am", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "subreddit_subscribers": 150204, "created_utc": 1704207813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Incremental View Maintenance (IVM)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq7jj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fGlSxhubku9S7PFnu4hbxinrk1J2lgRQLGjkZHHE2qc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704204367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?auto=webp&amp;s=c7c52ec3d38c17618f8e2d66171c27c43db16e74", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82e0a27fd835a1d0f5334c8262bc19d8cccdf4e7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af2b704d8bca9c88866a4bd14c3bbd3c53402624", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c87e5f77b1543f701882e490e96ed4c562eb835", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784c1dbb829b60e7ef650982b2e17b96040ebd8d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=39fb1dcb35bafe6e3018a12df56b63cd9b942ccc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4966f73d32a66db7bfee30cd8f334db98c528ec7", "width": 1080, "height": 540}], "variants": {}, "id": "lnHZk4nL9uJOOq-9-k4mRzI86-2_bCgX2l6koPtAzk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wq7jj", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq7jj/what_is_incremental_view_maintenance_ivm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "subreddit_subscribers": 150204, "created_utc": 1704204367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm thinking of migrating to Australia and if I'm to get a job offer with visa sponsorship what will be the technologies that I need to master? Basically what are the most demanding DE technologies used in most of the Australian companies?", "author_fullname": "t2_6b96x89c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are trending technologies to master as a DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xfjp7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704274701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of migrating to Australia and if I&amp;#39;m to get a job offer with visa sponsorship what will be the technologies that I need to master? Basically what are the most demanding DE technologies used in most of the Australian companies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xfjp7", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Buy-4947", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xfjp7/what_are_trending_technologies_to_master_as_a_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xfjp7/what_are_trending_technologies_to_master_as_a_de/", "subreddit_subscribers": 150204, "created_utc": 1704274701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, a Fractional Head of Growth here.  \nHas anyone implemented **Customer Level Unit economics**? (B2B or B2C)\n\nBasically have an event-driven data architecture that allows you to assign cost or revenue to every action a customer might take. The goal is to identify the most profitable customers and the actions that drive profitability.\n\nIt helps answer questions like:\n\n1. Bottom 10% of loss-making customers\n2. Profitability of customers in 20s vs 30s vs 40s\n3. Profitability of customers who signed up through Google vs Meta\n\nIf yes, how? Is there any resource I can refer to? I am looking to implement this at 2 companies (B2B and B2C)\n\nThanks!", "author_fullname": "t2_nvnticro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Customer Level Unit economics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xemop", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704270891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, a Fractional Head of Growth here.&lt;br/&gt;\nHas anyone implemented &lt;strong&gt;Customer Level Unit economics&lt;/strong&gt;? (B2B or B2C)&lt;/p&gt;\n\n&lt;p&gt;Basically have an event-driven data architecture that allows you to assign cost or revenue to every action a customer might take. The goal is to identify the most profitable customers and the actions that drive profitability.&lt;/p&gt;\n\n&lt;p&gt;It helps answer questions like:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Bottom 10% of loss-making customers&lt;/li&gt;\n&lt;li&gt;Profitability of customers in 20s vs 30s vs 40s&lt;/li&gt;\n&lt;li&gt;Profitability of customers who signed up through Google vs Meta&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If yes, how? Is there any resource I can refer to? I am looking to implement this at 2 companies (B2B and B2C)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xemop", "is_robot_indexable": true, "report_reasons": null, "author": "Scary-Swing2852", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xemop/customer_level_unit_economics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xemop/customer_level_unit_economics/", "subreddit_subscribers": 150204, "created_utc": 1704270891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm currently brainstorming ideas for my MSc Data Science/Comp Eng research project and I'm leaning towards a topic in Data Engineering. I'm particularly interested in areas involving pipelines, ETL, efficiency, cloud computing, and the use of Python, Spark, and SQL.\n\nHas anyone come across any recent research or projects in these areas that could inspire a solid project idea? I'm looking for something technically challenging and innovative.\n\nAppreciate any suggestions or pointers to recent papers, projects, or trends in this field!\n\nThanks!", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestions for a MSc Research Project in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wxzco", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704223945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently brainstorming ideas for my MSc Data Science/Comp Eng research project and I&amp;#39;m leaning towards a topic in Data Engineering. I&amp;#39;m particularly interested in areas involving pipelines, ETL, efficiency, cloud computing, and the use of Python, Spark, and SQL.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across any recent research or projects in these areas that could inspire a solid project idea? I&amp;#39;m looking for something technically challenging and innovative.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions or pointers to recent papers, projects, or trends in this field!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wxzco", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18wxzco/need_suggestions_for_a_msc_research_project_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wxzco/need_suggestions_for_a_msc_research_project_in/", "subreddit_subscribers": 150204, "created_utc": 1704223945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello y'all smart people,\n\nI'm a little frustrated by the experience of doing ad-hoc analyses in SQL.\n\nI love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it'd be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we'd have a nice documentation of the thought process that I can refer to in half a year when I've long forgotten whatever I was doing back then.\n\nHowever, this doesn't seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.\n\nDo I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it's properly documented? \n\nI'd be grateful for your input. Thanks!", "author_fullname": "t2_e7hyqhhe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using notebooks to analyze SQL - am I missing something?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wqg4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704205066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello y&amp;#39;all smart people,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little frustrated by the experience of doing ad-hoc analyses in SQL.&lt;/p&gt;\n\n&lt;p&gt;I love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it&amp;#39;d be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we&amp;#39;d have a nice documentation of the thought process that I can refer to in half a year when I&amp;#39;ve long forgotten whatever I was doing back then.&lt;/p&gt;\n\n&lt;p&gt;However, this doesn&amp;#39;t seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.&lt;/p&gt;\n\n&lt;p&gt;Do I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it&amp;#39;s properly documented? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be grateful for your input. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wqg4b", "is_robot_indexable": true, "report_reasons": null, "author": "UnusualCookieBox", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "subreddit_subscribers": 150204, "created_utc": 1704205066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the past few months, I have written about a number of different technologies ([Ray Data](https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/), [Ray Train](https://blog.min.io/distributed-training-with-ray-train-and-minio/), and [MLflow](https://blog.min.io/mlflow-tracking-and-minio/)). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my [Ray Train post](https://blog.min.io/distributed-training-with-ray-train-and-minio/) that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found [here](https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io).\n\n[https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=distributed\\_training\\_experiment\\_tracking\\_ray\\_train\\_mlflow+](https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Training and Experiment Tracking with Ray Train, MLflow, and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x0tav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704230704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past few months, I have written about a number of different technologies (&lt;a href=\"https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/\"&gt;Ray Data&lt;/a&gt;, &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train&lt;/a&gt;, and &lt;a href=\"https://blog.min.io/mlflow-tracking-and-minio/\"&gt;MLflow&lt;/a&gt;). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train post&lt;/a&gt; that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found &lt;a href=\"https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+\"&gt;https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?auto=webp&amp;s=60a9a5bb13ad52dc6f983d22a24a53b3b835a119", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db2aa38b11de95fa42f9dbf716d73c3b2a64e4f3", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c18b4e8bc4062930b8490b5721bf20e5d91b96a", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e065467bf6807e1f7014eb05a7e8aef79c5b4e88", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74d1b5b114a455946e7c372fa0cc71907ebdd6dd", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d0ea6ae4d3f00a6c97c690b38471318183ee2c8", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=011a2c8ca8070e4782e8cf151170e64f2db7fd85", "width": 1080, "height": 323}], "variants": {}, "id": "ycJVbgf3bDDq06hpn_WuGtC2gZGgwo5DkVBTrJOe5j8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18x0tav", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "subreddit_subscribers": 150204, "created_utc": 1704230704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a basic question , within Data bricks we have  Create SQL . .  \nMy question considering data bricks being clould portable   \nIf we are creating a SQL warehouse in databricks . is it fully managed . .if not how it will be portable across. . i am missing something .   \nIs it like sql storage is underlying cloud infrastructure. . where exactly sql warehouse is stored and manged is it be databricks or underlying cloud infra ", "author_fullname": "t2_hwqrk3yk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Bricks SQL warehouse where it is stored .", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18xhl15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704282522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a basic question , within Data bricks we have  Create SQL . .&lt;br/&gt;\nMy question considering data bricks being clould portable&lt;br/&gt;\nIf we are creating a SQL warehouse in databricks . is it fully managed . .if not how it will be portable across. . i am missing something .&lt;br/&gt;\nIs it like sql storage is underlying cloud infrastructure. . where exactly sql warehouse is stored and manged is it be databricks or underlying cloud infra &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18xhl15", "is_robot_indexable": true, "report_reasons": null, "author": "Data5kull", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xhl15/data_bricks_sql_warehouse_where_it_is_stored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xhl15/data_bricks_sql_warehouse_where_it_is_stored/", "subreddit_subscribers": 150204, "created_utc": 1704282522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,   \nI am currently researching low-code tools topic to integrate our company's SaaS solutions with business client systems (ERP, CRM, DW, or data lakes mostly). We are looking for something with capabilities to integrate with the most popular systems (aka SAP or Microsoft software, both cloud and on-premises) with predefined connectors, but also with options to connect to other less popular systems (it can be either API or direct db connection).   \nFor now, I am trying Azure Data Factory for this purpose, but still not sure if it will be suitable for us, do you have any other suggestions?  It might be both cloud or non-cloud tools. Thank you for your answers in advance and hope you have a great day :)", "author_fullname": "t2_hfmqvbux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most suitable low-code data integration tools for SaaS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xfota", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704275301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;br/&gt;\nI am currently researching low-code tools topic to integrate our company&amp;#39;s SaaS solutions with business client systems (ERP, CRM, DW, or data lakes mostly). We are looking for something with capabilities to integrate with the most popular systems (aka SAP or Microsoft software, both cloud and on-premises) with predefined connectors, but also with options to connect to other less popular systems (it can be either API or direct db connection).&lt;br/&gt;\nFor now, I am trying Azure Data Factory for this purpose, but still not sure if it will be suitable for us, do you have any other suggestions?  It might be both cloud or non-cloud tools. Thank you for your answers in advance and hope you have a great day :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18xfota", "is_robot_indexable": true, "report_reasons": null, "author": "Myhasik", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xfota/most_suitable_lowcode_data_integration_tools_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xfota/most_suitable_lowcode_data_integration_tools_for/", "subreddit_subscribers": 150204, "created_utc": 1704275301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrate from \"MySQL+ClickHouse\" combination to Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_18xf26l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/T3UD2tKFrOB6jAdRy-A60_4ykJ-USyAJNnnw5uvGCE8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704272672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yral-Wd1-NQxkZNm9LzietoLiTg68-zXv9mHGTdC4Tc.jpg?auto=webp&amp;s=608d80741ac0b7115eb402ce3ce1f3136ffc8351", "width": 900, "height": 384}, "resolutions": [{"url": "https://external-preview.redd.it/yral-Wd1-NQxkZNm9LzietoLiTg68-zXv9mHGTdC4Tc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e724ad56883930e26326f0e54f3a4a6831d1021", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/yral-Wd1-NQxkZNm9LzietoLiTg68-zXv9mHGTdC4Tc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de8c137831af57e5ac863e24de0c0b51d3d116e5", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/yral-Wd1-NQxkZNm9LzietoLiTg68-zXv9mHGTdC4Tc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9ae28aad74403f14875e67b89e45379b7ac9f82", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/yral-Wd1-NQxkZNm9LzietoLiTg68-zXv9mHGTdC4Tc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=325723888d2ad2335136a4cc45ac64d4a8a026db", "width": 640, "height": 273}], "variants": {}, "id": "6SVBI_cSoUf4wlFCQI1XDhEncJvO9WaLMamSKrSBZoQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18xf26l", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xf26l/migrate_from_mysqlclickhouse_combination_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics", "subreddit_subscribers": 150204, "created_utc": 1704272672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello all, first post here on reddit. Below I\u2019ve outlined a potential data architecture for a general sports betting analytics framework and would like to solicit feedback as well as any potential off the shelf alternatives that could accomplish the same goals. The data architecture needs to primarily support the following:\n\n1. entity definitions. Things like players, teams, games, etc.\n2. entity features. Some quantifiable piece of information about an entity. Usually associated with a given day. I.e. features are often time series for a given entity. Things like player ratings prior to a game, weather impacts on game day, etc. Can be arbitrarily complex - more than simple aggregations.\n3. point-in-timeliness. To support proper backtesting, all entities and metrics should be queryable as they existed at some point in time. \n4. ability to iterate quickly on feature research. Should be able to define a complex feature and run on the entire dataset. Entities \\* features \\* number of days \\* number of revisions could be in the millions. Queries here will often be vertical in nature. E.g. you might ask a question like \u201cgive me these 3 features across all entities on all days for all revisions\u201d.\n5. quick real time feature calculation. Will be used for real time predictions. These queries will often be horizontal in nature e.g. you might ask \u201cgive me these 100 features for these 5 entities on this day\u201d. Can also be vertical if a given feature requires history of itself or others.\n\nGiven the requirements for both 4/5, I\u2019m leaning towards a polyglot data model, utilizing multiple data architectures to support the different query patterns. Off the top of my head, I\u2019m thinking something like postgres for entity management and point-in-timeliness, MongoDB as the feature store for horizontal queries, and some type of columnar store like parquet files on s3 for fast querying of entire feature history.\n\nObvious complications are synchronization and consistency across data stores. My intuition says this could be managed crudely with triggers on the entity tables and/or tracking update timestamps across all 3 stores, but I\u2019m well aware that this could snowball into a fragile and extremely complex system. Happy to provide more details if necessary, but I\u2019m hoping there\u2019s enough here to know that I\u2019m either potentially headed down the right path or completely misguided. I\u2019d also be interested in any off the shelf, open source or relatively cheap solutions that exist to fulfill these requirements, as I haven\u2019t found any that satisfy all of them. While I'm committed to spending a decent amount of $ on this, I'm a solo dev who has a day job, so cost does play a major role. Thanks in advance!", "author_fullname": "t2_h3jdyzqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture for analytics framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x6piq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704245469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, first post here on reddit. Below I\u2019ve outlined a potential data architecture for a general sports betting analytics framework and would like to solicit feedback as well as any potential off the shelf alternatives that could accomplish the same goals. The data architecture needs to primarily support the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;entity definitions. Things like players, teams, games, etc.&lt;/li&gt;\n&lt;li&gt;entity features. Some quantifiable piece of information about an entity. Usually associated with a given day. I.e. features are often time series for a given entity. Things like player ratings prior to a game, weather impacts on game day, etc. Can be arbitrarily complex - more than simple aggregations.&lt;/li&gt;\n&lt;li&gt;point-in-timeliness. To support proper backtesting, all entities and metrics should be queryable as they existed at some point in time. &lt;/li&gt;\n&lt;li&gt;ability to iterate quickly on feature research. Should be able to define a complex feature and run on the entire dataset. Entities * features * number of days * number of revisions could be in the millions. Queries here will often be vertical in nature. E.g. you might ask a question like \u201cgive me these 3 features across all entities on all days for all revisions\u201d.&lt;/li&gt;\n&lt;li&gt;quick real time feature calculation. Will be used for real time predictions. These queries will often be horizontal in nature e.g. you might ask \u201cgive me these 100 features for these 5 entities on this day\u201d. Can also be vertical if a given feature requires history of itself or others.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given the requirements for both 4/5, I\u2019m leaning towards a polyglot data model, utilizing multiple data architectures to support the different query patterns. Off the top of my head, I\u2019m thinking something like postgres for entity management and point-in-timeliness, MongoDB as the feature store for horizontal queries, and some type of columnar store like parquet files on s3 for fast querying of entire feature history.&lt;/p&gt;\n\n&lt;p&gt;Obvious complications are synchronization and consistency across data stores. My intuition says this could be managed crudely with triggers on the entity tables and/or tracking update timestamps across all 3 stores, but I\u2019m well aware that this could snowball into a fragile and extremely complex system. Happy to provide more details if necessary, but I\u2019m hoping there\u2019s enough here to know that I\u2019m either potentially headed down the right path or completely misguided. I\u2019d also be interested in any off the shelf, open source or relatively cheap solutions that exist to fulfill these requirements, as I haven\u2019t found any that satisfy all of them. While I&amp;#39;m committed to spending a decent amount of $ on this, I&amp;#39;m a solo dev who has a day job, so cost does play a major role. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18x6piq", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed-Diamond-4859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x6piq/data_architecture_for_analytics_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x6piq/data_architecture_for_analytics_framework/", "subreddit_subscribers": 150204, "created_utc": 1704245469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a requirement to read mainframe ebcdic files using spark. Has anyone had any luck with this?\n\nUpon research I came across \"cobrix\". Has anyone used this package before? . Unfortunately that's the only one we have come across.\n\nPlease suggest if there are any other options that are avaliable to read these files on spark.", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EBCIDIC in spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wxfo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704223101.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704222635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a requirement to read mainframe ebcdic files using spark. Has anyone had any luck with this?&lt;/p&gt;\n\n&lt;p&gt;Upon research I came across &amp;quot;cobrix&amp;quot;. Has anyone used this package before? . Unfortunately that&amp;#39;s the only one we have come across.&lt;/p&gt;\n\n&lt;p&gt;Please suggest if there are any other options that are avaliable to read these files on spark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wxfo5", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wxfo5/ebcidic_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wxfo5/ebcidic_in_spark/", "subreddit_subscribers": 150204, "created_utc": 1704222635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.\n\nCurrent we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. \n\nThings I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.\n\nAnother one was trying on airflow however windows doesn't support airflow and vice versa, wsl works however that python doesn't work on any form of Linux because it uses some windows features.\n\nAm I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?\n\nAnswers to commonly asked question:\n\n1) No it cannot run in container or any form of linux, needs to be windows only.\n\n2) And it needs to run python\n\n3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.", "author_fullname": "t2_ozjx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a weird OnPrem Setup which uses Task Scheduler currently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq815", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704204406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.&lt;/p&gt;\n\n&lt;p&gt;Current we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. &lt;/p&gt;\n\n&lt;p&gt;Things I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.&lt;/p&gt;\n\n&lt;p&gt;Another one was trying on airflow however windows doesn&amp;#39;t support airflow and vice versa, wsl works however that python doesn&amp;#39;t work on any form of Linux because it uses some windows features.&lt;/p&gt;\n\n&lt;p&gt;Am I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?&lt;/p&gt;\n\n&lt;p&gt;Answers to commonly asked question:&lt;/p&gt;\n\n&lt;p&gt;1) No it cannot run in container or any form of linux, needs to be windows only.&lt;/p&gt;\n\n&lt;p&gt;2) And it needs to run python&lt;/p&gt;\n\n&lt;p&gt;3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wq815", "is_robot_indexable": true, "report_reasons": null, "author": "tecedu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "subreddit_subscribers": 150204, "created_utc": 1704204406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I completed my Bachelor of Engineering in Computer Science and have 1 year of experience in Oracle HCM Cloud, specializing in OTBI, BIP reports, and data extracts. I possess a good understanding of SQL and am looking to transition into data engineering. Can anyone recommend good paid courses with certification options to help me upskill in this area?   I've come across Andreas Kretz's course; does anyone have insights into its quality and effectiveness? \n\nThank you", "author_fullname": "t2_d9336uu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking career advice (India).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18xar0f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704257167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed my Bachelor of Engineering in Computer Science and have 1 year of experience in Oracle HCM Cloud, specializing in OTBI, BIP reports, and data extracts. I possess a good understanding of SQL and am looking to transition into data engineering. Can anyone recommend good paid courses with certification options to help me upskill in this area?   I&amp;#39;ve come across Andreas Kretz&amp;#39;s course; does anyone have insights into its quality and effectiveness? &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18xar0f", "is_robot_indexable": true, "report_reasons": null, "author": "ToughAd3865", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18xar0f/seeking_career_advice_india/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18xar0f/seeking_career_advice_india/", "subreddit_subscribers": 150204, "created_utc": 1704257167.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}