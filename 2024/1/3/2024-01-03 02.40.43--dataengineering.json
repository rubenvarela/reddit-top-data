{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.\n2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.\n3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.\n4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).\n5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.\n6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.\n7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.\n8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp; production environments.\n9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.\n10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.\n11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.\n12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.\n\n[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Testing Cheat Sheet: 12 Essential Rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mknzrwbvn0ac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ebb8f1cb9f499662fe7137401b81d40d8ff5eca"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=43803638b8a1176972a4065de0ad9917385dc318"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0df0c666c5e84ecd36586eb4951b19ff630dec92"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d31614e427f2b96d00308d4c0ae7f5bde5ce3d5a"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d393ab6192e671205c6854d4dbd32424e5f4f86d"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=341a0344accb93a99e02182da505b40f1058ffd6"}], "s": {"y": 3778, "x": 1887, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48"}, "id": "mknzrwbvn0ac1"}}, "name": "t3_18wnsqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 120, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xbH5B5vbrrV6rRJ15RnsiIr1X93x8bW4r0eHS9GlSAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704196767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Source vs Target Data Reconciliation:&lt;/strong&gt; Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ETL Transformation Test:&lt;/strong&gt; Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Source Data Validation:&lt;/strong&gt; Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Validation Rule:&lt;/strong&gt; Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Reconciliation Rule:&lt;/strong&gt; Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Referential Integrity Reconciliation:&lt;/strong&gt; Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Migration Reconciliation:&lt;/strong&gt; Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physical Schema Reconciliation:&lt;/strong&gt; Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp;amp; production environments.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cross Source Data Reconciliation:&lt;/strong&gt; Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Validation:&lt;/strong&gt; Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Reconciliation:&lt;/strong&gt; Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Cross-Environment Reconciliation:&lt;/strong&gt; Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48\"&gt;Data Testing Cheat Sheet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wnsqj", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "subreddit_subscribers": 150123, "created_utc": 1704196767.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. \n\nWhere would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.", "author_fullname": "t2_163ma7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ways to keep your SQL sharp with minimal effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18who2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704173395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m at a job that does doesn&amp;#39;t involve working daily with SQL as the project has matured and we&amp;#39;re not making many changes to the business logic anymore. So I&amp;#39;m thinking that I want to keep working on SQL problems somewhere else so that I&amp;#39;m interview ready. &lt;/p&gt;\n\n&lt;p&gt;Where would you recommend I can go let&amp;#39;s say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18who2l", "is_robot_indexable": true, "report_reasons": null, "author": "muhmeinchut69", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "subreddit_subscribers": 150123, "created_utc": 1704173395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wdkvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704161126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I haven&amp;#39;t accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I&amp;#39;ve been wanting to use. My current company is great but I&amp;#39;ve been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn&amp;#39;t performance based). Maybe I&amp;#39;m naive but that sounded fine to me. Am I crazy to leave my full-time role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wdkvp", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "subreddit_subscribers": 150123, "created_utc": 1704161126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted this on /rust and I thought /dataengineering might find it interesting! \n\nI saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)\n\nRunning the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.\n\n\n|Implementation|Time|Code/Gist Link|\n|:-|:-|:-|\n|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|\n|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|\n|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|\n|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\\_average\\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|\n\nThanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing a One Billion Row Challenge in with Rust and Python with Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x2214", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704244329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704233622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted this on /rust and I thought /dataengineering might find it interesting! &lt;/p&gt;\n\n&lt;p&gt;I saw this &lt;a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\"&gt;Blog Post&lt;/a&gt; on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;Code/Gist here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Implementation&lt;/th&gt;\n&lt;th align=\"left\"&gt;Time&lt;/th&gt;\n&lt;th align=\"left\"&gt;Code/Gist Link&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;39s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8\"&gt;https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Rust STD Libray&lt;/td&gt;\n&lt;td align=\"left\"&gt;19s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/coriolinus/1brc\"&gt;Coriolinus Solution&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Python + Polars&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.41 sec&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py\"&gt;https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Java &lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;royvanrijn&lt;/a&gt;&amp;#39;s Solution&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.366sec on the (8 core, 32 GB RAM)&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh\"&gt;https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Thanks to @&lt;a href=\"https://www.reddit.com/user/coriolinus/\"&gt;coriolinus&lt;/a&gt; and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @&lt;a href=\"https://www.reddit.com/user/ritchie46/\"&gt;ritchie46&lt;/a&gt; for the Polars recommendations and the great library!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x2214", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/", "subreddit_subscribers": 150123, "created_utc": 1704233622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it's counterpart, airflow. Is this a common thing, or is my PC not good enough for this?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Airbyte so buggy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18woe6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704198787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it&amp;#39;s counterpart, airflow. Is this a common thing, or is my PC not good enough for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18woe6i", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "subreddit_subscribers": 150123, "created_utc": 1704198787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Clean outliers with data painter](https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player)\n\nSometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.\n\n&amp;#x200B;\n\n[create new feature in your data](https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player)\n\nSometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.\n\n  \nOnline tutorial of Data painter in PyGWalker: [https://data-painter-tutorial.pygwalker.kanaries.io/](https://data-painter-tutorial.pygwalker.kanaries.io/)  \nPyGWalker's Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker's Data Painter, a new way to interact with your data in jupyter notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vuugpw2rfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/DASHPlaylist.mpd?a=1706841642%2CNzJiZmRkMjA0ZTVmYTA0NDIzY2Q5ODQ0YzBiZGQ2YzE3ZjZkZjg4NmZkZDdmYTliNTk0MzhiOTlkMDIxYWMwMg%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/HLSPlaylist.m3u8?a=1706841642%2COTU1YmYzN2U5NjYxMTk5ZmI0ZTAzN2QzOGM0M2IwNDMyMmFiMmJmYzA5YmRjODgxZjJhYjQ0NDRkNzMwMzYwYg%3D%3D&amp;v=1&amp;f=sd", "id": "vuugpw2rfz9c1", "isGif": false}, "ib92wepmfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/DASHPlaylist.mpd?a=1706841642%2CYjE1MjEzZTJjNjZhMzNiNDIzNGM0ZDNhZTM2YmM0YmQ3OWE4MWI4NWMxMGY3NDhlZjYxODYyYzNmN2Q2NjFlYw%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/HLSPlaylist.m3u8?a=1706841642%2COGY0ODcyMWQ3N2Q0NDVmMTAyZWZlNDUyMWVhODkyYjMxNjI3Zjc5NjA4ODgyMzBhNDU1M2EzNTRmZjhhMTBjMw%3D%3D&amp;v=1&amp;f=sd", "id": "ib92wepmfz9c1", "isGif": false}}, "name": "t3_18wk4rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704182283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player\"&gt;Clean outliers with data painter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player\"&gt;create new feature in your data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.&lt;/p&gt;\n\n&lt;p&gt;Online tutorial of Data painter in PyGWalker: &lt;a href=\"https://data-painter-tutorial.pygwalker.kanaries.io/\"&gt;https://data-painter-tutorial.pygwalker.kanaries.io/&lt;/a&gt;&lt;br/&gt;\nPyGWalker&amp;#39;s Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wk4rt", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "subreddit_subscribers": 150123, "created_utc": 1704182283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it's public sector to emphasise that both the people and processes there are a bit old school.\n\nSo, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.\n\nThe person who developed this workflow 10 years or so ago, which hasn't changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!", "author_fullname": "t2_y2r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Data Warehousing Methodology Viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wpk8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704202495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it&amp;#39;s public sector to emphasise that both the people and processes there are a bit old school.&lt;/p&gt;\n\n&lt;p&gt;So, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.&lt;/p&gt;\n\n&lt;p&gt;The person who developed this workflow 10 years or so ago, which hasn&amp;#39;t changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp;amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wpk8k", "is_robot_indexable": true, "report_reasons": null, "author": "Vextus420", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "subreddit_subscribers": 150123, "created_utc": 1704202495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I`m used to work with SP whenever I am etl`ing from database to database. I only use python when I need to work with csv, an pai, Json.\n\nBut I think I rear about using python and learning python (for begginers mainly) too much.\n\nDoes It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?", "author_fullname": "t2_8lo1pjes", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python over Stored Procedure in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x3ygn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704238335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&lt;code&gt;m used to work with SP whenever I am etl&lt;/code&gt;ing from database to database. I only use python when I need to work with csv, an pai, Json.&lt;/p&gt;\n\n&lt;p&gt;But I think I rear about using python and learning python (for begginers mainly) too much.&lt;/p&gt;\n\n&lt;p&gt;Does It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x3ygn", "is_robot_indexable": true, "report_reasons": null, "author": "DesperateBus362", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/", "subreddit_subscribers": 150123, "created_utc": 1704238335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everybody, HNY 2024 \ud83c\udf89\n\nI am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.\n\nCurrently I am focusing more on optimising the existing jobs in the current org.\n\nI use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. \n\nHowever could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.\n \nI have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. \n\nI would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.\n\nThanks !", "author_fullname": "t2_188qz428", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestions for Optimising Spark Jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wrfv0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704207813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everybody, HNY 2024 \ud83c\udf89&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.&lt;/p&gt;\n\n&lt;p&gt;Currently I am focusing more on optimising the existing jobs in the current org.&lt;/p&gt;\n\n&lt;p&gt;I use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. &lt;/p&gt;\n\n&lt;p&gt;However could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.&lt;/p&gt;\n\n&lt;p&gt;I have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. &lt;/p&gt;\n\n&lt;p&gt;I would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wrfv0", "is_robot_indexable": true, "report_reasons": null, "author": "swarup_i_am", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "subreddit_subscribers": 150123, "created_utc": 1704207813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Incremental View Maintenance (IVM)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq7jj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fGlSxhubku9S7PFnu4hbxinrk1J2lgRQLGjkZHHE2qc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704204367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?auto=webp&amp;s=c7c52ec3d38c17618f8e2d66171c27c43db16e74", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82e0a27fd835a1d0f5334c8262bc19d8cccdf4e7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af2b704d8bca9c88866a4bd14c3bbd3c53402624", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c87e5f77b1543f701882e490e96ed4c562eb835", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784c1dbb829b60e7ef650982b2e17b96040ebd8d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=39fb1dcb35bafe6e3018a12df56b63cd9b942ccc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4966f73d32a66db7bfee30cd8f334db98c528ec7", "width": 1080, "height": 540}], "variants": {}, "id": "lnHZk4nL9uJOOq-9-k4mRzI86-2_bCgX2l6koPtAzk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wq7jj", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq7jj/what_is_incremental_view_maintenance_ivm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "subreddit_subscribers": 150123, "created_utc": 1704204367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.\n\nTerraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?\n\nI suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?\n\nThanks in advance!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does terraform fit in with CI/CD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wg4lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704168485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.&lt;/p&gt;\n\n&lt;p&gt;Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?&lt;/p&gt;\n\n&lt;p&gt;I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the &lt;code&gt;.state&lt;/code&gt; and &lt;code&gt;.vars&lt;/code&gt; files may play a role in this. Could needing to securely manage &lt;code&gt;.state&lt;/code&gt; files give need for a CD pipeline for some reason?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wg4lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "subreddit_subscribers": 150123, "created_utc": 1704168485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some of the high level meeting discussed these buzz words.\n\nSpark\nAzure\nIceberg\nStarrocks\n\nIn understand infrastructure is Azure and computational is done by Spark.\n\nWhat does StarRocks and Iceberg do in this setup. Can some one please explain how all these concepts fit in architecture or pipeline", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to DE concepts. My company analytics team was working on setting up DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wyvdt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704226049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some of the high level meeting discussed these buzz words.&lt;/p&gt;\n\n&lt;p&gt;Spark\nAzure\nIceberg\nStarrocks&lt;/p&gt;\n\n&lt;p&gt;In understand infrastructure is Azure and computational is done by Spark.&lt;/p&gt;\n\n&lt;p&gt;What does StarRocks and Iceberg do in this setup. Can some one please explain how all these concepts fit in architecture or pipeline&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wyvdt", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wyvdt/new_to_de_concepts_my_company_analytics_team_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wyvdt/new_to_de_concepts_my_company_analytics_team_was/", "subreddit_subscribers": 150123, "created_utc": 1704226049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm currently brainstorming ideas for my MSc Data Science/Comp Eng research project and I'm leaning towards a topic in Data Engineering. I'm particularly interested in areas involving pipelines, ETL, efficiency, cloud computing, and the use of Python, Spark, and SQL.\n\nHas anyone come across any recent research or projects in these areas that could inspire a solid project idea? I'm looking for something technically challenging and innovative.\n\nAppreciate any suggestions or pointers to recent papers, projects, or trends in this field!\n\nThanks!", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestions for a MSc Research Project in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wxzco", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704223945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently brainstorming ideas for my MSc Data Science/Comp Eng research project and I&amp;#39;m leaning towards a topic in Data Engineering. I&amp;#39;m particularly interested in areas involving pipelines, ETL, efficiency, cloud computing, and the use of Python, Spark, and SQL.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across any recent research or projects in these areas that could inspire a solid project idea? I&amp;#39;m looking for something technically challenging and innovative.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions or pointers to recent papers, projects, or trends in this field!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wxzco", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18wxzco/need_suggestions_for_a_msc_research_project_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wxzco/need_suggestions_for_a_msc_research_project_in/", "subreddit_subscribers": 150123, "created_utc": 1704223945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?", "author_fullname": "t2_3wbyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your reverse ETL tool of choice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18x6y0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704246120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18x6y0i", "is_robot_indexable": true, "report_reasons": null, "author": "axlee", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/", "subreddit_subscribers": 150123, "created_utc": 1704246120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nFirst of all I'd like to say that I'm not sure I'm on the right subreddit for my question.\n\nI want to deploy an LLM via vLLM ([https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)) in order to be able to respond to a \"high\" request concurrency. Due to the constraints of the hosting solution I'm using, each server can only have one GPU, so I need several servers to have enough VRAM. So I'm going to use Ray ([https://github.com/ray-project/ray](https://github.com/ray-project/ray)) to connect the GPUs together.\n\nHowever, I can't decide whether it's more efficient to create several \"small\" server clusters or to increase the number of servers (and therefore GPUs) in a single large cluster.\n\nIs having several GPUs using Ray (e.g. 16 GPUs) as fast as (maximum number of concurrent requests) having 8 clusters of 2 GPUs to serve the LLM? Assuming that 2 GPUs with N GB of VRAM are enough to run the model.\n\nFrom a purely concurrency point of view, what's the best way to do this: create a large cluster of servers operating as one (accessible via a single API endpoint) or several smaller clusters? This raises the question: does using more GPUs increase inference time linearly?\n\nThis is all new to me, so thank you in advance if you take the time to reply.", "author_fullname": "t2_bh9x1s8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying an LLM model for high concurrency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x5e5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704241951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;First of all I&amp;#39;d like to say that I&amp;#39;m not sure I&amp;#39;m on the right subreddit for my question.&lt;/p&gt;\n\n&lt;p&gt;I want to deploy an LLM via vLLM (&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;https://github.com/vllm-project/vllm&lt;/a&gt;) in order to be able to respond to a &amp;quot;high&amp;quot; request concurrency. Due to the constraints of the hosting solution I&amp;#39;m using, each server can only have one GPU, so I need several servers to have enough VRAM. So I&amp;#39;m going to use Ray (&lt;a href=\"https://github.com/ray-project/ray\"&gt;https://github.com/ray-project/ray&lt;/a&gt;) to connect the GPUs together.&lt;/p&gt;\n\n&lt;p&gt;However, I can&amp;#39;t decide whether it&amp;#39;s more efficient to create several &amp;quot;small&amp;quot; server clusters or to increase the number of servers (and therefore GPUs) in a single large cluster.&lt;/p&gt;\n\n&lt;p&gt;Is having several GPUs using Ray (e.g. 16 GPUs) as fast as (maximum number of concurrent requests) having 8 clusters of 2 GPUs to serve the LLM? Assuming that 2 GPUs with N GB of VRAM are enough to run the model.&lt;/p&gt;\n\n&lt;p&gt;From a purely concurrency point of view, what&amp;#39;s the best way to do this: create a large cluster of servers operating as one (accessible via a single API endpoint) or several smaller clusters? This raises the question: does using more GPUs increase inference time linearly?&lt;/p&gt;\n\n&lt;p&gt;This is all new to me, so thank you in advance if you take the time to reply.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?auto=webp&amp;s=9aaab601e3ec8ca05c8849214250a9a7e05c38b4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c019eb9e27808a489433512ea52ee6462215fa1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=29b0917a4bdeedd6c0c7e7bd8c040191ce9c00c0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=38039aa34e0b7c792d40b7b63ae7c45088c56c66", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2d8df98e5aa22d7597cb9656bd2d7772a9ceb86", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=43d25f191624848a2b1adf18480604158153327a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QrAf46jk8qjJc9KmjsCoUAo71DzCzURojoYmVMEp4S4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a96e29104b468597a73fcf81e28a5fb3f343e4fe", "width": 1080, "height": 540}], "variants": {}, "id": "0Lt256p-mqRs3s-w_4aUbfSqwcT4guz_9tlkZ7vb-ZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18x5e5q", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Pangolin-4096", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x5e5q/deploying_an_llm_model_for_high_concurrency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x5e5q/deploying_an_llm_model_for_high_concurrency/", "subreddit_subscribers": 150123, "created_utc": 1704241951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello y'all smart people,\n\nI'm a little frustrated by the experience of doing ad-hoc analyses in SQL.\n\nI love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it'd be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we'd have a nice documentation of the thought process that I can refer to in half a year when I've long forgotten whatever I was doing back then.\n\nHowever, this doesn't seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.\n\nDo I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it's properly documented? \n\nI'd be grateful for your input. Thanks!", "author_fullname": "t2_e7hyqhhe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using notebooks to analyze SQL - am I missing something?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wqg4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704205066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello y&amp;#39;all smart people,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little frustrated by the experience of doing ad-hoc analyses in SQL.&lt;/p&gt;\n\n&lt;p&gt;I love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it&amp;#39;d be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we&amp;#39;d have a nice documentation of the thought process that I can refer to in half a year when I&amp;#39;ve long forgotten whatever I was doing back then.&lt;/p&gt;\n\n&lt;p&gt;However, this doesn&amp;#39;t seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.&lt;/p&gt;\n\n&lt;p&gt;Do I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it&amp;#39;s properly documented? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be grateful for your input. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wqg4b", "is_robot_indexable": true, "report_reasons": null, "author": "UnusualCookieBox", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "subreddit_subscribers": 150123, "created_utc": 1704205066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the past few months, I have written about a number of different technologies ([Ray Data](https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/), [Ray Train](https://blog.min.io/distributed-training-with-ray-train-and-minio/), and [MLflow](https://blog.min.io/mlflow-tracking-and-minio/)). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my [Ray Train post](https://blog.min.io/distributed-training-with-ray-train-and-minio/) that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found [here](https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io).\n\n[https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=distributed\\_training\\_experiment\\_tracking\\_ray\\_train\\_mlflow+](https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Training and Experiment Tracking with Ray Train, MLflow, and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18x0tav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704230704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past few months, I have written about a number of different technologies (&lt;a href=\"https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/\"&gt;Ray Data&lt;/a&gt;, &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train&lt;/a&gt;, and &lt;a href=\"https://blog.min.io/mlflow-tracking-and-minio/\"&gt;MLflow&lt;/a&gt;). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my &lt;a href=\"https://blog.min.io/distributed-training-with-ray-train-and-minio/\"&gt;Ray Train post&lt;/a&gt; that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found &lt;a href=\"https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+\"&gt;https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?auto=webp&amp;s=60a9a5bb13ad52dc6f983d22a24a53b3b835a119", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db2aa38b11de95fa42f9dbf716d73c3b2a64e4f3", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c18b4e8bc4062930b8490b5721bf20e5d91b96a", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e065467bf6807e1f7014eb05a7e8aef79c5b4e88", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74d1b5b114a455946e7c372fa0cc71907ebdd6dd", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d0ea6ae4d3f00a6c97c690b38471318183ee2c8", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=011a2c8ca8070e4782e8cf151170e64f2db7fd85", "width": 1080, "height": 323}], "variants": {}, "id": "ycJVbgf3bDDq06hpn_WuGtC2gZGgwo5DkVBTrJOe5j8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18x0tav", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/", "subreddit_subscribers": 150123, "created_utc": 1704230704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a requirement to read mainframe ebcdic files using spark. Has anyone had any luck with this?\n\nUpon research I came across \"cobrix\". Has anyone used this package before? . Unfortunately that's the only one we have come across.\n\nPlease suggest if there are any other options that are avaliable to read these files on spark.", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EBCIDIC in spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wxfo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704223101.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704222635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a requirement to read mainframe ebcdic files using spark. Has anyone had any luck with this?&lt;/p&gt;\n\n&lt;p&gt;Upon research I came across &amp;quot;cobrix&amp;quot;. Has anyone used this package before? . Unfortunately that&amp;#39;s the only one we have come across.&lt;/p&gt;\n\n&lt;p&gt;Please suggest if there are any other options that are avaliable to read these files on spark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wxfo5", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wxfo5/ebcidic_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wxfo5/ebcidic_in_spark/", "subreddit_subscribers": 150123, "created_utc": 1704222635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.\n\nCurrent we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. \n\nThings I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.\n\nAnother one was trying on airflow however windows doesn't support airflow and vice versa, wsl works however that python doesn't work on any form of Linux because it uses some windows features.\n\nAm I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?\n\nAnswers to commonly asked question:\n\n1) No it cannot run in container or any form of linux, needs to be windows only.\n\n2) And it needs to run python\n\n3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.", "author_fullname": "t2_ozjx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a weird OnPrem Setup which uses Task Scheduler currently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq815", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704204406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.&lt;/p&gt;\n\n&lt;p&gt;Current we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. &lt;/p&gt;\n\n&lt;p&gt;Things I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.&lt;/p&gt;\n\n&lt;p&gt;Another one was trying on airflow however windows doesn&amp;#39;t support airflow and vice versa, wsl works however that python doesn&amp;#39;t work on any form of Linux because it uses some windows features.&lt;/p&gt;\n\n&lt;p&gt;Am I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?&lt;/p&gt;\n\n&lt;p&gt;Answers to commonly asked question:&lt;/p&gt;\n\n&lt;p&gt;1) No it cannot run in container or any form of linux, needs to be windows only.&lt;/p&gt;\n\n&lt;p&gt;2) And it needs to run python&lt;/p&gt;\n\n&lt;p&gt;3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wq815", "is_robot_indexable": true, "report_reasons": null, "author": "tecedu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "subreddit_subscribers": 150123, "created_utc": 1704204406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks", "author_fullname": "t2_7hwnfxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing in a migration project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wjwgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704181362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wjwgf", "is_robot_indexable": true, "report_reasons": null, "author": "johndough990", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "subreddit_subscribers": 150123, "created_utc": 1704181362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? \n\n\nMy initial idea is to have a python script that scrape the website. It's scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don't really know how that would look like. \n\nCan you please give me tips and hints on how to approach this problem with the best practices.\n\nDo I need to use docker for example?\n\nThanks a lot", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to design a webscraping pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wggcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704169506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let&amp;#39;s say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? &lt;/p&gt;\n\n&lt;p&gt;My initial idea is to have a python script that scrape the website. It&amp;#39;s scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don&amp;#39;t really know how that would look like. &lt;/p&gt;\n\n&lt;p&gt;Can you please give me tips and hints on how to approach this problem with the best practices.&lt;/p&gt;\n\n&lt;p&gt;Do I need to use docker for example?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wggcl", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "subreddit_subscribers": 150123, "created_utc": 1704169506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?\n\nEdit 0:\n\nClarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).\n\nI am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))\n\nI want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?", "author_fullname": "t2_phuejnxhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE skill for LLM project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18we8em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704165727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704162949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?&lt;/p&gt;\n\n&lt;p&gt;Edit 0:&lt;/p&gt;\n\n&lt;p&gt;Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: &lt;em&gt;building foundation model, autonomous AI agent&lt;/em&gt;. Besides, the data source will very diversified (internet, book, paper report, ...).&lt;/p&gt;\n\n&lt;p&gt;I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))&lt;/p&gt;\n\n&lt;p&gt;I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18we8em", "is_robot_indexable": true, "report_reasons": null, "author": "basic_of_basic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "subreddit_subscribers": 150123, "created_utc": 1704162949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have prior coworkers who live in Ukraine. They\u2019re hard working, dedicated, and brilliant people with a good sense of humor. They often jokingly make light of the war:\n\n\u201cHow are you today?\u201d  \n\u201cEh, all good accept for those Russians still trying to kill us \ud83d\ude05\u201d\n\n Though I know the war is no light matter. \n\nI find myself wondering if it\u2019s possible to help them in some way, via providing a unique form of aid. It\u2019s a bit unrealistic I feel to fly over there and give them my labor in the combat zone, perhaps even not the best use of my ability as I\u2019m a smaller guy (despite some military experience and a Secret clearance).\n\nI\u2019m half decent with data engineering though. I have a degree in field, a few years working experience, and I read data engineering textbooks practically for fun. I think I could offer support via data engineering in an effective way if it were needed.\n\nMaybe their intelligence channels could use people working on a better data platform to facilitate efficient garnishment of insights from data? Who knows\u2026\n\nIs this a thing? Can it be a thing? Thanks in advance for any thoughts on the matter.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to volunteer for the Ukraine War as a remote data engineer / analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wu8lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.41, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704214974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have prior coworkers who live in Ukraine. They\u2019re hard working, dedicated, and brilliant people with a good sense of humor. They often jokingly make light of the war:&lt;/p&gt;\n\n&lt;p&gt;\u201cHow are you today?\u201d&lt;br/&gt;\n\u201cEh, all good accept for those Russians still trying to kill us \ud83d\ude05\u201d&lt;/p&gt;\n\n&lt;p&gt;Though I know the war is no light matter. &lt;/p&gt;\n\n&lt;p&gt;I find myself wondering if it\u2019s possible to help them in some way, via providing a unique form of aid. It\u2019s a bit unrealistic I feel to fly over there and give them my labor in the combat zone, perhaps even not the best use of my ability as I\u2019m a smaller guy (despite some military experience and a Secret clearance).&lt;/p&gt;\n\n&lt;p&gt;I\u2019m half decent with data engineering though. I have a degree in field, a few years working experience, and I read data engineering textbooks practically for fun. I think I could offer support via data engineering in an effective way if it were needed.&lt;/p&gt;\n\n&lt;p&gt;Maybe their intelligence channels could use people working on a better data platform to facilitate efficient garnishment of insights from data? Who knows\u2026&lt;/p&gt;\n\n&lt;p&gt;Is this a thing? Can it be a thing? Thanks in advance for any thoughts on the matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wu8lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wu8lm/is_it_possible_to_volunteer_for_the_ukraine_war/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wu8lm/is_it_possible_to_volunteer_for_the_ukraine_war/", "subreddit_subscribers": 150123, "created_utc": 1704214974.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}