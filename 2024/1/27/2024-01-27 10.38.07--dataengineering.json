{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "yes, I really said it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1abmrzv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 205, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 205, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IaEaqY3gaUV3kYdpcYR1IUh1hKrQbufDzuBZO3wMa0I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706287556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8jacgiphctec1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8jacgiphctec1.jpeg?auto=webp&amp;s=efd29ecd70a28b8288cf863fcd3a5695769ce223", "width": 500, "height": 533}, "resolutions": [{"url": "https://preview.redd.it/8jacgiphctec1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7fa99b544c849fb42a898133498f85405ec080e", "width": 108, "height": 115}, {"url": "https://preview.redd.it/8jacgiphctec1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9960cca84dfee4174a8d51f4393c6d996c0737a7", "width": 216, "height": 230}, {"url": "https://preview.redd.it/8jacgiphctec1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bfd5c0923e3dfdd11648169f8161f3df74983b3", "width": 320, "height": 341}], "variants": {}, "id": "Ppy2EGPzPp-8QQGhneq6NHKva89Vb3-0cccYq60jpko"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1abmrzv", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abmrzv/yes_i_really_said_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8jacgiphctec1.jpeg", "subreddit_subscribers": 156145, "created_utc": 1706287556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Something for fun, what abilities would you give this card?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1abov9g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qrvcvgIZsxQioR_qtHsV3i2JC60JO4Q7AFfChKsok7o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706292775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8om179jgstec1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8om179jgstec1.jpeg?auto=webp&amp;s=aa06aeb989c20985c5659dd0a802c6212a64209e", "width": 1079, "height": 1536}, "resolutions": [{"url": "https://preview.redd.it/8om179jgstec1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=18bece97bf3e9ddbfda9092179afa531c440ddc3", "width": 108, "height": 153}, {"url": "https://preview.redd.it/8om179jgstec1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a80425b558f0fbe406725cf445b9a14361ace95", "width": 216, "height": 307}, {"url": "https://preview.redd.it/8om179jgstec1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=109b631b53b199f35fc98e1d0ba86466fd7b8be2", "width": 320, "height": 455}, {"url": "https://preview.redd.it/8om179jgstec1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82eb0256fc417b467901a5c305f863c18248449e", "width": 640, "height": 911}, {"url": "https://preview.redd.it/8om179jgstec1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a1c595e140a970fd594c0c4ee7d1b3c5413068a", "width": 960, "height": 1366}], "variants": {}, "id": "4IFCKcG9DTFCMAzczawU1-DPQw6hEgcm35OwfBfwKb8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1abov9g", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abov9g/something_for_fun_what_abilities_would_you_give/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8om179jgstec1.jpeg", "subreddit_subscribers": 156145, "created_utc": 1706292775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a ETL developer. I have about 400/2500 tables that don\u2019t have primary keys. I asked my manager why they don\u2019t have primary keys and he said he doesn\u2019t know. I asked if there was any documentation and there isn\u2019t. I asked if I had to make them up for each table and he said yeah. I\u2019m supposed to make them up? How? He said yeah it\u2019s really annoying but it\u2019s something we have to do\u2026I need primary keys in the table for my code to work that migrates the data from source to target. \n\nAren\u2019t the DBA or the person who created the table supposed to do it? I\u2019m genuinely curious because I don\u2019t think I can sit there and go through 400 tables and make up a primary key for each table. Like I can\u2019t mentally handle sitting there and going through each table like that. I\u2019m considering finding a different job over this. Has anyone else done this before? It\u2019s 400 tables for christs sake\u2026", "author_fullname": "t2_4eu1efwn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whose job is it to add primary keys to tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abfe9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706265231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a ETL developer. I have about 400/2500 tables that don\u2019t have primary keys. I asked my manager why they don\u2019t have primary keys and he said he doesn\u2019t know. I asked if there was any documentation and there isn\u2019t. I asked if I had to make them up for each table and he said yeah. I\u2019m supposed to make them up? How? He said yeah it\u2019s really annoying but it\u2019s something we have to do\u2026I need primary keys in the table for my code to work that migrates the data from source to target. &lt;/p&gt;\n\n&lt;p&gt;Aren\u2019t the DBA or the person who created the table supposed to do it? I\u2019m genuinely curious because I don\u2019t think I can sit there and go through 400 tables and make up a primary key for each table. Like I can\u2019t mentally handle sitting there and going through each table like that. I\u2019m considering finding a different job over this. Has anyone else done this before? It\u2019s 400 tables for christs sake\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abfe9x", "is_robot_indexable": true, "report_reasons": null, "author": "briogeosucks", "discussion_type": null, "num_comments": 107, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abfe9x/whose_job_is_it_to_add_primary_keys_to_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abfe9x/whose_job_is_it_to_add_primary_keys_to_tables/", "subreddit_subscribers": 156145, "created_utc": 1706265231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "## Context\nI've seen a few posts/comments on the top of getting a new job. Talking about a tough job market, learning skills, etc. The comment I wanted to write on a few of those deserved a top level post IMO.\n\nThe market is somewhat saturated at the moment with the recent layoffs and hiring freezes/slowdowns, so you won't just get an offer or three thrown at you just because you applied a few places. Here's the simplest way to get hired.\n\n## Differentiate yourself\n\nA hiring manager or recruiter has to sift through tens to hundreds of resumes to fill a position; your resume needs to be have a reason why it should be chosen over your fellow applicants. **If your resume reads like every other applicants', then you're likely not going to get interviews**. The follow up question, then, is how do you differentiate yourself? Here are a few suggestions\n\n1. Tailor your resume to the job description. At the very least, use keywords that the company is looking for. If the company isn't looking for it or it isn't relevant, take it off your resume; it's wasted space.\n\n2. Write each bullet in your job descriptions demonstrating one of either two things (1) business impact or (2) differentiating skills. For business impact bullets, convey the outcome that your actions had on the business and not a description of what you did. For skills-related bullets, mention how you used specialized technology to solve a business problem. You're probably not being hired to be a technologist but to help drive business success.\n\n3. Clearly demonstrate your skill sets!  Saying you can write Python is fine. Having a link to an easy to navigate Github.\n\n4. Have side projects. This is an important carve out of #2 above. If your skill is passion for the industry or willingness to tackle unsolved problems, having a side project is the most effective way to demonstrate this and stand out from the crowd. \n\n5. Network. If you don't have enough experience to have differentiated job descriptions nor are you willing/able to put in time on a side project, your best bet to get interviews is to bypass the resume screening altogether. Go to events, meet people, tell them you're looking for a job, and/or offer to help them out however you can. If you can't travel, there are online meetups.", "author_fullname": "t2_ji5pjmjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why you're not getting hired -- Tips for those looking for a new job.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ablzb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706285619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Context&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;ve seen a few posts/comments on the top of getting a new job. Talking about a tough job market, learning skills, etc. The comment I wanted to write on a few of those deserved a top level post IMO.&lt;/p&gt;\n\n&lt;p&gt;The market is somewhat saturated at the moment with the recent layoffs and hiring freezes/slowdowns, so you won&amp;#39;t just get an offer or three thrown at you just because you applied a few places. Here&amp;#39;s the simplest way to get hired.&lt;/p&gt;\n\n&lt;h2&gt;Differentiate yourself&lt;/h2&gt;\n\n&lt;p&gt;A hiring manager or recruiter has to sift through tens to hundreds of resumes to fill a position; your resume needs to be have a reason why it should be chosen over your fellow applicants. &lt;strong&gt;If your resume reads like every other applicants&amp;#39;, then you&amp;#39;re likely not going to get interviews&lt;/strong&gt;. The follow up question, then, is how do you differentiate yourself? Here are a few suggestions&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Tailor your resume to the job description. At the very least, use keywords that the company is looking for. If the company isn&amp;#39;t looking for it or it isn&amp;#39;t relevant, take it off your resume; it&amp;#39;s wasted space.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Write each bullet in your job descriptions demonstrating one of either two things (1) business impact or (2) differentiating skills. For business impact bullets, convey the outcome that your actions had on the business and not a description of what you did. For skills-related bullets, mention how you used specialized technology to solve a business problem. You&amp;#39;re probably not being hired to be a technologist but to help drive business success.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Clearly demonstrate your skill sets!  Saying you can write Python is fine. Having a link to an easy to navigate Github.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have side projects. This is an important carve out of #2 above. If your skill is passion for the industry or willingness to tackle unsolved problems, having a side project is the most effective way to demonstrate this and stand out from the crowd. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Network. If you don&amp;#39;t have enough experience to have differentiated job descriptions nor are you willing/able to put in time on a side project, your best bet to get interviews is to bypass the resume screening altogether. Go to events, meet people, tell them you&amp;#39;re looking for a job, and/or offer to help them out however you can. If you can&amp;#39;t travel, there are online meetups.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ablzb7", "is_robot_indexable": true, "report_reasons": null, "author": "butwhhyy", "discussion_type": null, "num_comments": 20, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ablzb7/why_youre_not_getting_hired_tips_for_those/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ablzb7/why_youre_not_getting_hired_tips_for_those/", "subreddit_subscribers": 156145, "created_utc": 1706285619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it a lack of connectors for a particular tool, is your data shuffling too much, what are the things causing you pain these days?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your current biggest day to day pains in building pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abifu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706275992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it a lack of connectors for a particular tool, is your data shuffling too much, what are the things causing you pain these days?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abifu5", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 73, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abifu5/what_is_your_current_biggest_day_to_day_pains_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abifu5/what_is_your_current_biggest_day_to_day_pains_in/", "subreddit_subscribers": 156145, "created_utc": 1706275992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Geoglify is now open source! Explore tools for understanding the maritime universe in GIS space. A new tech stack, a whole new world for me, geared towards the geospatial maritime universe. Where I'll bring all ideas to life. The code is now available on GitHub. The first version is open for testing. I think everything you need for this exciting journey is there.\n\n[https://github.com/geoglify/geoglify](https://github.com/geoglify/geoglify)", "author_fullname": "t2_7svy5qp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geoglify is now Open Source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 112, "top_awarded_type": null, "hide_score": false, "name": "t3_1absxeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ogur4U1KGSji2HIRRNzHHWJoZjYNITJpQsGYjpSCiuI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706303138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Geoglify is now open source! Explore tools for understanding the maritime universe in GIS space. A new tech stack, a whole new world for me, geared towards the geospatial maritime universe. Where I&amp;#39;ll bring all ideas to life. The code is now available on GitHub. The first version is open for testing. I think everything you need for this exciting journey is there.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/geoglify/geoglify\"&gt;https://github.com/geoglify/geoglify&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rtk11tk9nuec1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?auto=webp&amp;s=089f9eabc5ca7ba810b6be84caaabf228aed8bfc", "width": 1177, "height": 946}, "resolutions": [{"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9794aa952d29085555559f25821fee23cb4cae4e", "width": 108, "height": 86}, {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f13a04a6e9c08dbbfb87478a5a3c329f7091bbef", "width": 216, "height": 173}, {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31e011877e8202405972909b932121cfe1ff6e16", "width": 320, "height": 257}, {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cdf996e7a4ac212752c7a85f49cc769c479f7924", "width": 640, "height": 514}, {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=51d5f1bf6c7bc44dd4bf51ef7a4d2b828f4d8c0e", "width": 960, "height": 771}, {"url": "https://preview.redd.it/rtk11tk9nuec1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87ad786161e0ac91c6df503cf2c714175e593d59", "width": 1080, "height": 868}], "variants": {}, "id": "fan7TNc_bikz65KgQzrHrubolBihBysTvihaCt-rcHY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1absxeh", "is_robot_indexable": true, "report_reasons": null, "author": "leoneljdias", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1absxeh/geoglify_is_now_open_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rtk11tk9nuec1.jpeg", "subreddit_subscribers": 156145, "created_utc": 1706303138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a developer, not a DE. I have the most experience in Python, and some in Pandas (more each day). \n\nI inherited a large repo of pandas scripts that run ~9 hours each day, every day on a 750 GB EC2 instance. They take a handful of large partitioned parquet datasets, a few other daily CSV files, calculate that day's data, and rewrite those datasets as the output. Each day, each dataset will have 1 additional parquet file. The repo was written by an individual with very very bad programming experience and skills, but good salesmanship. The code sucks. It's very hard to follow, multiple thousand line functions, I could go on.\n\nThe large parquet datasets are partitioned by date going back to 2007, so there's ~6000 individual parquet files in each dataset called something like 'action_date=2022-01-05.parquet'. This data has to be Point-In-Time, so previous day's data are read but not modified. The outputs are the up-to-date versions of each dataset, with a new parquet file for the current date. \n\n\nFor now this system can't change. Clients expect these parquet files structured as they are. \n\n\nThe memory requirement is huge, it's currently running on a 750 GB instance, and uses most of it. \nA single dataset (there's 5-10) is ~30 GB as a pq dataset, but once loaded into Pandas, 170GB is used.\n\n\n I'm doing everything I can to minimize the memory consumption. I want to at least make it run in a 500 GB instance and save a lot of money. I'm using more optimal Pandas dtypes, I'm verifying that all columns are actually needed when reading in a parquet dataset, and I'm extracting a lot of intermediate steps to separate functions, so dataframes not needed anymore are garbage-collected, instead of lingering in memory until the end. I also updated Pandas from 1.2 to 2.0, which helped lower the runtime a bit but memory consumption remained constant.\n\nI'd be happy to have a longer runtime for lower memory requirement. \n\nI can't change out of Pandas for the whole project in the short term. I'm trying to find a way to load these into Pandas more efficiently. Right now, data is loaded:\n\n    dataset = pq.ParquetDataset(filepath, filters=filters)\n    dataset = dataset.read()\n    dataset = dataset.to_pandas()\n\nI've been recommended other tools, like Dask. I tried it, but when you run .compute() on the lazy-loaded df, it uses the same amount of memory.\n\nI'm open to any suggestions or advice here. I have to maintain the date-partitioned parquet dataset, but any way I can load these into dataframes more efficiently would be welcomed.", "author_fullname": "t2_6k5atzmk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing memory-intensive pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1absst1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706302825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a developer, not a DE. I have the most experience in Python, and some in Pandas (more each day). &lt;/p&gt;\n\n&lt;p&gt;I inherited a large repo of pandas scripts that run ~9 hours each day, every day on a 750 GB EC2 instance. They take a handful of large partitioned parquet datasets, a few other daily CSV files, calculate that day&amp;#39;s data, and rewrite those datasets as the output. Each day, each dataset will have 1 additional parquet file. The repo was written by an individual with very very bad programming experience and skills, but good salesmanship. The code sucks. It&amp;#39;s very hard to follow, multiple thousand line functions, I could go on.&lt;/p&gt;\n\n&lt;p&gt;The large parquet datasets are partitioned by date going back to 2007, so there&amp;#39;s ~6000 individual parquet files in each dataset called something like &amp;#39;action_date=2022-01-05.parquet&amp;#39;. This data has to be Point-In-Time, so previous day&amp;#39;s data are read but not modified. The outputs are the up-to-date versions of each dataset, with a new parquet file for the current date. &lt;/p&gt;\n\n&lt;p&gt;For now this system can&amp;#39;t change. Clients expect these parquet files structured as they are. &lt;/p&gt;\n\n&lt;p&gt;The memory requirement is huge, it&amp;#39;s currently running on a 750 GB instance, and uses most of it. \nA single dataset (there&amp;#39;s 5-10) is ~30 GB as a pq dataset, but once loaded into Pandas, 170GB is used.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing everything I can to minimize the memory consumption. I want to at least make it run in a 500 GB instance and save a lot of money. I&amp;#39;m using more optimal Pandas dtypes, I&amp;#39;m verifying that all columns are actually needed when reading in a parquet dataset, and I&amp;#39;m extracting a lot of intermediate steps to separate functions, so dataframes not needed anymore are garbage-collected, instead of lingering in memory until the end. I also updated Pandas from 1.2 to 2.0, which helped lower the runtime a bit but memory consumption remained constant.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be happy to have a longer runtime for lower memory requirement. &lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t change out of Pandas for the whole project in the short term. I&amp;#39;m trying to find a way to load these into Pandas more efficiently. Right now, data is loaded:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dataset = pq.ParquetDataset(filepath, filters=filters)\ndataset = dataset.read()\ndataset = dataset.to_pandas()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve been recommended other tools, like Dask. I tried it, but when you run .compute() on the lazy-loaded df, it uses the same amount of memory.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to any suggestions or advice here. I have to maintain the date-partitioned parquet dataset, but any way I can load these into dataframes more efficiently would be welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1absst1", "is_robot_indexable": true, "report_reasons": null, "author": "foyslakesheriff", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1absst1/optimizing_memoryintensive_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1absst1/optimizing_memoryintensive_pandas/", "subreddit_subscribers": 156145, "created_utc": 1706302825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Check out [Medium article](https://medium.com/@thomas.dambrin/snowflake-worksheets-versioning-made-easy-4512d8ee5cc2)", "author_fullname": "t2_v3zgf0lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake worksheets Git versioning made easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abwti3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706313156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out &lt;a href=\"https://medium.com/@thomas.dambrin/snowflake-worksheets-versioning-made-easy-4512d8ee5cc2\"&gt;Medium article&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?auto=webp&amp;s=959490410ae9c533c386e4d15088f22c4c93a734", "width": 1118, "height": 1048}, "resolutions": [{"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=22f9dda26c775fe82bbd91116f36b0eee7f680ca", "width": 108, "height": 101}, {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff729d88c15b6e4da2c532e3bb635e0bdbc0e61b", "width": 216, "height": 202}, {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ee7ab9940831ff2e69803a1e2dedd625e3117fd", "width": 320, "height": 299}, {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=806ecc8a7a87eba045c4f464aacfbf73df6f39c1", "width": 640, "height": 599}, {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d079994376ec3e119fa28f1dadde8477666d7254", "width": 960, "height": 899}, {"url": "https://external-preview.redd.it/9iRsVZ2YTYO8EC4MfRQhyfDx5iAgcDFCkdWOiu0PoxI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cd1e320a0f894a486c5ccb53d63e04c39ea664b", "width": 1080, "height": 1012}], "variants": {}, "id": "WIcMJUze3Vks2RJQ_OBD9nR0l6E6AXYadMj21RnZDyI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1abwti3", "is_robot_indexable": true, "report_reasons": null, "author": "thomas-d11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abwti3/snowflake_worksheets_git_versioning_made_easy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abwti3/snowflake_worksheets_git_versioning_made_easy/", "subreddit_subscribers": 156145, "created_utc": 1706313156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working for a client as an ETL developer who is using 1010data . The tool uses XML to do the transformations compared to current SQL based tools. Having a good grip on this tool, will it help me in future or should I look for other opportunities elsewhere?", "author_fullname": "t2_9y85ksm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is 1010data still relevant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ac543u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706339494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working for a client as an ETL developer who is using 1010data . The tool uses XML to do the transformations compared to current SQL based tools. Having a good grip on this tool, will it help me in future or should I look for other opportunities elsewhere?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ac543u", "is_robot_indexable": true, "report_reasons": null, "author": "stinger_sks_22", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ac543u/is_1010data_still_relevant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ac543u/is_1010data_still_relevant/", "subreddit_subscribers": 156145, "created_utc": 1706339494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm so lucky, I landed my first full time analyst job at my dream company (large healthcare system). The pay isnt great, but the benefits and work/life balance are incredible. I now realize it's a great place to work if you're settling down, not so great if you're looking to move up the ladder in your mid 20s. \n\nI do a mix of ETL scripting (pandas, sqlalchemy, pyodbc) and mundane accounting with Excel. \n\nI originally had aspirations of landing a data engineering role 1-2 yrs later, but I think I would want ML engineer roles instead. I want to go into risk analytics for health insurance/healthcare systems, there's a lot of opportunities to use advanced stats and ML there. The data I currently work with doesn't have any clinical info to determine risk, so I'm limited to data engineering tasks. I also want to be more involved in delivering business insights, rather than strictly backend data stuff. \n\nAny ML engineers or data engineers here to share their thoughts?", "author_fullname": "t2_lfyfd0fn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data analyst to data/ML engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abnlf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706289585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m so lucky, I landed my first full time analyst job at my dream company (large healthcare system). The pay isnt great, but the benefits and work/life balance are incredible. I now realize it&amp;#39;s a great place to work if you&amp;#39;re settling down, not so great if you&amp;#39;re looking to move up the ladder in your mid 20s. &lt;/p&gt;\n\n&lt;p&gt;I do a mix of ETL scripting (pandas, sqlalchemy, pyodbc) and mundane accounting with Excel. &lt;/p&gt;\n\n&lt;p&gt;I originally had aspirations of landing a data engineering role 1-2 yrs later, but I think I would want ML engineer roles instead. I want to go into risk analytics for health insurance/healthcare systems, there&amp;#39;s a lot of opportunities to use advanced stats and ML there. The data I currently work with doesn&amp;#39;t have any clinical info to determine risk, so I&amp;#39;m limited to data engineering tasks. I also want to be more involved in delivering business insights, rather than strictly backend data stuff. &lt;/p&gt;\n\n&lt;p&gt;Any ML engineers or data engineers here to share their thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abnlf1", "is_robot_indexable": true, "report_reasons": null, "author": "velimino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abnlf1/data_analyst_to_dataml_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abnlf1/data_analyst_to_dataml_engineer/", "subreddit_subscribers": 156145, "created_utc": 1706289585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How is the job market for data engineering in south east Asia now? Or is it still bad as last year? I have been on market for six months now and not receiving any enquiries(not even HR calls).\n\n\nPS: I am a foreigner to these countries and so my job chances are limited to the opportunities which can sponsor visas in those countries.", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the job market up for DE in south east Asia now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ac74ox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706347700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is the job market for data engineering in south east Asia now? Or is it still bad as last year? I have been on market for six months now and not receiving any enquiries(not even HR calls).&lt;/p&gt;\n\n&lt;p&gt;PS: I am a foreigner to these countries and so my job chances are limited to the opportunities which can sponsor visas in those countries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ac74ox", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ac74ox/is_the_job_market_up_for_de_in_south_east_asia_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ac74ox/is_the_job_market_up_for_de_in_south_east_asia_now/", "subreddit_subscribers": 156145, "created_utc": 1706347700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Outside of like using copilot, I mean in the code base to use ai on support of functions to help abstract a layer ? Almost like maybe idk you pass in just a url or some endpoint and it automation write the correct request or idk even know I guess where you can push the rock so I wanted to open up a convo.\n\n\n***Maybe Simpler Put***\n\nHow can a data engineer in 2024 make the best etl library to digest net new sources &amp; is gen ai able to help that - if so how ? ", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If your developing an etl library ( for sources that are atypical or not covered by meltano ect) what\u2019s the best way to incorporate gen ai , if at all ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abteqs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706305486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706304374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Outside of like using copilot, I mean in the code base to use ai on support of functions to help abstract a layer ? Almost like maybe idk you pass in just a url or some endpoint and it automation write the correct request or idk even know I guess where you can push the rock so I wanted to open up a convo.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Maybe Simpler Put&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How can a data engineer in 2024 make the best etl library to digest net new sources &amp;amp; is gen ai able to help that - if so how ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abteqs", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abteqs/if_your_developing_an_etl_library_for_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abteqs/if_your_developing_an_etl_library_for_sources/", "subreddit_subscribers": 156145, "created_utc": 1706304374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone!\nI am starting as an Associate Data Ops Engineer at a Product Based Company.\nI have transitioned from being a Mechanical Engineer to the Data Field.\nI am very eager to learn and can't seem to get my head around what to focus on and what's the sequence of learning.\nCan anyone help me with the right source to follow through?\nAlso, how important is it to learn DSA?", "author_fullname": "t2_3h7f791q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abfnwg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706266342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone!\nI am starting as an Associate Data Ops Engineer at a Product Based Company.\nI have transitioned from being a Mechanical Engineer to the Data Field.\nI am very eager to learn and can&amp;#39;t seem to get my head around what to focus on and what&amp;#39;s the sequence of learning.\nCan anyone help me with the right source to follow through?\nAlso, how important is it to learn DSA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1abfnwg", "is_robot_indexable": true, "report_reasons": null, "author": "ruchirmittal", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abfnwg/guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abfnwg/guidance/", "subreddit_subscribers": 156145, "created_utc": 1706266342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a GitHub repo filled with scala code where the CI creates jar and submits to gcs bucket. Then airflow will use this jar. The jars in gcs bucket are timestamped. \n\nHow do I get airflow to use the latest jar always? Airflow python code in DAG which belongs to same GitHub repo is currently hard coded to a specific timestamp version and its tedious to constantly update it. Is there a way to just say, use the latest bucket timestamp for this jar name?", "author_fullname": "t2_f4rnp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use latest airflow jars?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ac4a9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706336253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a GitHub repo filled with scala code where the CI creates jar and submits to gcs bucket. Then airflow will use this jar. The jars in gcs bucket are timestamped. &lt;/p&gt;\n\n&lt;p&gt;How do I get airflow to use the latest jar always? Airflow python code in DAG which belongs to same GitHub repo is currently hard coded to a specific timestamp version and its tedious to constantly update it. Is there a way to just say, use the latest bucket timestamp for this jar name?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ac4a9i", "is_robot_indexable": true, "report_reasons": null, "author": "alwaysSearching23", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ac4a9i/how_to_use_latest_airflow_jars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ac4a9i/how_to_use_latest_airflow_jars/", "subreddit_subscribers": 156145, "created_utc": 1706336253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've been told our team is going agile. \n\nThe main issues in my mind are:\n\n  * We do unpredictable support work. Like &gt; 1/2 of the week being as-it-came-in support isn't unheard of. \n  * We (3 folks) are 90% siloed, so effectively we're 3 teams of 1\n\nDue to the 3 folks all being 'teams of 1', I'm told: \n\n  * No planning poker, we'll just each make our own estimates\n \nFor support work aspect...no-one seems quite sure yet. Either:\n\n  * We have a user story that represents support\n  * We just keep support out of Devops, and guesstimate our support time per week, and reduce planning points accordingly. \n\nAnyone tried agile in the above situation?  Cross training wasn't well received (Lack of time - and we're *very* siloed in all the different things we do: Not just data engineering - I'm just throwing this in dataengineering for wont of a better subreddit).", "author_fullname": "t2_3bc49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Agile: With no planning poker, siloed non-cross-trained members, and unpredictable support work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abu6ra", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706306361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been told our team is going agile. &lt;/p&gt;\n\n&lt;p&gt;The main issues in my mind are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We do unpredictable support work. Like &amp;gt; 1/2 of the week being as-it-came-in support isn&amp;#39;t unheard of. &lt;/li&gt;\n&lt;li&gt;We (3 folks) are 90% siloed, so effectively we&amp;#39;re 3 teams of 1&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Due to the 3 folks all being &amp;#39;teams of 1&amp;#39;, I&amp;#39;m told: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;No planning poker, we&amp;#39;ll just each make our own estimates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For support work aspect...no-one seems quite sure yet. Either:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We have a user story that represents support&lt;/li&gt;\n&lt;li&gt;We just keep support out of Devops, and guesstimate our support time per week, and reduce planning points accordingly. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone tried agile in the above situation?  Cross training wasn&amp;#39;t well received (Lack of time - and we&amp;#39;re &lt;em&gt;very&lt;/em&gt; siloed in all the different things we do: Not just data engineering - I&amp;#39;m just throwing this in dataengineering for wont of a better subreddit).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abu6ra", "is_robot_indexable": true, "report_reasons": null, "author": "cdigioia", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abu6ra/agile_with_no_planning_poker_siloed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abu6ra/agile_with_no_planning_poker_siloed/", "subreddit_subscribers": 156145, "created_utc": 1706306361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Difficulties of Senior Engineer \u2026. are not Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abjdyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1706278739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/the-difficulties-of-senior-engineer-are-not-engineering/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1abjdyg", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abjdyg/the_difficulties_of_senior_engineer_are_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/the-difficulties-of-senior-engineer-are-not-engineering/", "subreddit_subscribers": 156145, "created_utc": 1706278739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, supply chain data analyst here. I have a series of poorly ingested fields without primary keys. And for my own sanity I want to create them. And not just slap an auto incremented field on it with a unique constraint.\n\nI want to find the fields that would make a primary key unique (and then slap an auto increment on those).  For example, maybe load_num, shipment_status, and arrival_time make a unique key.\n\nBut im finding my phrasing for this process to be incorrect, bc when i google it all I get are links to creating constraints.\n\nAm I even thinking about this correctly?", "author_fullname": "t2_156swl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating new primary keys from multiple non-unique fields", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abj66s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706278110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, supply chain data analyst here. I have a series of poorly ingested fields without primary keys. And for my own sanity I want to create them. And not just slap an auto incremented field on it with a unique constraint.&lt;/p&gt;\n\n&lt;p&gt;I want to find the fields that would make a primary key unique (and then slap an auto increment on those).  For example, maybe load_num, shipment_status, and arrival_time make a unique key.&lt;/p&gt;\n\n&lt;p&gt;But im finding my phrasing for this process to be incorrect, bc when i google it all I get are links to creating constraints.&lt;/p&gt;\n\n&lt;p&gt;Am I even thinking about this correctly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1abj66s", "is_robot_indexable": true, "report_reasons": null, "author": "tjfrawl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abj66s/creating_new_primary_keys_from_multiple_nonunique/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abj66s/creating_new_primary_keys_from_multiple_nonunique/", "subreddit_subscribers": 156145, "created_utc": 1706278110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hope this is the right place to ask. I'm a Director at a startup and we're in the process of building our first analytics MVP for a customer. We've got a basic pipeline built and some raw data sitting in Postgres (RDS), and now we'd like to visualize. One of our engineers is in the process of building queries for specific metrics.\n\nThere are a ton of dashboard options out there. My first thought was Grafana since there's an AWS service for it and security would be integrated with everything else we're doing.\n\nWhat are some other good options? Prefer something I can at least evaluate fully for free, and doesn't cost a fortune after that. Prefer open source but we're flexible.\n\nRequirements:\n\n* Can produce dashboards from REST API and/or directly from Postgres\n* Reasonably flexible to embed in web applications (React)\n* Secure: Would like to be able to assign/manage users to specific dashboards or cards/metrics", "author_fullname": "t2_do270eaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Dashboard/Visualization options in 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abit3d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706277082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope this is the right place to ask. I&amp;#39;m a Director at a startup and we&amp;#39;re in the process of building our first analytics MVP for a customer. We&amp;#39;ve got a basic pipeline built and some raw data sitting in Postgres (RDS), and now we&amp;#39;d like to visualize. One of our engineers is in the process of building queries for specific metrics.&lt;/p&gt;\n\n&lt;p&gt;There are a ton of dashboard options out there. My first thought was Grafana since there&amp;#39;s an AWS service for it and security would be integrated with everything else we&amp;#39;re doing.&lt;/p&gt;\n\n&lt;p&gt;What are some other good options? Prefer something I can at least evaluate fully for free, and doesn&amp;#39;t cost a fortune after that. Prefer open source but we&amp;#39;re flexible.&lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Can produce dashboards from REST API and/or directly from Postgres&lt;/li&gt;\n&lt;li&gt;Reasonably flexible to embed in web applications (React)&lt;/li&gt;\n&lt;li&gt;Secure: Would like to be able to assign/manage users to specific dashboards or cards/metrics&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1abit3d", "is_robot_indexable": true, "report_reasons": null, "author": "zambizzi", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abit3d/best_dashboardvisualization_options_in_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abit3d/best_dashboardvisualization_options_in_2024/", "subreddit_subscribers": 156145, "created_utc": 1706277082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am the Head of Growth at a Silicon Valley startup and we've pivoted to build a new product and I would love to demo it to as many data engineers or consultants as I can. The tool we are building is an AI chat interface powered by our customers event data. The goal is to goal is to reduce ad-hoc data requests by 80% while also efficiently managing our customers data. We are in the phases of product development so it is not live just yet.\n\nPlease let me know your thoughts and let me know if I can demo it for you.", "author_fullname": "t2_9l5m3ewq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for product feedback on newly developed tool for data teams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abrv0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706300451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am the Head of Growth at a Silicon Valley startup and we&amp;#39;ve pivoted to build a new product and I would love to demo it to as many data engineers or consultants as I can. The tool we are building is an AI chat interface powered by our customers event data. The goal is to goal is to reduce ad-hoc data requests by 80% while also efficiently managing our customers data. We are in the phases of product development so it is not live just yet.&lt;/p&gt;\n\n&lt;p&gt;Please let me know your thoughts and let me know if I can demo it for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1abrv0q", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Rooster_2217", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abrv0q/looking_for_product_feedback_on_newly_developed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abrv0q/looking_for_product_feedback_on_newly_developed/", "subreddit_subscribers": 156145, "created_utc": 1706300451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title states, I am looking for an alternative to Microsoft Access. A system that can intake data as well as store it. A couple that come to mind for me include: Microsoft forms and/SharePoint lists. A bonus would be to be able to auto-populate fields on a form when filling it out. For example, if you entered a serial number, associated information would populate for that asset.", "author_fullname": "t2_9kxnux7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Access alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abpsxs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706295196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states, I am looking for an alternative to Microsoft Access. A system that can intake data as well as store it. A couple that come to mind for me include: Microsoft forms and/SharePoint lists. A bonus would be to be able to auto-populate fields on a form when filling it out. For example, if you entered a serial number, associated information would populate for that asset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1abpsxs", "is_robot_indexable": true, "report_reasons": null, "author": "FuelYourEpic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abpsxs/microsoft_access_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abpsxs/microsoft_access_alternative/", "subreddit_subscribers": 156145, "created_utc": 1706295196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an dataset in ADF, that is connected to an MSSQL database and pointing at a \\`BookReservations\\` table:\n\n&amp;#x200B;\n\n|*id*|*book\\_id*|*booking\\_date*|\n|:-|:-|:-|\n|512|99|2024-01-26T09:18:58.79|\n|1024|8732|2024-01-05T14:44:31.22|\n\nI want to add a column in my Data Flow activity called \\`booking\\_date\\_utc\\`, but I am struggling to figure out how to do it properly. I do not know what timezone is the server is running, and well it could change as part of the configuration.\n\nAfter some reasearch, I was able to find this:\n\n    SELECT GETUTCDATE() AS ServerUtcTime, GETDATE() AS ServerLocalTime;\n\nI added the \\`GetServerTime\\` lookup activity, then saved the time difference in minutes into a pipeline variable \\`offsetForUtcVar\\` with this expression:\n\n    @div(sub(\n        ticks(activity('GetServerDates').output.firstRow.ServerUtcTime), \n        ticks(activity('GetServerDates').output.firstRow.ServerLocalTime)),\n     600000000)\n\nBut I am stuck here. I cannot seem to figure out how to get the UTC time with the existing \\`booking\\_date\\` in the data flow. Can any1 help, please? ", "author_fullname": "t2_3xf6z4ut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting UTC time from an unknown timestamp in ADF pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ablw4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706285415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an dataset in ADF, that is connected to an MSSQL database and pointing at a `BookReservations` table:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;em&gt;id&lt;/em&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;em&gt;book_id&lt;/em&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;em&gt;booking_date&lt;/em&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;512&lt;/td&gt;\n&lt;td align=\"left\"&gt;99&lt;/td&gt;\n&lt;td align=\"left\"&gt;2024-01-26T09:18:58.79&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;8732&lt;/td&gt;\n&lt;td align=\"left\"&gt;2024-01-05T14:44:31.22&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I want to add a column in my Data Flow activity called `booking_date_utc`, but I am struggling to figure out how to do it properly. I do not know what timezone is the server is running, and well it could change as part of the configuration.&lt;/p&gt;\n\n&lt;p&gt;After some reasearch, I was able to find this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT GETUTCDATE() AS ServerUtcTime, GETDATE() AS ServerLocalTime;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I added the `GetServerTime` lookup activity, then saved the time difference in minutes into a pipeline variable `offsetForUtcVar` with this expression:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@div(sub(\n    ticks(activity(&amp;#39;GetServerDates&amp;#39;).output.firstRow.ServerUtcTime), \n    ticks(activity(&amp;#39;GetServerDates&amp;#39;).output.firstRow.ServerLocalTime)),\n 600000000)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But I am stuck here. I cannot seem to figure out how to get the UTC time with the existing `booking_date` in the data flow. Can any1 help, please? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ablw4u", "is_robot_indexable": true, "report_reasons": null, "author": "ealoles", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ablw4u/getting_utc_time_from_an_unknown_timestamp_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ablw4u/getting_utc_time_from_an_unknown_timestamp_in_adf/", "subreddit_subscribers": 156145, "created_utc": 1706285415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_yeda6sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Only Free Course You Need To Become a Professional Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_1abnkaz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.48, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KvlrsRfp2izARIeHHQAeAb8_S6YbAUSTSR7hpOkrK4M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706289506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-professional-data-engineer", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?auto=webp&amp;s=ed5db82b447e5b8c4c1e7d55838159c4d8d351d5", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0571073013296f617c90aea6b65f0b6ae7d7f7b", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=384eb6d1a8550f62ef95c458425c9ea65b4c5966", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4ff718cc262cc615a172ed17bb6c21f6ffdee96", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c92b6f2c5f0779c604b59b19b747f7032a3ccb91", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/AJIYBMabSPMrMrf0p9mFInM61x4lds9nA4YDwREe7c8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f1a39144b9148bfbfc6d83ee481ece5447e46dd", "width": 960, "height": 576}], "variants": {}, "id": "uFz9E-GBJ96KxwgIgkyeCB2gCJ4es-i7N-9NFyGtiVg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1abnkaz", "is_robot_indexable": true, "report_reasons": null, "author": "kingabzpro", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abnkaz/the_only_free_course_you_need_to_become_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-professional-data-engineer", "subreddit_subscribers": 156145, "created_utc": 1706289506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you guys think about this subreddit?   \nAre there some things that you don't like or would like to be different?\n\nHow many of you love programming and consider yourself software engineers specialized in Big Data?", "author_fullname": "t2_3wj092gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unusual question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1abkdrj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706281474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think about this subreddit?&lt;br/&gt;\nAre there some things that you don&amp;#39;t like or would like to be different?&lt;/p&gt;\n\n&lt;p&gt;How many of you love programming and consider yourself software engineers specialized in Big Data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1abkdrj", "is_robot_indexable": true, "report_reasons": null, "author": "yinshangyi", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1abkdrj/unusual_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1abkdrj/unusual_question/", "subreddit_subscribers": 156145, "created_utc": 1706281474.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}