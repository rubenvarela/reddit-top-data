{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a consultant and started to work as a Data Engineer on a startup. A lot of work but I really enjoyed working on it. However, they decided to fire a entire team and put me instead as the developer of that project. When I opened the files, it is just a mess. Spaghetti codes, multiple logics that just gets overwritten and no documentation at all. I discovered that the logic didn't work at all and I have been mostly finding issues with the code.\n\nI asked the business analyst to help me understand all the logic and he doesn't know either. There is no proper documentation of the reqs of the project (the original business analyst was fired for this too), they changed the project owner since the original moved to another company and this new guy doesn't know either what to do.\n\nWell, I suggested to spend time to try to understand the project itself but got called off by everyone since they need this done by February. It just happens that I am in a trial period in this job and it also ends in February. They are expecting me to finish this one but honestly I feel lost since there is no one to contact about the full scope of the project, I am working blinding and doing some \"screaming\" qa.\n\nBy the way, the original stack was Python Pandas, DBT, Airflow, AWS, Postgresql and they were planning to move to Snowflake, and now in this project is just plain SQL for MariaDB. The fricking test to enter the company were hard, I studied a lot of these data engineer tools to enter, even Kubernetes, Docker, Terraform and now I am just a SQL developer. Honestly, I don't mind working with SQL but because the incompetence of other people, they switched me over since they are out of people and they are no plans to hire a new team for this. I just hope that I can recover back my original role.", "author_fullname": "t2_gdhxcn2h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After finally getting my dream job, I was switched to other role against my will.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195tn9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1705168405.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705168197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a consultant and started to work as a Data Engineer on a startup. A lot of work but I really enjoyed working on it. However, they decided to fire a entire team and put me instead as the developer of that project. When I opened the files, it is just a mess. Spaghetti codes, multiple logics that just gets overwritten and no documentation at all. I discovered that the logic didn&amp;#39;t work at all and I have been mostly finding issues with the code.&lt;/p&gt;\n\n&lt;p&gt;I asked the business analyst to help me understand all the logic and he doesn&amp;#39;t know either. There is no proper documentation of the reqs of the project (the original business analyst was fired for this too), they changed the project owner since the original moved to another company and this new guy doesn&amp;#39;t know either what to do.&lt;/p&gt;\n\n&lt;p&gt;Well, I suggested to spend time to try to understand the project itself but got called off by everyone since they need this done by February. It just happens that I am in a trial period in this job and it also ends in February. They are expecting me to finish this one but honestly I feel lost since there is no one to contact about the full scope of the project, I am working blinding and doing some &amp;quot;screaming&amp;quot; qa.&lt;/p&gt;\n\n&lt;p&gt;By the way, the original stack was Python Pandas, DBT, Airflow, AWS, Postgresql and they were planning to move to Snowflake, and now in this project is just plain SQL for MariaDB. The fricking test to enter the company were hard, I studied a lot of these data engineer tools to enter, even Kubernetes, Docker, Terraform and now I am just a SQL developer. Honestly, I don&amp;#39;t mind working with SQL but because the incompetence of other people, they switched me over since they are out of people and they are no plans to hire a new team for this. I just hope that I can recover back my original role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "195tn9u", "is_robot_indexable": true, "report_reasons": null, "author": "DataSenpai", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195tn9u/after_finally_getting_my_dream_job_i_was_switched/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195tn9u/after_finally_getting_my_dream_job_i_was_switched/", "subreddit_subscribers": 152836, "created_utc": 1705168197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i6ulm8ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Guide to Data Lake Interview Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_196erg9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bo78L9h3mP3c8ZqpOxqRXHUuylpR-bghme4LCs1jXZs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705235073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itcertificate.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itcertificate.org/blog/data-lake-interview-questions", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?auto=webp&amp;s=25e6c249242342b7465f7dde367031c7dba962b4", "width": 612, "height": 412}, "resolutions": [{"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=feebdea14eb4a8ff904b50bc6bc91ec4f6b08a05", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31e8f1434a143f7b488df42f359572d314b8b32c", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89bd9dc6a08ccef63465a60a082f473445836cba", "width": 320, "height": 215}], "variants": {}, "id": "Yqass0Jy5N-IMcQo6kIJhHlHiqg68ZoeZSEXF7fnvro"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "196erg9", "is_robot_indexable": true, "report_reasons": null, "author": "Intelligent_Tune_392", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196erg9/a_guide_to_data_lake_interview_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itcertificate.org/blog/data-lake-interview-questions", "subreddit_subscribers": 152836, "created_utc": 1705235073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started my academic course where i would be learning how to perform ETL from multiple postgresql OLTP's to a postgresql OLAP using talend and AWS cloudstack for data engineering where im expected to do a individual project as my finals. My prof has told us that the coursework would be hectic with a lot of in class lab sessions. Would really appreciate any tips or suggestions that can help me through my academic course and to build a professional career as a data engineer.\n\nCurrently I know python as a programming language and intermediate mysql.", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips: Started to learn Data Engineering!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196bfau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705221609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started my academic course where i would be learning how to perform ETL from multiple postgresql OLTP&amp;#39;s to a postgresql OLAP using talend and AWS cloudstack for data engineering where im expected to do a individual project as my finals. My prof has told us that the coursework would be hectic with a lot of in class lab sessions. Would really appreciate any tips or suggestions that can help me through my academic course and to build a professional career as a data engineer.&lt;/p&gt;\n\n&lt;p&gt;Currently I know python as a programming language and intermediate mysql.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "196bfau", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196bfau/tips_started_to_learn_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196bfau/tips_started_to_learn_data_engineering/", "subreddit_subscribers": 152836, "created_utc": 1705221609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've noticed people say on some old Reddit posts that certain companies use Databricks for their Bronze and Silver data layers, and then transfer this data into Snowflake for the Gold layer.\n\nIn such scenarios, as Data Engineers, we often need to reconnect to Snowflake from Databricks to retrieve data\u2014sometimes a significant amount, depending on the table sizes and number of tables. This step is crucial when we have substantial data from sources not integrated into the data warehouse, as it allows us to enrich this data with warehouse data to create specialized datasets for data scientists' ML models.\n\nConsidering the use of both platforms, wouldn't it be more logical to fully establish the data warehouse in Snowflake and only transfer data into Databricks when necessary for creating these specialized, enriched datasets for data science and ML models?\n\nI\u2019m not familiar with the cost implications of these options, but I assume the latter approach might be more practical and efficient, especially for companies whose data warehouse teams have limited proficiency in Python.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Databricks for Data Science/ML and Snowflake for Data Warehousing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195txyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705168968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed people say on some old Reddit posts that certain companies use Databricks for their Bronze and Silver data layers, and then transfer this data into Snowflake for the Gold layer.&lt;/p&gt;\n\n&lt;p&gt;In such scenarios, as Data Engineers, we often need to reconnect to Snowflake from Databricks to retrieve data\u2014sometimes a significant amount, depending on the table sizes and number of tables. This step is crucial when we have substantial data from sources not integrated into the data warehouse, as it allows us to enrich this data with warehouse data to create specialized datasets for data scientists&amp;#39; ML models.&lt;/p&gt;\n\n&lt;p&gt;Considering the use of both platforms, wouldn&amp;#39;t it be more logical to fully establish the data warehouse in Snowflake and only transfer data into Databricks when necessary for creating these specialized, enriched datasets for data science and ML models?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not familiar with the cost implications of these options, but I assume the latter approach might be more practical and efficient, especially for companies whose data warehouse teams have limited proficiency in Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "195txyv", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195txyv/using_databricks_for_data_scienceml_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195txyv/using_databricks_for_data_scienceml_and_snowflake/", "subreddit_subscribers": 152836, "created_utc": 1705168968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am the sole data analyst/engineer for a smallish manufacturing company where I have practically built the whole thing from scratch: ETL from ERP/other sources using SSIS and Python, into a SQL data warehouse, and then into Power BI and SSRS for end users. \n\nI am starting to look at roles outside the company (for both my professional development as well as some red flags popping up at the office) but I am starting to feel that I am a bit of a cul-de-sac with my experience and don't have the hottest tools and software on my resume. \n\nI don't have \"formal\" data architecture/engineering knowledge (if that is a thing) so I have been trying to cobble together my knowledge from articles, youtube videos, etc, and have noticed there were several mistakes I made early on that I would have caught with best practices re: data warehouse design.\n\nFurthermore, I have not learned any of the newer \"cutting edge\" cloud tools as I don't think I can justify the cost and the time it would require to rebuild the whole thing in the cloud, nor do we need a lot of the high performance big data tools, even though I feel it would be personally beneficial for my career to get some experience with them.\n\nCan anyone recommend some books or something that would help me brush up on current best practices and also some suggestions how I might be able to get some experience with some of the newer tools? We are mostly Microsoft so I am taking some Udemy courses that cover the DP203 Azure Data Engineer Cert but if there are cheaper open source versions that I might be able to stick into my process somewhere that would be helpful as well.\n\nThanks for your help!", "author_fullname": "t2_466z52hl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shoring up resume with best practices/gaining cloud experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195trbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705168494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the sole data analyst/engineer for a smallish manufacturing company where I have practically built the whole thing from scratch: ETL from ERP/other sources using SSIS and Python, into a SQL data warehouse, and then into Power BI and SSRS for end users. &lt;/p&gt;\n\n&lt;p&gt;I am starting to look at roles outside the company (for both my professional development as well as some red flags popping up at the office) but I am starting to feel that I am a bit of a cul-de-sac with my experience and don&amp;#39;t have the hottest tools and software on my resume. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have &amp;quot;formal&amp;quot; data architecture/engineering knowledge (if that is a thing) so I have been trying to cobble together my knowledge from articles, youtube videos, etc, and have noticed there were several mistakes I made early on that I would have caught with best practices re: data warehouse design.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, I have not learned any of the newer &amp;quot;cutting edge&amp;quot; cloud tools as I don&amp;#39;t think I can justify the cost and the time it would require to rebuild the whole thing in the cloud, nor do we need a lot of the high performance big data tools, even though I feel it would be personally beneficial for my career to get some experience with them.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend some books or something that would help me brush up on current best practices and also some suggestions how I might be able to get some experience with some of the newer tools? We are mostly Microsoft so I am taking some Udemy courses that cover the DP203 Azure Data Engineer Cert but if there are cheaper open source versions that I might be able to stick into my process somewhere that would be helpful as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "195trbo", "is_robot_indexable": true, "report_reasons": null, "author": "Midnight_Old", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195trbo/shoring_up_resume_with_best_practicesgaining/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195trbo/shoring_up_resume_with_best_practicesgaining/", "subreddit_subscribers": 152836, "created_utc": 1705168494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At work we have dozens of use cases where scientists will track experiments and metadata in Excel files and share them through email or Microsoft Teams messages. I don't think that it will be easy to completely 180 their current processes, and because they still feel comfortable interacting with their data in editable tables, I thought that I could systemically upgrade their workflows to simple Streamlit apps with editable dataframes ([https://blog.streamlit.io/editable-dataframes-are-here/](https://blog.streamlit.io/editable-dataframes-are-here/)); with the hopes of:\n\n1. Validating data at the source\n2. Creating single source of truth for data\n3. Enabling downstream automation and data sharing\n\nSpecifically with #3, a goal would be to facilitate downstream automation and data sharing in the companies data lake. But technically I'm not sure how this could/should be structured? My initial idea was to have the backend data source be an Iceberg table and have the application make inserts, updates, and deletes directly to the Iceberg table. This would in theory update the data lake in real-time and prevent the need for any extra complexity with databases, orchestration, etc.\n\nBut I've never used Iceberg before so I may be making a lot of assumptions that might not actually work... Would this be a valid use case and are there any technical/data issues with this architecture?\n\n&amp;#x200B;", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading/writing directly to Iceberg table from web app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195vi3m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705172994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work we have dozens of use cases where scientists will track experiments and metadata in Excel files and share them through email or Microsoft Teams messages. I don&amp;#39;t think that it will be easy to completely 180 their current processes, and because they still feel comfortable interacting with their data in editable tables, I thought that I could systemically upgrade their workflows to simple Streamlit apps with editable dataframes (&lt;a href=\"https://blog.streamlit.io/editable-dataframes-are-here/\"&gt;https://blog.streamlit.io/editable-dataframes-are-here/&lt;/a&gt;); with the hopes of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Validating data at the source&lt;/li&gt;\n&lt;li&gt;Creating single source of truth for data&lt;/li&gt;\n&lt;li&gt;Enabling downstream automation and data sharing&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Specifically with #3, a goal would be to facilitate downstream automation and data sharing in the companies data lake. But technically I&amp;#39;m not sure how this could/should be structured? My initial idea was to have the backend data source be an Iceberg table and have the application make inserts, updates, and deletes directly to the Iceberg table. This would in theory update the data lake in real-time and prevent the need for any extra complexity with databases, orchestration, etc.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;ve never used Iceberg before so I may be making a lot of assumptions that might not actually work... Would this be a valid use case and are there any technical/data issues with this architecture?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "195vi3m", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195vi3m/readingwriting_directly_to_iceberg_table_from_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195vi3m/readingwriting_directly_to_iceberg_table_from_web/", "subreddit_subscribers": 152836, "created_utc": 1705172994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi to all Seniors and experienced one in the industry. \nI have a question how does you face a business problem and convert it into technical design. Like for various software Engineering post they follow norms of system design like scalability, load balancing, reliability. \nWhat's the norm or basic protocol followed while designing a data warehouse. \n\nI m ETL developer, fresh into industry. And I want to make it norm, to approach a problem with big picture in subconscious mind. Currently I had to faced a lot of rework due to modification in DWH architecture. But are various approach you follow. Do you have any great blogs!? Right now I m trying to learn from \"Design data intensive applications\" But its more like solving software issues.\n\nLike what common do you face and how you solved it. I'm great into blogs, books more than video. video is very tiring for me. Also I am looking forward enter real time processing market. So from that perspective I want to aware of scenarios normally faced in those situations\n\nAny suggestions will be appreciated by heart.", "author_fullname": "t2_1ubfs6x4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does a Data Warehouse Architect approach a business problem and convert it into technical design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_196hliq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705244070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi to all Seniors and experienced one in the industry. \nI have a question how does you face a business problem and convert it into technical design. Like for various software Engineering post they follow norms of system design like scalability, load balancing, reliability. \nWhat&amp;#39;s the norm or basic protocol followed while designing a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;I m ETL developer, fresh into industry. And I want to make it norm, to approach a problem with big picture in subconscious mind. Currently I had to faced a lot of rework due to modification in DWH architecture. But are various approach you follow. Do you have any great blogs!? Right now I m trying to learn from &amp;quot;Design data intensive applications&amp;quot; But its more like solving software issues.&lt;/p&gt;\n\n&lt;p&gt;Like what common do you face and how you solved it. I&amp;#39;m great into blogs, books more than video. video is very tiring for me. Also I am looking forward enter real time processing market. So from that perspective I want to aware of scenarios normally faced in those situations&lt;/p&gt;\n\n&lt;p&gt;Any suggestions will be appreciated by heart.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196hliq", "is_robot_indexable": true, "report_reasons": null, "author": "asud_w_asud", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196hliq/how_does_a_data_warehouse_architect_approach_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196hliq/how_does_a_data_warehouse_architect_approach_a/", "subreddit_subscribers": 152836, "created_utc": 1705244070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. \n\nAside my data science course work, I have enrolled in DataCamp's data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning \n- SQL Joins\n- Introduction to Relational Databases \n- Database Design\n- Intermediate Python \n\nI have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.", "author_fullname": "t2_v01madky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In dire need of guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1968ntq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705211078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. &lt;/p&gt;\n\n&lt;p&gt;Aside my data science course work, I have enrolled in DataCamp&amp;#39;s data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning \n- SQL Joins\n- Introduction to Relational Databases \n- Database Design\n- Intermediate Python &lt;/p&gt;\n\n&lt;p&gt;I have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1968ntq", "is_robot_indexable": true, "report_reasons": null, "author": "ETKojo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1968ntq/in_dire_need_of_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1968ntq/in_dire_need_of_guidance/", "subreddit_subscribers": 152836, "created_utc": 1705211078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have upgraded Databricks Community Edition to Databricks' free trial on AWS free tier. Since NAT Gateway was not part of the free tier and I am being charged, I deleted it. This results in me not being able to create clusters.\n\nI've seen suggestions about replacing it with VPC endpoint, but I have zero knowledge about networking so I don't know how to do it myself and I can't make sense of the tutorials I see on the internet how to customise it for Databricks use. Does anyone know any resource that I can follow so I can setup the needed VPC endpoints and for Databricks to work?\n\nOr if there are better options than VPC endpoints which a compete networking noob can easily follow, that would be great too.", "author_fullname": "t2_tomg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to replace AWS NAT Gateway being used by Databricks with VPC endpoints?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196beob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705221537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have upgraded Databricks Community Edition to Databricks&amp;#39; free trial on AWS free tier. Since NAT Gateway was not part of the free tier and I am being charged, I deleted it. This results in me not being able to create clusters.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen suggestions about replacing it with VPC endpoint, but I have zero knowledge about networking so I don&amp;#39;t know how to do it myself and I can&amp;#39;t make sense of the tutorials I see on the internet how to customise it for Databricks use. Does anyone know any resource that I can follow so I can setup the needed VPC endpoints and for Databricks to work?&lt;/p&gt;\n\n&lt;p&gt;Or if there are better options than VPC endpoints which a compete networking noob can easily follow, that would be great too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196beob", "is_robot_indexable": true, "report_reasons": null, "author": "AxenZh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196beob/how_to_replace_aws_nat_gateway_being_used_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196beob/how_to_replace_aws_nat_gateway_being_used_by/", "subreddit_subscribers": 152836, "created_utc": 1705221537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have anyone tried to automate the workflow deployment process using Jenkins. I want to migrate a workflow from QA to production environment, provided I should be able to modify the workflow variables.\nIs there a way to do this?", "author_fullname": "t2_7iy7e20h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collibra workflow deployment via Jenkins", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195vooq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705173459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have anyone tried to automate the workflow deployment process using Jenkins. I want to migrate a workflow from QA to production environment, provided I should be able to modify the workflow variables.\nIs there a way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "195vooq", "is_robot_indexable": true, "report_reasons": null, "author": "rags1230", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195vooq/collibra_workflow_deployment_via_jenkins/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195vooq/collibra_workflow_deployment_via_jenkins/", "subreddit_subscribers": 152836, "created_utc": 1705173459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm familar with reasources for beginning in Python, but am wondering if there are any specific to Synapse Serverless PySpark.\n\nGenerally it works best for me, to start with things I can immediately apply.  Not always, but with Python being a general purpose language, I'm thinking it might be best to try starting with that.", "author_fullname": "t2_3bc49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1 Python Beginner Resources, that focus on Synapse Serverless PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195t3cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705166748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m familar with reasources for beginning in Python, but am wondering if there are any specific to Synapse Serverless PySpark.&lt;/p&gt;\n\n&lt;p&gt;Generally it works best for me, to start with things I can immediately apply.  Not always, but with Python being a general purpose language, I&amp;#39;m thinking it might be best to try starting with that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "195t3cn", "is_robot_indexable": true, "report_reasons": null, "author": "cdigioia", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195t3cn/1_python_beginner_resources_that_focus_on_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195t3cn/1_python_beginner_resources_that_focus_on_synapse/", "subreddit_subscribers": 152836, "created_utc": 1705166748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kedro is often overlooked in Data Science projects despite offering structure, caching and tracking datasets, MLOps features as well as powerfull intergrations with other Data tools", "author_fullname": "t2_vbapbjo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kedro Intro and Hello World example", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_196gjd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Kedro Intro and Hello World example", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/r-oQ701wgDQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/196gjd0", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lEaH4fdOaFUim4MSa1dMXoEXZnisdrqh46i-mO9dpwE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705240892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kedro is often overlooked in Data Science projects despite offering structure, caching and tracking datasets, MLOps features as well as powerfull intergrations with other Data tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/r-oQ701wgDQ?si=roh4zqGCb7pithUY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?auto=webp&amp;s=c8a24bf13665b37f042154cdde85fb28c250c98d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fdd1cc5a2ddc5ae95e23b021f4eb4cf621f962b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=711d0a99e36cba6734cf4ab86a80970eb3e789d2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=29f32d1da2077405987e79acb96813efac6719d2", "width": 320, "height": 240}], "variants": {}, "id": "v_XpU9SkWBZnhYuzYwDmOvhQLE6L9_ed2E4WBP_NL1Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "196gjd0", "is_robot_indexable": true, "report_reasons": null, "author": "dnulcon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196gjd0/kedro_intro_and_hello_world_example/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/r-oQ701wgDQ?si=roh4zqGCb7pithUY", "subreddit_subscribers": 152836, "created_utc": 1705240892.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Kedro Intro and Hello World example", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/r-oQ701wgDQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company requires reporting on information within salesforce.\n\nThey want something like # open leads, # closed leads and # converted leads on a monthly basis. My issue is that often this data is not immutable: a lead (or an opportunity for example) might be closed, but then later on reopens since the lead/opp is now interested in the product.  \n\n\nThis would generally change data retrospectively.  \nIs the only way to do a complex query using lead and opportunity history tables and creating an events based data model?", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Salesforce Reporting (in DBT)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196funh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705238784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company requires reporting on information within salesforce.&lt;/p&gt;\n\n&lt;p&gt;They want something like # open leads, # closed leads and # converted leads on a monthly basis. My issue is that often this data is not immutable: a lead (or an opportunity for example) might be closed, but then later on reopens since the lead/opp is now interested in the product.  &lt;/p&gt;\n\n&lt;p&gt;This would generally change data retrospectively.&lt;br/&gt;\nIs the only way to do a complex query using lead and opportunity history tables and creating an events based data model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196funh", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196funh/managing_salesforce_reporting_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196funh/managing_salesforce_reporting_in_dbt/", "subreddit_subscribers": 152836, "created_utc": 1705238784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm following a Microsoft Tutorial on MS Fabric. I'm working on an ETL process using Azure Blob Storage and Apache Spark, and I've come across a step in the process that I'm trying to understand better. The workflow involves reading a large dataset from Azure Blob Storage, processing it, and then writing the first 1000 rows of this dataset back to Blob Storage. After this, the script reads this subset again, performs further transformations, and finally loads it into a Delta table.\n\nMy question is: What could be the rationale for writing a subset of the data (1000 rows) back to Azure Blob Storage, only to read it again for further processing and loading into a Delta table? Are there specific benefits or use cases for this approach, such as performance optimization, data segmentation, or testing purposes that I might be overlooking?. Here is the link to the tutorial:  \n\n\n[https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html)\n\nAny insights or experiences you can share regarding this method would be greatly appreciated, especially if there are more direct or efficient ways to handle such a workflow.\n\nThanks in advance.", "author_fullname": "t2_7r901d2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding the Rationale Behind Writing Subset of Data Back to Azure Blob in ETL Process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196eln8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705234509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m following a Microsoft Tutorial on MS Fabric. I&amp;#39;m working on an ETL process using Azure Blob Storage and Apache Spark, and I&amp;#39;ve come across a step in the process that I&amp;#39;m trying to understand better. The workflow involves reading a large dataset from Azure Blob Storage, processing it, and then writing the first 1000 rows of this dataset back to Blob Storage. After this, the script reads this subset again, performs further transformations, and finally loads it into a Delta table.&lt;/p&gt;\n\n&lt;p&gt;My question is: What could be the rationale for writing a subset of the data (1000 rows) back to Azure Blob Storage, only to read it again for further processing and loading into a Delta table? Are there specific benefits or use cases for this approach, such as performance optimization, data segmentation, or testing purposes that I might be overlooking?. Here is the link to the tutorial:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html\"&gt;https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any insights or experiences you can share regarding this method would be greatly appreciated, especially if there are more direct or efficient ways to handle such a workflow.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "196eln8", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Necessary-6455", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196eln8/understanding_the_rationale_behind_writing_subset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196eln8/understanding_the_rationale_behind_writing_subset/", "subreddit_subscribers": 152836, "created_utc": 1705234509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow redditors!\n\nSo I\u2019m beginning to get interested in building streaming pipelines. Things like a near real-time data ingestion (a bronze table, for instance), silver tables or even building near real-time specialized datasets (gold datasets).\n\nI know from some researches that in service oriented architectures companies use apache kafka as a integration layer/hub for consumers and producers.\n\nAs far as I\u2019m aware a producer generates some event and sends it for the kafka broker and the consumer can process it right after, and depending on the architecture the consumer even deletes (?) the event that already has been processed to avoid duplication.\n\nIf I\u2019m a data engineer interested in the data that this producer pushed to kafka, how would I reliably get access to this data, without intruding into the integration between the microservices? \n\nI\u2019m looking into kafka connect as something that tries to solve this issue, but I\u2019m quite unsure.\n\nCan someone with more experience in these streaming context bring some light to the subject?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Pipelines Architecture with Kafka and Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195z6b2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705182606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow redditors!&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m beginning to get interested in building streaming pipelines. Things like a near real-time data ingestion (a bronze table, for instance), silver tables or even building near real-time specialized datasets (gold datasets).&lt;/p&gt;\n\n&lt;p&gt;I know from some researches that in service oriented architectures companies use apache kafka as a integration layer/hub for consumers and producers.&lt;/p&gt;\n\n&lt;p&gt;As far as I\u2019m aware a producer generates some event and sends it for the kafka broker and the consumer can process it right after, and depending on the architecture the consumer even deletes (?) the event that already has been processed to avoid duplication.&lt;/p&gt;\n\n&lt;p&gt;If I\u2019m a data engineer interested in the data that this producer pushed to kafka, how would I reliably get access to this data, without intruding into the integration between the microservices? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking into kafka connect as something that tries to solve this issue, but I\u2019m quite unsure.&lt;/p&gt;\n\n&lt;p&gt;Can someone with more experience in these streaming context bring some light to the subject?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "195z6b2", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195z6b2/streaming_pipelines_architecture_with_kafka_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195z6b2/streaming_pipelines_architecture_with_kafka_and/", "subreddit_subscribers": 152836, "created_utc": 1705182606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I recently started a new job that is not related to data engineering, but there are a lot of things where I can apply ETL things to automate my tasks. I don't have experience as a data engineer but is my goal to switch careers and get a data engineering job eventually.\n\nAnyways, my problem is as follows: Everyday I get a new CSV that is the same table as the day before but with new info. Every day I need to rearrange that table and 1- create a new CSV file with the daily info and 2-  \"append\" it into the monthly cleaned table. \n\nThose things that take a lot of time manually I can use python to automate the task, and I am in the process of achieving it. But the thing is that I get to a point in which I don't know if I should use stuff like SQLalchemy, psycopg2 or if I should just stick to pandas. I want to understand the thought process of when I should use what. Let me develop. \n\nFor example, once I get the CSV as a DF, I filter it with pandas. But then, I need to modify the values of column F according to the values of column G. In column G I have description and each description corresponds to a group. In column F I need to specify the group. I know I can keep using pandas to also achieve this, but at the same time, this is a clear and plain simple JOIN function. \n\nI am reluctant to create a, let's say, PostgreSQL Database because I want to eventually be able to share my python scripts as an exe and someone like my teammate can simply execute it and don't need another program like PostgreSQL to successfully run the file. But maybe I should just do it. I don't really know. \n\nAlso I don't understand why should I use sql at all if pandas can cover for the things that I want to achieve. \n\nThanks in advance for your advice and if you can point me into maybe some useful learning books I would really appreciate it.", "author_fullname": "t2_9yaiy40i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie here trying to learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_196hvq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705244826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I recently started a new job that is not related to data engineering, but there are a lot of things where I can apply ETL things to automate my tasks. I don&amp;#39;t have experience as a data engineer but is my goal to switch careers and get a data engineering job eventually.&lt;/p&gt;\n\n&lt;p&gt;Anyways, my problem is as follows: Everyday I get a new CSV that is the same table as the day before but with new info. Every day I need to rearrange that table and 1- create a new CSV file with the daily info and 2-  &amp;quot;append&amp;quot; it into the monthly cleaned table. &lt;/p&gt;\n\n&lt;p&gt;Those things that take a lot of time manually I can use python to automate the task, and I am in the process of achieving it. But the thing is that I get to a point in which I don&amp;#39;t know if I should use stuff like SQLalchemy, psycopg2 or if I should just stick to pandas. I want to understand the thought process of when I should use what. Let me develop. &lt;/p&gt;\n\n&lt;p&gt;For example, once I get the CSV as a DF, I filter it with pandas. But then, I need to modify the values of column F according to the values of column G. In column G I have description and each description corresponds to a group. In column F I need to specify the group. I know I can keep using pandas to also achieve this, but at the same time, this is a clear and plain simple JOIN function. &lt;/p&gt;\n\n&lt;p&gt;I am reluctant to create a, let&amp;#39;s say, PostgreSQL Database because I want to eventually be able to share my python scripts as an exe and someone like my teammate can simply execute it and don&amp;#39;t need another program like PostgreSQL to successfully run the file. But maybe I should just do it. I don&amp;#39;t really know. &lt;/p&gt;\n\n&lt;p&gt;Also I don&amp;#39;t understand why should I use sql at all if pandas can cover for the things that I want to achieve. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your advice and if you can point me into maybe some useful learning books I would really appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196hvq8", "is_robot_indexable": true, "report_reasons": null, "author": "GFM41", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196hvq8/newbie_here_trying_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196hvq8/newbie_here_trying_to_learn/", "subreddit_subscribers": 152836, "created_utc": 1705244826.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}