{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i6ulm8ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Guide to Data Lake Interview Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_196erg9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bo78L9h3mP3c8ZqpOxqRXHUuylpR-bghme4LCs1jXZs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705235073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itcertificate.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itcertificate.org/blog/data-lake-interview-questions", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?auto=webp&amp;s=25e6c249242342b7465f7dde367031c7dba962b4", "width": 612, "height": 412}, "resolutions": [{"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=feebdea14eb4a8ff904b50bc6bc91ec4f6b08a05", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31e8f1434a143f7b488df42f359572d314b8b32c", "width": 216, "height": 145}, {"url": "https://external-preview.redd.it/cHkwD7ukAvg_vQGvsjs8wjDt5_sEjsIl-0ndpPgdz60.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89bd9dc6a08ccef63465a60a082f473445836cba", "width": 320, "height": 215}], "variants": {}, "id": "Yqass0Jy5N-IMcQo6kIJhHlHiqg68ZoeZSEXF7fnvro"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "196erg9", "is_robot_indexable": true, "report_reasons": null, "author": "Intelligent_Tune_392", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196erg9/a_guide_to_data_lake_interview_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itcertificate.org/blog/data-lake-interview-questions", "subreddit_subscribers": 152852, "created_utc": 1705235073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started my academic course where i would be learning how to perform ETL from multiple postgresql OLTP's to a postgresql OLAP using talend and AWS cloudstack for data engineering where im expected to do a individual project as my finals. My prof has told us that the coursework would be hectic with a lot of in class lab sessions. Would really appreciate any tips or suggestions that can help me through my academic course and to build a professional career as a data engineer.\n\nCurrently I know python as a programming language and intermediate mysql.", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips: Started to learn Data Engineering!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196bfau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705221609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started my academic course where i would be learning how to perform ETL from multiple postgresql OLTP&amp;#39;s to a postgresql OLAP using talend and AWS cloudstack for data engineering where im expected to do a individual project as my finals. My prof has told us that the coursework would be hectic with a lot of in class lab sessions. Would really appreciate any tips or suggestions that can help me through my academic course and to build a professional career as a data engineer.&lt;/p&gt;\n\n&lt;p&gt;Currently I know python as a programming language and intermediate mysql.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "196bfau", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196bfau/tips_started_to_learn_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196bfau/tips_started_to_learn_data_engineering/", "subreddit_subscribers": 152852, "created_utc": 1705221609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi to all Seniors and experienced one in the industry. \nI have a question how does you face a business problem and convert it into technical design. Like for various software Engineering post they follow norms of system design like scalability, load balancing, reliability. \nWhat's the norm or basic protocol followed while designing a data warehouse. \n\nI m ETL developer, fresh into industry. And I want to make it norm, to approach a problem with big picture in subconscious mind. Currently I had to faced a lot of rework due to modification in DWH architecture. But are various approach you follow. Do you have any great blogs!? Right now I m trying to learn from \"Design data intensive applications\" But its more like solving software issues.\n\nLike what common do you face and how you solved it. I'm great into blogs, books more than video. video is very tiring for me. Also I am looking forward enter real time processing market. So from that perspective I want to aware of scenarios normally faced in those situations\n\nAny suggestions will be appreciated by heart.", "author_fullname": "t2_1ubfs6x4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does a Data Warehouse Architect approach a business problem and convert it into technical design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196hliq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705244070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi to all Seniors and experienced one in the industry. \nI have a question how does you face a business problem and convert it into technical design. Like for various software Engineering post they follow norms of system design like scalability, load balancing, reliability. \nWhat&amp;#39;s the norm or basic protocol followed while designing a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;I m ETL developer, fresh into industry. And I want to make it norm, to approach a problem with big picture in subconscious mind. Currently I had to faced a lot of rework due to modification in DWH architecture. But are various approach you follow. Do you have any great blogs!? Right now I m trying to learn from &amp;quot;Design data intensive applications&amp;quot; But its more like solving software issues.&lt;/p&gt;\n\n&lt;p&gt;Like what common do you face and how you solved it. I&amp;#39;m great into blogs, books more than video. video is very tiring for me. Also I am looking forward enter real time processing market. So from that perspective I want to aware of scenarios normally faced in those situations&lt;/p&gt;\n\n&lt;p&gt;Any suggestions will be appreciated by heart.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196hliq", "is_robot_indexable": true, "report_reasons": null, "author": "asud_w_asud", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196hliq/how_does_a_data_warehouse_architect_approach_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196hliq/how_does_a_data_warehouse_architect_approach_a/", "subreddit_subscribers": 152852, "created_utc": 1705244070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At work we have dozens of use cases where scientists will track experiments and metadata in Excel files and share them through email or Microsoft Teams messages. I don't think that it will be easy to completely 180 their current processes, and because they still feel comfortable interacting with their data in editable tables, I thought that I could systemically upgrade their workflows to simple Streamlit apps with editable dataframes ([https://blog.streamlit.io/editable-dataframes-are-here/](https://blog.streamlit.io/editable-dataframes-are-here/)); with the hopes of:\n\n1. Validating data at the source\n2. Creating single source of truth for data\n3. Enabling downstream automation and data sharing\n\nSpecifically with #3, a goal would be to facilitate downstream automation and data sharing in the companies data lake. But technically I'm not sure how this could/should be structured? My initial idea was to have the backend data source be an Iceberg table and have the application make inserts, updates, and deletes directly to the Iceberg table. This would in theory update the data lake in real-time and prevent the need for any extra complexity with databases, orchestration, etc.\n\nBut I've never used Iceberg before so I may be making a lot of assumptions that might not actually work... Would this be a valid use case and are there any technical/data issues with this architecture?\n\n&amp;#x200B;", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading/writing directly to Iceberg table from web app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195vi3m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705172994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work we have dozens of use cases where scientists will track experiments and metadata in Excel files and share them through email or Microsoft Teams messages. I don&amp;#39;t think that it will be easy to completely 180 their current processes, and because they still feel comfortable interacting with their data in editable tables, I thought that I could systemically upgrade their workflows to simple Streamlit apps with editable dataframes (&lt;a href=\"https://blog.streamlit.io/editable-dataframes-are-here/\"&gt;https://blog.streamlit.io/editable-dataframes-are-here/&lt;/a&gt;); with the hopes of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Validating data at the source&lt;/li&gt;\n&lt;li&gt;Creating single source of truth for data&lt;/li&gt;\n&lt;li&gt;Enabling downstream automation and data sharing&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Specifically with #3, a goal would be to facilitate downstream automation and data sharing in the companies data lake. But technically I&amp;#39;m not sure how this could/should be structured? My initial idea was to have the backend data source be an Iceberg table and have the application make inserts, updates, and deletes directly to the Iceberg table. This would in theory update the data lake in real-time and prevent the need for any extra complexity with databases, orchestration, etc.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;ve never used Iceberg before so I may be making a lot of assumptions that might not actually work... Would this be a valid use case and are there any technical/data issues with this architecture?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "195vi3m", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195vi3m/readingwriting_directly_to_iceberg_table_from_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195vi3m/readingwriting_directly_to_iceberg_table_from_web/", "subreddit_subscribers": 152852, "created_utc": 1705172994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. \n\nAside my data science course work, I have enrolled in DataCamp's data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning \n- SQL Joins\n- Introduction to Relational Databases \n- Database Design\n- Intermediate Python \n\nI have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.", "author_fullname": "t2_v01madky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In dire need of guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1968ntq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705211078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. &lt;/p&gt;\n\n&lt;p&gt;Aside my data science course work, I have enrolled in DataCamp&amp;#39;s data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning \n- SQL Joins\n- Introduction to Relational Databases \n- Database Design\n- Intermediate Python &lt;/p&gt;\n\n&lt;p&gt;I have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1968ntq", "is_robot_indexable": true, "report_reasons": null, "author": "ETKojo", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1968ntq/in_dire_need_of_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1968ntq/in_dire_need_of_guidance/", "subreddit_subscribers": 152852, "created_utc": 1705211078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company requires reporting on information within salesforce.\n\nThey want something like # open leads, # closed leads and # converted leads on a monthly basis. My issue is that often this data is not immutable: a lead (or an opportunity for example) might be closed, but then later on reopens since the lead/opp is now interested in the product.  \n\n\nThis would generally change data retrospectively.  \nIs the only way to do a complex query using lead and opportunity history tables and creating an events based data model?", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Salesforce Reporting (in DBT)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196funh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705238784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company requires reporting on information within salesforce.&lt;/p&gt;\n\n&lt;p&gt;They want something like # open leads, # closed leads and # converted leads on a monthly basis. My issue is that often this data is not immutable: a lead (or an opportunity for example) might be closed, but then later on reopens since the lead/opp is now interested in the product.  &lt;/p&gt;\n\n&lt;p&gt;This would generally change data retrospectively.&lt;br/&gt;\nIs the only way to do a complex query using lead and opportunity history tables and creating an events based data model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196funh", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196funh/managing_salesforce_reporting_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196funh/managing_salesforce_reporting_in_dbt/", "subreddit_subscribers": 152852, "created_utc": 1705238784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have upgraded Databricks Community Edition to Databricks' free trial on AWS free tier. Since NAT Gateway was not part of the free tier and I am being charged, I deleted it. This results in me not being able to create clusters.\n\nI've seen suggestions about replacing it with VPC endpoint, but I have zero knowledge about networking so I don't know how to do it myself and I can't make sense of the tutorials I see on the internet how to customise it for Databricks use. Does anyone know any resource that I can follow so I can setup the needed VPC endpoints and for Databricks to work?\n\nOr if there are better options than VPC endpoints which a compete networking noob can easily follow, that would be great too.", "author_fullname": "t2_tomg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to replace AWS NAT Gateway being used by Databricks with VPC endpoints?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196beob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705221537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have upgraded Databricks Community Edition to Databricks&amp;#39; free trial on AWS free tier. Since NAT Gateway was not part of the free tier and I am being charged, I deleted it. This results in me not being able to create clusters.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen suggestions about replacing it with VPC endpoint, but I have zero knowledge about networking so I don&amp;#39;t know how to do it myself and I can&amp;#39;t make sense of the tutorials I see on the internet how to customise it for Databricks use. Does anyone know any resource that I can follow so I can setup the needed VPC endpoints and for Databricks to work?&lt;/p&gt;\n\n&lt;p&gt;Or if there are better options than VPC endpoints which a compete networking noob can easily follow, that would be great too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196beob", "is_robot_indexable": true, "report_reasons": null, "author": "AxenZh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196beob/how_to_replace_aws_nat_gateway_being_used_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196beob/how_to_replace_aws_nat_gateway_being_used_by/", "subreddit_subscribers": 152852, "created_utc": 1705221537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have anyone tried to automate the workflow deployment process using Jenkins. I want to migrate a workflow from QA to production environment, provided I should be able to modify the workflow variables.\nIs there a way to do this?", "author_fullname": "t2_7iy7e20h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collibra workflow deployment via Jenkins", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195vooq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705173459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have anyone tried to automate the workflow deployment process using Jenkins. I want to migrate a workflow from QA to production environment, provided I should be able to modify the workflow variables.\nIs there a way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "195vooq", "is_robot_indexable": true, "report_reasons": null, "author": "rags1230", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195vooq/collibra_workflow_deployment_via_jenkins/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195vooq/collibra_workflow_deployment_via_jenkins/", "subreddit_subscribers": 152852, "created_utc": 1705173459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "By way of introduction - I\u2019m primarily an econometrician by trade and focus on building smaller scale econometric/ML models for clients. I typically work with a range of datasets, but importantly it\u2019s collectively small enough to fit in to local memory and work with using R or Python.\n\nAs the lines between data science and econometrics continue to blur, I find myself working more and more with much larger datasets. Too large to fit in to local memory.\n\nAs an organisation we use Snowflake to store data that we don\u2019t want on our network drives (100gb+). As of now, I use Python's Snowpark library to aggregate large datasets on snowflake in to a level where I can transfer locally and then deploy an ML model locally. I'm also comfortable writing the SQL directly. \n\nHowever using cloud or online data platforms hasn't really been my bread and butter and wondered if anybody could offer any advice around the following:\n\n1. I'm unsure if its best practice to aggregate on the cloud and then analyse locally. Supposing its not - is it possible to deploy a ML model within a snowflake environment instead? In other words \u2013 all data wrangling and model execution is done on the snowflake engine.\n2. I hear a lot about databricks as an alternative to snowflake, but struggling to understand what it offers, and what it can do that\u2019s different? Could anybody help to explain this to me like you would to a small dog?\n\nThanks a lot! ", "author_fullname": "t2_uqor47tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some guidance around online data platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_196lma9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705254663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By way of introduction - I\u2019m primarily an econometrician by trade and focus on building smaller scale econometric/ML models for clients. I typically work with a range of datasets, but importantly it\u2019s collectively small enough to fit in to local memory and work with using R or Python.&lt;/p&gt;\n\n&lt;p&gt;As the lines between data science and econometrics continue to blur, I find myself working more and more with much larger datasets. Too large to fit in to local memory.&lt;/p&gt;\n\n&lt;p&gt;As an organisation we use Snowflake to store data that we don\u2019t want on our network drives (100gb+). As of now, I use Python&amp;#39;s Snowpark library to aggregate large datasets on snowflake in to a level where I can transfer locally and then deploy an ML model locally. I&amp;#39;m also comfortable writing the SQL directly. &lt;/p&gt;\n\n&lt;p&gt;However using cloud or online data platforms hasn&amp;#39;t really been my bread and butter and wondered if anybody could offer any advice around the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I&amp;#39;m unsure if its best practice to aggregate on the cloud and then analyse locally. Supposing its not - is it possible to deploy a ML model within a snowflake environment instead? In other words \u2013 all data wrangling and model execution is done on the snowflake engine.&lt;/li&gt;\n&lt;li&gt;I hear a lot about databricks as an alternative to snowflake, but struggling to understand what it offers, and what it can do that\u2019s different? Could anybody help to explain this to me like you would to a small dog?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks a lot! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196lma9", "is_robot_indexable": true, "report_reasons": null, "author": "LDM-88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196lma9/looking_for_some_guidance_around_online_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196lma9/looking_for_some_guidance_around_online_data/", "subreddit_subscribers": 152852, "created_utc": 1705254663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Posting here as I'm not big into the DE world, but figure yall probably know where noobs like me could find data sets to brush up with.  \n\nDoes anyone know if there are any downloadable data sets for downloads of open source packages? NPM / Maven / Composer / etc \u2013 any statistics like that would be helpful!\n\n&amp;#x200B;\n\nP.s. I know I could try harvesting from APIs, but there's a LOT of stats to download, and I can't help but imagine someone's done this.", "author_fullname": "t2_n1kbf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data sets for Open Source package downloads (Maven / NPM / etc)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_196kexn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705251589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Posting here as I&amp;#39;m not big into the DE world, but figure yall probably know where noobs like me could find data sets to brush up with.  &lt;/p&gt;\n\n&lt;p&gt;Does anyone know if there are any downloadable data sets for downloads of open source packages? NPM / Maven / Composer / etc \u2013 any statistics like that would be helpful!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.s. I know I could try harvesting from APIs, but there&amp;#39;s a LOT of stats to download, and I can&amp;#39;t help but imagine someone&amp;#39;s done this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "196kexn", "is_robot_indexable": true, "report_reasons": null, "author": "dwelch2344", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196kexn/data_sets_for_open_source_package_downloads_maven/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196kexn/data_sets_for_open_source_package_downloads_maven/", "subreddit_subscribers": 152852, "created_utc": 1705251589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I recently started a new job that is not related to data engineering, but there are a lot of things where I can apply ETL things to automate my tasks. I don't have experience as a data engineer but is my goal to switch careers and get a data engineering job eventually.\n\nAnyways, my problem is as follows: Everyday I get a new CSV that is the same table as the day before but with new info. Every day I need to rearrange that table and 1- create a new CSV file with the daily info and 2-  \"append\" it into the monthly cleaned table. \n\nThose things that take a lot of time manually I can use python to automate the task, and I am in the process of achieving it. But the thing is that I get to a point in which I don't know if I should use stuff like SQLalchemy, psycopg2 or if I should just stick to pandas. I want to understand the thought process of when I should use what. Let me develop. \n\nFor example, once I get the CSV as a DF, I filter it with pandas. But then, I need to modify the values of column F according to the values of column G. In column G I have description and each description corresponds to a group. In column F I need to specify the group. I know I can keep using pandas to also achieve this, but at the same time, this is a clear and plain simple JOIN function. \n\nI am reluctant to create a, let's say, PostgreSQL Database because I want to eventually be able to share my python scripts as an exe and someone like my teammate can simply execute it and don't need another program like PostgreSQL to successfully run the file. But maybe I should just do it. I don't really know. \n\nAlso I don't understand why should I use sql at all if pandas can cover for the things that I want to achieve. \n\nThanks in advance for your advice and if you can point me into maybe some useful learning books I would really appreciate it.", "author_fullname": "t2_9yaiy40i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie here trying to learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196hvq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705244826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I recently started a new job that is not related to data engineering, but there are a lot of things where I can apply ETL things to automate my tasks. I don&amp;#39;t have experience as a data engineer but is my goal to switch careers and get a data engineering job eventually.&lt;/p&gt;\n\n&lt;p&gt;Anyways, my problem is as follows: Everyday I get a new CSV that is the same table as the day before but with new info. Every day I need to rearrange that table and 1- create a new CSV file with the daily info and 2-  &amp;quot;append&amp;quot; it into the monthly cleaned table. &lt;/p&gt;\n\n&lt;p&gt;Those things that take a lot of time manually I can use python to automate the task, and I am in the process of achieving it. But the thing is that I get to a point in which I don&amp;#39;t know if I should use stuff like SQLalchemy, psycopg2 or if I should just stick to pandas. I want to understand the thought process of when I should use what. Let me develop. &lt;/p&gt;\n\n&lt;p&gt;For example, once I get the CSV as a DF, I filter it with pandas. But then, I need to modify the values of column F according to the values of column G. In column G I have description and each description corresponds to a group. In column F I need to specify the group. I know I can keep using pandas to also achieve this, but at the same time, this is a clear and plain simple JOIN function. &lt;/p&gt;\n\n&lt;p&gt;I am reluctant to create a, let&amp;#39;s say, PostgreSQL Database because I want to eventually be able to share my python scripts as an exe and someone like my teammate can simply execute it and don&amp;#39;t need another program like PostgreSQL to successfully run the file. But maybe I should just do it. I don&amp;#39;t really know. &lt;/p&gt;\n\n&lt;p&gt;Also I don&amp;#39;t understand why should I use sql at all if pandas can cover for the things that I want to achieve. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your advice and if you can point me into maybe some useful learning books I would really appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "196hvq8", "is_robot_indexable": true, "report_reasons": null, "author": "GFM41", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196hvq8/newbie_here_trying_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196hvq8/newbie_here_trying_to_learn/", "subreddit_subscribers": 152852, "created_utc": 1705244826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kedro is often overlooked in Data Science projects despite offering structure, caching and tracking datasets, MLOps features as well as powerfull intergrations with other Data tools", "author_fullname": "t2_vbapbjo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kedro Intro and Hello World example", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_196gjd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Kedro Intro and Hello World example", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/r-oQ701wgDQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/196gjd0", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lEaH4fdOaFUim4MSa1dMXoEXZnisdrqh46i-mO9dpwE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1705240892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kedro is often overlooked in Data Science projects despite offering structure, caching and tracking datasets, MLOps features as well as powerfull intergrations with other Data tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/r-oQ701wgDQ?si=roh4zqGCb7pithUY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?auto=webp&amp;s=c8a24bf13665b37f042154cdde85fb28c250c98d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fdd1cc5a2ddc5ae95e23b021f4eb4cf621f962b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=711d0a99e36cba6734cf4ab86a80970eb3e789d2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/6Fr1bqghrzjNeHGjCmNPl2IwtpoJ68OtMVimm8CYA7U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=29f32d1da2077405987e79acb96813efac6719d2", "width": 320, "height": 240}], "variants": {}, "id": "v_XpU9SkWBZnhYuzYwDmOvhQLE6L9_ed2E4WBP_NL1Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "196gjd0", "is_robot_indexable": true, "report_reasons": null, "author": "dnulcon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196gjd0/kedro_intro_and_hello_world_example/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/r-oQ701wgDQ?si=roh4zqGCb7pithUY", "subreddit_subscribers": 152852, "created_utc": 1705240892.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Kedro Intro and Hello World example", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/r-oQ701wgDQ?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Kedro Intro and Hello World example\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/r-oQ701wgDQ/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm following a Microsoft Tutorial on MS Fabric. I'm working on an ETL process using Azure Blob Storage and Apache Spark, and I've come across a step in the process that I'm trying to understand better. The workflow involves reading a large dataset from Azure Blob Storage, processing it, and then writing the first 1000 rows of this dataset back to Blob Storage. After this, the script reads this subset again, performs further transformations, and finally loads it into a Delta table.\n\nMy question is: What could be the rationale for writing a subset of the data (1000 rows) back to Azure Blob Storage, only to read it again for further processing and loading into a Delta table? Are there specific benefits or use cases for this approach, such as performance optimization, data segmentation, or testing purposes that I might be overlooking?. Here is the link to the tutorial:  \n\n\n[https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html)\n\nAny insights or experiences you can share regarding this method would be greatly appreciated, especially if there are more direct or efficient ways to handle such a workflow.\n\nThanks in advance.", "author_fullname": "t2_7r901d2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding the Rationale Behind Writing Subset of Data Back to Azure Blob in ETL Process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_196eln8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705234509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m following a Microsoft Tutorial on MS Fabric. I&amp;#39;m working on an ETL process using Azure Blob Storage and Apache Spark, and I&amp;#39;ve come across a step in the process that I&amp;#39;m trying to understand better. The workflow involves reading a large dataset from Azure Blob Storage, processing it, and then writing the first 1000 rows of this dataset back to Blob Storage. After this, the script reads this subset again, performs further transformations, and finally loads it into a Delta table.&lt;/p&gt;\n\n&lt;p&gt;My question is: What could be the rationale for writing a subset of the data (1000 rows) back to Azure Blob Storage, only to read it again for further processing and loading into a Delta table? Are there specific benefits or use cases for this approach, such as performance optimization, data segmentation, or testing purposes that I might be overlooking?. Here is the link to the tutorial:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html\"&gt;https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any insights or experiences you can share regarding this method would be greatly appreciated, especially if there are more direct or efficient ways to handle such a workflow.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "196eln8", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Necessary-6455", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/196eln8/understanding_the_rationale_behind_writing_subset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/196eln8/understanding_the_rationale_behind_writing_subset/", "subreddit_subscribers": 152852, "created_utc": 1705234509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow redditors!\n\nSo I\u2019m beginning to get interested in building streaming pipelines. Things like a near real-time data ingestion (a bronze table, for instance), silver tables or even building near real-time specialized datasets (gold datasets).\n\nI know from some researches that in service oriented architectures companies use apache kafka as a integration layer/hub for consumers and producers.\n\nAs far as I\u2019m aware a producer generates some event and sends it for the kafka broker and the consumer can process it right after, and depending on the architecture the consumer even deletes (?) the event that already has been processed to avoid duplication.\n\nIf I\u2019m a data engineer interested in the data that this producer pushed to kafka, how would I reliably get access to this data, without intruding into the integration between the microservices? \n\nI\u2019m looking into kafka connect as something that tries to solve this issue, but I\u2019m quite unsure.\n\nCan someone with more experience in these streaming context bring some light to the subject?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Pipelines Architecture with Kafka and Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_195z6b2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705182606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow redditors!&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m beginning to get interested in building streaming pipelines. Things like a near real-time data ingestion (a bronze table, for instance), silver tables or even building near real-time specialized datasets (gold datasets).&lt;/p&gt;\n\n&lt;p&gt;I know from some researches that in service oriented architectures companies use apache kafka as a integration layer/hub for consumers and producers.&lt;/p&gt;\n\n&lt;p&gt;As far as I\u2019m aware a producer generates some event and sends it for the kafka broker and the consumer can process it right after, and depending on the architecture the consumer even deletes (?) the event that already has been processed to avoid duplication.&lt;/p&gt;\n\n&lt;p&gt;If I\u2019m a data engineer interested in the data that this producer pushed to kafka, how would I reliably get access to this data, without intruding into the integration between the microservices? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking into kafka connect as something that tries to solve this issue, but I\u2019m quite unsure.&lt;/p&gt;\n\n&lt;p&gt;Can someone with more experience in these streaming context bring some light to the subject?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "195z6b2", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/195z6b2/streaming_pipelines_architecture_with_kafka_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/195z6b2/streaming_pipelines_architecture_with_kafka_and/", "subreddit_subscribers": 152852, "created_utc": 1705182606.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}