{"kind": "Listing", "data": {"after": "t3_19e2m1m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.\n\nIt has been an Apache top-level project since 2018 ([https://pulsar.apache.org](https://pulsar.apache.org/)). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.\n\nUnlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.\n\nWhen I first heard about the Pulsar, I thought my dreams had come true. \ud83c\udf08\ud83e\udd84\n\nNow I'm trying to understand why there is not much buzz in public about it:\n\n* Is it a marketing flaw and people just didn't ever hear about it?\n* Or is there something wrong or missing in Pulsar?\n* Maybe it is just an overkill for most new projects?\n\nI would greatly appreciate hearing about your experience and thoughts on Pulsar.", "author_fullname": "t2_15hg9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think about Apache Pulsar?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dry3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706026742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706026337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.&lt;/p&gt;\n\n&lt;p&gt;It has been an Apache top-level project since 2018 (&lt;a href=\"https://pulsar.apache.org/\"&gt;https://pulsar.apache.org&lt;/a&gt;). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.&lt;/p&gt;\n\n&lt;p&gt;Unlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.&lt;/p&gt;\n\n&lt;p&gt;When I first heard about the Pulsar, I thought my dreams had come true. \ud83c\udf08\ud83e\udd84&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m trying to understand why there is not much buzz in public about it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it a marketing flaw and people just didn&amp;#39;t ever hear about it?&lt;/li&gt;\n&lt;li&gt;Or is there something wrong or missing in Pulsar?&lt;/li&gt;\n&lt;li&gt;Maybe it is just an overkill for most new projects?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would greatly appreciate hearing about your experience and thoughts on Pulsar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?auto=webp&amp;s=1a58c711911d84b2f519a1ebd08ffaf45660be13", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01aacbda20b76f446c0b68d0f0bcee12860a82ed", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca04e0b0d8c0f2f204fa4d37a4e9513be4b5d06d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a90d61ab4a25f6f305d2ef4c1b13d9096f42412b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b154af8ec3ffb86dffb7caf2c7b2afa2e48bca6e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=421a03368d603cfbce7be51e351c8581fbce11ee", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fV8wrBLVOHa4NS-TvBl_Rggq7OADLbBRMKWIDWh-wTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79cdb62ae01d72d03f7dce263d7d04f41e345207", "width": 1080, "height": 567}], "variants": {}, "id": "IpR2rPjTijRtTpBc0LhDqAYBTgh-AKSxmasMwlwTsew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dry3i", "is_robot_indexable": true, "report_reasons": null, "author": "visortelle", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dry3i/what_do_you_think_about_apache_pulsar/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dry3i/what_do_you_think_about_apache_pulsar/", "subreddit_subscribers": 155310, "created_utc": 1706026337.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently working on an ETL pipeline to move tables from SQL Server on a server to Postgres on another server, and then do operations on those tables in Postgres on a daily schedule. Some of those tables contain geospatial data.\n\nI currently implemented the pipeline through python scripts using pandas and sqlalchemy\n\nFor tables with spatial data I use geopandas and geosqlalchemy\n\nThe process is functional so far but it takes too long to move large tables \n\nHow can I speed up moving tables from SQL Server to Postgres? What kind of tools/ideas would be better to implement?", "author_fullname": "t2_b0bsz436o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL pipeline from SQL Server to Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19djant", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705996388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently working on an ETL pipeline to move tables from SQL Server on a server to Postgres on another server, and then do operations on those tables in Postgres on a daily schedule. Some of those tables contain geospatial data.&lt;/p&gt;\n\n&lt;p&gt;I currently implemented the pipeline through python scripts using pandas and sqlalchemy&lt;/p&gt;\n\n&lt;p&gt;For tables with spatial data I use geopandas and geosqlalchemy&lt;/p&gt;\n\n&lt;p&gt;The process is functional so far but it takes too long to move large tables &lt;/p&gt;\n\n&lt;p&gt;How can I speed up moving tables from SQL Server to Postgres? What kind of tools/ideas would be better to implement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19djant", "is_robot_indexable": true, "report_reasons": null, "author": "bolt_runner", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19djant/etl_pipeline_from_sql_server_to_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19djant/etl_pipeline_from_sql_server_to_postgres/", "subreddit_subscribers": 155310, "created_utc": 1705996388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Summary:\n\n* B.S. in Microbiology &amp; Chemistry \n* 10 years of work experience - clinical research, teaching, &amp; sales\n* No coding experience \n* Strong soft skills\n* Currently working fully remote\n\nDebating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it's positive or negative.  \n\nAm I turning this career path into a \"feel good\" dream or is it feasible? \n\nMuch love. ", "author_fullname": "t2_ex7mq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I romanticizing DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dp67h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706018753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Summary:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;B.S. in Microbiology &amp;amp; Chemistry &lt;/li&gt;\n&lt;li&gt;10 years of work experience - clinical research, teaching, &amp;amp; sales&lt;/li&gt;\n&lt;li&gt;No coding experience &lt;/li&gt;\n&lt;li&gt;Strong soft skills&lt;/li&gt;\n&lt;li&gt;Currently working fully remote&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Debating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it&amp;#39;s positive or negative.  &lt;/p&gt;\n\n&lt;p&gt;Am I turning this career path into a &amp;quot;feel good&amp;quot; dream or is it feasible? &lt;/p&gt;\n\n&lt;p&gt;Much love. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dp67h", "is_robot_indexable": true, "report_reasons": null, "author": "ItsWetInPortland", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dp67h/am_i_romanticizing_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dp67h/am_i_romanticizing_de/", "subreddit_subscribers": 155310, "created_utc": 1706018753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, to provide some background, I currently run a very small data team in an industry that\u2019s heavily biased towards on-prem. After facing some limitations with SSIS, we\u2019re looking to move to a more modern ETL solution. My issue is we are currently built to run on-prem ETL while modern architecture seems to focus cloud ELT.\n\nMy current setup is as follows: \n1. Extract data from a variety of structured SQL servers (most on-prem, some accessed through VPN. All have direct SQL access)\n2. Transform that data using SQL into a more usable, de-normalized format\n3. Upload the data into our on-prem data warehouse for analytics. \n\nA few complicating factors:\n1. Several of our vendors are moving to the cloud and I\u2019ll need to export the data via API (and one vendor through snowflake)\n2. Our largest data table is under 30 million rows, I don\u2019t expect I need anything more complicated than polars/pandas and SQL\n3. In the future, we\u2019d like to be able to ingest and process high frequency (~5-15s) data loaded via 1 hour batches. I don\u2019t need streaming yet nor do I want to implement it. \n\nTools I\u2019ve considered:\nMage*\nDagster\nAirflow\nPandas/Polars*\nDuckDB\nDBT*\nMinio*\n\n*Current favorite for that role\n\n\nMy thought is, if I\u2019m rebuilding this anyway, would it make more sense to move to an ELT architecture with DBT or maintain my ETL architecture with Python/SQL. What would that look like? Load everything raw into parquet files and process from there? Load raw data into intermediate tables on the same server as the data warehouse? If the majority of my data is already structured, should I bother with parquet? I apologize for the deluge of questions, I\u2019m trying to wrap my head around this new world order after spending too long in the comfortable embrace of SSIS. I appreciate your help!", "author_fullname": "t2_95pplng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT for On-Prem ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dtqiu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706031025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, to provide some background, I currently run a very small data team in an industry that\u2019s heavily biased towards on-prem. After facing some limitations with SSIS, we\u2019re looking to move to a more modern ETL solution. My issue is we are currently built to run on-prem ETL while modern architecture seems to focus cloud ELT.&lt;/p&gt;\n\n&lt;p&gt;My current setup is as follows: \n1. Extract data from a variety of structured SQL servers (most on-prem, some accessed through VPN. All have direct SQL access)\n2. Transform that data using SQL into a more usable, de-normalized format\n3. Upload the data into our on-prem data warehouse for analytics. &lt;/p&gt;\n\n&lt;p&gt;A few complicating factors:\n1. Several of our vendors are moving to the cloud and I\u2019ll need to export the data via API (and one vendor through snowflake)\n2. Our largest data table is under 30 million rows, I don\u2019t expect I need anything more complicated than polars/pandas and SQL\n3. In the future, we\u2019d like to be able to ingest and process high frequency (~5-15s) data loaded via 1 hour batches. I don\u2019t need streaming yet nor do I want to implement it. &lt;/p&gt;\n\n&lt;p&gt;Tools I\u2019ve considered:\nMage*\nDagster\nAirflow\nPandas/Polars*\nDuckDB\nDBT*\nMinio*&lt;/p&gt;\n\n&lt;p&gt;*Current favorite for that role&lt;/p&gt;\n\n&lt;p&gt;My thought is, if I\u2019m rebuilding this anyway, would it make more sense to move to an ELT architecture with DBT or maintain my ETL architecture with Python/SQL. What would that look like? Load everything raw into parquet files and process from there? Load raw data into intermediate tables on the same server as the data warehouse? If the majority of my data is already structured, should I bother with parquet? I apologize for the deluge of questions, I\u2019m trying to wrap my head around this new world order after spending too long in the comfortable embrace of SSIS. I appreciate your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dtqiu", "is_robot_indexable": true, "report_reasons": null, "author": "Lord_Lloydd", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dtqiu/dbt_for_onprem_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dtqiu/dbt_for_onprem_etl/", "subreddit_subscribers": 155310, "created_utc": 1706031025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI\u2019m trying to improve a system that maintains a local copy of a dataset retrieved via API. The dataset can fluctuate by having records added, removed, or changed. It gets synchronized hourly via the results of an ETL job moving API data into the database.\n\nAt the moment, the pipeline queries API for the full dataset, normalizes the data, and compares against the database data in memory. Python code getting both the API data and database data in the same place, normalizing the API data, and doing in memory comparisons. Missing records get added to the database. Gone records get removed. Changed records get updated.\n\nI\u2019m thinking it would be better to just truncate the whole dataset every hour and reload all the data, regardless of whether it\u2019s changed or not. This sounds less expensive.\n\nCurious if there\u2019s an even more efficient approach given records can be deleted- we don\u2019t know if the set of records will be the same set after each update.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to efficiently maintain a current state of dataset from an API when records can be removed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e4sc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706059125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to improve a system that maintains a local copy of a dataset retrieved via API. The dataset can fluctuate by having records added, removed, or changed. It gets synchronized hourly via the results of an ETL job moving API data into the database.&lt;/p&gt;\n\n&lt;p&gt;At the moment, the pipeline queries API for the full dataset, normalizes the data, and compares against the database data in memory. Python code getting both the API data and database data in the same place, normalizing the API data, and doing in memory comparisons. Missing records get added to the database. Gone records get removed. Changed records get updated.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking it would be better to just truncate the whole dataset every hour and reload all the data, regardless of whether it\u2019s changed or not. This sounds less expensive.&lt;/p&gt;\n\n&lt;p&gt;Curious if there\u2019s an even more efficient approach given records can be deleted- we don\u2019t know if the set of records will be the same set after each update.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19e4sc5", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e4sc5/how_to_efficiently_maintain_a_current_state_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e4sc5/how_to_efficiently_maintain_a_current_state_of/", "subreddit_subscribers": 155310, "created_utc": 1706059125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to create a python app that will output data e.g. every 5s (ideally should be parametrizable to e.g. 10s or 1 day). Input is realtime data stream from RabbitMQ/Kafka. Every 5s app should look into the queue and take all data between defined start and end time - i.e. every run instance should be idempotent as it is defined by static start/end. Job should have some retrying logic in case something fails. App should run in on-premise Kubernetes cluster.\n\nI thought of 3 possible solutions:\n\na) custom python app with some scheduler such as Airflow that could schedule the app in cluster every 5s\n\nb) custom python app that would run in an infinite loop as standalone pod in the cluster and every 5s or so would do the job\n\nc) use some existing solution that would solve both issues (data logic and scheduling) for me\n\nThe issue with a) is we need a robust scheduler that could handle all the job spawns (each 5s, each 10s etc.)\n\nThe issue with b) is there is less transparency in case app crashes and also since its only one app in case it crashes all runs that should run after the crash won't run until the app is fixed (scheduler would schedule all the following runs despite previous fails)\n\nThe issue with c) is the only potential solution is Prefect but it has memory leak so when we run it for inifity it will eventually run out of memory.\n\nWhat would be the best approach to implement this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do design a near real-time data job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dtn2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706030793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to create a python app that will output data e.g. every 5s (ideally should be parametrizable to e.g. 10s or 1 day). Input is realtime data stream from RabbitMQ/Kafka. Every 5s app should look into the queue and take all data between defined start and end time - i.e. every run instance should be idempotent as it is defined by static start/end. Job should have some retrying logic in case something fails. App should run in on-premise Kubernetes cluster.&lt;/p&gt;\n\n&lt;p&gt;I thought of 3 possible solutions:&lt;/p&gt;\n\n&lt;p&gt;a) custom python app with some scheduler such as Airflow that could schedule the app in cluster every 5s&lt;/p&gt;\n\n&lt;p&gt;b) custom python app that would run in an infinite loop as standalone pod in the cluster and every 5s or so would do the job&lt;/p&gt;\n\n&lt;p&gt;c) use some existing solution that would solve both issues (data logic and scheduling) for me&lt;/p&gt;\n\n&lt;p&gt;The issue with a) is we need a robust scheduler that could handle all the job spawns (each 5s, each 10s etc.)&lt;/p&gt;\n\n&lt;p&gt;The issue with b) is there is less transparency in case app crashes and also since its only one app in case it crashes all runs that should run after the crash won&amp;#39;t run until the app is fixed (scheduler would schedule all the following runs despite previous fails)&lt;/p&gt;\n\n&lt;p&gt;The issue with c) is the only potential solution is Prefect but it has memory leak so when we run it for inifity it will eventually run out of memory.&lt;/p&gt;\n\n&lt;p&gt;What would be the best approach to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dtn2u", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dtn2u/how_to_do_design_a_near_realtime_data_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dtn2u/how_to_do_design_a_near_realtime_data_job/", "subreddit_subscribers": 155310, "created_utc": 1706030793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had a phone screen yesterday for a data analytics engineer role. \n\nI was asked how do I monitor the data pipelines and ensure its accuracy. My response was, I enjoy working with the end user and am really great about getting constant feedback. I said how in my current role, as a Product Engineer, i spend a lot of time with users and going through user data/feedback to determine the success of a feature. \n\nNow that I'm thinking about it -- they may have been asking me what tools I use.\n\nEarlier, I described a FastAPI poller I built that detected any new data from an AWS EC2 where I dumped everything. Then it took the new data, transformed it in into the \"pretty\" staging structures then updated the appropriate (separate) EC2 tables. In this case, I use pydantic models to ensure that the data is structured correctly. Any issues I can see in the logs. \n\nNow that time has passed I think they were asking about testing (in dbt) and monitoring tools. \n\nIs it worth following-up and clarifying?", "author_fullname": "t2_qn4mfqku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maybe bombed this interview question? Asked about data validation and accuracy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsy6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706028848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a phone screen yesterday for a data analytics engineer role. &lt;/p&gt;\n\n&lt;p&gt;I was asked how do I monitor the data pipelines and ensure its accuracy. My response was, I enjoy working with the end user and am really great about getting constant feedback. I said how in my current role, as a Product Engineer, i spend a lot of time with users and going through user data/feedback to determine the success of a feature. &lt;/p&gt;\n\n&lt;p&gt;Now that I&amp;#39;m thinking about it -- they may have been asking me what tools I use.&lt;/p&gt;\n\n&lt;p&gt;Earlier, I described a FastAPI poller I built that detected any new data from an AWS EC2 where I dumped everything. Then it took the new data, transformed it in into the &amp;quot;pretty&amp;quot; staging structures then updated the appropriate (separate) EC2 tables. In this case, I use pydantic models to ensure that the data is structured correctly. Any issues I can see in the logs. &lt;/p&gt;\n\n&lt;p&gt;Now that time has passed I think they were asking about testing (in dbt) and monitoring tools. &lt;/p&gt;\n\n&lt;p&gt;Is it worth following-up and clarifying?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "19dsy6j", "is_robot_indexable": true, "report_reasons": null, "author": "No_Egg1537", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsy6j/maybe_bombed_this_interview_question_asked_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsy6j/maybe_bombed_this_interview_question_asked_about/", "subreddit_subscribers": 155310, "created_utc": 1706028848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, everyone!\n\nI'd like to move to the West in the near future working now more like in 2d world region. To do this, I think I need to master the appropriate stack (and understand the diff - as-is vs. to-be-done). I tried to make it reading this subreddit (thank you guys!) but want to calibrate it.\n\nMy own is (biased ofc + couldn't sort):  \n1. Airflow (dagster or similar, airflow just preferable in my country; tool of orchestration, it's clear)  \n2. Postgres (relational db, very widespread, also no comment)  \n3. Spark (processing large data)  \n4. Analytical storage (dunno what to include here since stack varies greatly - on-prem (GP/CH) are used mostly here but not cloud)  \n5. Streaming (?)  \n.. and other stuff on roughly basic level (docker, superset/grafana, k8s, ELK/EFK, CI/CD etc.)\n\nI think mine may be cringy so could you share your own list and elaborate if possible (from top to bottom in terms of value)?\n\nFollow-up: when engineer can put \"experience with cloud\" on their resume? I have luck to interact with cloud on my current place, but I do simple stuff from first chapters of aws (compatible) docs. I think this is not even nearly enough.\n\nThank you! If I missed similar discussion here share link please.", "author_fullname": "t2_b3ayn5hsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE hard-skills (using Pareto principle)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dzhdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706045170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to move to the West in the near future working now more like in 2d world region. To do this, I think I need to master the appropriate stack (and understand the diff - as-is vs. to-be-done). I tried to make it reading this subreddit (thank you guys!) but want to calibrate it.&lt;/p&gt;\n\n&lt;p&gt;My own is (biased ofc + couldn&amp;#39;t sort):&lt;br/&gt;\n1. Airflow (dagster or similar, airflow just preferable in my country; tool of orchestration, it&amp;#39;s clear)&lt;br/&gt;\n2. Postgres (relational db, very widespread, also no comment)&lt;br/&gt;\n3. Spark (processing large data)&lt;br/&gt;\n4. Analytical storage (dunno what to include here since stack varies greatly - on-prem (GP/CH) are used mostly here but not cloud)&lt;br/&gt;\n5. Streaming (?)&lt;br/&gt;\n.. and other stuff on roughly basic level (docker, superset/grafana, k8s, ELK/EFK, CI/CD etc.)&lt;/p&gt;\n\n&lt;p&gt;I think mine may be cringy so could you share your own list and elaborate if possible (from top to bottom in terms of value)?&lt;/p&gt;\n\n&lt;p&gt;Follow-up: when engineer can put &amp;quot;experience with cloud&amp;quot; on their resume? I have luck to interact with cloud on my current place, but I do simple stuff from first chapters of aws (compatible) docs. I think this is not even nearly enough.&lt;/p&gt;\n\n&lt;p&gt;Thank you! If I missed similar discussion here share link please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dzhdr", "is_robot_indexable": true, "report_reasons": null, "author": "isk14yo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dzhdr/de_hardskills_using_pareto_principle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dzhdr/de_hardskills_using_pareto_principle/", "subreddit_subscribers": 155310, "created_utc": 1706045170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table set up with 3 foreign keys. In the case of one of those foreign keys, the lack of a value (NULL) is considered important information.\n\nIt works out something like this:\n\n```\nCREATE TABLE foo (\n    id SERIAL,\n    name TEXT UNIQUE\n);\n\nCREATE TABLE bar (\n    id SERIAL\n    name TEXT UNIQUE\n);\n\nCREATE TABLE baz (\n    id SERIAL\n    name TEXT UNIQUE\n);\n\nCREATE TABLE foobarbaz (\n    id SERIAL\n    foo_id FK NOT NULL\n    bar_id FK NOT NULL\n    baz_id FK\n    value INT\n\n    CONSTRAINT uniq_distnct_baz UNIQUE NULL NOT DISTINCT baz_id\n);\n```\n\nThe thing is, does this make it impossible to reliably insert rows using subqueries?\n\n```\nINSERT INTO foobarbaz (foo_id, bar_id, baz_id, value)\nVALUES (\n    (SELECT id FROM foo WHERE name = \u201cpea\u201d),\n    (SELECT id FROM bar WHERE name = \u201cnut\u201d)\n     (SELECT id FROM baz WHERE name = \u201cfan\u201d),\n    100\n);\n```\n\nIn this case, the existence of the subquery for `name = \u201cfan\u201d` on `baz` indicates that the user **knows** a record should exist for this. If the record does not exist, the INSERT should fail. If there was not supposed to be a value there, the INSERT would use NULL in place of that subquery,\n\nHowever, it\u2019s possible that the condition `name = \u201cfan\u201d` on `baz` doesn\u2019t exist. This would return a NULL value, and that would be acceptable by the tables constraints so long as a similar record doesn\u2019t already exist. \n\nThis makes the INSERT statement inherently risky, as you must first confirm that the record you seek actually exists.\n\nIs there a way to leverage the database to take care of this automatically? Make the INSERT fail if it should. \n\nIs the only way a VIEW with an INSTEAD OF INSERT trigger?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to make an INSERT statement fail if a subquery returns NULL? (Postgres)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e88iq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706069337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table set up with 3 foreign keys. In the case of one of those foreign keys, the lack of a value (NULL) is considered important information.&lt;/p&gt;\n\n&lt;p&gt;It works out something like this:&lt;/p&gt;\n\n&lt;p&gt;```\nCREATE TABLE foo (\n    id SERIAL,\n    name TEXT UNIQUE\n);&lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE bar (\n    id SERIAL\n    name TEXT UNIQUE\n);&lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE baz (\n    id SERIAL\n    name TEXT UNIQUE\n);&lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE foobarbaz (\n    id SERIAL\n    foo_id FK NOT NULL\n    bar_id FK NOT NULL\n    baz_id FK\n    value INT&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CONSTRAINT uniq_distnct_baz UNIQUE NULL NOT DISTINCT baz_id\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;);\n```&lt;/p&gt;\n\n&lt;p&gt;The thing is, does this make it impossible to reliably insert rows using subqueries?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nINSERT INTO foobarbaz (foo_id, bar_id, baz_id, value)\nVALUES (\n    (SELECT id FROM foo WHERE name = \u201cpea\u201d),\n    (SELECT id FROM bar WHERE name = \u201cnut\u201d)\n     (SELECT id FROM baz WHERE name = \u201cfan\u201d),\n    100\n);\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In this case, the existence of the subquery for &lt;code&gt;name = \u201cfan\u201d&lt;/code&gt; on &lt;code&gt;baz&lt;/code&gt; indicates that the user &lt;strong&gt;knows&lt;/strong&gt; a record should exist for this. If the record does not exist, the INSERT should fail. If there was not supposed to be a value there, the INSERT would use NULL in place of that subquery,&lt;/p&gt;\n\n&lt;p&gt;However, it\u2019s possible that the condition &lt;code&gt;name = \u201cfan\u201d&lt;/code&gt; on &lt;code&gt;baz&lt;/code&gt; doesn\u2019t exist. This would return a NULL value, and that would be acceptable by the tables constraints so long as a similar record doesn\u2019t already exist. &lt;/p&gt;\n\n&lt;p&gt;This makes the INSERT statement inherently risky, as you must first confirm that the record you seek actually exists.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to leverage the database to take care of this automatically? Make the INSERT fail if it should. &lt;/p&gt;\n\n&lt;p&gt;Is the only way a VIEW with an INSTEAD OF INSERT trigger?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19e88iq", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e88iq/any_way_to_make_an_insert_statement_fail_if_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e88iq/any_way_to_make_an_insert_statement_fail_if_a/", "subreddit_subscribers": 155310, "created_utc": 1706069337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a manufactoring project I have two sensors:\n\n1. Sensor 1: temperature data sampled at 10Hz continously.\n2. Sensor 2: 3-axis accelerometer data sampled at 6kHz in a window of 10s every 10m. In other words, every 10m I have a windows of 10s containing 10\\*6k=60000 records. Every record has a timestamp, a value for axis x, y, and z. 60000x4 table.\n\nOn sensors 2 data:\n\nThe ideas is to perform, at some stage, a \"data engineering\" phase where the \"raw data\" from sensors 2 mentionted before are processed in order to output some informative and less-dimensional data. For instance, letting the inputs be:\n\n* Window 1 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).\n* Window 2 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).\n* ...\n* Window M: ...\n\nThe output would be:\n\n* MxN table/matrix (windows\\_id, timestamp\\_start\\_window, feature1, feature2, ..., feature N-2).\n\nWhere N is the number of synthetic features created (e.g. mean x, median y, max z, min z, etc..) plus a timestamp (for instance the start of the window) and the windows ID and M is the number of windows.\n\nIf I want to save these two data raw sources (inputs) into a file system or database, and also the synthetic data (outputs), how would you save them in order to be flexible and efficient with later data analysis? The analysis will be based on time-series algorithm in order to dedect patterns and anomaly detections.\n\nNote, the two sensors are an example of different sources with different requirements but the use case is not \"that simple\". I would like to discuss the design of modeling and storing/extraction of these time-series with easiness, scaling, and efficiency in mind.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model and save these two data source.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dt9fn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706029646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a manufactoring project I have two sensors:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Sensor 1: temperature data sampled at 10Hz continously.&lt;/li&gt;\n&lt;li&gt;Sensor 2: 3-axis accelerometer data sampled at 6kHz in a window of 10s every 10m. In other words, every 10m I have a windows of 10s containing 10*6k=60000 records. Every record has a timestamp, a value for axis x, y, and z. 60000x4 table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;On sensors 2 data:&lt;/p&gt;\n\n&lt;p&gt;The ideas is to perform, at some stage, a &amp;quot;data engineering&amp;quot; phase where the &amp;quot;raw data&amp;quot; from sensors 2 mentionted before are processed in order to output some informative and less-dimensional data. For instance, letting the inputs be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Window 1 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).&lt;/li&gt;\n&lt;li&gt;Window 2 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;li&gt;Window M: ...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The output would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MxN table/matrix (windows_id, timestamp_start_window, feature1, feature2, ..., feature N-2).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Where N is the number of synthetic features created (e.g. mean x, median y, max z, min z, etc..) plus a timestamp (for instance the start of the window) and the windows ID and M is the number of windows.&lt;/p&gt;\n\n&lt;p&gt;If I want to save these two data raw sources (inputs) into a file system or database, and also the synthetic data (outputs), how would you save them in order to be flexible and efficient with later data analysis? The analysis will be based on time-series algorithm in order to dedect patterns and anomaly detections.&lt;/p&gt;\n\n&lt;p&gt;Note, the two sensors are an example of different sources with different requirements but the use case is not &amp;quot;that simple&amp;quot;. I would like to discuss the design of modeling and storing/extraction of these time-series with easiness, scaling, and efficiency in mind.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dt9fn", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dt9fn/how_to_model_and_save_these_two_data_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dt9fn/how_to_model_and_save_these_two_data_source/", "subreddit_subscribers": 155310, "created_utc": 1706029646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to figure out how to size resources for our data pipeline and warehouse. My boss wants me to calculate what each pipeline needs in terms of compute and RAM. Any ideas on how to evaluate and track these requirements? Would love to hear what's worked for you.", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you decide on resource sizes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dor5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706017540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out how to size resources for our data pipeline and warehouse. My boss wants me to calculate what each pipeline needs in terms of compute and RAM. Any ideas on how to evaluate and track these requirements? Would love to hear what&amp;#39;s worked for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dor5g", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dor5g/how_do_you_decide_on_resource_sizes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dor5g/how_do_you_decide_on_resource_sizes/", "subreddit_subscribers": 155310, "created_utc": 1706017540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like any other optimization, parquet row group sizing has trade-offs, when do you choose to go for more row groups or less row groups?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet Row Group Sizing, how do you choose your sizing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dnoxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706014188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like any other optimization, parquet row group sizing has trade-offs, when do you choose to go for more row groups or less row groups?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dnoxa", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dnoxa/parquet_row_group_sizing_how_do_you_choose_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dnoxa/parquet_row_group_sizing_how_do_you_choose_your/", "subreddit_subscribers": 155310, "created_utc": 1706014188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am working on data engineering project, and am thinking what data modelling technique to use? Can someone give a suggestion?\n\nThank you very much for reading and helping.", "author_fullname": "t2_it5y6tdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault or Dimensional Modelling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dmm6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706010355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am working on data engineering project, and am thinking what data modelling technique to use? Can someone give a suggestion?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much for reading and helping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19dmm6t", "is_robot_indexable": true, "report_reasons": null, "author": "Aggressive-Nebula-44", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dmm6t/data_vault_or_dimensional_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dmm6t/data_vault_or_dimensional_modelling/", "subreddit_subscribers": 155310, "created_utc": 1706010355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m reading avro messages from Kafka by connecting to the schema registry through a Delta Live Tables pipeline (DLT). I then use the from_avro method to deserialize this message stream. The records are present as json objects, how do I use from_json to load this data inside delta? It\u2019s asking me for the schema, is there any way I can dynamically infer the schema? \n\nTL;DR: how to dynamically infer schema from a JSON stream and load inside delta?", "author_fullname": "t2_htau4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deserialize json data from an avro message stream in Databricks using DLT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_19ea6tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706075987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m reading avro messages from Kafka by connecting to the schema registry through a Delta Live Tables pipeline (DLT). I then use the from_avro method to deserialize this message stream. The records are present as json objects, how do I use from_json to load this data inside delta? It\u2019s asking me for the schema, is there any way I can dynamically infer the schema? &lt;/p&gt;\n\n&lt;p&gt;TL;DR: how to dynamically infer schema from a JSON stream and load inside delta?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19ea6tf", "is_robot_indexable": true, "report_reasons": null, "author": "sampasha007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19ea6tf/how_to_deserialize_json_data_from_an_avro_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19ea6tf/how_to_deserialize_json_data_from_an_avro_message/", "subreddit_subscribers": 155310, "created_utc": 1706075987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i am a mathematics student, and i would like to pursue a carrer in data science, idelly in finance. I have some doubts about how to get started.\n\nDo i also need to graduate in a specific data sciencie program, or is a mathematics degree sufficient?\n\nI read that the nest way to begin learninf is to work on projects and continually improve. Are there other methods you know of to get started?\n\nThank you very much.", "author_fullname": "t2_sl8md8fw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e9ggh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706073365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i am a mathematics student, and i would like to pursue a carrer in data science, idelly in finance. I have some doubts about how to get started.&lt;/p&gt;\n\n&lt;p&gt;Do i also need to graduate in a specific data sciencie program, or is a mathematics degree sufficient?&lt;/p&gt;\n\n&lt;p&gt;I read that the nest way to begin learninf is to work on projects and continually improve. Are there other methods you know of to get started?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19e9ggh", "is_robot_indexable": true, "report_reasons": null, "author": "Low-Pack4738", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e9ggh/how_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e9ggh/how_to_start/", "subreddit_subscribers": 155310, "created_utc": 1706073365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is a massive growth in SaaS data tools, and I was wondering how much people rely on such product. Excluding cloud data warehouses (Snowflake/Redshift/Data bricks).\n\nHow much does your company rely on Salas tools for orchestration, data quality, data catalogs, ingestion, transformation? How big is your data team and your company's revenue? \n\nWhich products do you use as Salas and which do you self-host? Why? \n\nWhat are the pros and cons of each?", "author_fullname": "t2_j3gqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much does your data team rely on SaaS products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e1jv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706050292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a massive growth in SaaS data tools, and I was wondering how much people rely on such product. Excluding cloud data warehouses (Snowflake/Redshift/Data bricks).&lt;/p&gt;\n\n&lt;p&gt;How much does your company rely on Salas tools for orchestration, data quality, data catalogs, ingestion, transformation? How big is your data team and your company&amp;#39;s revenue? &lt;/p&gt;\n\n&lt;p&gt;Which products do you use as Salas and which do you self-host? Why? &lt;/p&gt;\n\n&lt;p&gt;What are the pros and cons of each?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "19e1jv5", "is_robot_indexable": true, "report_reasons": null, "author": "exact-approximate", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e1jv5/how_much_does_your_data_team_rely_on_saas_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e1jv5/how_much_does_your_data_team_rely_on_saas_products/", "subreddit_subscribers": 155310, "created_utc": 1706050292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI-like interface for data modeling in the semantic layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_19dwfaq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iqGkI6_XbT3aOnh5CKpfJKdIJ_sqTRUHW-Ro8E8znFk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706037577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-playground-2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?auto=webp&amp;s=598a2011d7cc1843b9f148a90fa2935b50f5ed64", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=873ccb13da7029d29c99220eaf9dac2d90de459c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=28488b21c37f052cdb27e0d4e658b8e5f3698389", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0ffbb86cbe85df321dd1357b777bcf1dc2165cc", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffd3a38a0dbc76bb13bea6f9e06551ed65442f95", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=38affae2cdcecdee105d887c39acbd152b2e8d09", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-s9C9iQ4Q8kEEqmSK0QSS4rvcmLCNRcHBpaMyuYrfJg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f58b64228375ec116bbb449412ffb0f299ac464", "width": 1080, "height": 567}], "variants": {}, "id": "9I19JyWLZtFgtwaXM9XPcdIrs9yeJOFCJI-1OhiVzx4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19dwfaq", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dwfaq/bilike_interface_for_data_modeling_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-playground-2", "subreddit_subscribers": 155310, "created_utc": 1706037577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to move and schedule the data copy operation from the company Sharepoint to the Azure Blob Storage. I am new to the Azure and found two candidate solutions.\n\n* Use Azure Data Factory predefined jobs.\n* Use Azure Functions with Python scripts.\n\nFrom what I gathered Azure Functions would be cheaper, this is not super hard to code so I wonder if there is any benefit to the ADF. Do you have any thoughts/recommendations?", "author_fullname": "t2_8wtp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy Data from Sharepoint to the Azure Blob storage.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19dsv7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706028645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to move and schedule the data copy operation from the company Sharepoint to the Azure Blob Storage. I am new to the Azure and found two candidate solutions.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Azure Data Factory predefined jobs.&lt;/li&gt;\n&lt;li&gt;Use Azure Functions with Python scripts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;From what I gathered Azure Functions would be cheaper, this is not super hard to code so I wonder if there is any benefit to the ADF. Do you have any thoughts/recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19dsv7p", "is_robot_indexable": true, "report_reasons": null, "author": "bartosaq", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19dsv7p/copy_data_from_sharepoint_to_the_azure_blob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19dsv7p/copy_data_from_sharepoint_to_the_azure_blob/", "subreddit_subscribers": 155310, "created_utc": 1706028645.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to backfill via the Airflow UI? I know that it's possible via CLI, but I have a requirement to do it over the UI, did some searching couldn't find much info, wonder if it's possible to do via UI. ", "author_fullname": "t2_15rwxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow backfill via UI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19doe1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706016425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to backfill via the Airflow UI? I know that it&amp;#39;s possible via CLI, but I have a requirement to do it over the UI, did some searching couldn&amp;#39;t find much info, wonder if it&amp;#39;s possible to do via UI. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19doe1t", "is_robot_indexable": true, "report_reasons": null, "author": "cjj1120", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19doe1t/airflow_backfill_via_ui/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19doe1t/airflow_backfill_via_ui/", "subreddit_subscribers": 155310, "created_utc": 1706016425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started as a Data Analyst after a career change. As with many others, I was quickly identified as a candidate for a JDE role. Ever since (~2 months) I have been mainly working with spatial data in FME and handling pipelines and dashboards.\n\nMy organization have suggested I take some training on spatial data, but I also want to know where else I can apply myself to progress becoming a Data Engineer. Any tips on specific skills, certifications, or experiences that could help me? I'd say I have moderate ability with SQL and basic Python understanding. Should I be putting all my time in data handling / manipulation in Python? Should I be looking at any other ETL tools (we currently use FME exclusively). Should I be considering database managent?? \n\nAny help and advice is really appreciated!", "author_fullname": "t2_3ewzzqbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New player to the game", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19djcsf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1705996666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started as a Data Analyst after a career change. As with many others, I was quickly identified as a candidate for a JDE role. Ever since (~2 months) I have been mainly working with spatial data in FME and handling pipelines and dashboards.&lt;/p&gt;\n\n&lt;p&gt;My organization have suggested I take some training on spatial data, but I also want to know where else I can apply myself to progress becoming a Data Engineer. Any tips on specific skills, certifications, or experiences that could help me? I&amp;#39;d say I have moderate ability with SQL and basic Python understanding. Should I be putting all my time in data handling / manipulation in Python? Should I be looking at any other ETL tools (we currently use FME exclusively). Should I be considering database managent?? &lt;/p&gt;\n\n&lt;p&gt;Any help and advice is really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19djcsf", "is_robot_indexable": true, "report_reasons": null, "author": "BrittleTupperwareBox", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19djcsf/new_player_to_the_game/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19djcsf/new_player_to_the_game/", "subreddit_subscribers": 155310, "created_utc": 1705996666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has started a referral program for DataEengineers and Data Scientists and other posts.\n\nI am attaching a pdf, please go through them, if some position suits you, please DM me.\n\nAlthough you can directly apply to the positions, but it would be help if you let me refer it so that I get the referral bonus once you join and complete 3 months.\n\nMost of the positions are for India, 1 for USA and 1 for Canada\n\n[open positions](https://drive.google.com/file/d/1dJakn9IImz49kLsxLWghCA5SkhhvbOWZ/view?usp=share_link)", "author_fullname": "t2_9qgesljo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering referral program.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e909p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706071848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has started a referral program for DataEengineers and Data Scientists and other posts.&lt;/p&gt;\n\n&lt;p&gt;I am attaching a pdf, please go through them, if some position suits you, please DM me.&lt;/p&gt;\n\n&lt;p&gt;Although you can directly apply to the positions, but it would be help if you let me refer it so that I get the referral bonus once you join and complete 3 months.&lt;/p&gt;\n\n&lt;p&gt;Most of the positions are for India, 1 for USA and 1 for Canada&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/file/d/1dJakn9IImz49kLsxLWghCA5SkhhvbOWZ/view?usp=share_link\"&gt;open positions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19e909p", "is_robot_indexable": true, "report_reasons": null, "author": "Ready-Ad3141", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e909p/data_engineering_referral_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e909p/data_engineering_referral_program/", "subreddit_subscribers": 155310, "created_utc": 1706071848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Must watch if you have ever been confused/frustrated with Python paths and packaging.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_19e7pf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v6tALyc4C10?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Packaging Your Python Code With pyproject.toml | Complete Code Conversation\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Packaging Your Python Code With pyproject.toml | Complete Code Conversation", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v6tALyc4C10?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Packaging Your Python Code With pyproject.toml | Complete Code Conversation\"&gt;&lt;/iframe&gt;", "author_name": "Real Python", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/v6tALyc4C10/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@realpython"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v6tALyc4C10?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Packaging Your Python Code With pyproject.toml | Complete Code Conversation\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/19e7pf8", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3kuudUaEorxNEiKRX0XoC9rQXM9Np4-y2MFhrMBzX30.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706067632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=v6tALyc4C10", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/35v3F8X8Bkp_U7u5tlU6rkZop4WfJXKIdtzxehCbdfY.jpg?auto=webp&amp;s=15c96d0a57990daad9ceb7044847115735098688", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/35v3F8X8Bkp_U7u5tlU6rkZop4WfJXKIdtzxehCbdfY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1796fdee5b48e6ac8851ca7b8d236a56c250b8e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/35v3F8X8Bkp_U7u5tlU6rkZop4WfJXKIdtzxehCbdfY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d482728dd5c3d0b6d3c4845255f7cbdf6816cc6", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/35v3F8X8Bkp_U7u5tlU6rkZop4WfJXKIdtzxehCbdfY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=336ce71509e721be4da6454ebd7c7ac9571a465d", "width": 320, "height": 240}], "variants": {}, "id": "bJ82cZJ_dSSei8mziFPvY5jwtpPYIZMcrX7dkta6BY0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "19e7pf8", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e7pf8/must_watch_if_you_have_ever_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=v6tALyc4C10", "subreddit_subscribers": 155310, "created_utc": 1706067632.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Packaging Your Python Code With pyproject.toml | Complete Code Conversation", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v6tALyc4C10?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Packaging Your Python Code With pyproject.toml | Complete Code Conversation\"&gt;&lt;/iframe&gt;", "author_name": "Real Python", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/v6tALyc4C10/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@realpython"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Software engineer here just recently joined a team heavily focused on data science and data engineering. The expectation is that i take their scripts and refine and improve their automation. While still ramping up on the data science and data engineering problem space, coming from a software engineer perspective would it make sense to use a tool that offer ci/cd pipeline capabilities to create an ETL pipeline? Ive just normally been exposed to ci/cd pipelines with tests, static code analysis tools, etc that promote/deploy an application to an environment. Whereas, from my understanding, the new team im on wants to gather data, normalize said data, and upload it to a data warehouse. So from a tech stack perspective would it make sense to also use tools that offer ci/cd pipelines to achieve that? Like basically break up their script into stages so i can also create stages to test/ensure quality of the data prior to moving on to the next step, as opposed to traditionally having devop-specific stages .Or are there some obvious reasons that im just not aware of as to why people use tools like domino, aws glue, snowflake or airflow instead? Like maybe its an issue of scalability when dealing with large volumes of data? Any feedback would be appreciated.", "author_fullname": "t2_npzm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SW engineer learning about ETL pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e6ofb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706064579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Software engineer here just recently joined a team heavily focused on data science and data engineering. The expectation is that i take their scripts and refine and improve their automation. While still ramping up on the data science and data engineering problem space, coming from a software engineer perspective would it make sense to use a tool that offer ci/cd pipeline capabilities to create an ETL pipeline? Ive just normally been exposed to ci/cd pipelines with tests, static code analysis tools, etc that promote/deploy an application to an environment. Whereas, from my understanding, the new team im on wants to gather data, normalize said data, and upload it to a data warehouse. So from a tech stack perspective would it make sense to also use tools that offer ci/cd pipelines to achieve that? Like basically break up their script into stages so i can also create stages to test/ensure quality of the data prior to moving on to the next step, as opposed to traditionally having devop-specific stages .Or are there some obvious reasons that im just not aware of as to why people use tools like domino, aws glue, snowflake or airflow instead? Like maybe its an issue of scalability when dealing with large volumes of data? Any feedback would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19e6ofb", "is_robot_indexable": true, "report_reasons": null, "author": "monkeypaw64", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e6ofb/sw_engineer_learning_about_etl_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e6ofb/sw_engineer_learning_about_etl_pipelines/", "subreddit_subscribers": 155310, "created_utc": 1706064579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm attempting to connect to a vendor API (Invoca). They provide only an OAuth token, which I'm supposed to pass as a header/parameter. My options for a REST linked service in ADF either require a username and password (when using Basic method) or need a token generating endpoint, client ID, and secret (when using OAuth2). Any idea how I can make this connection?", "author_fullname": "t2_cknqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF Linked Service to REST API with only token - how?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e6bpi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706063556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m attempting to connect to a vendor API (Invoca). They provide only an OAuth token, which I&amp;#39;m supposed to pass as a header/parameter. My options for a REST linked service in ADF either require a username and password (when using Basic method) or need a token generating endpoint, client ID, and secret (when using OAuth2). Any idea how I can make this connection?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "19e6bpi", "is_robot_indexable": true, "report_reasons": null, "author": "Sub1ime14", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e6bpi/adf_linked_service_to_rest_api_with_only_token_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e6bpi/adf_linked_service_to_rest_api_with_only_token_how/", "subreddit_subscribers": 155310, "created_utc": 1706063556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm currently doing Masters in Ai and Data and I don't have a CS bachelor degree not even related to it. It's in Finance/Business. \n\nI have a few months till I graduate. I have had few projects using Python, I'm familiar with SQL. I know I need to get better at both of them. \n\nI also wanted to do the Microsoft DP-203 and the AWS Data Engineer Associate / Solution Architect. 1. To fill cv and 2. To be familiar with cloud. \n\nWhat else should I spend the next few months doing? And do I have a chance at landing my first job soon or no? Is this enough or not? \n\nI'm in London (UK) if that helps.", "author_fullname": "t2_rrtbk3gwh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you do it? Make my plan better.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_19e2m1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706053071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m currently doing Masters in Ai and Data and I don&amp;#39;t have a CS bachelor degree not even related to it. It&amp;#39;s in Finance/Business. &lt;/p&gt;\n\n&lt;p&gt;I have a few months till I graduate. I have had few projects using Python, I&amp;#39;m familiar with SQL. I know I need to get better at both of them. &lt;/p&gt;\n\n&lt;p&gt;I also wanted to do the Microsoft DP-203 and the AWS Data Engineer Associate / Solution Architect. 1. To fill cv and 2. To be familiar with cloud. &lt;/p&gt;\n\n&lt;p&gt;What else should I spend the next few months doing? And do I have a chance at landing my first job soon or no? Is this enough or not? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in London (UK) if that helps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "19e2m1m", "is_robot_indexable": true, "report_reasons": null, "author": "Timeframe98", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/19e2m1m/how_would_you_do_it_make_my_plan_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/19e2m1m/how_would_you_do_it_make_my_plan_better/", "subreddit_subscribers": 155310, "created_utc": 1706053071.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}