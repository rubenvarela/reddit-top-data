{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don't know how to react to this situation, and the code he sent is flawless, I'm at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?", "author_fullname": "t2_jtekxc8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I be offended? Project manager send me a code from Chatgpt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vdch8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704045945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don&amp;#39;t know how to react to this situation, and the code he sent is flawless, I&amp;#39;m at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vdch8", "is_robot_indexable": true, "report_reasons": null, "author": "Zack-s21", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/", "subreddit_subscribers": 149775, "created_utc": 1704045945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What\u2019s the differentiator/ draw from an architectural standpoint apart from the obvious cost savings facet?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why can\u2019t I just rely on AWS native services instead of running Databricks clusters on AWS for running the analogous workloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vb0gt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704039404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the differentiator/ draw from an architectural standpoint apart from the obvious cost savings facet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vb0gt", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vb0gt/why_cant_i_just_rely_on_aws_native_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vb0gt/why_cant_i_just_rely_on_aws_native_services/", "subreddit_subscribers": 149775, "created_utc": 1704039404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm currently evaluating different data storage and transformation strategies for a complex data set and would appreciate your insights or recommendations.\n\n### Current Setup and Challenge\n\n1. **Data Collection**: We extract data from sources such as AWS IAM service.  \nThis data includes entities such as users, groups, and policies, which inherently have many-to-many relationships.\n2. **Data Storage**: The extracted data is initially loaded into an S3 bucket in a JSON format.\n3. **Data Transformation**: Currently, we perform data transformation in-memory. This process involves handling the complex many-to-many relationships and preparing the data for final storage.\n4. **Final Storage**: The transformed data is then loaded into a PostgreSQL database.\n\nManaging these complex relationships is becoming increasingly challenging, especially as our data volume grows (can be 1M+ records for every entity).  \nWe are considering whether a traditional RDBMS like PostgreSQL is the best approach or if we should pivot to a Data Warehouse solution or even explore other database types like GraphDBs or Lakehouse solutions such as Databricks.\n\n### Data Modeling in PostgreSQL\n\nFor PostgreSQL, our model involves tables for users, groups, and policies, along with bridge tables for the many-to-many relationships:\n\n* **Tables**: Users, Groups, Policies.\n* **Bridge Tables**: Users\\_Groups (connecting users to groups), Groups\\_Policies (connecting groups to policies), Users\\_Policies (connecting users to policies)\n\n#### Example PostgreSQL Query\n\nA typical query we use to find all policies of a specific user (including those obtained through groups) is:\n\n    SELECT DISTINCT p.policy_name FROM users u \n    LEFT JOIN users_groups ug ON u.user_id = ug.user_id \n    LEFT JOIN groups_policies gp ON ug.group_id = gp.group_id \n    LEFT JOIN users_policies up ON ug.group_id = up.group_id \n    JOIN policies p ON gp.permission_id = p.policy_id OR up.policy_id = p.policy_id WHERE u.user_name = 'XYZ';\n\n### Considering Data Warehouse (e.g., BigQuery)\n\nIn contrast, a Data Warehouse approach like BigQuery would involve a denormalized fact table, potentially simplifying queries:\n\n* **Fact Table**: User\\_Group\\_Policy\\_Facts (consolidating user, group, and permission data).\n* **Dimension Tables**: Users, Groups, Policies.\n\n#### Example Data Warehouse Query\n\nTo find all policy IDs associated with a specific user (both directly and through groups):\n\n    SELECT DISTINCT f.policy_id FROM User_Group_Policy_Facts f \n    WHERE f.user_id = [User ID] OR (\n                                    f.group_id IS NOT NULL AND\n                                    f.group_id IN (SELECT group_id \n                                                    FROM User_Group_Policy_Facts\n                                                    WHERE user_id = [User ID]));\n\n### \n\n### Seeking Suggestions On\n\n* **Database Selection**: Considering alternatives like Data Warehouses (Snowflake, BigQuery), GraphDBs, or Lakehouse solutions (Databricks) for our data storage and querying needs. Key parameters for our database selection include:\n   * Query Performance\n   * Pricing Model\n   * On-Premise capabilities\n   * Scalability\n   * Streaming Data Support (not mandatory in our case, but it is a consideration)\n   * \u2026\n* **Data Transformation**: Exploring more efficient transformation processes, possibly using DBT or transforming data on S3 (with Spark) or directly in the database.\n* **Handling Complex Relationships**: Advice on managing many-to-many relationships effectively, especially in a scalable and performant manner.\n* **Scalability and Performance**: Best practices or recommendations for architectural changes to improve scalability and query efficiency.  \n\n\nI'm particularly interested in hearing about experiences with similar data models and the trade-offs you've encountered in different database environments.\n\nThank you for your time and insights!", "author_fullname": "t2_64lh2w2sq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rethinking Data Architecture: What's Your Ideal Setup for Standard Many-to-Many relationships model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vbprm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704041346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently evaluating different data storage and transformation strategies for a complex data set and would appreciate your insights or recommendations.&lt;/p&gt;\n\n&lt;h3&gt;Current Setup and Challenge&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: We extract data from sources such as AWS IAM service.&lt;br/&gt;\nThis data includes entities such as users, groups, and policies, which inherently have many-to-many relationships.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt;: The extracted data is initially loaded into an S3 bucket in a JSON format.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Transformation&lt;/strong&gt;: Currently, we perform data transformation in-memory. This process involves handling the complex many-to-many relationships and preparing the data for final storage.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Final Storage&lt;/strong&gt;: The transformed data is then loaded into a PostgreSQL database.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Managing these complex relationships is becoming increasingly challenging, especially as our data volume grows (can be 1M+ records for every entity).&lt;br/&gt;\nWe are considering whether a traditional RDBMS like PostgreSQL is the best approach or if we should pivot to a Data Warehouse solution or even explore other database types like GraphDBs or Lakehouse solutions such as Databricks.&lt;/p&gt;\n\n&lt;h3&gt;Data Modeling in PostgreSQL&lt;/h3&gt;\n\n&lt;p&gt;For PostgreSQL, our model involves tables for users, groups, and policies, along with bridge tables for the many-to-many relationships:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt;: Users, Groups, Policies.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bridge Tables&lt;/strong&gt;: Users_Groups (connecting users to groups), Groups_Policies (connecting groups to policies), Users_Policies (connecting users to policies)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4&gt;Example PostgreSQL Query&lt;/h4&gt;\n\n&lt;p&gt;A typical query we use to find all policies of a specific user (including those obtained through groups) is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT p.policy_name FROM users u \nLEFT JOIN users_groups ug ON u.user_id = ug.user_id \nLEFT JOIN groups_policies gp ON ug.group_id = gp.group_id \nLEFT JOIN users_policies up ON ug.group_id = up.group_id \nJOIN policies p ON gp.permission_id = p.policy_id OR up.policy_id = p.policy_id WHERE u.user_name = &amp;#39;XYZ&amp;#39;;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Considering Data Warehouse (e.g., BigQuery)&lt;/h3&gt;\n\n&lt;p&gt;In contrast, a Data Warehouse approach like BigQuery would involve a denormalized fact table, potentially simplifying queries:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fact Table&lt;/strong&gt;: User_Group_Policy_Facts (consolidating user, group, and permission data).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dimension Tables&lt;/strong&gt;: Users, Groups, Policies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4&gt;Example Data Warehouse Query&lt;/h4&gt;\n\n&lt;p&gt;To find all policy IDs associated with a specific user (both directly and through groups):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT f.policy_id FROM User_Group_Policy_Facts f \nWHERE f.user_id = [User ID] OR (\n                                f.group_id IS NOT NULL AND\n                                f.group_id IN (SELECT group_id \n                                                FROM User_Group_Policy_Facts\n                                                WHERE user_id = [User ID]));\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Seeking Suggestions On&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Database Selection&lt;/strong&gt;: Considering alternatives like Data Warehouses (Snowflake, BigQuery), GraphDBs, or Lakehouse solutions (Databricks) for our data storage and querying needs. Key parameters for our database selection include:\n\n&lt;ul&gt;\n&lt;li&gt;Query Performance&lt;/li&gt;\n&lt;li&gt;Pricing Model&lt;/li&gt;\n&lt;li&gt;On-Premise capabilities&lt;/li&gt;\n&lt;li&gt;Scalability&lt;/li&gt;\n&lt;li&gt;Streaming Data Support (not mandatory in our case, but it is a consideration)&lt;/li&gt;\n&lt;li&gt;\u2026&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Transformation&lt;/strong&gt;: Exploring more efficient transformation processes, possibly using DBT or transforming data on S3 (with Spark) or directly in the database.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handling Complex Relationships&lt;/strong&gt;: Advice on managing many-to-many relationships effectively, especially in a scalable and performant manner.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalability and Performance&lt;/strong&gt;: Best practices or recommendations for architectural changes to improve scalability and query efficiency.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in hearing about experiences with similar data models and the trade-offs you&amp;#39;ve encountered in different database environments.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vbprm", "is_robot_indexable": true, "report_reasons": null, "author": "ewenField", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vbprm/rethinking_data_architecture_whats_your_ideal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vbprm/rethinking_data_architecture_whats_your_ideal/", "subreddit_subscribers": 149775, "created_utc": 1704041346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What\u2019s the best way so I can\u2019t practice writing my etl before pushing it to prod?", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have an etl that is using python to extract data from an api and land it in s3 the write to Postgres - how do you manage local / dev / qa/ prod envs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vsz8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704099917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the best way so I can\u2019t practice writing my etl before pushing it to prod?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vsz8v", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vsz8v/if_you_have_an_etl_that_is_using_python_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vsz8v/if_you_have_an_etl_that_is_using_python_to/", "subreddit_subscribers": 149775, "created_utc": 1704099917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   \n\n\nIf you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. ", "author_fullname": "t2_auriunhuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happy new year engineers and data enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vubnp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704105850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   &lt;/p&gt;\n\n&lt;p&gt;If you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vubnp", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Rub-3984", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "subreddit_subscribers": 149775, "created_utc": 1704105850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\nMy questions are,\nIs there a SQL software that permits this? Please suggest some names that are available for free or are open source.\nIs there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)\nIs there anything we can do to improve the project?", "author_fullname": "t2_hqiwxblm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB solutions for remote student project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vkqlm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704068088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big &amp;#39;data&amp;#39; set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\nMy questions are,\nIs there a SQL software that permits this? Please suggest some names that are available for free or are open source.\nIs there a way to connect the flow of data? (I&amp;#39;m considering to completely use SQL to perform the ETL tasks)\nIs there anything we can do to improve the project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vkqlm", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Reason59", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vkqlm/db_solutions_for_remote_student_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vkqlm/db_solutions_for_remote_student_project/", "subreddit_subscribers": 149775, "created_utc": 1704068088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone using Unity Catalog extensively at their org? Looking for honest reviews on performance, ease of use, and whether the value add is worth having the additional overhead of yet another tool.\n\nI\u2019ve been skeptical of some goals Databricks has claimed in the past to be open and compatible with a variety of open source technologies. However, with the announcement of [Unity Lakehouse Federation](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog) and the [Open Apache Hive Metastore API](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api), I\u2019m starting to see that they are pretty serious about this. \n\nWe\u2019ve got a few Postgres databases that have been used as both ODS and historically as data warehouse but also have a BigQuery instance where we\u2019ve put larger datasets, for reference. Direct query performance of Postgres has been good but we usually find BigQuery lacking. Also have yet to work in Databricks at all and honestly not a huge fan of their transformation framework.", "author_fullname": "t2_ahqse5d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unity Catalog Opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vg9oj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704054233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone using Unity Catalog extensively at their org? Looking for honest reviews on performance, ease of use, and whether the value add is worth having the additional overhead of yet another tool.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been skeptical of some goals Databricks has claimed in the past to be open and compatible with a variety of open source technologies. However, with the announcement of &lt;a href=\"https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog\"&gt;Unity Lakehouse Federation&lt;/a&gt; and the &lt;a href=\"https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api\"&gt;Open Apache Hive Metastore API&lt;/a&gt;, I\u2019m starting to see that they are pretty serious about this. &lt;/p&gt;\n\n&lt;p&gt;We\u2019ve got a few Postgres databases that have been used as both ODS and historically as data warehouse but also have a BigQuery instance where we\u2019ve put larger datasets, for reference. Direct query performance of Postgres has been good but we usually find BigQuery lacking. Also have yet to work in Databricks at all and honestly not a huge fan of their transformation framework.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?auto=webp&amp;s=96ccb9c5fa71e5862cea53afeaeafa5d1ebba14f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9dccb735338e1f279dbfb21ea3128600f216b866", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=079a23daa31cf6fe382b3871c9e1e3f4d0f9bd1e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=543e8436e65ba5aaf5ef93a623d721ae2ef4dea6", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2103d4e7d59721b2eece5daddcbd945c296f8b12", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7f9a8fd13fa11c74badfdf02145d9773c6ae300", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9e66a6561f2be08cbb3ddf1ab97f5e6ce6bb913", "width": 1080, "height": 565}], "variants": {}, "id": "8ooGHNa4aTZTS_509BAc4_8M-bm112QUmcYDDgtCxXA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vg9oj", "is_robot_indexable": true, "report_reasons": null, "author": "Express-Comb8675", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vg9oj/unity_catalog_opinions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vg9oj/unity_catalog_opinions/", "subreddit_subscribers": 149775, "created_utc": 1704054233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.\n\nConsider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. \n\nSince the data volume is so huge, I'm currently finding delta of A between yesterday vs today's record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I'm looking for a plan over here. I've a buffer time of 8-9 hours for completion of the job.\n\nAny suggestions would be appreciated!!\n\nUpdate: logic for exact match, partial match and no match.\n\nSo when it comes to comparison logic on high level, for an IP, if source, destination,port, and protocols matches exactly then that is an exact match. The logic of partial match is if any one of them does not matches keeping other fields matching then it's a partial match. And if none of them is matching then it's no match.\n", "author_fullname": "t2_pl5rcgmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Reconciliation in PySpark SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vr5ja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704106042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704092066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.&lt;/p&gt;\n\n&lt;p&gt;Consider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. &lt;/p&gt;\n\n&lt;p&gt;Since the data volume is so huge, I&amp;#39;m currently finding delta of A between yesterday vs today&amp;#39;s record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I&amp;#39;m looking for a plan over here. I&amp;#39;ve a buffer time of 8-9 hours for completion of the job.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Update: logic for exact match, partial match and no match.&lt;/p&gt;\n\n&lt;p&gt;So when it comes to comparison logic on high level, for an IP, if source, destination,port, and protocols matches exactly then that is an exact match. The logic of partial match is if any one of them does not matches keeping other fields matching then it&amp;#39;s a partial match. And if none of them is matching then it&amp;#39;s no match.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vr5ja", "is_robot_indexable": true, "report_reasons": null, "author": "AdQueasy6234", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "subreddit_subscribers": 149775, "created_utc": 1704092066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to copy data from a snowflake warehouse to my organizations bigquery warehouse. There seem to be a dizzying array of authentication, authorization, and extract methods. Just wondering what is the typical way to do this? I see there is a way to copy directly into gcs but not sure who runs or how the COPY INTO command works on the snowflake side.", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake to BQ extract", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vd2es", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704045146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to copy data from a snowflake warehouse to my organizations bigquery warehouse. There seem to be a dizzying array of authentication, authorization, and extract methods. Just wondering what is the typical way to do this? I see there is a way to copy directly into gcs but not sure who runs or how the COPY INTO command works on the snowflake side.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vd2es", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vd2es/snowflake_to_bq_extract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vd2es/snowflake_to_bq_extract/", "subreddit_subscribers": 149775, "created_utc": 1704045146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cleaning up data with sed\n\nSo at work. I have this giant pipe delimited file from a dos server and ofncourse I'm dealing with carriage returns line feed he'll. Without giving away too much info, a few fields have essay sized note columns and I need to basically erase those. But there were multiple carriage return line feeds that make it hard to parse. \n\nEnter Sed.\n\nSed combined with tr have been amazing but still having issues.ive used from Unix and unixtodos to get rid of these line breaks but then I'm stuck with the same issue, just parsing \\n instead of \\r\\n.\n\nMy colleagues are lacking in advice bc they say shkt like \"oh just load it in pandas \" \u2620\ufe0f like you can see my issue here. Also the spreadsheers a few gigabytes so vash scrupting is much more efficient for parsing than an interpreter language.\n\nIt's proprietary data so I can't share examples other than vague recreations but it looks like this rn:\n\nCol1|col2|...|bad_col|bad_col2|...|end_col\\n\n\nAaaa|bbb|...|\\n\n\nBla bla blabla\\n\n\n\\n\n\nMore bad data..\\n\n\n|Another essay data...\\n\n\n\\n\n\nmore data...\\n\n\n|more bad data...\\n\n\n\\n\n\nMore worthless text data\\n\n\n|good|from|...|here|row should end.\\n\n\nNew|row|starts\\n\n\nAnd now it beings...\\n\n\n|all over again.\\n\n\n...\\n\n\n|and so on...\n\n\n\nAll pipe delimiters are at the front of each line that would be it's own field i git that far. I need.to find a way to erase all.the junk between a line that starts with a pipe until the next line that starts with a pipe\nI've been hitting the sed faq and Messing with conditionals but in driving myself nuts. Any advice or wisdom on using sed or tr or any other packages to parse spreadsheets delimited ny pipes, with no quotations, where some columns have multiple paragraphs? \n\nI'll upload my sed script I'm working with but I don't wanna sign into my reddit on my company owned lapt9p lol", "author_fullname": "t2_3vm76xzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning data with sed and tr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vkg7o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704067152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cleaning up data with sed&lt;/p&gt;\n\n&lt;p&gt;So at work. I have this giant pipe delimited file from a dos server and ofncourse I&amp;#39;m dealing with carriage returns line feed he&amp;#39;ll. Without giving away too much info, a few fields have essay sized note columns and I need to basically erase those. But there were multiple carriage return line feeds that make it hard to parse. &lt;/p&gt;\n\n&lt;p&gt;Enter Sed.&lt;/p&gt;\n\n&lt;p&gt;Sed combined with tr have been amazing but still having issues.ive used from Unix and unixtodos to get rid of these line breaks but then I&amp;#39;m stuck with the same issue, just parsing \\n instead of \\r\\n.&lt;/p&gt;\n\n&lt;p&gt;My colleagues are lacking in advice bc they say shkt like &amp;quot;oh just load it in pandas &amp;quot; \u2620\ufe0f like you can see my issue here. Also the spreadsheers a few gigabytes so vash scrupting is much more efficient for parsing than an interpreter language.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s proprietary data so I can&amp;#39;t share examples other than vague recreations but it looks like this rn:&lt;/p&gt;\n\n&lt;p&gt;Col1|col2|...|bad_col|bad_col2|...|end_col\\n&lt;/p&gt;\n\n&lt;p&gt;Aaaa|bbb|...|\\n&lt;/p&gt;\n\n&lt;p&gt;Bla bla blabla\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;More bad data..\\n&lt;/p&gt;\n\n&lt;p&gt;|Another essay data...\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;more data...\\n&lt;/p&gt;\n\n&lt;p&gt;|more bad data...\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;More worthless text data\\n&lt;/p&gt;\n\n&lt;p&gt;|good|from|...|here|row should end.\\n&lt;/p&gt;\n\n&lt;p&gt;New|row|starts\\n&lt;/p&gt;\n\n&lt;p&gt;And now it beings...\\n&lt;/p&gt;\n\n&lt;p&gt;|all over again.\\n&lt;/p&gt;\n\n&lt;p&gt;...\\n&lt;/p&gt;\n\n&lt;p&gt;|and so on...&lt;/p&gt;\n\n&lt;p&gt;All pipe delimiters are at the front of each line that would be it&amp;#39;s own field i git that far. I need.to find a way to erase all.the junk between a line that starts with a pipe until the next line that starts with a pipe\nI&amp;#39;ve been hitting the sed faq and Messing with conditionals but in driving myself nuts. Any advice or wisdom on using sed or tr or any other packages to parse spreadsheets delimited ny pipes, with no quotations, where some columns have multiple paragraphs? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll upload my sed script I&amp;#39;m working with but I don&amp;#39;t wanna sign into my reddit on my company owned lapt9p lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vkg7o", "is_robot_indexable": true, "report_reasons": null, "author": "JucheCouture69420", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vkg7o/cleaning_data_with_sed_and_tr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vkg7o/cleaning_data_with_sed_and_tr/", "subreddit_subscribers": 149775, "created_utc": 1704067152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs", "author_fullname": "t2_8fb47pbl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Data Engineering Icon Packs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18vw89a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704113805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vw89a", "is_robot_indexable": true, "report_reasons": null, "author": "ForMrKite", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "subreddit_subscribers": 149775, "created_utc": 1704113805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to migrate files from Aliyun OSS to GCP Cloud Storage. As I see, Transfer service just supports AWS and Azure Blob storage natively.\n\nWorkaround will be to use Rclone for this job, but is there any other alternative?\n\nAlso, is there any better easy way to transfer files from private blob storage located in public cloud such as Aliyun to GCP cloud storage?", "author_fullname": "t2_8xdeh5fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aliyun OSS to GCP Cloud Storage migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18vw3fi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704113251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to migrate files from Aliyun OSS to GCP Cloud Storage. As I see, Transfer service just supports AWS and Azure Blob storage natively.&lt;/p&gt;\n\n&lt;p&gt;Workaround will be to use Rclone for this job, but is there any other alternative?&lt;/p&gt;\n\n&lt;p&gt;Also, is there any better easy way to transfer files from private blob storage located in public cloud such as Aliyun to GCP cloud storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vw3fi", "is_robot_indexable": true, "report_reasons": null, "author": "Winter-Activity-6938", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vw3fi/aliyun_oss_to_gcp_cloud_storage_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vw3fi/aliyun_oss_to_gcp_cloud_storage_migration/", "subreddit_subscribers": 149775, "created_utc": 1704113251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?", "author_fullname": "t2_igetxkds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Contribute to DE projects or consultancy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vuscb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704107870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vuscb", "is_robot_indexable": true, "report_reasons": null, "author": "Azar_e", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "subreddit_subscribers": 149775, "created_utc": 1704107870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\n\nMy questions are, \n\n* Is there a SQL software that permits this? Please suggest some names that are available for free or are open source. \n* Is there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)\n* Is there anything we can do to improve the project? \n\n&amp;#x200B;", "author_fullname": "t2_qpoqsjlmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB solution for remote student project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vko16", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704067853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big &amp;#39;data&amp;#39; set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.&lt;/p&gt;\n\n&lt;p&gt;My questions are, &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there a SQL software that permits this? Please suggest some names that are available for free or are open source. &lt;/li&gt;\n&lt;li&gt;Is there a way to connect the flow of data? (I&amp;#39;m considering to completely use SQL to perform the ETL tasks)&lt;/li&gt;\n&lt;li&gt;Is there anything we can do to improve the project? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vko16", "is_robot_indexable": true, "report_reasons": null, "author": "Dull-Atmosphere8478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vko16/db_solution_for_remote_student_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vko16/db_solution_for_remote_student_project/", "subreddit_subscribers": 149775, "created_utc": 1704067853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can't seem to piece that in the puzzle", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What product/service under GCP/AWS equate to delta live tables in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vb7l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704039948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can&amp;#39;t seem to piece that in the puzzle&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vb7l8", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vb7l8/what_productservice_under_gcpaws_equate_to_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vb7l8/what_productservice_under_gcpaws_equate_to_delta/", "subreddit_subscribers": 149775, "created_utc": 1704039948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi reddit community, I have been working on personal project and I am unsure on  how to do calculations on tuple data. My dataframe has data in the form  (x, y) in every cell and I would like to add numbers of all the y data,  depending on what x is, to a row total. I have about 1500 rows. what is  the best way of doing this? ", "author_fullname": "t2_63vh4yz6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Manipulation in pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18vv8ss", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704109827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit community, I have been working on personal project and I am unsure on  how to do calculations on tuple data. My dataframe has data in the form  (x, y) in every cell and I would like to add numbers of all the y data,  depending on what x is, to a row total. I have about 1500 rows. what is  the best way of doing this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vv8ss", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous-Country31", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vv8ss/data_manipulation_in_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vv8ss/data_manipulation_in_pandas/", "subreddit_subscribers": 149775, "created_utc": 1704109827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Griffin 2.0: Instacart Revamps Its Machine Learning Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18vubap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dtZt2quQ90KBzGqak1araM_D3A0xl6MjD3nRiSpN1fs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704105802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2024/01/instacart-machine-learning/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?auto=webp&amp;s=40d73d890134f8debeb9b96f62ae899fa5137741", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdaed62e5aae9af720747f45b2c4990185dac678", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a034d55362c9cf10c7d369e0b13681e12ccec17c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5504f8f832ca7a6c2dd47efc7ac020c7a1489b2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5653e23be2fb1c7b4dbf69c631b787bdb01b24da", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d5b0b7dd3c7f802b23851fc9b13239541230834", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/g81woglbcRJNHGbKwjjig-q9UXPn8nBOsBeeygQDKKE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c36ae786435767a916aabc8a77dd222e7c6ec655", "width": 1080, "height": 567}], "variants": {}, "id": "8aDoaX35qei2Db37igecvIif-BVwaGHRzMNcWptqVzM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18vubap", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vubap/griffin_20_instacart_revamps_its_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2024/01/instacart-machine-learning/", "subreddit_subscribers": 149775, "created_utc": 1704105802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \nWondering if there's any experienced data engineers / SWE out there who would be willing to chat a bit and share some perspective.\n\nContext:  \nNew to data engineering role, with 1 year experience. Previously 4 years as a data analyst.  \nLive &amp; work in Australia  \nLooking to understand:  \n\\- where I sit in terms of skills - particularly in industry outside of Australia as it's siloed  \n\\- next steps or how I could 'level-up' in skills or professionally  \n\\- understanding how remote jobs work  \n\n\nYour time and perspective would really be appreciated and valuable to me  \n\n\nThanks,", "author_fullname": "t2_bazyi61n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Data Engineer Seeking Experienced Engineers Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vlz7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704072233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nWondering if there&amp;#39;s any experienced data engineers / SWE out there who would be willing to chat a bit and share some perspective.&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;br/&gt;\nNew to data engineering role, with 1 year experience. Previously 4 years as a data analyst.&lt;br/&gt;\nLive &amp;amp; work in Australia&lt;br/&gt;\nLooking to understand:&lt;br/&gt;\n- where I sit in terms of skills - particularly in industry outside of Australia as it&amp;#39;s siloed&lt;br/&gt;\n- next steps or how I could &amp;#39;level-up&amp;#39; in skills or professionally&lt;br/&gt;\n- understanding how remote jobs work  &lt;/p&gt;\n\n&lt;p&gt;Your time and perspective would really be appreciated and valuable to me  &lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vlz7h", "is_robot_indexable": true, "report_reasons": null, "author": "Foe317", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vlz7h/new_data_engineer_seeking_experienced_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vlz7h/new_data_engineer_seeking_experienced_engineers/", "subreddit_subscribers": 149775, "created_utc": 1704072233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI\u2019m considering creating a data product that combines a data lake service with AI-generated insights. Despite existing products like Databricks and Snowflake, along with various AI-based insight companies - Do you think it\u2019s still a viable venture?\n\nAny comments or suggestions? Are there any gaps in the current market that you think should be addressed?\n\nThanks!", "author_fullname": "t2_8vjbfemu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Startup Idea Validation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vlbsy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704070061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering creating a data product that combines a data lake service with AI-generated insights. Despite existing products like Databricks and Snowflake, along with various AI-based insight companies - Do you think it\u2019s still a viable venture?&lt;/p&gt;\n\n&lt;p&gt;Any comments or suggestions? Are there any gaps in the current market that you think should be addressed?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vlbsy", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Guy81", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vlbsy/startup_idea_validation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vlbsy/startup_idea_validation/", "subreddit_subscribers": 149775, "created_utc": 1704070061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It's time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. \n\nFirstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. \n\nSecondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.\n\nAlso, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. \n\nThanks a lot. Looking forward to listening from you industry folks.", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Engineer Interview Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vi3gn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704059663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It&amp;#39;s time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. &lt;/p&gt;\n\n&lt;p&gt;Firstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. &lt;/p&gt;\n\n&lt;p&gt;Secondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.&lt;/p&gt;\n\n&lt;p&gt;Also, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot. Looking forward to listening from you industry folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18vi3gn", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vi3gn/azure_data_engineer_interview_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vi3gn/azure_data_engineer_interview_help/", "subreddit_subscribers": 149775, "created_utc": 1704059663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am apologizing beforehand if somehow is offended by my question.\n\nI am in a dilemma on which role should I venture into - data engineer or data scientist. I am more inclined towards data engineer as it pertains to my skill set but I am in doubt if there is a ceiling in the career growth. Like can a data engineer be offered a executive role or be a part of the board of directors?", "author_fullname": "t2_6aijectm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is hierarchically superior - data engineer or data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vev5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.13, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704050250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am apologizing beforehand if somehow is offended by my question.&lt;/p&gt;\n\n&lt;p&gt;I am in a dilemma on which role should I venture into - data engineer or data scientist. I am more inclined towards data engineer as it pertains to my skill set but I am in doubt if there is a ceiling in the career growth. Like can a data engineer be offered a executive role or be a part of the board of directors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vev5r", "is_robot_indexable": true, "report_reasons": null, "author": "X_Warrior361", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vev5r/who_is_hierarchically_superior_data_engineer_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vev5r/who_is_hierarchically_superior_data_engineer_or/", "subreddit_subscribers": 149775, "created_utc": 1704050250.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}