{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don't know how to react to this situation, and the code he sent is flawless, I'm at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?", "author_fullname": "t2_jtekxc8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I be offended? Project manager send me a code from Chatgpt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vdch8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704045945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don&amp;#39;t know how to react to this situation, and the code he sent is flawless, I&amp;#39;m at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vdch8", "is_robot_indexable": true, "report_reasons": null, "author": "Zack-s21", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/", "subreddit_subscribers": 149725, "created_utc": 1704045945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This sounds a bit impossible because versioning schema changes sounds like keeping an sql file that builds the database schema from scratch. However, you don\u2019t `alter` a database the same way that you `create` one. So there\u2019s a translation layer there that I\u2019m not sure would be easy to overcome for automation.\n\nIs this sort of pipeline possible? Has it already been built?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to set up git for versioning schema changes to Postgres, with GitHub Actions for CI/CD ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18v2vpm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704009494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This sounds a bit impossible because versioning schema changes sounds like keeping an sql file that builds the database schema from scratch. However, you don\u2019t &lt;code&gt;alter&lt;/code&gt; a database the same way that you &lt;code&gt;create&lt;/code&gt; one. So there\u2019s a translation layer there that I\u2019m not sure would be easy to overcome for automation.&lt;/p&gt;\n\n&lt;p&gt;Is this sort of pipeline possible? Has it already been built?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18v2vpm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18v2vpm/is_it_possible_to_set_up_git_for_versioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18v2vpm/is_it_possible_to_set_up_git_for_versioning/", "subreddit_subscribers": 149725, "created_utc": 1704009494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What\u2019s the differentiator/ draw from an architectural standpoint apart from the obvious cost savings facet?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why can\u2019t I just rely on AWS native services instead of running Databricks clusters on AWS for running the analogous workloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vb0gt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704039404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the differentiator/ draw from an architectural standpoint apart from the obvious cost savings facet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vb0gt", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vb0gt/why_cant_i_just_rely_on_aws_native_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vb0gt/why_cant_i_just_rely_on_aws_native_services/", "subreddit_subscribers": 149725, "created_utc": 1704039404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm currently evaluating different data storage and transformation strategies for a complex data set and would appreciate your insights or recommendations.\n\n### Current Setup and Challenge\n\n1. **Data Collection**: We extract data from sources such as AWS IAM service.  \nThis data includes entities such as users, groups, and policies, which inherently have many-to-many relationships.\n2. **Data Storage**: The extracted data is initially loaded into an S3 bucket in a JSON format.\n3. **Data Transformation**: Currently, we perform data transformation in-memory. This process involves handling the complex many-to-many relationships and preparing the data for final storage.\n4. **Final Storage**: The transformed data is then loaded into a PostgreSQL database.\n\nManaging these complex relationships is becoming increasingly challenging, especially as our data volume grows (can be 1M+ records for every entity).  \nWe are considering whether a traditional RDBMS like PostgreSQL is the best approach or if we should pivot to a Data Warehouse solution or even explore other database types like GraphDBs or Lakehouse solutions such as Databricks.\n\n### Data Modeling in PostgreSQL\n\nFor PostgreSQL, our model involves tables for users, groups, and policies, along with bridge tables for the many-to-many relationships:\n\n* **Tables**: Users, Groups, Policies.\n* **Bridge Tables**: Users\\_Groups (connecting users to groups), Groups\\_Policies (connecting groups to policies), Users\\_Policies (connecting users to policies)\n\n#### Example PostgreSQL Query\n\nA typical query we use to find all policies of a specific user (including those obtained through groups) is:\n\n    SELECT DISTINCT p.policy_name FROM users u \n    LEFT JOIN users_groups ug ON u.user_id = ug.user_id \n    LEFT JOIN groups_policies gp ON ug.group_id = gp.group_id \n    LEFT JOIN users_policies up ON ug.group_id = up.group_id \n    JOIN policies p ON gp.permission_id = p.policy_id OR up.policy_id = p.policy_id WHERE u.user_name = 'XYZ';\n\n### Considering Data Warehouse (e.g., BigQuery)\n\nIn contrast, a Data Warehouse approach like BigQuery would involve a denormalized fact table, potentially simplifying queries:\n\n* **Fact Table**: User\\_Group\\_Policy\\_Facts (consolidating user, group, and permission data).\n* **Dimension Tables**: Users, Groups, Policies.\n\n#### Example Data Warehouse Query\n\nTo find all policy IDs associated with a specific user (both directly and through groups):\n\n    SELECT DISTINCT f.policy_id FROM User_Group_Policy_Facts f \n    WHERE f.user_id = [User ID] OR (\n                                    f.group_id IS NOT NULL AND\n                                    f.group_id IN (SELECT group_id \n                                                    FROM User_Group_Policy_Facts\n                                                    WHERE user_id = [User ID]));\n\n### \n\n### Seeking Suggestions On\n\n* **Database Selection**: Considering alternatives like Data Warehouses (Snowflake, BigQuery), GraphDBs, or Lakehouse solutions (Databricks) for our data storage and querying needs. Key parameters for our database selection include:\n   * Query Performance\n   * Pricing Model\n   * On-Premise capabilities\n   * Scalability\n   * Streaming Data Support (not mandatory in our case, but it is a consideration)\n   * \u2026\n* **Data Transformation**: Exploring more efficient transformation processes, possibly using DBT or transforming data on S3 (with Spark) or directly in the database.\n* **Handling Complex Relationships**: Advice on managing many-to-many relationships effectively, especially in a scalable and performant manner.\n* **Scalability and Performance**: Best practices or recommendations for architectural changes to improve scalability and query efficiency.  \n\n\nI'm particularly interested in hearing about experiences with similar data models and the trade-offs you've encountered in different database environments.\n\nThank you for your time and insights!", "author_fullname": "t2_64lh2w2sq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rethinking Data Architecture: What's Your Ideal Setup for Standard Many-to-Many relationships model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vbprm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704041346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently evaluating different data storage and transformation strategies for a complex data set and would appreciate your insights or recommendations.&lt;/p&gt;\n\n&lt;h3&gt;Current Setup and Challenge&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: We extract data from sources such as AWS IAM service.&lt;br/&gt;\nThis data includes entities such as users, groups, and policies, which inherently have many-to-many relationships.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt;: The extracted data is initially loaded into an S3 bucket in a JSON format.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Transformation&lt;/strong&gt;: Currently, we perform data transformation in-memory. This process involves handling the complex many-to-many relationships and preparing the data for final storage.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Final Storage&lt;/strong&gt;: The transformed data is then loaded into a PostgreSQL database.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Managing these complex relationships is becoming increasingly challenging, especially as our data volume grows (can be 1M+ records for every entity).&lt;br/&gt;\nWe are considering whether a traditional RDBMS like PostgreSQL is the best approach or if we should pivot to a Data Warehouse solution or even explore other database types like GraphDBs or Lakehouse solutions such as Databricks.&lt;/p&gt;\n\n&lt;h3&gt;Data Modeling in PostgreSQL&lt;/h3&gt;\n\n&lt;p&gt;For PostgreSQL, our model involves tables for users, groups, and policies, along with bridge tables for the many-to-many relationships:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt;: Users, Groups, Policies.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bridge Tables&lt;/strong&gt;: Users_Groups (connecting users to groups), Groups_Policies (connecting groups to policies), Users_Policies (connecting users to policies)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4&gt;Example PostgreSQL Query&lt;/h4&gt;\n\n&lt;p&gt;A typical query we use to find all policies of a specific user (including those obtained through groups) is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT p.policy_name FROM users u \nLEFT JOIN users_groups ug ON u.user_id = ug.user_id \nLEFT JOIN groups_policies gp ON ug.group_id = gp.group_id \nLEFT JOIN users_policies up ON ug.group_id = up.group_id \nJOIN policies p ON gp.permission_id = p.policy_id OR up.policy_id = p.policy_id WHERE u.user_name = &amp;#39;XYZ&amp;#39;;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Considering Data Warehouse (e.g., BigQuery)&lt;/h3&gt;\n\n&lt;p&gt;In contrast, a Data Warehouse approach like BigQuery would involve a denormalized fact table, potentially simplifying queries:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fact Table&lt;/strong&gt;: User_Group_Policy_Facts (consolidating user, group, and permission data).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dimension Tables&lt;/strong&gt;: Users, Groups, Policies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4&gt;Example Data Warehouse Query&lt;/h4&gt;\n\n&lt;p&gt;To find all policy IDs associated with a specific user (both directly and through groups):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT f.policy_id FROM User_Group_Policy_Facts f \nWHERE f.user_id = [User ID] OR (\n                                f.group_id IS NOT NULL AND\n                                f.group_id IN (SELECT group_id \n                                                FROM User_Group_Policy_Facts\n                                                WHERE user_id = [User ID]));\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Seeking Suggestions On&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Database Selection&lt;/strong&gt;: Considering alternatives like Data Warehouses (Snowflake, BigQuery), GraphDBs, or Lakehouse solutions (Databricks) for our data storage and querying needs. Key parameters for our database selection include:\n\n&lt;ul&gt;\n&lt;li&gt;Query Performance&lt;/li&gt;\n&lt;li&gt;Pricing Model&lt;/li&gt;\n&lt;li&gt;On-Premise capabilities&lt;/li&gt;\n&lt;li&gt;Scalability&lt;/li&gt;\n&lt;li&gt;Streaming Data Support (not mandatory in our case, but it is a consideration)&lt;/li&gt;\n&lt;li&gt;\u2026&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Transformation&lt;/strong&gt;: Exploring more efficient transformation processes, possibly using DBT or transforming data on S3 (with Spark) or directly in the database.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handling Complex Relationships&lt;/strong&gt;: Advice on managing many-to-many relationships effectively, especially in a scalable and performant manner.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalability and Performance&lt;/strong&gt;: Best practices or recommendations for architectural changes to improve scalability and query efficiency.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in hearing about experiences with similar data models and the trade-offs you&amp;#39;ve encountered in different database environments.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vbprm", "is_robot_indexable": true, "report_reasons": null, "author": "ewenField", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vbprm/rethinking_data_architecture_whats_your_ideal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vbprm/rethinking_data_architecture_whats_your_ideal/", "subreddit_subscribers": 149725, "created_utc": 1704041346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone using Unity Catalog extensively at their org? Looking for honest reviews on performance, ease of use, and whether the value add is worth having the additional overhead of yet another tool.\n\nI\u2019ve been skeptical of some goals Databricks has claimed in the past to be open and compatible with a variety of open source technologies. However, with the announcement of [Unity Lakehouse Federation](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog) and the [Open Apache Hive Metastore API](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api), I\u2019m starting to see that they are pretty serious about this. \n\nWe\u2019ve got a few Postgres databases that have been used as both ODS and historically as data warehouse but also have a BigQuery instance where we\u2019ve put larger datasets, for reference. Direct query performance of Postgres has been good but we usually find BigQuery lacking. Also have yet to work in Databricks at all and honestly not a huge fan of their transformation framework.", "author_fullname": "t2_ahqse5d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unity Catalog Opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vg9oj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704054233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone using Unity Catalog extensively at their org? Looking for honest reviews on performance, ease of use, and whether the value add is worth having the additional overhead of yet another tool.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been skeptical of some goals Databricks has claimed in the past to be open and compatible with a variety of open source technologies. However, with the announcement of &lt;a href=\"https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog\"&gt;Unity Lakehouse Federation&lt;/a&gt; and the &lt;a href=\"https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api\"&gt;Open Apache Hive Metastore API&lt;/a&gt;, I\u2019m starting to see that they are pretty serious about this. &lt;/p&gt;\n\n&lt;p&gt;We\u2019ve got a few Postgres databases that have been used as both ODS and historically as data warehouse but also have a BigQuery instance where we\u2019ve put larger datasets, for reference. Direct query performance of Postgres has been good but we usually find BigQuery lacking. Also have yet to work in Databricks at all and honestly not a huge fan of their transformation framework.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?auto=webp&amp;s=96ccb9c5fa71e5862cea53afeaeafa5d1ebba14f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9dccb735338e1f279dbfb21ea3128600f216b866", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=079a23daa31cf6fe382b3871c9e1e3f4d0f9bd1e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=543e8436e65ba5aaf5ef93a623d721ae2ef4dea6", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2103d4e7d59721b2eece5daddcbd945c296f8b12", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7f9a8fd13fa11c74badfdf02145d9773c6ae300", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/LYuZWUPaC0Y5y846LBxlaGedfzW9ylpT8kLzA05oH88.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9e66a6561f2be08cbb3ddf1ab97f5e6ce6bb913", "width": 1080, "height": 565}], "variants": {}, "id": "8ooGHNa4aTZTS_509BAc4_8M-bm112QUmcYDDgtCxXA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vg9oj", "is_robot_indexable": true, "report_reasons": null, "author": "Express-Comb8675", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vg9oj/unity_catalog_opinions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vg9oj/unity_catalog_opinions/", "subreddit_subscribers": 149725, "created_utc": 1704054233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\nMy questions are,\nIs there a SQL software that permits this? Please suggest some names that are available for free or are open source.\nIs there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)\nIs there anything we can do to improve the project?", "author_fullname": "t2_hqiwxblm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB solutions for remote student project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vkqlm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704068088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big &amp;#39;data&amp;#39; set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\nMy questions are,\nIs there a SQL software that permits this? Please suggest some names that are available for free or are open source.\nIs there a way to connect the flow of data? (I&amp;#39;m considering to completely use SQL to perform the ETL tasks)\nIs there anything we can do to improve the project?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vkqlm", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Reason59", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vkqlm/db_solutions_for_remote_student_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vkqlm/db_solutions_for_remote_student_project/", "subreddit_subscribers": 149725, "created_utc": 1704068088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn Spark  via Databricks Community edition or Local Hadoop environment in Ubuntu\n\n\nHi DEs\nI am a data analyst with 5YOE from India and have worked on ETL  using SQL(Redshift, Oracle, MySQL). I also worked on Python mainly pandas for data crunching. \n\nI want to get into Data Engineering and I have to learn Spark. Should I learn and hands on Using browser based Databricks community edition or install hadoop, Hive, Spark on my Ubuntu laptop .\n\n\nDoes it even makes any difference learning from Pre-configured Data bricks and installing and configuring Hadoop enviromenr on my laptop?", "author_fullname": "t2_l6b7kbp7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn Spark via Databricks Community edition or Local Hadoop environment in Ubuntu", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18v4s1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704017271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn Spark  via Databricks Community edition or Local Hadoop environment in Ubuntu&lt;/p&gt;\n\n&lt;p&gt;Hi DEs\nI am a data analyst with 5YOE from India and have worked on ETL  using SQL(Redshift, Oracle, MySQL). I also worked on Python mainly pandas for data crunching. &lt;/p&gt;\n\n&lt;p&gt;I want to get into Data Engineering and I have to learn Spark. Should I learn and hands on Using browser based Databricks community edition or install hadoop, Hive, Spark on my Ubuntu laptop .&lt;/p&gt;\n\n&lt;p&gt;Does it even makes any difference learning from Pre-configured Data bricks and installing and configuring Hadoop enviromenr on my laptop?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18v4s1q", "is_robot_indexable": true, "report_reasons": null, "author": "vainothisside", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18v4s1q/learn_spark_via_databricks_community_edition_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18v4s1q/learn_spark_via_databricks_community_edition_or/", "subreddit_subscribers": 149725, "created_utc": 1704017271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing Webhooks and Event Consumption", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18v2o1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/yGaRLQJ8TGvUs_lbyU-jgN5vg7jMmJ5wkL90qZ_h2H8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704008615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@memphis-dev/comparing-webhooks-and-event-consumption-38225e3b5d9d", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?auto=webp&amp;s=d095abcba7f13fa9c9cdbd34e06f59233270f8c7", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c4df145009a2062cb3ccb60917f4dadb1fbac8c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d238ef3a92900924695febaf861063ca2c38d49", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=01bcaa644d016c9e92007ecd4eb4f4702979e1ad", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=536feeb16a046abb7953a84b7c713a15820069d8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc72ce4895e347a9aa2129db9039eb8415f4eb40", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/pdiFNMavIr9dRAwee28tx3dsqVomCxO20zA50KPP5MQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3db2d728f6eab4e85d2cd90ad9161ce986182f4", "width": 1080, "height": 607}], "variants": {}, "id": "LMW5vuEmRJ9Heh5ykxM7cxKnlZ1oSE0svlI8SOmjhA0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18v2o1m", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18v2o1m/comparing_webhooks_and_event_consumption/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@memphis-dev/comparing-webhooks-and-event-consumption-38225e3b5d9d", "subreddit_subscribers": 149725, "created_utc": 1704008615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to copy data from a snowflake warehouse to my organizations bigquery warehouse. There seem to be a dizzying array of authentication, authorization, and extract methods. Just wondering what is the typical way to do this? I see there is a way to copy directly into gcs but not sure who runs or how the COPY INTO command works on the snowflake side.", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake to BQ extract", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vd2es", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704045146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to copy data from a snowflake warehouse to my organizations bigquery warehouse. There seem to be a dizzying array of authentication, authorization, and extract methods. Just wondering what is the typical way to do this? I see there is a way to copy directly into gcs but not sure who runs or how the COPY INTO command works on the snowflake side.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vd2es", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vd2es/snowflake_to_bq_extract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vd2es/snowflake_to_bq_extract/", "subreddit_subscribers": 149725, "created_utc": 1704045146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cleaning up data with sed\n\nSo at work. I have this giant pipe delimited file from a dos server and ofncourse I'm dealing with carriage returns line feed he'll. Without giving away too much info, a few fields have essay sized note columns and I need to basically erase those. But there were multiple carriage return line feeds that make it hard to parse. \n\nEnter Sed.\n\nSed combined with tr have been amazing but still having issues.ive used from Unix and unixtodos to get rid of these line breaks but then I'm stuck with the same issue, just parsing \\n instead of \\r\\n.\n\nMy colleagues are lacking in advice bc they say shkt like \"oh just load it in pandas \" \u2620\ufe0f like you can see my issue here. Also the spreadsheers a few gigabytes so vash scrupting is much more efficient for parsing than an interpreter language.\n\nIt's proprietary data so I can't share examples other than vague recreations but it looks like this rn:\n\nCol1|col2|...|bad_col|bad_col2|...|end_col\\n\n\nAaaa|bbb|...|\\n\n\nBla bla blabla\\n\n\n\\n\n\nMore bad data..\\n\n\n|Another essay data...\\n\n\n\\n\n\nmore data...\\n\n\n|more bad data...\\n\n\n\\n\n\nMore worthless text data\\n\n\n|good|from|...|here|row should end.\\n\n\nNew|row|starts\\n\n\nAnd now it beings...\\n\n\n|all over again.\\n\n\n...\\n\n\n|and so on...\n\n\n\nAll pipe delimiters are at the front of each line that would be it's own field i git that far. I need.to find a way to erase all.the junk between a line that starts with a pipe until the next line that starts with a pipe\nI've been hitting the sed faq and Messing with conditionals but in driving myself nuts. Any advice or wisdom on using sed or tr or any other packages to parse spreadsheets delimited ny pipes, with no quotations, where some columns have multiple paragraphs? \n\nI'll upload my sed script I'm working with but I don't wanna sign into my reddit on my company owned lapt9p lol", "author_fullname": "t2_3vm76xzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning data with sed and tr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vkg7o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704067152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cleaning up data with sed&lt;/p&gt;\n\n&lt;p&gt;So at work. I have this giant pipe delimited file from a dos server and ofncourse I&amp;#39;m dealing with carriage returns line feed he&amp;#39;ll. Without giving away too much info, a few fields have essay sized note columns and I need to basically erase those. But there were multiple carriage return line feeds that make it hard to parse. &lt;/p&gt;\n\n&lt;p&gt;Enter Sed.&lt;/p&gt;\n\n&lt;p&gt;Sed combined with tr have been amazing but still having issues.ive used from Unix and unixtodos to get rid of these line breaks but then I&amp;#39;m stuck with the same issue, just parsing \\n instead of \\r\\n.&lt;/p&gt;\n\n&lt;p&gt;My colleagues are lacking in advice bc they say shkt like &amp;quot;oh just load it in pandas &amp;quot; \u2620\ufe0f like you can see my issue here. Also the spreadsheers a few gigabytes so vash scrupting is much more efficient for parsing than an interpreter language.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s proprietary data so I can&amp;#39;t share examples other than vague recreations but it looks like this rn:&lt;/p&gt;\n\n&lt;p&gt;Col1|col2|...|bad_col|bad_col2|...|end_col\\n&lt;/p&gt;\n\n&lt;p&gt;Aaaa|bbb|...|\\n&lt;/p&gt;\n\n&lt;p&gt;Bla bla blabla\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;More bad data..\\n&lt;/p&gt;\n\n&lt;p&gt;|Another essay data...\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;more data...\\n&lt;/p&gt;\n\n&lt;p&gt;|more bad data...\\n&lt;/p&gt;\n\n&lt;p&gt;\\n&lt;/p&gt;\n\n&lt;p&gt;More worthless text data\\n&lt;/p&gt;\n\n&lt;p&gt;|good|from|...|here|row should end.\\n&lt;/p&gt;\n\n&lt;p&gt;New|row|starts\\n&lt;/p&gt;\n\n&lt;p&gt;And now it beings...\\n&lt;/p&gt;\n\n&lt;p&gt;|all over again.\\n&lt;/p&gt;\n\n&lt;p&gt;...\\n&lt;/p&gt;\n\n&lt;p&gt;|and so on...&lt;/p&gt;\n\n&lt;p&gt;All pipe delimiters are at the front of each line that would be it&amp;#39;s own field i git that far. I need.to find a way to erase all.the junk between a line that starts with a pipe until the next line that starts with a pipe\nI&amp;#39;ve been hitting the sed faq and Messing with conditionals but in driving myself nuts. Any advice or wisdom on using sed or tr or any other packages to parse spreadsheets delimited ny pipes, with no quotations, where some columns have multiple paragraphs? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll upload my sed script I&amp;#39;m working with but I don&amp;#39;t wanna sign into my reddit on my company owned lapt9p lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vkg7o", "is_robot_indexable": true, "report_reasons": null, "author": "JucheCouture69420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vkg7o/cleaning_data_with_sed_and_tr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vkg7o/cleaning_data_with_sed_and_tr/", "subreddit_subscribers": 149725, "created_utc": 1704067152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.\n\nConsider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. \n\nSince the data volume is so huge, I'm currently finding delta of A between yesterday vs today's record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I'm looking for a plan over here. I've a buffer time of 8-9 hours for completion of the job.\n\nAny suggestions would be appreciated!!", "author_fullname": "t2_pl5rcgmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Reconciliation in PySpark SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18vr5ja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704092066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.&lt;/p&gt;\n\n&lt;p&gt;Consider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. &lt;/p&gt;\n\n&lt;p&gt;Since the data volume is so huge, I&amp;#39;m currently finding delta of A between yesterday vs today&amp;#39;s record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I&amp;#39;m looking for a plan over here. I&amp;#39;ve a buffer time of 8-9 hours for completion of the job.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be appreciated!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vr5ja", "is_robot_indexable": true, "report_reasons": null, "author": "AdQueasy6234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "subreddit_subscribers": 149725, "created_utc": 1704092066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.\n\nMy questions are, \n\n* Is there a SQL software that permits this? Please suggest some names that are available for free or are open source. \n* Is there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)\n* Is there anything we can do to improve the project? \n\n&amp;#x200B;", "author_fullname": "t2_qpoqsjlmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB solution for remote student project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vko16", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704067853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big &amp;#39;data&amp;#39; set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.&lt;/p&gt;\n\n&lt;p&gt;My questions are, &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there a SQL software that permits this? Please suggest some names that are available for free or are open source. &lt;/li&gt;\n&lt;li&gt;Is there a way to connect the flow of data? (I&amp;#39;m considering to completely use SQL to perform the ETL tasks)&lt;/li&gt;\n&lt;li&gt;Is there anything we can do to improve the project? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vko16", "is_robot_indexable": true, "report_reasons": null, "author": "Dull-Atmosphere8478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vko16/db_solution_for_remote_student_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vko16/db_solution_for_remote_student_project/", "subreddit_subscribers": 149725, "created_utc": 1704067853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can't seem to piece that in the puzzle", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What product/service under GCP/AWS equate to delta live tables in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vb7l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704039948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can&amp;#39;t seem to piece that in the puzzle&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vb7l8", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vb7l8/what_productservice_under_gcpaws_equate_to_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vb7l8/what_productservice_under_gcpaws_equate_to_delta/", "subreddit_subscribers": 149725, "created_utc": 1704039948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have started trying to use dagster for a personal project and am trying to migrate some legacy windows scheduled jobs  (I'm just using subprocess with a meaningless return value to execute these).  It's generally working well, but I have one particular issue I am struggling with.  If any ther dagster users can give me any pointers I would be very grateful.\n\nUpstream asset is a web scraper:\n\n* The target website has one page per 'url-date', and the content of the page changes.\n* Using a scheduled job, on each day, (t) the scraper should scrape data for pages dated t-1, t-2 and t-7.\n* I want to partition the asset.\n* 'url-date' is a natural partition dimension\n* age *might* be a good partition dimension too (to allow simple recording of age)\n* the scraper should only be run for one partition concurrently (due to throttling)\n\nI have no experience on dagster (other than manual, videos and trial and error I have been doing) or any similar system so could really just do with **someone telling me if I am going about things the wrong way or on the right track, please**?\n\nHere is what I have tried:\n\nSetup the scraper as an asset with multi-partition of ('url-date', 'ageInDays-at-materialization'=\\[1,2,7\\])\n\n* Attempt 1 - set up an scheduled op-job which accepts a run-date and calls materialize() 3 times on the asset for partitions:(run-date-1,2,7)\\*(t-1,2,7).  The issue here is that in the GUI the asset-partition never shows as materialized (possibly due to the job being an 'ephemeral\\_asset\\_job' and running in a temp folder / with a different default io-manager).  I suspect his is incorrect/unintended usage of materialize()\n* Attempt 2 - set up a scheduler which returns a 3 runRequests one for each (url-date=\\[run-date-1,2,7\\])\\*(age=\\[1,2,7\\]).  The issue here is that the runRequests run concurrently. To solve this I will try using 'Limiting specific runs using tags' [https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines](https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines).  Perhaps this will work...  I did originally expect the whole scheduled task to run in one run, but that might be unrealistic.\n* Attempt 3 - set up a scheduler to run a single runRequest for a range of partitions. As well as scraping the incorrect partitions this returns ''dagster.\\_core.storage.fs\\_io\\_manager.PickledObjectFilesystemIOManager'&gt; does not support persisting an output associated with multiple partitions. '\n\nOther thoughts:\n\n* Partitioning by url-date\\*age means I will have many existing and empty partitions (which we are not able to materialize yet) and so will have to handle/ignore these in downstream dependencies.  Generally seems less than ideal.\n* Partitioning by sample-date\\*age seems like it would be so much more straightforward to implement, but provide a worse user experience (since sample-date is nowhere near as important to the user as url-date)", "author_fullname": "t2_ip724", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dagster project design help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18v64o8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704023196.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704022996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have started trying to use dagster for a personal project and am trying to migrate some legacy windows scheduled jobs  (I&amp;#39;m just using subprocess with a meaningless return value to execute these).  It&amp;#39;s generally working well, but I have one particular issue I am struggling with.  If any ther dagster users can give me any pointers I would be very grateful.&lt;/p&gt;\n\n&lt;p&gt;Upstream asset is a web scraper:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The target website has one page per &amp;#39;url-date&amp;#39;, and the content of the page changes.&lt;/li&gt;\n&lt;li&gt;Using a scheduled job, on each day, (t) the scraper should scrape data for pages dated t-1, t-2 and t-7.&lt;/li&gt;\n&lt;li&gt;I want to partition the asset.&lt;/li&gt;\n&lt;li&gt;&amp;#39;url-date&amp;#39; is a natural partition dimension&lt;/li&gt;\n&lt;li&gt;age &lt;em&gt;might&lt;/em&gt; be a good partition dimension too (to allow simple recording of age)&lt;/li&gt;\n&lt;li&gt;the scraper should only be run for one partition concurrently (due to throttling)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have no experience on dagster (other than manual, videos and trial and error I have been doing) or any similar system so could really just do with &lt;strong&gt;someone telling me if I am going about things the wrong way or on the right track, please&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;Here is what I have tried:&lt;/p&gt;\n\n&lt;p&gt;Setup the scraper as an asset with multi-partition of (&amp;#39;url-date&amp;#39;, &amp;#39;ageInDays-at-materialization&amp;#39;=[1,2,7])&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Attempt 1 - set up an scheduled op-job which accepts a run-date and calls materialize() 3 times on the asset for partitions:(run-date-1,2,7)*(t-1,2,7).  The issue here is that in the GUI the asset-partition never shows as materialized (possibly due to the job being an &amp;#39;ephemeral_asset_job&amp;#39; and running in a temp folder / with a different default io-manager).  I suspect his is incorrect/unintended usage of materialize()&lt;/li&gt;\n&lt;li&gt;Attempt 2 - set up a scheduler which returns a 3 runRequests one for each (url-date=[run-date-1,2,7])*(age=[1,2,7]).  The issue here is that the runRequests run concurrently. To solve this I will try using &amp;#39;Limiting specific runs using tags&amp;#39; &lt;a href=\"https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines\"&gt;https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines&lt;/a&gt;.  Perhaps this will work...  I did originally expect the whole scheduled task to run in one run, but that might be unrealistic.&lt;/li&gt;\n&lt;li&gt;Attempt 3 - set up a scheduler to run a single runRequest for a range of partitions. As well as scraping the incorrect partitions this returns &amp;#39;&amp;#39;dagster._core.storage.fs_io_manager.PickledObjectFilesystemIOManager&amp;#39;&amp;gt; does not support persisting an output associated with multiple partitions. &amp;#39;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Other thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Partitioning by url-date*age means I will have many existing and empty partitions (which we are not able to materialize yet) and so will have to handle/ignore these in downstream dependencies.  Generally seems less than ideal.&lt;/li&gt;\n&lt;li&gt;Partitioning by sample-date*age seems like it would be so much more straightforward to implement, but provide a worse user experience (since sample-date is nowhere near as important to the user as url-date)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?auto=webp&amp;s=d9eec3bfbfc3fd565643229f8a84c399bc1fe73b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=499be57a5b257c149c9417e04fb72b227c3fc6bc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41c03559a31ecfba73ee7ff44bd6293216bcd5f6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1f3696c3a4f7025280b0a98110eadfe130536c4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a12aeb324dd791c5dbd277a3131545efcfa5b3fe", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bceae394da1ca70512dad8f5ec6ce1e142472e86", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/f_qGYoavWVH20j0QhjrqbfrQFEuALkYkaXBnJtU_xs4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29800a80de6287c32b91b05133f7c8e4a2d57eb2", "width": 1080, "height": 567}], "variants": {}, "id": "gUU0nwKC8lE8qWjp6y6pjjOolTxbCTTaSrkKGe3Kq6A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18v64o8", "is_robot_indexable": true, "report_reasons": null, "author": "BeigePerson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18v64o8/dagster_project_design_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18v64o8/dagster_project_design_help/", "subreddit_subscribers": 149725, "created_utc": 1704022996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \nWondering if there's any experienced data engineers / SWE out there who would be willing to chat a bit and share some perspective.\n\nContext:  \nNew to data engineering role, with 1 year experience. Previously 4 years as a data analyst.  \nLive &amp; work in Australia  \nLooking to understand:  \n\\- where I sit in terms of skills - particularly in industry outside of Australia as it's siloed  \n\\- next steps or how I could 'level-up' in skills or professionally  \n\\- understanding how remote jobs work  \n\n\nYour time and perspective would really be appreciated and valuable to me  \n\n\nThanks,", "author_fullname": "t2_bazyi61n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Data Engineer Seeking Experienced Engineers Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vlz7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704072233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nWondering if there&amp;#39;s any experienced data engineers / SWE out there who would be willing to chat a bit and share some perspective.&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;br/&gt;\nNew to data engineering role, with 1 year experience. Previously 4 years as a data analyst.&lt;br/&gt;\nLive &amp;amp; work in Australia&lt;br/&gt;\nLooking to understand:&lt;br/&gt;\n- where I sit in terms of skills - particularly in industry outside of Australia as it&amp;#39;s siloed&lt;br/&gt;\n- next steps or how I could &amp;#39;level-up&amp;#39; in skills or professionally&lt;br/&gt;\n- understanding how remote jobs work  &lt;/p&gt;\n\n&lt;p&gt;Your time and perspective would really be appreciated and valuable to me  &lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vlz7h", "is_robot_indexable": true, "report_reasons": null, "author": "Foe317", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vlz7h/new_data_engineer_seeking_experienced_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vlz7h/new_data_engineer_seeking_experienced_engineers/", "subreddit_subscribers": 149725, "created_utc": 1704072233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI\u2019m considering creating a data product that combines a data lake service with AI-generated insights. Despite existing products like Databricks and Snowflake, along with various AI-based insight companies - Do you think it\u2019s still a viable venture?\n\nAny comments or suggestions? Are there any gaps in the current market that you think should be addressed?\n\nThanks!", "author_fullname": "t2_8vjbfemu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Startup Idea Validation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vlbsy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704070061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering creating a data product that combines a data lake service with AI-generated insights. Despite existing products like Databricks and Snowflake, along with various AI-based insight companies - Do you think it\u2019s still a viable venture?&lt;/p&gt;\n\n&lt;p&gt;Any comments or suggestions? Are there any gaps in the current market that you think should be addressed?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vlbsy", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Guy81", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vlbsy/startup_idea_validation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vlbsy/startup_idea_validation/", "subreddit_subscribers": 149725, "created_utc": 1704070061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It's time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. \n\nFirstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. \n\nSecondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.\n\nAlso, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. \n\nThanks a lot. Looking forward to listening from you industry folks.", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Engineer Interview Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vi3gn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704059663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It&amp;#39;s time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. &lt;/p&gt;\n\n&lt;p&gt;Firstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. &lt;/p&gt;\n\n&lt;p&gt;Secondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.&lt;/p&gt;\n\n&lt;p&gt;Also, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot. Looking forward to listening from you industry folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18vi3gn", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vi3gn/azure_data_engineer_interview_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vi3gn/azure_data_engineer_interview_help/", "subreddit_subscribers": 149725, "created_utc": 1704059663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am apologizing beforehand if somehow is offended by my question.\n\nI am in a dilemma on which role should I venture into - data engineer or data scientist. I am more inclined towards data engineer as it pertains to my skill set but I am in doubt if there is a ceiling in the career growth. Like can a data engineer be offered a executive role or be a part of the board of directors?", "author_fullname": "t2_6aijectm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is hierarchically superior - data engineer or data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vev5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704050250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am apologizing beforehand if somehow is offended by my question.&lt;/p&gt;\n\n&lt;p&gt;I am in a dilemma on which role should I venture into - data engineer or data scientist. I am more inclined towards data engineer as it pertains to my skill set but I am in doubt if there is a ceiling in the career growth. Like can a data engineer be offered a executive role or be a part of the board of directors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18vev5r", "is_robot_indexable": true, "report_reasons": null, "author": "X_Warrior361", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vev5r/who_is_hierarchically_superior_data_engineer_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vev5r/who_is_hierarchically_superior_data_engineer_or/", "subreddit_subscribers": 149725, "created_utc": 1704050250.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}