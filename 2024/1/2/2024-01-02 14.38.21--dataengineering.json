{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. \n\nWhere would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.", "author_fullname": "t2_163ma7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ways to keep your SQL sharp with minimal effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18who2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704173395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m at a job that does doesn&amp;#39;t involve working daily with SQL as the project has matured and we&amp;#39;re not making many changes to the business logic anymore. So I&amp;#39;m thinking that I want to keep working on SQL problems somewhere else so that I&amp;#39;m interview ready. &lt;/p&gt;\n\n&lt;p&gt;Where would you recommend I can go let&amp;#39;s say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18who2l", "is_robot_indexable": true, "report_reasons": null, "author": "muhmeinchut69", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "subreddit_subscribers": 149998, "created_utc": 1704173395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wdkvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704161126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I haven&amp;#39;t accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I&amp;#39;ve been wanting to use. My current company is great but I&amp;#39;ve been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn&amp;#39;t performance based). Maybe I&amp;#39;m naive but that sounded fine to me. Am I crazy to leave my full-time role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wdkvp", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "subreddit_subscribers": 149998, "created_utc": 1704161126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I'd like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don't and why. E.g.:\n\n**Centralized vs decentralized** \\- do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?\n\n**Streaming/real-time** \\- Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?\n\n**Data modeling** \\- How do you approach data modeling, have you tried creating and maintaining one huge or several smaller \"generic\" data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?\n\n**Self-service** \\- what do you do to maximize the ability of your consumers to self-serve?\n\nAlternatively, if anybody knows of more public docs like the [Gitlabs handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/) let me know.\n\nThanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enterprise data solutions - how does your look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyykn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704122901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I&amp;#39;d like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don&amp;#39;t and why. E.g.:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Centralized vs decentralized&lt;/strong&gt; - do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Streaming/real-time&lt;/strong&gt; - Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data modeling&lt;/strong&gt; - How do you approach data modeling, have you tried creating and maintaining one huge or several smaller &amp;quot;generic&amp;quot; data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Self-service&lt;/strong&gt; - what do you do to maximize the ability of your consumers to self-serve?&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if anybody knows of more public docs like the &lt;a href=\"https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/\"&gt;Gitlabs handbook&lt;/a&gt; let me know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?auto=webp&amp;s=af544e78828882798d7fe4d1c42454d6bd21dc22", "width": 875, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=828e0ee040e1722af1bf0dbb719d2b846ec766b4", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f99184941ef4a831059c95c0f3bfb24a7de84249", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b5e1d3118a33c25bab44e78f2a95e971cbca6e5", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a04b3ad42aa3e307db53deca2024a610d384848", "width": 640, "height": 447}], "variants": {}, "id": "v9bukcUTEDutaePTQidDeR95NYA8AyYs-tp2j6EyUkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyykn", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "subreddit_subscribers": 149998, "created_utc": 1704122901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.\n2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.\n3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.\n4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).\n5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.\n6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.\n7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.\n8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp; production environments.\n9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.\n10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.\n11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.\n12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.\n\n[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Testing Cheat Sheet: 12 Essential Rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mknzrwbvn0ac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ebb8f1cb9f499662fe7137401b81d40d8ff5eca"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=43803638b8a1176972a4065de0ad9917385dc318"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0df0c666c5e84ecd36586eb4951b19ff630dec92"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d31614e427f2b96d00308d4c0ae7f5bde5ce3d5a"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d393ab6192e671205c6854d4dbd32424e5f4f86d"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=341a0344accb93a99e02182da505b40f1058ffd6"}], "s": {"y": 3778, "x": 1887, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48"}, "id": "mknzrwbvn0ac1"}}, "name": "t3_18wnsqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xbH5B5vbrrV6rRJ15RnsiIr1X93x8bW4r0eHS9GlSAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704196767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Source vs Target Data Reconciliation:&lt;/strong&gt; Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ETL Transformation Test:&lt;/strong&gt; Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Source Data Validation:&lt;/strong&gt; Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Validation Rule:&lt;/strong&gt; Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Reconciliation Rule:&lt;/strong&gt; Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Referential Integrity Reconciliation:&lt;/strong&gt; Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Migration Reconciliation:&lt;/strong&gt; Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physical Schema Reconciliation:&lt;/strong&gt; Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp;amp; production environments.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cross Source Data Reconciliation:&lt;/strong&gt; Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Validation:&lt;/strong&gt; Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Reconciliation:&lt;/strong&gt; Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Cross-Environment Reconciliation:&lt;/strong&gt; Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48\"&gt;Data Testing Cheat Sheet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wnsqj", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "subreddit_subscribers": 149998, "created_utc": 1704196767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!  \nI released a new open source library and would like to get some feedback from r/dataengineering!\n\nThe library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues\n\nLink to the github page: [https://github.com/dataflint/spark](https://github.com/dataflint/spark)\n\n[DataFlint Demo](https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player)", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataFlint, a new open source Performance Monitoring for Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"td74k6056v9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/DASHPlaylist.mpd?a=1706798301%2CYWYyNTEwNmFjYWQ2ZGQ1NjgzOGE4YTFmMjk5MTRhNTkzZGU2ZTFmYzE3MmIwOGJlNjRhZTc5OWNiNzA2MGM2Mw%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 648, "hlsUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/HLSPlaylist.m3u8?a=1706798301%2CYWJhMzU1ZTBkZjdkZjJlNzZiMjM4YWE4NWY2OGY0YzJmMTFiMjA4Zjk3OTQ4Y2U4NTkxYjNjYzlhMGQ1YTI4Yg%3D%3D&amp;v=1&amp;f=sd", "id": "td74k6056v9c1", "isGif": false}}, "name": "t3_18w1ufk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4GtLoknj2YGr9NK8ORfJeOtwa1MJrjG74w63T95dxSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704130812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nI released a new open source library and would like to get some feedback from &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;The library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues&lt;/p&gt;\n\n&lt;p&gt;Link to the github page: &lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player\"&gt;DataFlint Demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?auto=webp&amp;s=be0e90451258a2a1ce63f90016308f0f15c99cc8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d294196e9e00d6ee716ce16e37596f833b1efb4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a38c54ff87953dc68f413ea4ab90e4715a91119b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=274f4749ee094e46fbfe2e9f589fe79ed4db1e71", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a33116b184654f9656cb450b1956fec287bff28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=656fc3465823807c018550fde06f998957335d1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58c3b293e884cd329cee524426278e873cfbb63c", "width": 1080, "height": 540}], "variants": {}, "id": "hzPBYn3K6qtbd4X8ppKl43rauR-Q_QAujjbYrduf49k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18w1ufk", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "subreddit_subscribers": 149998, "created_utc": 1704130812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Clean outliers with data painter](https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player)\n\nSometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.\n\n&amp;#x200B;\n\n[create new feature in your data](https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player)\n\nSometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.\n\n  \nOnline tutorial of Data painter in PyGWalker: [https://data-painter-tutorial.pygwalker.kanaries.io/](https://data-painter-tutorial.pygwalker.kanaries.io/)  \nPyGWalker's Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker's Data Painter, a new way to interact with your data in jupyter notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vuugpw2rfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/DASHPlaylist.mpd?a=1706798301%2CZDhkOGZhMTMzNDkzYjFhOWFkMzI5NmM1YTQ3MjA1NjhlMGQyOWMxOTUxOWFiYjRmYjVmY2Q5YWViNzIyODhjYw%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/HLSPlaylist.m3u8?a=1706798301%2CYWY1ZjU5MjBkOWNkZDViNDY1NmI5NGYzNmNjM2Y2NjFmM2RhZmQxMmVjNWI3Zjk5MGI2MzBjZjQ0OWNmYzgxOQ%3D%3D&amp;v=1&amp;f=sd", "id": "vuugpw2rfz9c1", "isGif": false}, "ib92wepmfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/DASHPlaylist.mpd?a=1706798301%2CZGE0MzEwOTBiNWZmYzZmOTM5NDljMDI4ZjVlYzdiMWJkNzMxNzhkOTliOGE2YTlkYTRiYjA4NGNkOTM4MmI5Mw%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/HLSPlaylist.m3u8?a=1706798301%2CNDlmNzM5NGY2YmVjZWNlNjJkZmYxZGMxN2QyZGVhNjAwMzM4MWMxNDY2OTZmMzRkOTc4N2MyZTJhNTA0ZjE1ZQ%3D%3D&amp;v=1&amp;f=sd", "id": "ib92wepmfz9c1", "isGif": false}}, "name": "t3_18wk4rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704182283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player\"&gt;Clean outliers with data painter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player\"&gt;create new feature in your data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.&lt;/p&gt;\n\n&lt;p&gt;Online tutorial of Data painter in PyGWalker: &lt;a href=\"https://data-painter-tutorial.pygwalker.kanaries.io/\"&gt;https://data-painter-tutorial.pygwalker.kanaries.io/&lt;/a&gt;&lt;br/&gt;\nPyGWalker&amp;#39;s Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wk4rt", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "subreddit_subscribers": 149998, "created_utc": 1704182283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.\n\nTerraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?\n\nI suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?\n\nThanks in advance!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does terraform fit in with CI/CD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wg4lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704168485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.&lt;/p&gt;\n\n&lt;p&gt;Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?&lt;/p&gt;\n\n&lt;p&gt;I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the &lt;code&gt;.state&lt;/code&gt; and &lt;code&gt;.vars&lt;/code&gt; files may play a role in this. Could needing to securely manage &lt;code&gt;.state&lt;/code&gt; files give need for a CD pipeline for some reason?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wg4lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "subreddit_subscribers": 149998, "created_utc": 1704168485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it's public sector to emphasise that both the people and processes there are a bit old school.\n\nSo, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.\n\nThe person who developed this workflow 10 years or so ago, which hasn't changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!", "author_fullname": "t2_y2r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Data Warehousing Methodology Viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18wpk8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704202495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it&amp;#39;s public sector to emphasise that both the people and processes there are a bit old school.&lt;/p&gt;\n\n&lt;p&gt;So, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.&lt;/p&gt;\n\n&lt;p&gt;The person who developed this workflow 10 years or so ago, which hasn&amp;#39;t changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp;amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wpk8k", "is_robot_indexable": true, "report_reasons": null, "author": "Vextus420", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "subreddit_subscribers": 149998, "created_utc": 1704202495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Jan 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1704128435.952, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w0y5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704128435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?auto=webp&amp;s=a3d9e8461a9baf9bb27e06e1d6b27e2c85baf1e8", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e72878bdb12c2e2103d5e7121ca806e468e31f0f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db6ed5960f73ec86eaca80ba9b350806442c0294", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=444f5f54270aea66f490b2eebc963362d9d0917f", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce8c3041b9fe9dad9d8da214b8bdfa280adf7b73", "width": 640, "height": 333}], "variants": {}, "id": "rRXk_aE_pxsAg4FEkSKNuvScY3QSkzt6dMIdXDYr2-U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w0y5n", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "subreddit_subscribers": 149998, "created_utc": 1704128435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a handful of questions pertaining to DE careers, with some minor background to help understand where the motivation. I've been in analytics/BI/DE for roughly 10 years at this point, coming from a controllership background where I functioned as data analyst/BI engineer (mostly startups). My current official title is senior analytics engineer, but I do many DE-dominant activities like pipeline administration (infrastructure and ops/orchestration), CICD, code reviews, tool selection/replacement. Very seldom am I doing any dashboard work unless it's in my domain (finance/sales). The company I work for is starting to fail from a cash flow perspective and I question how long they'll survive. My questions center on job market and resume strategies.\n\nHere's the series of questions:  \n\n\n1.) Should I tailor my resume to be more analytics engineer or DE focused at this point? Meaning, should my resume bullet points be more about the aforementioned DE centric activities, or should I try to showcase the business impact from my analytics engineering work?\n\n2.) I don't have any flink/streaming nor any experience with pyspark since most of my experience has been for small/medium sized startups/scale ups, which has forced most of my attention on the data rather than engineering. Many of these companies has opted for buy vs. build because the value-add I bring centers on how to effectively use/cleanse the data.\n\n3.) Is data warehousing experience important when searching for the next role? I've constructed 3 data warehouses over the past 6 years from the ground up, and prior to that, mostly worked on constructing finance data marts. Mostly Kimball + OBT, no data vault.\n\n4.) Given the background and the context from the first three questions, what other ways can I stand out? My longest stint at a company has been 2 years, almost entirely a function of working at startups. I've held two director level roles, but prefer to be in IC positions going forward.\n\n&amp;#x200B;", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Few career related questions for a nervous engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18woy97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704200635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a handful of questions pertaining to DE careers, with some minor background to help understand where the motivation. I&amp;#39;ve been in analytics/BI/DE for roughly 10 years at this point, coming from a controllership background where I functioned as data analyst/BI engineer (mostly startups). My current official title is senior analytics engineer, but I do many DE-dominant activities like pipeline administration (infrastructure and ops/orchestration), CICD, code reviews, tool selection/replacement. Very seldom am I doing any dashboard work unless it&amp;#39;s in my domain (finance/sales). The company I work for is starting to fail from a cash flow perspective and I question how long they&amp;#39;ll survive. My questions center on job market and resume strategies.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the series of questions:  &lt;/p&gt;\n\n&lt;p&gt;1.) Should I tailor my resume to be more analytics engineer or DE focused at this point? Meaning, should my resume bullet points be more about the aforementioned DE centric activities, or should I try to showcase the business impact from my analytics engineering work?&lt;/p&gt;\n\n&lt;p&gt;2.) I don&amp;#39;t have any flink/streaming nor any experience with pyspark since most of my experience has been for small/medium sized startups/scale ups, which has forced most of my attention on the data rather than engineering. Many of these companies has opted for buy vs. build because the value-add I bring centers on how to effectively use/cleanse the data.&lt;/p&gt;\n\n&lt;p&gt;3.) Is data warehousing experience important when searching for the next role? I&amp;#39;ve constructed 3 data warehouses over the past 6 years from the ground up, and prior to that, mostly worked on constructing finance data marts. Mostly Kimball + OBT, no data vault.&lt;/p&gt;\n\n&lt;p&gt;4.) Given the background and the context from the first three questions, what other ways can I stand out? My longest stint at a company has been 2 years, almost entirely a function of working at startups. I&amp;#39;ve held two director level roles, but prefer to be in IC positions going forward.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18woy97", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woy97/few_career_related_questions_for_a_nervous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woy97/few_career_related_questions_for_a_nervous/", "subreddit_subscribers": 149998, "created_utc": 1704200635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? \n\n\nMy initial idea is to have a python script that scrape the website. It's scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don't really know how that would look like. \n\nCan you please give me tips and hints on how to approach this problem with the best practices.\n\nDo I need to use docker for example?\n\nThanks a lot", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to design a webscraping pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wggcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704169506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let&amp;#39;s say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? &lt;/p&gt;\n\n&lt;p&gt;My initial idea is to have a python script that scrape the website. It&amp;#39;s scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don&amp;#39;t really know how that would look like. &lt;/p&gt;\n\n&lt;p&gt;Can you please give me tips and hints on how to approach this problem with the best practices.&lt;/p&gt;\n\n&lt;p&gt;Do I need to use docker for example?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wggcl", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "subreddit_subscribers": 149998, "created_utc": 1704169506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious people's thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I'm 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.\n\nI think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.", "author_fullname": "t2_75f7qjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Individual contributor or Management career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w7k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704145254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious people&amp;#39;s thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I&amp;#39;m 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.&lt;/p&gt;\n\n&lt;p&gt;I think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w7k0o", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-74514", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "subreddit_subscribers": 149998, "created_utc": 1704145254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?\n\nI ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can't live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).\n\nTherefore..  the ask.\n\nWhat was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.\n\nSo .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).\n\nThe data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes\n\nExporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I'm not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;c.\n\nSo considering those limitations, whats the best way to get all that data into memory without breaking the bank?\n\nhttps://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\n\nhttps://docs.snowflake.com/en/user-guide/data-unload-s3", "author_fullname": "t2_96muxygv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Snowflake Table to Pandas Dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w4wg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704140228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704138621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?&lt;/p&gt;\n\n&lt;p&gt;I ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can&amp;#39;t live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).&lt;/p&gt;\n\n&lt;p&gt;Therefore..  the ask.&lt;/p&gt;\n\n&lt;p&gt;What was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.&lt;/p&gt;\n\n&lt;p&gt;So .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).&lt;/p&gt;\n\n&lt;p&gt;The data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes&lt;/p&gt;\n\n&lt;p&gt;Exporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I&amp;#39;m not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;amp;c.&lt;/p&gt;\n\n&lt;p&gt;So considering those limitations, whats the best way to get all that data into memory without breaking the bank?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\"&gt;https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-unload-s3\"&gt;https://docs.snowflake.com/en/user-guide/data-unload-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w4wg1", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Tailor992", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "subreddit_subscribers": 149998, "created_utc": 1704138621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it's counterpart, airflow. Is this a common thing, or is my PC not good enough for this?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Airbyte so buggy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18woe6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704198787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it&amp;#39;s counterpart, airflow. Is this a common thing, or is my PC not good enough for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18woe6i", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "subreddit_subscribers": 149998, "created_utc": 1704198787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks", "author_fullname": "t2_7hwnfxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing in a migration project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wjwgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704181362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wjwgf", "is_robot_indexable": true, "report_reasons": null, "author": "johndough990", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "subreddit_subscribers": 149998, "created_utc": 1704181362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?\n\n[View Poll](https://www.reddit.com/poll/18w6mjf)", "author_fullname": "t2_2bc3qulu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which book do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w6mjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704142910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18w6mjf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18w6mjf", "is_robot_indexable": true, "report_reasons": null, "author": "Bassemustafa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1704747710115, "options": [{"text": "Fundamentals of Data Engineering (Joe &amp; Matt)", "id": "26587189"}, {"text": "Designing Data-Intensive Applications (Martin Kleppmann)", "id": "26587190"}, {"text": "Another option (please specify)", "id": "26587191"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 47, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "subreddit_subscribers": 149998, "created_utc": 1704142910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3dyum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "50+ Incredible Big Data Statistics for 2024: Facts, Market Size &amp; Industry Growth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyu06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vlUxDD9weyBDUs1V_r0_Cit73bUGmLUmn1G37-p24jI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704122512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bigdataanalyticsnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bigdataanalyticsnews.com/big-data-statistics/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?auto=webp&amp;s=e46f2536cf6003bdab10a463f0507baac3de8313", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12a9e1a1ae12628529d03d0fe1a47dab780668de", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f4f605ab4f7873e7b8e2fc133da8a5997472342", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=848ede9c87c68a93f5e962ea8bf47fd67d26be4c", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a25dd76325ab2e4d6722c7bab79580326fd70b01", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2cbed5b59f02d466d889fa188e81f571c800a1bc", "width": 960, "height": 576}], "variants": {}, "id": "G7vw-wHW51NEHsU3rbN0_5-agzBurbGYMoMOLsl1mPY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyu06", "is_robot_indexable": true, "report_reasons": null, "author": "Veerans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyu06/50_incredible_big_data_statistics_for_2024_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bigdataanalyticsnews.com/big-data-statistics/", "subreddit_subscribers": 149998, "created_utc": 1704122512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?\n\nEdit 0:\n\nClarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).\n\nI am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))\n\nI want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?", "author_fullname": "t2_phuejnxhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE skill for LLM project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18we8em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704165727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704162949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?&lt;/p&gt;\n\n&lt;p&gt;Edit 0:&lt;/p&gt;\n\n&lt;p&gt;Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: &lt;em&gt;building foundation model, autonomous AI agent&lt;/em&gt;. Besides, the data source will very diversified (internet, book, paper report, ...).&lt;/p&gt;\n\n&lt;p&gt;I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))&lt;/p&gt;\n\n&lt;p&gt;I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18we8em", "is_robot_indexable": true, "report_reasons": null, "author": "basic_of_basic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "subreddit_subscribers": 149998, "created_utc": 1704162949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A small blog post on open-source, data engineering, and blogging", "author_fullname": "t2_2bhtmk4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reflecting on the Year 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyiap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0tw-pydEEaymWbeizXjFotyfm1_QaYPZFXz5YQz-rdM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704121534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A small blog post on open-source, data engineering, and blogging&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/reflecting-on-year-2023/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?auto=webp&amp;s=1a09e56e7641075040486f7189b54cb98076a572", "width": 3840, "height": 2158}, "resolutions": [{"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33dfb55ddd030d2cb537108514aa51b2987ded35", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd1fa87258d8d9eae7ee6f264faa9d4d2f1dbd13", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4140802654b52f5edbd27deb8fd691cd45aa187", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70a80e27dafc8321817b8568b20171bf2892fa50", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f80477c5cca7c521a5b72fab1610482d85e704f0", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cce6622ee2cffb892e313574243dc4f4ae02a17", "width": 1080, "height": 606}], "variants": {}, "id": "JBTcceHBOjoeZxknyf0btIks9RWwd2sWR5Yo1qBt_6s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18vyiap", "is_robot_indexable": true, "report_reasons": null, "author": "nf_x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyiap/reflecting_on_the_year_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/reflecting-on-year-2023/", "subreddit_subscribers": 149998, "created_utc": 1704121534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,   \nI've been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We've ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:\n\nLets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:\n\nSB1 for US is Strength Evaluation\n\nSB2 for US is Power Evaluation\n\nwhile \n\nSB1 for GB is Power Evaluation\n\nSB2 for GB is Strength Evaluation  \n\n\nand its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that\n\n  \n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 3   | 5   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 5   | 3   | 9   | 9    |\n\nWhat is expected output in that scenario I guess is (look at SB1 SB2 cols)\n\n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 5   | 3   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 3   | 5   | 9   | 9    |\n\nand I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can't be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to tackle inconsistency in schemas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vzsfe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704126689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704125282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI&amp;#39;ve been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We&amp;#39;ve ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:&lt;/p&gt;\n\n&lt;p&gt;Lets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:&lt;/p&gt;\n\n&lt;p&gt;SB1 for US is Strength Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for US is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;while &lt;/p&gt;\n\n&lt;p&gt;SB1 for GB is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for GB is Strength Evaluation  &lt;/p&gt;\n\n&lt;p&gt;and its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;What is expected output in that scenario I guess is (look at SB1 SB2 cols)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can&amp;#39;t be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vzsfe", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "subreddit_subscribers": 149998, "created_utc": 1704125282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. \n\nAll these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. \n\nWhat do you think about learning JS if working in data?\n\nDoes I make any sense even considering these requirements?\n\nI want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)\n\nYour take on this?", "author_fullname": "t2_dnl6ymzoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Javascript for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w58rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704139455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. &lt;/p&gt;\n\n&lt;p&gt;All these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. &lt;/p&gt;\n\n&lt;p&gt;What do you think about learning JS if working in data?&lt;/p&gt;\n\n&lt;p&gt;Does I make any sense even considering these requirements?&lt;/p&gt;\n\n&lt;p&gt;I want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)&lt;/p&gt;\n\n&lt;p&gt;Your take on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w58rt", "is_robot_indexable": true, "report_reasons": null, "author": "BlackBird-28", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "subreddit_subscribers": 149998, "created_utc": 1704139455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wondered how databases celebrate the New Year? Let's dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! [https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake](https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake)  Happy New Year everyone!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celebrating New Year with Postgres and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3lcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered how databases celebrate the New Year? Let&amp;#39;s dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! &lt;a href=\"https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake\"&gt;https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake&lt;/a&gt;  Happy New Year everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?auto=webp&amp;s=8164bd3cb8e962d833fe03e7fdab4447e6fd7f00", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2ee4a2da97d848af4040345017a17c389659d2e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7bb14b841963cec4703e9665e20371017854a41", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5264063b4db33de7f7f9eca7590afc2669d34815", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ad5e0a18488351be2fd7df572418cfe79a48dc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e386d61723a87bc4167a52399d970a106702836", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae9872cec4d6f6929a9f1e52fb00bd5220f96bf", "width": 1080, "height": 567}], "variants": {}, "id": "78TcVOKOSrUZItTpaTSL7dLL7sXsrVV93nmtsXgWnws"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18w3lcc", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "subreddit_subscribers": 149998, "created_utc": 1704135332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody, I started lurking in this community just a few months ago.\n\nI just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn't look like it's against the rules, but I'll stop if that's not received well.\n\nSo, here we are:\n\n\ud83d\udd38\u00a0Neglecting Documentation\n\n\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.\n\nFor instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.\n\nI\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.\n\n\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.\n\nCheck links and standard parameters, and read plain English text. You never know what you\u2019ll find.\n\n\n\n\ud83d\udd38\u00a0Ignoring Error Handling\n\n\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.\n\nThere are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.\n\nLet\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.\n\n\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.\n\nThink of what can fail and handle those scenarios before the scenario where everything works.\n\n\n\n\ud83d\udd38\u00a0Fetching Too Much Information\n\n\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.\n\nOn top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.\n\n\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.\n\nImplement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.\n\n\u2014\nWhat else?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Mistakes Data Engineers Make When Consuming APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3pvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704135915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody, I started lurking in this community just a few months ago.&lt;/p&gt;\n\n&lt;p&gt;I just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn&amp;#39;t look like it&amp;#39;s against the rules, but I&amp;#39;ll stop if that&amp;#39;s not received well.&lt;/p&gt;\n\n&lt;p&gt;So, here we are:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Neglecting Documentation&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.&lt;/p&gt;\n\n&lt;p&gt;Check links and standard parameters, and read plain English text. You never know what you\u2019ll find.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Ignoring Error Handling&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.&lt;/p&gt;\n\n&lt;p&gt;There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.&lt;/p&gt;\n\n&lt;p&gt;Think of what can fail and handle those scenarios before the scenario where everything works.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Fetching Too Much Information&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.&lt;/p&gt;\n\n&lt;p&gt;On top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.&lt;/p&gt;\n\n&lt;p&gt;Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.&lt;/p&gt;\n\n&lt;p&gt;\u2014\nWhat else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w3pvz", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "subreddit_subscribers": 149998, "created_utc": 1704135651.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}