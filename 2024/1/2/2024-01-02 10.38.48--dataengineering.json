{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   \n\n\nIf you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. ", "author_fullname": "t2_auriunhuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happy new year engineers and data enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vubnp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704105850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   &lt;/p&gt;\n\n&lt;p&gt;If you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vubnp", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Rub-3984", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "subreddit_subscribers": 149970, "created_utc": 1704105850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a column named 'normalized-losses' in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can't do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks\u00a0in\u00a0advance\n\n&amp;#x200B;\n\nhttps://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;format=png&amp;auto=webp&amp;s=f29e1af711003115fe90d00b85fec080c01eb271\n\nhttps://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;format=png&amp;auto=webp&amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7", "author_fullname": "t2_l8jdxq43u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I replace NaN values?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"iq0ty9pyut9c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a887e9971fc7b14d28f050f5c0d329e25f45298"}, {"y": 158, "x": 216, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb77d43ddb02b0135b8f144e65388dfd3693ac64"}, {"y": 234, "x": 320, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45e232946f26905a39f4f91d257da924e041bf09"}, {"y": 469, "x": 640, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94643b189a39f45d36136d2beac12d439d258dbe"}], "s": {"y": 521, "x": 710, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;format=png&amp;auto=webp&amp;s=f29e1af711003115fe90d00b85fec080c01eb271"}, "id": "iq0ty9pyut9c1"}, "ignyqkpyut9c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bb5bb5c96a0de412dbd8364d8c32f5521ef15ae"}, {"y": 43, "x": 216, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa0748d850ae605bfd97fd87aacc2905f83323d3"}, {"y": 64, "x": 320, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55a3a0bd94684e1866b1df3c6b11251d90f0791"}, {"y": 128, "x": 640, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29203c9a3dd6d22d202f19a96c50ac873530cd83"}, {"y": 192, "x": 960, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbfe9f76067ad771889ec9a781f87ca9698a7a31"}], "s": {"y": 208, "x": 1037, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;format=png&amp;auto=webp&amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7"}, "id": "ignyqkpyut9c1"}}, "name": "t3_18vwegq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HwLnBitzCqeVUmXoyOYcL8tX8A-8CGpWfWzk7ZjN5_I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704114404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a column named &amp;#39;normalized-losses&amp;#39; in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can&amp;#39;t do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks\u00a0in\u00a0advance&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f29e1af711003115fe90d00b85fec080c01eb271\"&gt;https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f29e1af711003115fe90d00b85fec080c01eb271&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7\"&gt;https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vwegq", "is_robot_indexable": true, "report_reasons": null, "author": "Darktrader21", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vwegq/how_should_i_replace_nan_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vwegq/how_should_i_replace_nan_values/", "subreddit_subscribers": 149970, "created_utc": 1704114404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I'd like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don't and why. E.g.:\n\n**Centralized vs decentralized** \\- do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?\n\n**Streaming/real-time** \\- Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?\n\n**Data modeling** \\- How do you approach data modeling, have you tried creating and maintaining one huge or several smaller \"generic\" data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?\n\n**Self-service** \\- what do you do to maximize the ability of your consumers to self-serve?\n\nAlternatively, if anybody knows of more public docs like the [Gitlabs handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/) let me know.\n\nThanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enterprise data solutions - how does your look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyykn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704122901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I&amp;#39;d like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don&amp;#39;t and why. E.g.:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Centralized vs decentralized&lt;/strong&gt; - do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Streaming/real-time&lt;/strong&gt; - Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data modeling&lt;/strong&gt; - How do you approach data modeling, have you tried creating and maintaining one huge or several smaller &amp;quot;generic&amp;quot; data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Self-service&lt;/strong&gt; - what do you do to maximize the ability of your consumers to self-serve?&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if anybody knows of more public docs like the &lt;a href=\"https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/\"&gt;Gitlabs handbook&lt;/a&gt; let me know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?auto=webp&amp;s=af544e78828882798d7fe4d1c42454d6bd21dc22", "width": 875, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=828e0ee040e1722af1bf0dbb719d2b846ec766b4", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f99184941ef4a831059c95c0f3bfb24a7de84249", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b5e1d3118a33c25bab44e78f2a95e971cbca6e5", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a04b3ad42aa3e307db53deca2024a610d384848", "width": 640, "height": 447}], "variants": {}, "id": "v9bukcUTEDutaePTQidDeR95NYA8AyYs-tp2j6EyUkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyykn", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "subreddit_subscribers": 149970, "created_utc": 1704122901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wdkvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704161126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I haven&amp;#39;t accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I&amp;#39;ve been wanting to use. My current company is great but I&amp;#39;ve been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn&amp;#39;t performance based). Maybe I&amp;#39;m naive but that sounded fine to me. Am I crazy to leave my full-time role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wdkvp", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "subreddit_subscribers": 149970, "created_utc": 1704161126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI'm on the lookout for some cool data engineering courses to level up my skills and hopefully snag a great job. Any tips or suggestions? Thanks a bunch!", "author_fullname": "t2_v9jjyjtc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the best data engineering course from basic to advanced", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vws8a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704115784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the lookout for some cool data engineering courses to level up my skills and hopefully snag a great job. Any tips or suggestions? Thanks a bunch!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vws8a", "is_robot_indexable": true, "report_reasons": null, "author": "brainvale", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vws8a/looking_for_the_best_data_engineering_course_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vws8a/looking_for_the_best_data_engineering_course_from/", "subreddit_subscribers": 149970, "created_utc": 1704115784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. \n\nWhere would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.", "author_fullname": "t2_163ma7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ways to keep your SQL sharp with minimal effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18who2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704173395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m at a job that does doesn&amp;#39;t involve working daily with SQL as the project has matured and we&amp;#39;re not making many changes to the business logic anymore. So I&amp;#39;m thinking that I want to keep working on SQL problems somewhere else so that I&amp;#39;m interview ready. &lt;/p&gt;\n\n&lt;p&gt;Where would you recommend I can go let&amp;#39;s say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18who2l", "is_robot_indexable": true, "report_reasons": null, "author": "muhmeinchut69", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "subreddit_subscribers": 149970, "created_utc": 1704173395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!  \nI released a new open source library and would like to get some feedback from r/dataengineering!\n\nThe library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues\n\nLink to the github page: [https://github.com/dataflint/spark](https://github.com/dataflint/spark)\n\n[DataFlint Demo](https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player)", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataFlint, a new open source Performance Monitoring for Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"td74k6056v9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/DASHPlaylist.mpd?a=1706783927%2CNGY4MDk3YzM1MTYyOGEzNzU0M2M4ZjQ3YzViYjc1ODI1NDAzOGI1YTExYjdlZjU0MzA5NjA4ZDM0ZTk0NjZhOA%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 648, "hlsUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/HLSPlaylist.m3u8?a=1706783927%2CMDlkMzcyYWU2NmI1NDUwZTk2ZGEyYmM1YjhkOTM3MGY1MzE0ZjdmYWE3Zjg5NjRiYTVmMjg2MzUzYWYyOWI0Zg%3D%3D&amp;v=1&amp;f=sd", "id": "td74k6056v9c1", "isGif": false}}, "name": "t3_18w1ufk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4GtLoknj2YGr9NK8ORfJeOtwa1MJrjG74w63T95dxSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704130812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nI released a new open source library and would like to get some feedback from &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;The library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues&lt;/p&gt;\n\n&lt;p&gt;Link to the github page: &lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player\"&gt;DataFlint Demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?auto=webp&amp;s=be0e90451258a2a1ce63f90016308f0f15c99cc8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d294196e9e00d6ee716ce16e37596f833b1efb4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a38c54ff87953dc68f413ea4ab90e4715a91119b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=274f4749ee094e46fbfe2e9f589fe79ed4db1e71", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a33116b184654f9656cb450b1956fec287bff28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=656fc3465823807c018550fde06f998957335d1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58c3b293e884cd329cee524426278e873cfbb63c", "width": 1080, "height": 540}], "variants": {}, "id": "hzPBYn3K6qtbd4X8ppKl43rauR-Q_QAujjbYrduf49k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18w1ufk", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "subreddit_subscribers": 149970, "created_utc": 1704130812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.\n\nTerraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?\n\nI suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?\n\nThanks in advance!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does terraform fit in with CI/CD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wg4lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704168485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.&lt;/p&gt;\n\n&lt;p&gt;Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?&lt;/p&gt;\n\n&lt;p&gt;I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the &lt;code&gt;.state&lt;/code&gt; and &lt;code&gt;.vars&lt;/code&gt; files may play a role in this. Could needing to securely manage &lt;code&gt;.state&lt;/code&gt; files give need for a CD pipeline for some reason?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wg4lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "subreddit_subscribers": 149970, "created_utc": 1704168485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Clean outliers with data painter](https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player)\n\nSometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.\n\n&amp;#x200B;\n\n[create new feature in your data](https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player)\n\nSometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.\n\n  \nOnline tutorial of Data painter in PyGWalker: [https://data-painter-tutorial.pygwalker.kanaries.io/](https://data-painter-tutorial.pygwalker.kanaries.io/)  \nPyGWalker's Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker's Data Painter, a new way to interact with your data in jupyter notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vuugpw2rfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/DASHPlaylist.mpd?a=1706783927%2CYzg3ZTRlMDJiZTFhMmVjMTlkNjI2NjVlYjNkYjQ1Y2M1NjEwY2FhZjMzNjc1ZWM0MzE1NDc1YzM5MjNlOGJkYw%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/HLSPlaylist.m3u8?a=1706783927%2CNjRmZjEzMDJmY2NlODhkYTUwYTIyNjJkNzQwYzZhMDg3NTkxZWQ1NWJjMzA4ZGE0NTM0NWM2ZDZlZWUyNmE5OA%3D%3D&amp;v=1&amp;f=sd", "id": "vuugpw2rfz9c1", "isGif": false}, "ib92wepmfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/DASHPlaylist.mpd?a=1706783927%2CNTFlNmNhZTJjOWQxZDc3MDYzNTE5ZTJjYTc3ODdkZDU1NmQ0OWZiZWYyZmYzM2Y0YWMzN2JkZDVjYjdmZWJlOA%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/HLSPlaylist.m3u8?a=1706783927%2CMDBmODU2NjUzYzYxYjZkNDJhMTdlZTM0OTgxNmU4ZTU2ZWYwMThiMDllNGFhZDdiMmY5NDA3MTJjYjJmNTJiOA%3D%3D&amp;v=1&amp;f=sd", "id": "ib92wepmfz9c1", "isGif": false}}, "name": "t3_18wk4rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704182283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player\"&gt;Clean outliers with data painter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player\"&gt;create new feature in your data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.&lt;/p&gt;\n\n&lt;p&gt;Online tutorial of Data painter in PyGWalker: &lt;a href=\"https://data-painter-tutorial.pygwalker.kanaries.io/\"&gt;https://data-painter-tutorial.pygwalker.kanaries.io/&lt;/a&gt;&lt;br/&gt;\nPyGWalker&amp;#39;s Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18wk4rt", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "subreddit_subscribers": 149970, "created_utc": 1704182283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Jan 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1704128435.952, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w0y5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704128435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?auto=webp&amp;s=a3d9e8461a9baf9bb27e06e1d6b27e2c85baf1e8", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e72878bdb12c2e2103d5e7121ca806e468e31f0f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db6ed5960f73ec86eaca80ba9b350806442c0294", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=444f5f54270aea66f490b2eebc963362d9d0917f", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce8c3041b9fe9dad9d8da214b8bdfa280adf7b73", "width": 640, "height": 333}], "variants": {}, "id": "rRXk_aE_pxsAg4FEkSKNuvScY3QSkzt6dMIdXDYr2-U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w0y5n", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "subreddit_subscribers": 149970, "created_utc": 1704128435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs", "author_fullname": "t2_8fb47pbl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Data Engineering Icon Packs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vw89a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704113805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vw89a", "is_robot_indexable": true, "report_reasons": null, "author": "ForMrKite", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "subreddit_subscribers": 149970, "created_utc": 1704113805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? \n\n\nMy initial idea is to have a python script that scrape the website. It's scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don't really know how that would look like. \n\nCan you please give me tips and hints on how to approach this problem with the best practices.\n\nDo I need to use docker for example?\n\nThanks a lot", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to design a webscraping pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wggcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704169506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let&amp;#39;s say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? &lt;/p&gt;\n\n&lt;p&gt;My initial idea is to have a python script that scrape the website. It&amp;#39;s scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don&amp;#39;t really know how that would look like. &lt;/p&gt;\n\n&lt;p&gt;Can you please give me tips and hints on how to approach this problem with the best practices.&lt;/p&gt;\n\n&lt;p&gt;Do I need to use docker for example?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18wggcl", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "subreddit_subscribers": 149970, "created_utc": 1704169506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious people's thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I'm 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.\n\nI think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.", "author_fullname": "t2_75f7qjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Individual contributor or Management career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w7k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704145254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious people&amp;#39;s thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I&amp;#39;m 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.&lt;/p&gt;\n\n&lt;p&gt;I think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w7k0o", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-74514", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "subreddit_subscribers": 149970, "created_utc": 1704145254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?\n\nI ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can't live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).\n\nTherefore..  the ask.\n\nWhat was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.\n\nSo .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).\n\nThe data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes\n\nExporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I'm not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;c.\n\nSo considering those limitations, whats the best way to get all that data into memory without breaking the bank?\n\nhttps://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\n\nhttps://docs.snowflake.com/en/user-guide/data-unload-s3", "author_fullname": "t2_96muxygv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Snowflake Table to Pandas Dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w4wg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704140228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704138621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?&lt;/p&gt;\n\n&lt;p&gt;I ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can&amp;#39;t live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).&lt;/p&gt;\n\n&lt;p&gt;Therefore..  the ask.&lt;/p&gt;\n\n&lt;p&gt;What was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.&lt;/p&gt;\n\n&lt;p&gt;So .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).&lt;/p&gt;\n\n&lt;p&gt;The data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes&lt;/p&gt;\n\n&lt;p&gt;Exporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I&amp;#39;m not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;amp;c.&lt;/p&gt;\n\n&lt;p&gt;So considering those limitations, whats the best way to get all that data into memory without breaking the bank?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\"&gt;https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-unload-s3\"&gt;https://docs.snowflake.com/en/user-guide/data-unload-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w4wg1", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Tailor992", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "subreddit_subscribers": 149970, "created_utc": 1704138621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?", "author_fullname": "t2_igetxkds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Contribute to DE projects or consultancy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vuscb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704107870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vuscb", "is_robot_indexable": true, "report_reasons": null, "author": "Azar_e", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "subreddit_subscribers": 149970, "created_utc": 1704107870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks", "author_fullname": "t2_7hwnfxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing in a migration project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wjwgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704181362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wjwgf", "is_robot_indexable": true, "report_reasons": null, "author": "johndough990", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "subreddit_subscribers": 149970, "created_utc": 1704181362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?\n\nEdit 0:\n\nClarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).\n\nI am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))\n\nI want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?", "author_fullname": "t2_phuejnxhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE skill for LLM project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18we8em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704165727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704162949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?&lt;/p&gt;\n\n&lt;p&gt;Edit 0:&lt;/p&gt;\n\n&lt;p&gt;Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: &lt;em&gt;building foundation model, autonomous AI agent&lt;/em&gt;. Besides, the data source will very diversified (internet, book, paper report, ...).&lt;/p&gt;\n\n&lt;p&gt;I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))&lt;/p&gt;\n\n&lt;p&gt;I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18we8em", "is_robot_indexable": true, "report_reasons": null, "author": "basic_of_basic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "subreddit_subscribers": 149970, "created_utc": 1704162949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?\n\n[View Poll](https://www.reddit.com/poll/18w6mjf)", "author_fullname": "t2_2bc3qulu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which book do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w6mjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704142910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18w6mjf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18w6mjf", "is_robot_indexable": true, "report_reasons": null, "author": "Bassemustafa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1704747710115, "options": [{"text": "Fundamentals of Data Engineering (Joe &amp; Matt)", "id": "26587189"}, {"text": "Designing Data-Intensive Applications (Martin Kleppmann)", "id": "26587190"}, {"text": "Another option (please specify)", "id": "26587191"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 45, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "subreddit_subscribers": 149970, "created_utc": 1704142910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3dyum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "50+ Incredible Big Data Statistics for 2024: Facts, Market Size &amp; Industry Growth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyu06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vlUxDD9weyBDUs1V_r0_Cit73bUGmLUmn1G37-p24jI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704122512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bigdataanalyticsnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bigdataanalyticsnews.com/big-data-statistics/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?auto=webp&amp;s=e46f2536cf6003bdab10a463f0507baac3de8313", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12a9e1a1ae12628529d03d0fe1a47dab780668de", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f4f605ab4f7873e7b8e2fc133da8a5997472342", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=848ede9c87c68a93f5e962ea8bf47fd67d26be4c", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a25dd76325ab2e4d6722c7bab79580326fd70b01", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2cbed5b59f02d466d889fa188e81f571c800a1bc", "width": 960, "height": 576}], "variants": {}, "id": "G7vw-wHW51NEHsU3rbN0_5-agzBurbGYMoMOLsl1mPY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyu06", "is_robot_indexable": true, "report_reasons": null, "author": "Veerans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyu06/50_incredible_big_data_statistics_for_2024_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bigdataanalyticsnews.com/big-data-statistics/", "subreddit_subscribers": 149970, "created_utc": 1704122512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A small blog post on open-source, data engineering, and blogging", "author_fullname": "t2_2bhtmk4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reflecting on the Year 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyiap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0tw-pydEEaymWbeizXjFotyfm1_QaYPZFXz5YQz-rdM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704121534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A small blog post on open-source, data engineering, and blogging&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/reflecting-on-year-2023/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?auto=webp&amp;s=1a09e56e7641075040486f7189b54cb98076a572", "width": 3840, "height": 2158}, "resolutions": [{"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33dfb55ddd030d2cb537108514aa51b2987ded35", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd1fa87258d8d9eae7ee6f264faa9d4d2f1dbd13", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4140802654b52f5edbd27deb8fd691cd45aa187", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70a80e27dafc8321817b8568b20171bf2892fa50", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f80477c5cca7c521a5b72fab1610482d85e704f0", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cce6622ee2cffb892e313574243dc4f4ae02a17", "width": 1080, "height": 606}], "variants": {}, "id": "JBTcceHBOjoeZxknyf0btIks9RWwd2sWR5Yo1qBt_6s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18vyiap", "is_robot_indexable": true, "report_reasons": null, "author": "nf_x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyiap/reflecting_on_the_year_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/reflecting-on-year-2023/", "subreddit_subscribers": 149970, "created_utc": 1704121534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. \n\nAll these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. \n\nWhat do you think about learning JS if working in data?\n\nDoes I make any sense even considering these requirements?\n\nI want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)\n\nYour take on this?", "author_fullname": "t2_dnl6ymzoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Javascript for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w58rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704139455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. &lt;/p&gt;\n\n&lt;p&gt;All these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. &lt;/p&gt;\n\n&lt;p&gt;What do you think about learning JS if working in data?&lt;/p&gt;\n\n&lt;p&gt;Does I make any sense even considering these requirements?&lt;/p&gt;\n\n&lt;p&gt;I want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)&lt;/p&gt;\n\n&lt;p&gt;Your take on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w58rt", "is_robot_indexable": true, "report_reasons": null, "author": "BlackBird-28", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "subreddit_subscribers": 149970, "created_utc": 1704139455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,   \nI've been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We've ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:\n\nLets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:\n\nSB1 for US is Strength Evaluation\n\nSB2 for US is Power Evaluation\n\nwhile \n\nSB1 for GB is Power Evaluation\n\nSB2 for GB is Strength Evaluation  \n\n\nand its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that\n\n  \n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 3   | 5   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 5   | 3   | 9   | 9    |\n\nWhat is expected output in that scenario I guess is (look at SB1 SB2 cols)\n\n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 5   | 3   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 3   | 5   | 9   | 9    |\n\nand I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can't be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to tackle inconsistency in schemas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vzsfe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704126689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704125282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI&amp;#39;ve been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We&amp;#39;ve ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:&lt;/p&gt;\n\n&lt;p&gt;Lets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:&lt;/p&gt;\n\n&lt;p&gt;SB1 for US is Strength Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for US is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;while &lt;/p&gt;\n\n&lt;p&gt;SB1 for GB is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for GB is Strength Evaluation  &lt;/p&gt;\n\n&lt;p&gt;and its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;What is expected output in that scenario I guess is (look at SB1 SB2 cols)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can&amp;#39;t be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vzsfe", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "subreddit_subscribers": 149970, "created_utc": 1704125282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wondered how databases celebrate the New Year? Let's dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! [https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake](https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake)  Happy New Year everyone!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celebrating New Year with Postgres and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3lcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered how databases celebrate the New Year? Let&amp;#39;s dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! &lt;a href=\"https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake\"&gt;https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake&lt;/a&gt;  Happy New Year everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?auto=webp&amp;s=8164bd3cb8e962d833fe03e7fdab4447e6fd7f00", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2ee4a2da97d848af4040345017a17c389659d2e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7bb14b841963cec4703e9665e20371017854a41", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5264063b4db33de7f7f9eca7590afc2669d34815", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ad5e0a18488351be2fd7df572418cfe79a48dc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e386d61723a87bc4167a52399d970a106702836", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae9872cec4d6f6929a9f1e52fb00bd5220f96bf", "width": 1080, "height": 567}], "variants": {}, "id": "78TcVOKOSrUZItTpaTSL7dLL7sXsrVV93nmtsXgWnws"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18w3lcc", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "subreddit_subscribers": 149970, "created_utc": 1704135332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody, I started lurking in this community just a few months ago.\n\nI just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn't look like it's against the rules, but I'll stop if that's not received well.\n\nSo, here we are:\n\n\ud83d\udd38\u00a0Neglecting Documentation\n\n\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.\n\nFor instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.\n\nI\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.\n\n\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.\n\nCheck links and standard parameters, and read plain English text. You never know what you\u2019ll find.\n\n\n\n\ud83d\udd38\u00a0Ignoring Error Handling\n\n\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.\n\nThere are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.\n\nLet\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.\n\n\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.\n\nThink of what can fail and handle those scenarios before the scenario where everything works.\n\n\n\n\ud83d\udd38\u00a0Fetching Too Much Information\n\n\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.\n\nOn top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.\n\n\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.\n\nImplement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.\n\n\u2014\nWhat else?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Mistakes Data Engineers Make When Consuming APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3pvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704135915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody, I started lurking in this community just a few months ago.&lt;/p&gt;\n\n&lt;p&gt;I just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn&amp;#39;t look like it&amp;#39;s against the rules, but I&amp;#39;ll stop if that&amp;#39;s not received well.&lt;/p&gt;\n\n&lt;p&gt;So, here we are:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Neglecting Documentation&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.&lt;/p&gt;\n\n&lt;p&gt;Check links and standard parameters, and read plain English text. You never know what you\u2019ll find.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Ignoring Error Handling&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.&lt;/p&gt;\n\n&lt;p&gt;There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.&lt;/p&gt;\n\n&lt;p&gt;Think of what can fail and handle those scenarios before the scenario where everything works.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Fetching Too Much Information&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.&lt;/p&gt;\n\n&lt;p&gt;On top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.&lt;/p&gt;\n\n&lt;p&gt;Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.&lt;/p&gt;\n\n&lt;p&gt;\u2014\nWhat else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w3pvz", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "subreddit_subscribers": 149970, "created_utc": 1704135651.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}