{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   \n\n\nIf you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. ", "author_fullname": "t2_auriunhuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happy new year engineers and data enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vubnp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704105850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   &lt;/p&gt;\n\n&lt;p&gt;If you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vubnp", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Rub-3984", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/", "subreddit_subscribers": 149914, "created_utc": 1704105850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I'd like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don't and why. E.g.:\n\n**Centralized vs decentralized** \\- do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?\n\n**Streaming/real-time** \\- Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?\n\n**Data modeling** \\- How do you approach data modeling, have you tried creating and maintaining one huge or several smaller \"generic\" data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?\n\n**Self-service** \\- what do you do to maximize the ability of your consumers to self-serve?\n\nAlternatively, if anybody knows of more public docs like the [Gitlabs handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/) let me know.\n\nThanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enterprise data solutions - how does your look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyykn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704122901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I&amp;#39;d like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don&amp;#39;t and why. E.g.:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Centralized vs decentralized&lt;/strong&gt; - do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Streaming/real-time&lt;/strong&gt; - Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data modeling&lt;/strong&gt; - How do you approach data modeling, have you tried creating and maintaining one huge or several smaller &amp;quot;generic&amp;quot; data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Self-service&lt;/strong&gt; - what do you do to maximize the ability of your consumers to self-serve?&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if anybody knows of more public docs like the &lt;a href=\"https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/\"&gt;Gitlabs handbook&lt;/a&gt; let me know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?auto=webp&amp;s=af544e78828882798d7fe4d1c42454d6bd21dc22", "width": 875, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=828e0ee040e1722af1bf0dbb719d2b846ec766b4", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f99184941ef4a831059c95c0f3bfb24a7de84249", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b5e1d3118a33c25bab44e78f2a95e971cbca6e5", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a04b3ad42aa3e307db53deca2024a610d384848", "width": 640, "height": 447}], "variants": {}, "id": "v9bukcUTEDutaePTQidDeR95NYA8AyYs-tp2j6EyUkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyykn", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/", "subreddit_subscribers": 149914, "created_utc": 1704122901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a column named 'normalized-losses' in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can't do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks\u00a0in\u00a0advance\n\n&amp;#x200B;\n\nhttps://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;format=png&amp;auto=webp&amp;s=f29e1af711003115fe90d00b85fec080c01eb271\n\nhttps://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;format=png&amp;auto=webp&amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7", "author_fullname": "t2_l8jdxq43u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I replace NaN values?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"iq0ty9pyut9c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a887e9971fc7b14d28f050f5c0d329e25f45298"}, {"y": 158, "x": 216, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb77d43ddb02b0135b8f144e65388dfd3693ac64"}, {"y": 234, "x": 320, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45e232946f26905a39f4f91d257da924e041bf09"}, {"y": 469, "x": 640, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94643b189a39f45d36136d2beac12d439d258dbe"}], "s": {"y": 521, "x": 710, "u": "https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;format=png&amp;auto=webp&amp;s=f29e1af711003115fe90d00b85fec080c01eb271"}, "id": "iq0ty9pyut9c1"}, "ignyqkpyut9c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bb5bb5c96a0de412dbd8364d8c32f5521ef15ae"}, {"y": 43, "x": 216, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa0748d850ae605bfd97fd87aacc2905f83323d3"}, {"y": 64, "x": 320, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55a3a0bd94684e1866b1df3c6b11251d90f0791"}, {"y": 128, "x": 640, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29203c9a3dd6d22d202f19a96c50ac873530cd83"}, {"y": 192, "x": 960, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbfe9f76067ad771889ec9a781f87ca9698a7a31"}], "s": {"y": 208, "x": 1037, "u": "https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;format=png&amp;auto=webp&amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7"}, "id": "ignyqkpyut9c1"}}, "name": "t3_18vwegq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HwLnBitzCqeVUmXoyOYcL8tX8A-8CGpWfWzk7ZjN5_I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704114404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a column named &amp;#39;normalized-losses&amp;#39; in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can&amp;#39;t do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks\u00a0in\u00a0advance&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f29e1af711003115fe90d00b85fec080c01eb271\"&gt;https://preview.redd.it/iq0ty9pyut9c1.png?width=710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f29e1af711003115fe90d00b85fec080c01eb271&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7\"&gt;https://preview.redd.it/ignyqkpyut9c1.png?width=1037&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a2858f63d99edbe72dbd030f582f18a87197c7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vwegq", "is_robot_indexable": true, "report_reasons": null, "author": "Darktrader21", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vwegq/how_should_i_replace_nan_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vwegq/how_should_i_replace_nan_values/", "subreddit_subscribers": 149914, "created_utc": 1704114404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What\u2019s the best way so I can\u2019t practice writing my etl before pushing it to prod?", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have an etl that is using python to extract data from an api and land it in s3 the write to Postgres - how do you manage local / dev / qa/ prod envs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vsz8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704099917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the best way so I can\u2019t practice writing my etl before pushing it to prod?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vsz8v", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vsz8v/if_you_have_an_etl_that_is_using_python_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vsz8v/if_you_have_an_etl_that_is_using_python_to/", "subreddit_subscribers": 149914, "created_utc": 1704099917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI'm on the lookout for some cool data engineering courses to level up my skills and hopefully snag a great job. Any tips or suggestions? Thanks a bunch!", "author_fullname": "t2_v9jjyjtc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the best data engineering course from basic to advanced", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vws8a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704115784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the lookout for some cool data engineering courses to level up my skills and hopefully snag a great job. Any tips or suggestions? Thanks a bunch!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vws8a", "is_robot_indexable": true, "report_reasons": null, "author": "brainvale", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vws8a/looking_for_the_best_data_engineering_course_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vws8a/looking_for_the_best_data_engineering_course_from/", "subreddit_subscribers": 149914, "created_utc": 1704115784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!  \nI released a new open source library and would like to get some feedback from r/dataengineering!\n\nThe library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues\n\nLink to the github page: [https://github.com/dataflint/spark](https://github.com/dataflint/spark)\n\n[DataFlint Demo](https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player)", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataFlint, a new open source Performance Monitoring for Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"td74k6056v9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/DASHPlaylist.mpd?a=1706762314%2COTUyZmYxYmYzYzU5NTNmMWQ3ZjY2OTIxOTZkZTQzMWUzYjM0MmRlODY3YjgxMzY0YjRmYjAwYzM4NjAyOGYzMA%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 648, "hlsUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/HLSPlaylist.m3u8?a=1706762314%2CN2NlMTljMzk0ZmE4MWJiMWU4MWM0YWQ0NTk4MzZiYWE5NTA5YzFlODkxMzAxM2NlZTcxMzBmYTViMzEwZmY0Mw%3D%3D&amp;v=1&amp;f=sd", "id": "td74k6056v9c1", "isGif": false}}, "name": "t3_18w1ufk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4GtLoknj2YGr9NK8ORfJeOtwa1MJrjG74w63T95dxSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704130812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nI released a new open source library and would like to get some feedback from &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;The library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues&lt;/p&gt;\n\n&lt;p&gt;Link to the github page: &lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player\"&gt;DataFlint Demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?auto=webp&amp;s=be0e90451258a2a1ce63f90016308f0f15c99cc8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d294196e9e00d6ee716ce16e37596f833b1efb4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a38c54ff87953dc68f413ea4ab90e4715a91119b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=274f4749ee094e46fbfe2e9f589fe79ed4db1e71", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a33116b184654f9656cb450b1956fec287bff28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=656fc3465823807c018550fde06f998957335d1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58c3b293e884cd329cee524426278e873cfbb63c", "width": 1080, "height": 540}], "variants": {}, "id": "hzPBYn3K6qtbd4X8ppKl43rauR-Q_QAujjbYrduf49k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18w1ufk", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "subreddit_subscribers": 149914, "created_utc": 1704130812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.\n\nConsider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. \n\nSince the data volume is so huge, I'm currently finding delta of A between yesterday vs today's record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I'm looking for a plan over here. I've a buffer time of 8-9 hours for completion of the job.\n\nAny suggestions would be appreciated!!\n\nUpdate: logic for exact match, partial match and no match.\n\nSo when it comes to comparison logic on high level, for an IP, if source, destination,port, and protocols matches exactly then that is an exact match. The logic of partial match is if any one of them does not matches keeping other fields matching then it's a partial match. And if none of them is matching then it's no match.\n", "author_fullname": "t2_pl5rcgmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Reconciliation in PySpark SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vr5ja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704106042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704092066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.&lt;/p&gt;\n\n&lt;p&gt;Consider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. &lt;/p&gt;\n\n&lt;p&gt;Since the data volume is so huge, I&amp;#39;m currently finding delta of A between yesterday vs today&amp;#39;s record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I&amp;#39;m looking for a plan over here. I&amp;#39;ve a buffer time of 8-9 hours for completion of the job.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Update: logic for exact match, partial match and no match.&lt;/p&gt;\n\n&lt;p&gt;So when it comes to comparison logic on high level, for an IP, if source, destination,port, and protocols matches exactly then that is an exact match. The logic of partial match is if any one of them does not matches keeping other fields matching then it&amp;#39;s a partial match. And if none of them is matching then it&amp;#39;s no match.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vr5ja", "is_robot_indexable": true, "report_reasons": null, "author": "AdQueasy6234", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/", "subreddit_subscribers": 149914, "created_utc": 1704092066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHas anyone here experienced using the Graphframes library to use Spark GraphX in python?\n\nMy use case is I have dataset with two columns, an original company and a parent company. And if the original company is closed I need to replace it with the parent company.\n\nI need to do this as many times as necessary until I reach either a dead end or a company who isn't closed.\n\nThis could be done with recursive joins, but graphs would be much better.\n\nI'd like to know if it can be done easily using **graphframes**.\n\nThanks for the advice.", "author_fullname": "t2_uhki1q4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Graphframes with Pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18wcben", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-2CUlYpFfKxmSz3cNPecZyESUSHnTPLuWUqDPbE7XNU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704157542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here experienced using the Graphframes library to use Spark GraphX in python?&lt;/p&gt;\n\n&lt;p&gt;My use case is I have dataset with two columns, an original company and a parent company. And if the original company is closed I need to replace it with the parent company.&lt;/p&gt;\n\n&lt;p&gt;I need to do this as many times as necessary until I reach either a dead end or a company who isn&amp;#39;t closed.&lt;/p&gt;\n\n&lt;p&gt;This could be done with recursive joins, but graphs would be much better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if it can be done easily using &lt;strong&gt;graphframes&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thanks for the advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/graphframes/graphframes", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?auto=webp&amp;s=106677a05f0fee28046c3ecbdd3dcd59e04d9747", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47791d80ed4c1402a606f629f25cc623b40e652a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f60cecf6d0ceebcc5b165aa07110abd05993525", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d70f151fc1e6989361585989ea34d612627c6dad", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e7e9a32957f002bee6a9b56fc5fd04d72ebb9091", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d74c84a235d8012f8a32bab62ba252b952dcc8d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TIlsv5Cui8INWhYmDdF4uiQDHToVQdQntzDdEM2eJmI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=607963f55c3fd00b5b67cecbd3df37b5508de7be", "width": 1080, "height": 540}], "variants": {}, "id": "aikwf0V_U0qNGTqvlAdnyPjlWnL_E0O3faBAs1i3rX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wcben", "is_robot_indexable": true, "report_reasons": null, "author": "DoNotFeedTheSnakes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wcben/using_graphframes_with_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/graphframes/graphframes", "subreddit_subscribers": 149914, "created_utc": 1704157542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wdkvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704161126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I haven&amp;#39;t accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I&amp;#39;ve been wanting to use. My current company is great but I&amp;#39;ve been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn&amp;#39;t performance based). Maybe I&amp;#39;m naive but that sounded fine to me. Am I crazy to leave my full-time role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wdkvp", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "subreddit_subscribers": 149914, "created_utc": 1704161126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Jan 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1704128435.952, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w0y5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704128435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?auto=webp&amp;s=a3d9e8461a9baf9bb27e06e1d6b27e2c85baf1e8", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e72878bdb12c2e2103d5e7121ca806e468e31f0f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db6ed5960f73ec86eaca80ba9b350806442c0294", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=444f5f54270aea66f490b2eebc963362d9d0917f", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/6vEzzTTcLuxs_WhXm15FGX9Q8lSScnSbFNjm5w1mP58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce8c3041b9fe9dad9d8da214b8bdfa280adf7b73", "width": 640, "height": 333}], "variants": {}, "id": "rRXk_aE_pxsAg4FEkSKNuvScY3QSkzt6dMIdXDYr2-U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w0y5n", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/", "subreddit_subscribers": 149914, "created_utc": 1704128435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs", "author_fullname": "t2_8fb47pbl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Data Engineering Icon Packs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vw89a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704113805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vw89a", "is_robot_indexable": true, "report_reasons": null, "author": "ForMrKite", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/", "subreddit_subscribers": 149914, "created_utc": 1704113805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?\n\nEdit 0:\n\nClarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).\n\nI am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))\n\nI want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?", "author_fullname": "t2_phuejnxhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE skill for LLM project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18we8em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704165727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704162949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?&lt;/p&gt;\n\n&lt;p&gt;Edit 0:&lt;/p&gt;\n\n&lt;p&gt;Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: &lt;em&gt;building foundation model, autonomous AI agent&lt;/em&gt;. Besides, the data source will very diversified (internet, book, paper report, ...).&lt;/p&gt;\n\n&lt;p&gt;I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))&lt;/p&gt;\n\n&lt;p&gt;I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18we8em", "is_robot_indexable": true, "report_reasons": null, "author": "basic_of_basic", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "subreddit_subscribers": 149914, "created_utc": 1704162949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious people's thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I'm 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.\n\nI think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.", "author_fullname": "t2_75f7qjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Individual contributor or Management career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w7k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704145254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious people&amp;#39;s thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I&amp;#39;m 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.&lt;/p&gt;\n\n&lt;p&gt;I think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w7k0o", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-74514", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "subreddit_subscribers": 149914, "created_utc": 1704145254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?\n\n[View Poll](https://www.reddit.com/poll/18w6mjf)", "author_fullname": "t2_2bc3qulu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which book do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w6mjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704142910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18w6mjf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18w6mjf", "is_robot_indexable": true, "report_reasons": null, "author": "Bassemustafa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1704747710115, "options": [{"text": "Fundamentals of Data Engineering (Joe &amp; Matt)", "id": "26587189"}, {"text": "Designing Data-Intensive Applications (Martin Kleppmann)", "id": "26587190"}, {"text": "Another option (please specify)", "id": "26587191"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 39, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "subreddit_subscribers": 149914, "created_utc": 1704142910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?\n\nI ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can't live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).\n\nTherefore..  the ask.\n\nWhat was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.\n\nSo .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).\n\nThe data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes\n\nExporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I'm not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;c.\n\nSo considering those limitations, whats the best way to get all that data into memory without breaking the bank?\n\nhttps://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\n\nhttps://docs.snowflake.com/en/user-guide/data-unload-s3", "author_fullname": "t2_96muxygv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Snowflake Table to Pandas Dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w4wg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704140228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704138621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?&lt;/p&gt;\n\n&lt;p&gt;I ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can&amp;#39;t live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).&lt;/p&gt;\n\n&lt;p&gt;Therefore..  the ask.&lt;/p&gt;\n\n&lt;p&gt;What was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.&lt;/p&gt;\n\n&lt;p&gt;So .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).&lt;/p&gt;\n\n&lt;p&gt;The data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes&lt;/p&gt;\n\n&lt;p&gt;Exporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I&amp;#39;m not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;amp;c.&lt;/p&gt;\n\n&lt;p&gt;So considering those limitations, whats the best way to get all that data into memory without breaking the bank?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\"&gt;https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-unload-s3\"&gt;https://docs.snowflake.com/en/user-guide/data-unload-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w4wg1", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Tailor992", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "subreddit_subscribers": 149914, "created_utc": 1704138621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?", "author_fullname": "t2_igetxkds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Contribute to DE projects or consultancy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vuscb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704107870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vuscb", "is_robot_indexable": true, "report_reasons": null, "author": "Azar_e", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/", "subreddit_subscribers": 149914, "created_utc": 1704107870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3dyum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "50+ Incredible Big Data Statistics for 2024: Facts, Market Size &amp; Industry Growth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyu06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vlUxDD9weyBDUs1V_r0_Cit73bUGmLUmn1G37-p24jI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704122512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bigdataanalyticsnews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bigdataanalyticsnews.com/big-data-statistics/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?auto=webp&amp;s=e46f2536cf6003bdab10a463f0507baac3de8313", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12a9e1a1ae12628529d03d0fe1a47dab780668de", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f4f605ab4f7873e7b8e2fc133da8a5997472342", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=848ede9c87c68a93f5e962ea8bf47fd67d26be4c", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a25dd76325ab2e4d6722c7bab79580326fd70b01", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/t-Mw7SPnNyfNDd5wdkie00ZKooBWlaNe7f9kdoVl_GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2cbed5b59f02d466d889fa188e81f571c800a1bc", "width": 960, "height": 576}], "variants": {}, "id": "G7vw-wHW51NEHsU3rbN0_5-agzBurbGYMoMOLsl1mPY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vyu06", "is_robot_indexable": true, "report_reasons": null, "author": "Veerans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyu06/50_incredible_big_data_statistics_for_2024_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bigdataanalyticsnews.com/big-data-statistics/", "subreddit_subscribers": 149914, "created_utc": 1704122512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A small blog post on open-source, data engineering, and blogging", "author_fullname": "t2_2bhtmk4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reflecting on the Year 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18vyiap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0tw-pydEEaymWbeizXjFotyfm1_QaYPZFXz5YQz-rdM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704121534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A small blog post on open-source, data engineering, and blogging&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/reflecting-on-year-2023/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?auto=webp&amp;s=1a09e56e7641075040486f7189b54cb98076a572", "width": 3840, "height": 2158}, "resolutions": [{"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33dfb55ddd030d2cb537108514aa51b2987ded35", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd1fa87258d8d9eae7ee6f264faa9d4d2f1dbd13", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4140802654b52f5edbd27deb8fd691cd45aa187", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70a80e27dafc8321817b8568b20171bf2892fa50", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f80477c5cca7c521a5b72fab1610482d85e704f0", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/d4tFgB2-s2sWsVt1q4Pal5x10s2EFEcIDpYjKzevOE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cce6622ee2cffb892e313574243dc4f4ae02a17", "width": 1080, "height": 606}], "variants": {}, "id": "JBTcceHBOjoeZxknyf0btIks9RWwd2sWR5Yo1qBt_6s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18vyiap", "is_robot_indexable": true, "report_reasons": null, "author": "nf_x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vyiap/reflecting_on_the_year_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/reflecting-on-year-2023/", "subreddit_subscribers": 149914, "created_utc": 1704121534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to migrate files from Aliyun OSS to GCP Cloud Storage. As I see, Transfer service just supports AWS and Azure Blob storage natively.\n\nWorkaround will be to use Rclone for this job, but is there any other alternative?\n\nAlso, is there any better easy way to transfer files from private blob storage located in public cloud such as Aliyun to GCP cloud storage?", "author_fullname": "t2_8xdeh5fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aliyun OSS to GCP Cloud Storage migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vw3fi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704113251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to migrate files from Aliyun OSS to GCP Cloud Storage. As I see, Transfer service just supports AWS and Azure Blob storage natively.&lt;/p&gt;\n\n&lt;p&gt;Workaround will be to use Rclone for this job, but is there any other alternative?&lt;/p&gt;\n\n&lt;p&gt;Also, is there any better easy way to transfer files from private blob storage located in public cloud such as Aliyun to GCP cloud storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18vw3fi", "is_robot_indexable": true, "report_reasons": null, "author": "Winter-Activity-6938", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vw3fi/aliyun_oss_to_gcp_cloud_storage_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vw3fi/aliyun_oss_to_gcp_cloud_storage_migration/", "subreddit_subscribers": 149914, "created_utc": 1704113251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,   \nI've been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We've ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:\n\nLets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:\n\nSB1 for US is Strength Evaluation\n\nSB2 for US is Power Evaluation\n\nwhile \n\nSB1 for GB is Power Evaluation\n\nSB2 for GB is Strength Evaluation  \n\n\nand its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that\n\n  \n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 3   | 5   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 5   | 3   | 9   | 9    |\n\nWhat is expected output in that scenario I guess is (look at SB1 SB2 cols)\n\n| ID | Market | CK  | SB1 | SB2 | SbX | ColX |\n|----|--------|-----|-----|-----|-----|------|\n| 1  | US     | 1US | 2   | 1   | 9   | 9    |\n| 2  | US     | 2US | 2   | 2   | 9   | 9    |\n| 3  | US     | 3US | 1   | 1   | 9   | 9    |\n| 1  | GB     | 1GB | 5   | 3   | 9   | 9    |\n| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |\n| 3  | GB     | 3GB | 3   | 5   | 9   | 9    |\n\nand I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can't be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to tackle inconsistency in schemas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vzsfe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704126689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704125282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI&amp;#39;ve been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We&amp;#39;ve ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:&lt;/p&gt;\n\n&lt;p&gt;Lets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:&lt;/p&gt;\n\n&lt;p&gt;SB1 for US is Strength Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for US is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;while &lt;/p&gt;\n\n&lt;p&gt;SB1 for GB is Power Evaluation&lt;/p&gt;\n\n&lt;p&gt;SB2 for GB is Strength Evaluation  &lt;/p&gt;\n\n&lt;p&gt;and its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;What is expected output in that scenario I guess is (look at SB1 SB2 cols)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Market&lt;/th&gt;\n&lt;th&gt;CK&lt;/th&gt;\n&lt;th&gt;SB1&lt;/th&gt;\n&lt;th&gt;SB2&lt;/th&gt;\n&lt;th&gt;SbX&lt;/th&gt;\n&lt;th&gt;ColX&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;1US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;2US&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;US&lt;/td&gt;\n&lt;td&gt;3US&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;1GB&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;2GB&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;GB&lt;/td&gt;\n&lt;td&gt;3GB&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can&amp;#39;t be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18vzsfe", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/", "subreddit_subscribers": 149914, "created_utc": 1704125282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. \n\nAll these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. \n\nWhat do you think about learning JS if working in data?\n\nDoes I make any sense even considering these requirements?\n\nI want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)\n\nYour take on this?", "author_fullname": "t2_dnl6ymzoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Javascript for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w58rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704139455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. &lt;/p&gt;\n\n&lt;p&gt;All these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. &lt;/p&gt;\n\n&lt;p&gt;What do you think about learning JS if working in data?&lt;/p&gt;\n\n&lt;p&gt;Does I make any sense even considering these requirements?&lt;/p&gt;\n\n&lt;p&gt;I want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)&lt;/p&gt;\n\n&lt;p&gt;Your take on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w58rt", "is_robot_indexable": true, "report_reasons": null, "author": "BlackBird-28", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "subreddit_subscribers": 149914, "created_utc": 1704139455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wondered how databases celebrate the New Year? Let's dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! [https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake](https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake)  Happy New Year everyone!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celebrating New Year with Postgres and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3lcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered how databases celebrate the New Year? Let&amp;#39;s dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! &lt;a href=\"https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake\"&gt;https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake&lt;/a&gt;  Happy New Year everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?auto=webp&amp;s=8164bd3cb8e962d833fe03e7fdab4447e6fd7f00", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2ee4a2da97d848af4040345017a17c389659d2e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7bb14b841963cec4703e9665e20371017854a41", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5264063b4db33de7f7f9eca7590afc2669d34815", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ad5e0a18488351be2fd7df572418cfe79a48dc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e386d61723a87bc4167a52399d970a106702836", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae9872cec4d6f6929a9f1e52fb00bd5220f96bf", "width": 1080, "height": 567}], "variants": {}, "id": "78TcVOKOSrUZItTpaTSL7dLL7sXsrVV93nmtsXgWnws"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18w3lcc", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "subreddit_subscribers": 149914, "created_utc": 1704135332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody, I started lurking in this community just a few months ago.\n\nI just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn't look like it's against the rules, but I'll stop if that's not received well.\n\nSo, here we are:\n\n\ud83d\udd38\u00a0Neglecting Documentation\n\n\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.\n\nFor instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.\n\nI\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.\n\n\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.\n\nCheck links and standard parameters, and read plain English text. You never know what you\u2019ll find.\n\n\n\n\ud83d\udd38\u00a0Ignoring Error Handling\n\n\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.\n\nThere are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.\n\nLet\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.\n\n\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.\n\nThink of what can fail and handle those scenarios before the scenario where everything works.\n\n\n\n\ud83d\udd38\u00a0Fetching Too Much Information\n\n\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.\n\nOn top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.\n\n\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.\n\nImplement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.\n\n\u2014\nWhat else?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Mistakes Data Engineers Make When Consuming APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3pvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704135915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody, I started lurking in this community just a few months ago.&lt;/p&gt;\n\n&lt;p&gt;I just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn&amp;#39;t look like it&amp;#39;s against the rules, but I&amp;#39;ll stop if that&amp;#39;s not received well.&lt;/p&gt;\n\n&lt;p&gt;So, here we are:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Neglecting Documentation&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.&lt;/p&gt;\n\n&lt;p&gt;Check links and standard parameters, and read plain English text. You never know what you\u2019ll find.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Ignoring Error Handling&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.&lt;/p&gt;\n\n&lt;p&gt;There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.&lt;/p&gt;\n\n&lt;p&gt;Think of what can fail and handle those scenarios before the scenario where everything works.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Fetching Too Much Information&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.&lt;/p&gt;\n\n&lt;p&gt;On top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.&lt;/p&gt;\n\n&lt;p&gt;Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.&lt;/p&gt;\n\n&lt;p&gt;\u2014\nWhat else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w3pvz", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "subreddit_subscribers": 149914, "created_utc": 1704135651.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}