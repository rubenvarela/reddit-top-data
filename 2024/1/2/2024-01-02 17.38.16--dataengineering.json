{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.\n2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.\n3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.\n4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).\n5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.\n6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.\n7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.\n8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp; production environments.\n9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.\n10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.\n11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.\n12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.\n\n[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Testing Cheat Sheet: 12 Essential Rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mknzrwbvn0ac1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ebb8f1cb9f499662fe7137401b81d40d8ff5eca"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=43803638b8a1176972a4065de0ad9917385dc318"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0df0c666c5e84ecd36586eb4951b19ff630dec92"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d31614e427f2b96d00308d4c0ae7f5bde5ce3d5a"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d393ab6192e671205c6854d4dbd32424e5f4f86d"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=341a0344accb93a99e02182da505b40f1058ffd6"}], "s": {"y": 3778, "x": 1887, "u": "https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;format=png&amp;auto=webp&amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48"}, "id": "mknzrwbvn0ac1"}}, "name": "t3_18wnsqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xbH5B5vbrrV6rRJ15RnsiIr1X93x8bW4r0eHS9GlSAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704196767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Source vs Target Data Reconciliation:&lt;/strong&gt; Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ETL Transformation Test:&lt;/strong&gt; Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Source Data Validation:&lt;/strong&gt; Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Validation Rule:&lt;/strong&gt; Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Reconciliation Rule:&lt;/strong&gt; Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Referential Integrity Reconciliation:&lt;/strong&gt; Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Migration Reconciliation:&lt;/strong&gt; Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physical Schema Reconciliation:&lt;/strong&gt; Ensure the physical schema consistency between systems. Useful during releases to sync QA &amp;amp; production environments.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cross Source Data Reconciliation:&lt;/strong&gt; Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Validation:&lt;/strong&gt; Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Reconciliation:&lt;/strong&gt; Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BI Report Cross-Environment Reconciliation:&lt;/strong&gt; Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48\"&gt;Data Testing Cheat Sheet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wnsqj", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/", "subreddit_subscribers": 150026, "created_utc": 1704196767.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. \n\nWhere would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.", "author_fullname": "t2_163ma7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ways to keep your SQL sharp with minimal effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18who2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704173395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m at a job that does doesn&amp;#39;t involve working daily with SQL as the project has matured and we&amp;#39;re not making many changes to the business logic anymore. So I&amp;#39;m thinking that I want to keep working on SQL problems somewhere else so that I&amp;#39;m interview ready. &lt;/p&gt;\n\n&lt;p&gt;Where would you recommend I can go let&amp;#39;s say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18who2l", "is_robot_indexable": true, "report_reasons": null, "author": "muhmeinchut69", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/", "subreddit_subscribers": 150026, "created_utc": 1704173395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wdkvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704161126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I haven&amp;#39;t accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I&amp;#39;ve been wanting to use. My current company is great but I&amp;#39;ve been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn&amp;#39;t performance based). Maybe I&amp;#39;m naive but that sounded fine to me. Am I crazy to leave my full-time role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18wdkvp", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/", "subreddit_subscribers": 150026, "created_utc": 1704161126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!  \nI released a new open source library and would like to get some feedback from r/dataengineering!\n\nThe library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues\n\nLink to the github page: [https://github.com/dataflint/spark](https://github.com/dataflint/spark)\n\n[DataFlint Demo](https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player)", "author_fullname": "t2_i2b8380mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataFlint, a new open source Performance Monitoring for Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"td74k6056v9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/DASHPlaylist.mpd?a=1706809096%2CYzYxMGFkZWQxZDhjYmVmZTI2MjBiZTAzNjM2OWI1ZmY2NjkxYjQ4OGE0OTc0Y2MzOTA2MDVmMmNjYTg3NjgzMg%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 648, "hlsUrl": "https://v.redd.it/link/18w1ufk/asset/td74k6056v9c1/HLSPlaylist.m3u8?a=1706809096%2COGQxMTljZjU5YjQ4ZTIwOTc2YTAyYmFkNTdmNGQ5NGVlMjE2Zjc2NDZjOGUyMzI5NDRhNGMxN2RhZDRlNTliNQ%3D%3D&amp;v=1&amp;f=sd", "id": "td74k6056v9c1", "isGif": false}}, "name": "t3_18w1ufk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4GtLoknj2YGr9NK8ORfJeOtwa1MJrjG74w63T95dxSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1704130812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nI released a new open source library and would like to get some feedback from &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;The library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues&lt;/p&gt;\n\n&lt;p&gt;Link to the github page: &lt;a href=\"https://github.com/dataflint/spark\"&gt;https://github.com/dataflint/spark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player\"&gt;DataFlint Demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?auto=webp&amp;s=be0e90451258a2a1ce63f90016308f0f15c99cc8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d294196e9e00d6ee716ce16e37596f833b1efb4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a38c54ff87953dc68f413ea4ab90e4715a91119b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=274f4749ee094e46fbfe2e9f589fe79ed4db1e71", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a33116b184654f9656cb450b1956fec287bff28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=656fc3465823807c018550fde06f998957335d1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DnNyIXlcfEQFBiTT1yn2gMQWZ92ys-tjrPK5ULyiE_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58c3b293e884cd329cee524426278e873cfbb63c", "width": 1080, "height": 540}], "variants": {}, "id": "hzPBYn3K6qtbd4X8ppKl43rauR-Q_QAujjbYrduf49k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18w1ufk", "is_robot_indexable": true, "report_reasons": null, "author": "menishmueli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/", "subreddit_subscribers": 150026, "created_utc": 1704130812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Clean outliers with data painter](https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player)\n\nSometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.\n\n&amp;#x200B;\n\n[create new feature in your data](https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player)\n\nSometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.\n\n  \nOnline tutorial of Data painter in PyGWalker: [https://data-painter-tutorial.pygwalker.kanaries.io/](https://data-painter-tutorial.pygwalker.kanaries.io/)  \nPyGWalker's Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker's Data Painter, a new way to interact with your data in jupyter notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vuugpw2rfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/DASHPlaylist.mpd?a=1706809096%2CN2IxMGZhMzM5NDY1NDU3MDZiMTFjODZhMGZkZDFmNDJjNTRkNTJjNGJkMzFiNDg4MTk3OWJkZTQ5ZWVlZWJlOQ%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/vuugpw2rfz9c1/HLSPlaylist.m3u8?a=1706809096%2COTc4NWIyMGI2ODRiMWYxYjE3NDBkZmI2NjBmMDRjMGNkMjBmMWIyMTZkN2VhNTAxNDI1MzRiNWI3ZGNlMDM5YQ%3D%3D&amp;v=1&amp;f=sd", "id": "vuugpw2rfz9c1", "isGif": false}, "ib92wepmfz9c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/DASHPlaylist.mpd?a=1706809096%2CZmM3NTBkNTAwZTJkZGE0MzkxYjU4MWM2NmVjYWZlYzlhMDQ5ZmRlZDcxOWNlMTBkMzk1MTIxYzczYTcxNGVhOA%3D%3D&amp;v=1&amp;f=sd", "x": 1508, "y": 1080, "hlsUrl": "https://v.redd.it/link/18wk4rt/asset/ib92wepmfz9c1/HLSPlaylist.m3u8?a=1706809096%2CMWUwYjZhMTYwOGU4OTAzMWY5N2MwMWYyMWExMmFiMjBkOTcwOTYwY2EyY2VhYTAzZjAxNmIwMjlmYTAwMjgwZg%3D%3D&amp;v=1&amp;f=sd", "id": "ib92wepmfz9c1", "isGif": false}}, "name": "t3_18wk4rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704182283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player\"&gt;Clean outliers with data painter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player\"&gt;create new feature in your data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.&lt;/p&gt;\n\n&lt;p&gt;Online tutorial of Data painter in PyGWalker: &lt;a href=\"https://data-painter-tutorial.pygwalker.kanaries.io/\"&gt;https://data-painter-tutorial.pygwalker.kanaries.io/&lt;/a&gt;&lt;br/&gt;\nPyGWalker&amp;#39;s Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wk4rt", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/", "subreddit_subscribers": 150026, "created_utc": 1704182283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.\n\nTerraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?\n\nI suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?\n\nThanks in advance!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does terraform fit in with CI/CD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wg4lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704168485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there\u2019s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.&lt;/p&gt;\n\n&lt;p&gt;Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?&lt;/p&gt;\n\n&lt;p&gt;I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the &lt;code&gt;.state&lt;/code&gt; and &lt;code&gt;.vars&lt;/code&gt; files may play a role in this. Could needing to securely manage &lt;code&gt;.state&lt;/code&gt; files give need for a CD pipeline for some reason?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wg4lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/", "subreddit_subscribers": 150026, "created_utc": 1704168485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Incremental View Maintenance (IVM)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq7jj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fGlSxhubku9S7PFnu4hbxinrk1J2lgRQLGjkZHHE2qc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1704204367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?auto=webp&amp;s=c7c52ec3d38c17618f8e2d66171c27c43db16e74", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82e0a27fd835a1d0f5334c8262bc19d8cccdf4e7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af2b704d8bca9c88866a4bd14c3bbd3c53402624", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c87e5f77b1543f701882e490e96ed4c562eb835", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784c1dbb829b60e7ef650982b2e17b96040ebd8d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=39fb1dcb35bafe6e3018a12df56b63cd9b942ccc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0iro8VeNHZKyMbO38lZJdz6Qrg0HJiDjMKAY0hNrR7w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4966f73d32a66db7bfee30cd8f334db98c528ec7", "width": 1080, "height": 540}], "variants": {}, "id": "lnHZk4nL9uJOOq-9-k4mRzI86-2_bCgX2l6koPtAzk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18wq7jj", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq7jj/what_is_incremental_view_maintenance_ivm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "subreddit_subscribers": 150026, "created_utc": 1704204367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it's public sector to emphasise that both the people and processes there are a bit old school.\n\nSo, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.\n\nThe person who developed this workflow 10 years or so ago, which hasn't changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!", "author_fullname": "t2_y2r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Data Warehousing Methodology Viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wpk8k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704202495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it&amp;#39;s public sector to emphasise that both the people and processes there are a bit old school.&lt;/p&gt;\n\n&lt;p&gt;So, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.&lt;/p&gt;\n\n&lt;p&gt;The person who developed this workflow 10 years or so ago, which hasn&amp;#39;t changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp;amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wpk8k", "is_robot_indexable": true, "report_reasons": null, "author": "Vextus420", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wpk8k/current_data_warehousing_methodology_viability/", "subreddit_subscribers": 150026, "created_utc": 1704202495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everybody, HNY 2024 \ud83c\udf89\n\nI am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.\n\nCurrently I am focusing more on optimising the existing jobs in the current org.\n\nI use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. \n\nHowever could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.\n \nI have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. \n\nI would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.\n\nThanks !", "author_fullname": "t2_188qz428", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestions for Optimising Spark Jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wrfv0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704207813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everybody, HNY 2024 \ud83c\udf89&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.&lt;/p&gt;\n\n&lt;p&gt;Currently I am focusing more on optimising the existing jobs in the current org.&lt;/p&gt;\n\n&lt;p&gt;I use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. &lt;/p&gt;\n\n&lt;p&gt;However could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.&lt;/p&gt;\n\n&lt;p&gt;I have a basic understanding of spark UI however I don\u2019t know where to look at when I am optimising a job. &lt;/p&gt;\n\n&lt;p&gt;I would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wrfv0", "is_robot_indexable": true, "report_reasons": null, "author": "swarup_i_am", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/", "subreddit_subscribers": 150026, "created_utc": 1704207813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello y'all smart people,\n\nI'm a little frustrated by the experience of doing ad-hoc analyses in SQL.\n\nI love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it'd be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we'd have a nice documentation of the thought process that I can refer to in half a year when I've long forgotten whatever I was doing back then.\n\nHowever, this doesn't seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.\n\nDo I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it's properly documented? \n\nI'd be grateful for your input. Thanks!", "author_fullname": "t2_e7hyqhhe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using notebooks to analyze SQL - am I missing something?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wqg4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704205066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello y&amp;#39;all smart people,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little frustrated by the experience of doing ad-hoc analyses in SQL.&lt;/p&gt;\n\n&lt;p&gt;I love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it&amp;#39;d be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we&amp;#39;d have a nice documentation of the thought process that I can refer to in half a year when I&amp;#39;ve long forgotten whatever I was doing back then.&lt;/p&gt;\n\n&lt;p&gt;However, this doesn&amp;#39;t seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.&lt;/p&gt;\n\n&lt;p&gt;Do I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it&amp;#39;s properly documented? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be grateful for your input. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wqg4b", "is_robot_indexable": true, "report_reasons": null, "author": "UnusualCookieBox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/", "subreddit_subscribers": 150026, "created_utc": 1704205066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? \n\n\nMy initial idea is to have a python script that scrape the website. It's scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don't really know how that would look like. \n\nCan you please give me tips and hints on how to approach this problem with the best practices.\n\nDo I need to use docker for example?\n\nThanks a lot", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to design a webscraping pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wggcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704169506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let&amp;#39;s say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? &lt;/p&gt;\n\n&lt;p&gt;My initial idea is to have a python script that scrape the website. It&amp;#39;s scheduled by airflow. \nI was considering integrating the scraping functions in api calls, but I don&amp;#39;t really know how that would look like. &lt;/p&gt;\n\n&lt;p&gt;Can you please give me tips and hints on how to approach this problem with the best practices.&lt;/p&gt;\n\n&lt;p&gt;Do I need to use docker for example?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wggcl", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/", "subreddit_subscribers": 150026, "created_utc": 1704169506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious people's thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I'm 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.\n\nI think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.", "author_fullname": "t2_75f7qjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Individual contributor or Management career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w7k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704145254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious people&amp;#39;s thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I&amp;#39;m 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.&lt;/p&gt;\n\n&lt;p&gt;I think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w7k0o", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-74514", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/", "subreddit_subscribers": 150026, "created_utc": 1704145254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?\n\nI ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can't live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).\n\nTherefore..  the ask.\n\nWhat was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.\n\nSo .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).\n\nThe data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes\n\nExporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I'm not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;c.\n\nSo considering those limitations, whats the best way to get all that data into memory without breaking the bank?\n\nhttps://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\n\nhttps://docs.snowflake.com/en/user-guide/data-unload-s3", "author_fullname": "t2_96muxygv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Snowflake Table to Pandas Dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w4wg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704140228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704138621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats the smoothest way to materialize a Snowflake table into a pandas Dataframe?&lt;/p&gt;\n\n&lt;p&gt;I ask because of ivory tower bureaucrats (tsk tsk, your friendly Security Team) who are mandating that certain data elements can&amp;#39;t live in Snowflake on an individualized identifiable basis (a Security requirement but not a Legal/Privacy requirement).&lt;/p&gt;\n\n&lt;p&gt;Therefore..  the ask.&lt;/p&gt;\n\n&lt;p&gt;What was previously done in Snowflake now has to be ported over to a Python setup for sake of doing the processing/aggregating outside of Snowflake, and then persist the output table back into Snowflake.  Only for this particular pipeline due to it containing sensitive data fields.&lt;/p&gt;\n\n&lt;p&gt;So .. apropos this particular pipeline, the starting point is that I have to somehow export a 5 million record table into a Pandas dataframe, or something equivalent (for sake of argument, lets just keep pandas as the framework of choice).&lt;/p&gt;\n\n&lt;p&gt;The data from Snowflake for this table is approx 1GB [natively on Snowflake].  When converting to dataframe (i.e. snowflake connector to data structure to data frame) the in-memory explodes to 50GB+.  Yikes&lt;/p&gt;\n\n&lt;p&gt;Exporting the snowflake table to S3 is not an option due to the IT Team (who controls Snowflake) not being able to create individualized integration accounts.  And S3 buckets are locked down to the max.  I&amp;#39;m not gonna even get into the convoluted unergonomical setup that the Corporation has in place for S3 bucket access/&amp;amp;c.&lt;/p&gt;\n\n&lt;p&gt;So considering those limitations, whats the best way to get all that data into memory without breaking the bank?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25\"&gt;https://stackoverflow.com/questions/76953214/snowflake-python-connector-fetch-pandas-all-is-really-slow-with-less-than-25&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-unload-s3\"&gt;https://docs.snowflake.com/en/user-guide/data-unload-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w4wg1", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Tailor992", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w4wg1/help_with_snowflake_table_to_pandas_dataframe/", "subreddit_subscribers": 150026, "created_utc": 1704138621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have prior coworkers who live in Ukraine. They\u2019re hard working, dedicated, and brilliant people with a good sense of humor. They often jokingly make light of the war:\n\n\u201cHow are you today?\u201d  \n\u201cEh, all good accept for those Russians still trying to kill us \ud83d\ude05\u201d\n\n Though I know the war is no light matter. \n\nI find myself wondering if it\u2019s possible to help them in some way, via providing a unique form of aid. It\u2019s a bit unrealistic I feel to fly over there and give them my labor in the combat zone, perhaps even not the best use of my ability as I\u2019m a smaller guy (despite some military experience and a Secret clearance).\n\nI\u2019m half decent with data engineering though. I have a degree in field, a few years working experience, and I read data engineering textbooks practically for fun. I think I could offer support via data engineering in an effective way if it were needed.\n\nMaybe their intelligence channels could use people working on a better data platform to facilitate efficient garnishment of insights from data? Who knows\u2026\n\nIs this a thing? Can it be a thing? Thanks in advance for any thoughts on the matter.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to volunteer for the Ukraine War as a remote data engineer / analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18wu8lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704214974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have prior coworkers who live in Ukraine. They\u2019re hard working, dedicated, and brilliant people with a good sense of humor. They often jokingly make light of the war:&lt;/p&gt;\n\n&lt;p&gt;\u201cHow are you today?\u201d&lt;br/&gt;\n\u201cEh, all good accept for those Russians still trying to kill us \ud83d\ude05\u201d&lt;/p&gt;\n\n&lt;p&gt;Though I know the war is no light matter. &lt;/p&gt;\n\n&lt;p&gt;I find myself wondering if it\u2019s possible to help them in some way, via providing a unique form of aid. It\u2019s a bit unrealistic I feel to fly over there and give them my labor in the combat zone, perhaps even not the best use of my ability as I\u2019m a smaller guy (despite some military experience and a Secret clearance).&lt;/p&gt;\n\n&lt;p&gt;I\u2019m half decent with data engineering though. I have a degree in field, a few years working experience, and I read data engineering textbooks practically for fun. I think I could offer support via data engineering in an effective way if it were needed.&lt;/p&gt;\n\n&lt;p&gt;Maybe their intelligence channels could use people working on a better data platform to facilitate efficient garnishment of insights from data? Who knows\u2026&lt;/p&gt;\n\n&lt;p&gt;Is this a thing? Can it be a thing? Thanks in advance for any thoughts on the matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wu8lm", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wu8lm/is_it_possible_to_volunteer_for_the_ukraine_war/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wu8lm/is_it_possible_to_volunteer_for_the_ukraine_war/", "subreddit_subscribers": 150026, "created_utc": 1704214974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.\n\nCurrent we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. \n\nThings I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.\n\nAnother one was trying on airflow however windows doesn't support airflow and vice versa, wsl works however that python doesn't work on any form of Linux because it uses some windows features.\n\nAm I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?\n\nAnswers to commonly asked question:\n\n1) No it cannot run in container or any form of linux, needs to be windows only.\n\n2) And it needs to run python\n\n3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.", "author_fullname": "t2_ozjx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a weird OnPrem Setup which uses Task Scheduler currently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wq815", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704204406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.&lt;/p&gt;\n\n&lt;p&gt;Current we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. &lt;/p&gt;\n\n&lt;p&gt;Things I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.&lt;/p&gt;\n\n&lt;p&gt;Another one was trying on airflow however windows doesn&amp;#39;t support airflow and vice versa, wsl works however that python doesn&amp;#39;t work on any form of Linux because it uses some windows features.&lt;/p&gt;\n\n&lt;p&gt;Am I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?&lt;/p&gt;\n\n&lt;p&gt;Answers to commonly asked question:&lt;/p&gt;\n\n&lt;p&gt;1) No it cannot run in container or any form of linux, needs to be windows only.&lt;/p&gt;\n\n&lt;p&gt;2) And it needs to run python&lt;/p&gt;\n\n&lt;p&gt;3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18wq815", "is_robot_indexable": true, "report_reasons": null, "author": "tecedu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/", "subreddit_subscribers": 150026, "created_utc": 1704204406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it's counterpart, airflow. Is this a common thing, or is my PC not good enough for this?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Airbyte so buggy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18woe6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704198787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it&amp;#39;s counterpart, airflow. Is this a common thing, or is my PC not good enough for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18woe6i", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/", "subreddit_subscribers": 150026, "created_utc": 1704198787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks", "author_fullname": "t2_7hwnfxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing in a migration project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18wjwgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704181362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data people,\nI am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.\nJust wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18wjwgf", "is_robot_indexable": true, "report_reasons": null, "author": "johndough990", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/", "subreddit_subscribers": 150026, "created_utc": 1704181362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?\n\n[View Poll](https://www.reddit.com/poll/18w6mjf)", "author_fullname": "t2_2bc3qulu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which book do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w6mjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704142910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.\nI need a book to improve my conceptual understanding of data engineering not only the practical part, note that I\u2019m someone who is losing interest on reading continuously and don\u2019t read books easily.\nWhich one should I read first?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18w6mjf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18w6mjf", "is_robot_indexable": true, "report_reasons": null, "author": "Bassemustafa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1704747710115, "options": [{"text": "Fundamentals of Data Engineering (Joe &amp; Matt)", "id": "26587189"}, {"text": "Designing Data-Intensive Applications (Martin Kleppmann)", "id": "26587190"}, {"text": "Another option (please specify)", "id": "26587191"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 53, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18w6mjf/which_book_do_i_need/", "subreddit_subscribers": 150026, "created_utc": 1704142910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a handful of questions pertaining to DE careers, with some minor background to help understand where the motivation comes from. I've been in analytics/BI/DE for roughly 10 years at this point, coming from a controllership background where I functioned as data analyst/BI engineer (mostly startups). My current official title is senior analytics engineer, but I do many DE-dominant activities like pipeline administration (infrastructure and ops/orchestration), CICD, code reviews, tool selection/replacement. Very seldom am I doing any dashboard work unless it's in my domain (finance/sales). The company I work for is starting to fail from a cash flow perspective and I question how long they'll survive. My questions center on job market and resume strategies.\n\nHere's the series of questions:\n\n1.) Should I tailor my resume to be more analytics engineer or DE focused at this point? Meaning, should my resume bullet points be more about the aforementioned DE centric activities, or should I try to showcase the business impact from my analytics engineering work?\n\n2.) I don't have any flink/streaming nor any experience with pyspark since most of my experience has been for small/medium sized startups/scale ups, which has forced most of my attention on the data rather than engineering. Many of these companies have opted for buy vs. build because the value-add I bring centers on how to effectively use/cleanse the data.\n\n3.) Is data warehousing experience important when searching for the next role? I've constructed 3 data warehouses over the past 6 years from the ground up, and prior to that, mostly worked on constructing finance data marts. Mostly Kimball + OBT, no data vault.\n\n4.) Given the background and the context from the first three questions, what other ways can I stand out? My longest stint at a company has been 2 years, almost entirely a function of working at startups. I've held two director level roles, but prefer to be in IC positions going forward.\n\n&amp;#x200B;", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Few career related questions for a nervous engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18woy97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704215341.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704200635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a handful of questions pertaining to DE careers, with some minor background to help understand where the motivation comes from. I&amp;#39;ve been in analytics/BI/DE for roughly 10 years at this point, coming from a controllership background where I functioned as data analyst/BI engineer (mostly startups). My current official title is senior analytics engineer, but I do many DE-dominant activities like pipeline administration (infrastructure and ops/orchestration), CICD, code reviews, tool selection/replacement. Very seldom am I doing any dashboard work unless it&amp;#39;s in my domain (finance/sales). The company I work for is starting to fail from a cash flow perspective and I question how long they&amp;#39;ll survive. My questions center on job market and resume strategies.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the series of questions:&lt;/p&gt;\n\n&lt;p&gt;1.) Should I tailor my resume to be more analytics engineer or DE focused at this point? Meaning, should my resume bullet points be more about the aforementioned DE centric activities, or should I try to showcase the business impact from my analytics engineering work?&lt;/p&gt;\n\n&lt;p&gt;2.) I don&amp;#39;t have any flink/streaming nor any experience with pyspark since most of my experience has been for small/medium sized startups/scale ups, which has forced most of my attention on the data rather than engineering. Many of these companies have opted for buy vs. build because the value-add I bring centers on how to effectively use/cleanse the data.&lt;/p&gt;\n\n&lt;p&gt;3.) Is data warehousing experience important when searching for the next role? I&amp;#39;ve constructed 3 data warehouses over the past 6 years from the ground up, and prior to that, mostly worked on constructing finance data marts. Mostly Kimball + OBT, no data vault.&lt;/p&gt;\n\n&lt;p&gt;4.) Given the background and the context from the first three questions, what other ways can I stand out? My longest stint at a company has been 2 years, almost entirely a function of working at startups. I&amp;#39;ve held two director level roles, but prefer to be in IC positions going forward.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18woy97", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18woy97/few_career_related_questions_for_a_nervous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18woy97/few_career_related_questions_for_a_nervous/", "subreddit_subscribers": 150026, "created_utc": 1704200635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?\n\nEdit 0:\n\nClarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).\n\nI am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))\n\nI want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?", "author_fullname": "t2_phuejnxhc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE skill for LLM project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18we8em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704165727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704162949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?&lt;/p&gt;\n\n&lt;p&gt;Edit 0:&lt;/p&gt;\n\n&lt;p&gt;Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: &lt;em&gt;building foundation model, autonomous AI agent&lt;/em&gt;. Besides, the data source will very diversified (internet, book, paper report, ...).&lt;/p&gt;\n\n&lt;p&gt;I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))&lt;/p&gt;\n\n&lt;p&gt;I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18we8em", "is_robot_indexable": true, "report_reasons": null, "author": "basic_of_basic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/", "subreddit_subscribers": 150026, "created_utc": 1704162949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. \n\nAll these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. \n\nWhat do you think about learning JS if working in data?\n\nDoes I make any sense even considering these requirements?\n\nI want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)\n\nYour take on this?", "author_fullname": "t2_dnl6ymzoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Javascript for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w58rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704139455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys! In recent months I\u2019ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. &lt;/p&gt;\n\n&lt;p&gt;All these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. &lt;/p&gt;\n\n&lt;p&gt;What do you think about learning JS if working in data?&lt;/p&gt;\n\n&lt;p&gt;Does I make any sense even considering these requirements?&lt;/p&gt;\n\n&lt;p&gt;I want to believe this won\u2019t become the norm in upcoming months or years, but in the past I\u2019ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)&lt;/p&gt;\n\n&lt;p&gt;Your take on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18w58rt", "is_robot_indexable": true, "report_reasons": null, "author": "BlackBird-28", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/", "subreddit_subscribers": 150026, "created_utc": 1704139455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wondered how databases celebrate the New Year? Let's dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! [https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake](https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake)  Happy New Year everyone!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celebrating New Year with Postgres and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3lcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered how databases celebrate the New Year? Let&amp;#39;s dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! &lt;a href=\"https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake\"&gt;https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake&lt;/a&gt;  Happy New Year everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?auto=webp&amp;s=8164bd3cb8e962d833fe03e7fdab4447e6fd7f00", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2ee4a2da97d848af4040345017a17c389659d2e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7bb14b841963cec4703e9665e20371017854a41", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5264063b4db33de7f7f9eca7590afc2669d34815", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ad5e0a18488351be2fd7df572418cfe79a48dc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e386d61723a87bc4167a52399d970a106702836", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q60i6tOYOs0AoytHStI27_TP0DcmzRiMGZSU4MKHY3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae9872cec4d6f6929a9f1e52fb00bd5220f96bf", "width": 1080, "height": 567}], "variants": {}, "id": "78TcVOKOSrUZItTpaTSL7dLL7sXsrVV93nmtsXgWnws"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18w3lcc", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/", "subreddit_subscribers": 150026, "created_utc": 1704135332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody, I started lurking in this community just a few months ago.\n\nI just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn't look like it's against the rules, but I'll stop if that's not received well.\n\nSo, here we are:\n\n\ud83d\udd38\u00a0Neglecting Documentation\n\n\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.\n\nFor instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.\n\nI\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.\n\n\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.\n\nCheck links and standard parameters, and read plain English text. You never know what you\u2019ll find.\n\n\n\n\ud83d\udd38\u00a0Ignoring Error Handling\n\n\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.\n\nThere are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.\n\nLet\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.\n\n\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.\n\nThink of what can fail and handle those scenarios before the scenario where everything works.\n\n\n\n\ud83d\udd38\u00a0Fetching Too Much Information\n\n\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.\n\nOn top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.\n\n\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.\n\nImplement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.\n\n\u2014\nWhat else?", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Mistakes Data Engineers Make When Consuming APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18w3pvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704135915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704135651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody, I started lurking in this community just a few months ago.&lt;/p&gt;\n\n&lt;p&gt;I just love how responsive the community is to questions that appear.\nBut I also think there are many skilled people who could spark discussions by sharing there knowledge.\nMy idea is to post often, maybe even daily and test what happens.\nIt doesn&amp;#39;t look like it&amp;#39;s against the rules, but I&amp;#39;ll stop if that&amp;#39;s not received well.&lt;/p&gt;\n\n&lt;p&gt;So, here we are:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Neglecting Documentation&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0If you don\u2019t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.&lt;/p&gt;\n\n&lt;p&gt;For instance, if you don\u2019t specify the response format, you might get a default format that\u2019s difficult to parse.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve even seen one service promoting their \u201ceasier\u201d SOAP API while having a much more flexible and modern RESTful or GraphQL API.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Thoroughly review the API documentation to understand its terms of use and how to make requests.&lt;/p&gt;\n\n&lt;p&gt;Check links and standard parameters, and read plain English text. You never know what you\u2019ll find.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Ignoring Error Handling&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.&lt;/p&gt;\n\n&lt;p&gt;There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.&lt;/p&gt;\n\n&lt;p&gt;Think of what can fail and handle those scenarios before the scenario where everything works.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd38\u00a0Fetching Too Much Information&lt;/p&gt;\n\n&lt;p&gt;\u2753\u00a0Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.&lt;/p&gt;\n\n&lt;p&gt;On top of that, you\u2019ll need to pay more for computing costs and API calls if that\u2019s how your provider works.&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Minimize unnecessary data transfer and implement caching mechanisms to improve performance.&lt;/p&gt;\n\n&lt;p&gt;Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don\u2019t fetch objects you don\u2019t need.&lt;/p&gt;\n\n&lt;p&gt;\u2014\nWhat else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18w3pvz", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/", "subreddit_subscribers": 150026, "created_utc": 1704135651.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}