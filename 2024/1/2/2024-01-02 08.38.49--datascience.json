{"kind": "Listing", "data": {"after": null, "dist": 2, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I purchased 5000 [GenderAPI](https://gender-api.com/en/) credits last June and only ended up needing 500 of them.\n\nI have 4500 left over that I will not use before they expire in June 2024.\n\nIf anybody has a personal use case for these credits, I would be more than happy to donate them for free. Just reply to this thread and I'll DM you.", "author_fullname": "t2_shjly", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4500 spare GenderAPI credits for anyone that needs them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vwqc4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1704115593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased 5000 &lt;a href=\"https://gender-api.com/en/\"&gt;GenderAPI&lt;/a&gt; credits last June and only ended up needing 500 of them.&lt;/p&gt;\n\n&lt;p&gt;I have 4500 left over that I will not use before they expire in June 2024.&lt;/p&gt;\n\n&lt;p&gt;If anybody has a personal use case for these credits, I would be more than happy to donate them for free. Just reply to this thread and I&amp;#39;ll DM you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?auto=webp&amp;s=f91f3146749800bddfe428acd24607af9c7b0a63", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95cdf485b90a72b3233873e9af8367fc588594df", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87115fa8518715b0f46f73e4498aa40c9cc8b780", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9463f2ba0e8455aae0efab95433d4f4719c2af0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf03d82c45a7d0958cb4c170ea0eeb077cffbfd0", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d848de88a22cb2a8f7f85d2c8296d51d5337de0f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SAQMDJ_hhq8YZ8vKM-R9g3aI-8fTrFUhzYQFcteKFM4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89f2f48a64b1263745d08627e59da784a267ec90", "width": 1080, "height": 567}], "variants": {}, "id": "iGomVTm5XIDT2gCp7Z562HBn2Ft1ShzLOFeDwTnXtkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "18vwqc4", "is_robot_indexable": true, "report_reasons": null, "author": "TobyTheCamel", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18vwqc4/4500_spare_genderapi_credits_for_anyone_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18vwqc4/4500_spare_genderapi_credits_for_anyone_that/", "subreddit_subscribers": 1220305, "created_utc": 1704115593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "While working with a timeseries that has multiple dependant values for different variables, does it make sense to invest time in feature engineering artificial features related to overall state? Or am I just redundantly using the same information and should focus on a model capable of capturing the complexity?\n\nThis given we ignore trivial lag features and the dataset is small (100s of examples).\n\n\nE.g. Say I have a dataset of students that compete against each other in debate class. I want to predict which student will win against another, given a topic. I can construct an internal state, with a rating system, historical statistics, maybe normalizing results given ratings. \n\nBut am I just reusing and rehashing the same information? Are these features really creating useful training information? Is it possible to gain accuracy by more feature engineering?\n\nI think what I'm asking is: should I focus on engineering independent dimensions that achieve better class separation or should I focus on a model that captures the dependencies? Seeing as the former adds little accuracy.", "author_fullname": "t2_mmaop8yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Timeseries artificial features", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vt0bv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1704101509.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704100062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While working with a timeseries that has multiple dependant values for different variables, does it make sense to invest time in feature engineering artificial features related to overall state? Or am I just redundantly using the same information and should focus on a model capable of capturing the complexity?&lt;/p&gt;\n\n&lt;p&gt;This given we ignore trivial lag features and the dataset is small (100s of examples).&lt;/p&gt;\n\n&lt;p&gt;E.g. Say I have a dataset of students that compete against each other in debate class. I want to predict which student will win against another, given a topic. I can construct an internal state, with a rating system, historical statistics, maybe normalizing results given ratings. &lt;/p&gt;\n\n&lt;p&gt;But am I just reusing and rehashing the same information? Are these features really creating useful training information? Is it possible to gain accuracy by more feature engineering?&lt;/p&gt;\n\n&lt;p&gt;I think what I&amp;#39;m asking is: should I focus on engineering independent dimensions that achieve better class separation or should I focus on a model that captures the dependencies? Seeing as the former adds little accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18vt0bv", "is_robot_indexable": true, "report_reasons": null, "author": "sciencesebi3", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18vt0bv/timeseries_artificial_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18vt0bv/timeseries_artificial_features/", "subreddit_subscribers": 1220305, "created_utc": 1704100062.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}