{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_phodw6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Browsing through twitter/x, I came across the following post about snowflake/dbt. Any thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0f3ie", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;From 2019-2023 I ran a data analytics consulting firm, Upright Analytics.&lt;br&gt;&lt;br&gt;My firm is named &amp;quot;Upright&amp;quot; Analytics because I always want to do the right thing and be upright in a sea of IT con artists selling weird compute toys.&lt;br&gt;&lt;br&gt;My firm&amp;#39;s name is the direct result of the behaviors\u2026&lt;/p&gt;&amp;mdash; Lauren Balik (@laurenbalik) &lt;a href=\"https://twitter.com/laurenbalik/status/1761804446114230312?ref_src=twsrc%5Etfw\"&gt;February 25, 2024&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"oembed": {"provider_url": "https://twitter.com", "url": "https://twitter.com/laurenbalik/status/1761804446114230312", "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;From 2019-2023 I ran a data analytics consulting firm, Upright Analytics.&lt;br&gt;&lt;br&gt;My firm is named &amp;quot;Upright&amp;quot; Analytics because I always want to do the right thing and be upright in a sea of IT con artists selling weird compute toys.&lt;br&gt;&lt;br&gt;My firm&amp;#39;s name is the direct result of the behaviors\u2026&lt;/p&gt;&amp;mdash; Lauren Balik (@laurenbalik) &lt;a href=\"https://twitter.com/laurenbalik/status/1761804446114230312?ref_src=twsrc%5Etfw\"&gt;February 25, 2024&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_name": "Lauren Balik", "height": null, "width": 350, "version": "1.0", "author_url": "https://twitter.com/laurenbalik", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}, "type": "twitter.com"}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;From 2019-2023 I ran a data analytics consulting firm, Upright Analytics.&lt;br&gt;&lt;br&gt;My firm is named &amp;quot;Upright&amp;quot; Analytics because I always want to do the right thing and be upright in a sea of IT con artists selling weird compute toys.&lt;br&gt;&lt;br&gt;My firm&amp;#39;s name is the direct result of the behaviors\u2026&lt;/p&gt;&amp;mdash; Lauren Balik (@laurenbalik) &lt;a href=\"https://twitter.com/laurenbalik/status/1761804446114230312?ref_src=twsrc%5Etfw\"&gt;February 25, 2024&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1b0f3ie", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/t4ePAVC4aDSWBM7UxzPEeNYr2Df4ryuqLQ8VNkESUW0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708946175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/laurenbalik/status/1761804446114230312", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qR5aaBkRq9Jjjeq8GFUmUNtoWhZaw2zRaMuy2yrwIVA.jpg?auto=webp&amp;s=51d64f7e5993981c3e3ddb81418779f056ed6552", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/qR5aaBkRq9Jjjeq8GFUmUNtoWhZaw2zRaMuy2yrwIVA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b790b76f46a8c042bb78b201fe77f0516b1f5eff", "width": 108, "height": 108}], "variants": {}, "id": "-HMX-XcWOngZfxevcSvnam63aH_pyBU8nzrbIYt4aRs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0f3ie", "is_robot_indexable": true, "report_reasons": null, "author": "SPORTSfANALYTICS", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0f3ie/browsing_through_twitterx_i_came_across_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/laurenbalik/status/1761804446114230312", "subreddit_subscribers": 163914, "created_utc": 1708946175.0, "num_crossposts": 0, "media": {"oembed": {"provider_url": "https://twitter.com", "url": "https://twitter.com/laurenbalik/status/1761804446114230312", "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;From 2019-2023 I ran a data analytics consulting firm, Upright Analytics.&lt;br&gt;&lt;br&gt;My firm is named &amp;quot;Upright&amp;quot; Analytics because I always want to do the right thing and be upright in a sea of IT con artists selling weird compute toys.&lt;br&gt;&lt;br&gt;My firm&amp;#39;s name is the direct result of the behaviors\u2026&lt;/p&gt;&amp;mdash; Lauren Balik (@laurenbalik) &lt;a href=\"https://twitter.com/laurenbalik/status/1761804446114230312?ref_src=twsrc%5Etfw\"&gt;February 25, 2024&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_name": "Lauren Balik", "height": null, "width": 350, "version": "1.0", "author_url": "https://twitter.com/laurenbalik", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}, "type": "twitter.com"}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reaching a mental breaking point.\n\nI have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column 'ID' is in a predetermined set of ID's (roughly 135,000,000) . I've tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.\n\nI really don't know what to do at this point or if it is possible to make an efficient script to do this.", "author_fullname": "t2_ecc5gnt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read/Filter a 1.7 TB CSV File in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0up0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708986031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reaching a mental breaking point.&lt;/p&gt;\n\n&lt;p&gt;I have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column &amp;#39;ID&amp;#39; is in a predetermined set of ID&amp;#39;s (roughly 135,000,000) . I&amp;#39;ve tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.&lt;/p&gt;\n\n&lt;p&gt;I really don&amp;#39;t know what to do at this point or if it is possible to make an efficient script to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0up0e", "is_robot_indexable": true, "report_reasons": null, "author": "The-Salamander-Fan", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "subreddit_subscribers": 163914, "created_utc": 1708986031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, UK based here. I'm transitioning from a data analyst role to a junior data engineer. I've been offered a job that uses the Google stack (GCP, BigQuery etc). From what I can tell, Google is the least common of the three big cloud providers with AWS being the most popular. As I am a junior, do you think it is worth holding out for an AWS based role or is it relatively easy to transition between the three?\nThanks", "author_fullname": "t2_ro8dcljg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google stack worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0e99f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708942918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, UK based here. I&amp;#39;m transitioning from a data analyst role to a junior data engineer. I&amp;#39;ve been offered a job that uses the Google stack (GCP, BigQuery etc). From what I can tell, Google is the least common of the three big cloud providers with AWS being the most popular. As I am a junior, do you think it is worth holding out for an AWS based role or is it relatively easy to transition between the three?\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b0e99f", "is_robot_indexable": true, "report_reasons": null, "author": "mybigolthrowaway1234", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0e99f/google_stack_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0e99f/google_stack_worth_it/", "subreddit_subscribers": 163914, "created_utc": 1708942918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8vv7mvb57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When marketing needs your help (heard through Hightouch)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0lgyz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/z6JK0URsKazrZJg_1gchcdXyEKI2mLBxUtYAHvvFXgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708964375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dnivh4m3gykc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?auto=webp&amp;s=5a0f919391c9cf55fccee5f9470f236e70ed34e9", "width": 500, "height": 558}, "resolutions": [{"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5d662aee614780ed8cdff0d32f83f7b693a78a3", "width": 108, "height": 120}, {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8bd1af3f1d325c442bee2d285ca13fdb1418076", "width": 216, "height": 241}, {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb4c6fc9c2668ccb4c4d43afc1c045d76c772f7f", "width": 320, "height": 357}], "variants": {}, "id": "-Bx4u7D20iRyvf7K1LxxOqyjpLczguKZv4s96kNoWWo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1b0lgyz", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Cartographer4232", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0lgyz/when_marketing_needs_your_help_heard_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dnivh4m3gykc1.jpeg", "subreddit_subscribers": 163914, "created_utc": 1708964375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Data Peeps,\n\nI was recently asked this question and failed miserably at it. Wanted to get people's opinion on how I can do better on it.   \n\n\nIngestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. \n\nProcessing: We can run a daily spark pipeline to update the playlist.   \n\n\nThey told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   \n\n\nI wasn't able to answer that. Can anyone please help me with how to design such things? ", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture: Generate Apple Music's Top 50 multiple playlists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xiyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708992919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Data Peeps,&lt;/p&gt;\n\n&lt;p&gt;I was recently asked this question and failed miserably at it. Wanted to get people&amp;#39;s opinion on how I can do better on it.   &lt;/p&gt;\n\n&lt;p&gt;Ingestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. &lt;/p&gt;\n\n&lt;p&gt;Processing: We can run a daily spark pipeline to update the playlist.   &lt;/p&gt;\n\n&lt;p&gt;They told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   &lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t able to answer that. Can anyone please help me with how to design such things? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0xiyh", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "subreddit_subscribers": 163914, "created_utc": 1708992919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the general feedback of AWS Glue, Amazon Redshift and Amazon Data Zone?    \n\n\nHave you used it? If yes, what do you like and what do you not like?\n\n&amp;#x200B;\n\n  \n", "author_fullname": "t2_ineht0nke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "General feedback of AWS Glue / Amazon Redshift / Amazon Data Zone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0cnzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708936301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the general feedback of AWS Glue, Amazon Redshift and Amazon Data Zone?    &lt;/p&gt;\n\n&lt;p&gt;Have you used it? If yes, what do you like and what do you not like?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0cnzw", "is_robot_indexable": true, "report_reasons": null, "author": "DirectionPrize3281", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0cnzw/general_feedback_of_aws_glue_amazon_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0cnzw/general_feedback_of_aws_glue_amazon_redshift/", "subreddit_subscribers": 163914, "created_utc": 1708936301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone \n", "author_fullname": "t2_84ztczxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to Dive Deeper into Data Modeling. Recommendations for In-Depth Learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0o4bb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/t6bucAzFX9Qv_76pKS6_eTpWtudZ6I3_jAXjfmSeHoQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708970629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1q4grlv2zykc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?auto=webp&amp;s=ad493fc45be45cf6330afbc6a218203f0a401e8c", "width": 700, "height": 394}, "resolutions": [{"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b02fb77805dc60307c2937b0bbef70bdc98d7d1", "width": 108, "height": 60}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a57ee70754f3c7aa563aaeebbc32f7297678a46", "width": 216, "height": 121}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c58659241bb8f3c8658c432e2e84698657b98a1", "width": 320, "height": 180}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=050eb410f54f8b7a8ea8a4b1799151bf0c72aafe", "width": 640, "height": 360}], "variants": {}, "id": "iwWyl6t2YUzpOC_MCfgLg7f7JQrgL8tB9oFp9H92Rnk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0o4bb", "is_robot_indexable": true, "report_reasons": null, "author": "chaachans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0o4bb/looking_to_dive_deeper_into_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1q4grlv2zykc1.jpeg", "subreddit_subscribers": 163914, "created_utc": 1708970629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n**Introduction**\n\nAs a developer with a passion for music and open-source technologies, I recently created EventMusic Producer, a Dockerized application that reads data and outputs it to a Kafka topic, using Avro schemas for data serialization. This application seamlessly integrates with Kafka and Schema Registry to manage the flow of event data related to music event information.\n\n**Key Features**\n\nEventMusic Producer offers a comprehensive set of features for efficiently managing music event data:\n\n* **Kafka Integration**: Sends event data to Kafka topics for real-time processing and analysis.\n* **Avro Serialization**: Utilizes Avro schemas to serialize event data, ensuring efficiency and data validation.\n* **Docker Support**: Fully Dockerized for easy deployment and scaling in any environment.\n* **Batch Processing**: Sends event data in batches to optimize network usage and processing times.\n* **Flexible Configuration**: Easily configurable using environment variables for Kafka, Schema Registry, and other settings.\n\n**Event Types**\n\nEventMusic Producer supports three types of events:\n\n* **Page View Events**: Generated when users browse a music-related website.\n\n{  \n \"ts\": 151302389,  \n \"sessionId\": \"4301\",  \n \"page\": \"/my-music\",  \n \"auth\": \"Logged Out\",  \n \"method\": \"GET\",  \n \"status\": \"500\",  \n \"level\": \"paid\",  \n \"itemInSession\": \"40\",  \n \"city\": {  \n \"string\": \"Valentin\"  \n },  \n \"zip\": {  \n \"string\": \"11136\"  \n },  \n \"state\": {  \n \"string\": \"\"  \n },  \n \"userAgent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10\\_7\\_9 rv:6.0; bn-IN) AppleWebKit/533.8.5 (KHTML, like Gecko) Version/4.0.2 Safari/533.8.5\",  \n \"lon\": \"-30.419926\",  \n \"lat\": \"-11.5486195\",  \n \"userId\": \"57982\",  \n \"lastName\": \"Watson\",  \n \"firstName\": \"Courtney\",  \n \"gender\": \"M\",  \n \"registration\": \"337305197\",  \n \"artist\": \"David Bowie\",  \n \"song\": \"Let\u2019s Dance\",  \n \"duration\": \"224.5126215638037\"  \n}\n\n* **Listen Events**: Generated when users listen to songs or albums.\n\nlisten\\_events: {  \n \"artist\": \"The Rolling Stones\",  \n \"song\": \"(I Can\u2019t Get No) Satisfaction\",  \n \"duration\": \"473.79469057974416\",  \n \"ts\": 6678921,  \n \"sessionid\": \"8202\",  \n \"auth\": \"Logged Out\",  \n \"level\": \"free\",  \n \"itemInSession\": \"18\",  \n \"city\": \"New Sandrachester\",  \n \"zip\": \"22955\",  \n \"state\": \"MS\",  \n \"country\": \"US\",  \n \"userAgent\": \"Mozilla/5.0 (iPod; U; CPU iPhone OS 3\\_3 like Mac OS X; hi-IN) AppleWebKit/535.47.6 (KHTML, like Gecko) Version/3.0.5 Mobile/8B118 Safari/6535.47.6\",  \n \"lon\": \"-99.431258\",  \n \"lat\": \"75.0709225\",  \n \"userId\": \"78902\",  \n \"lastName\": \"Williams\",  \n \"firstName\": \"Matthew\",  \n \"gender\": \"M\",  \n \"registration\": \"1584877204\"  \n}\n\n* **Authentication Events**: Generated when users log in or out of a music-related service.\n\nauth\\_events: {  \n \"ts\": 380291172,  \n \"sessionId\": \"6616\",  \n \"level\": \"paid\",  \n \"itemInSession\": \"79\",  \n \"city\": {  \n \"string\": \"West Coletown\"  \n },  \n \"zip\": {  \n \"string\": \"03297\"  \n },  \n \"state\": {  \n \"string\": \"MI\"  \n },  \n \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10\\_6\\_5) AppleWebKit/533.1 (KHTML, like Gecko) Chrome/35.0.812.0 Safari/533.1\",  \n \"lon\": \"-95.78613\",  \n \"lat\": \"57.1778745\",  \n \"userId\": \"91224\",  \n \"lastName\": \"Thomas\",  \n \"firstName\": \"Greg\",  \n \"gender\": \"M\",  \n \"registration\": \"341396702\",  \n \"success\": \"False\"  \n}\n\n**Benefits**\n\nEventMusic Producer provides numerous benefits for managing music event data:\n\n* **Easy Deployment and Scaling**: Docker support enables quick deployment and seamless scaling.\n* **Performance Optimization**: Batch processing optimizes network usage and reduces processing times.\n* **Efficient Data Validation**: Avro serialization ensures the integrity and validity of event data.\n* **Flexibility and Customization**: Flexible configuration allows the application to be tailored to specific needs.\n\n**Conclusion**\n\nEventMusic Producer is a valuable tool for developers, data analysts, and music enthusiasts looking to manage and analyze music event data. Its seamless integration with Kafka, efficient Avro serialization, and flexible features make it an ideal solution for real-time applications and data analytics.\n\n**Links**\n\n* Source Code: [https://github.com/Stefen-Taime/eventmusic](https://github.com/Stefen-Taime/eventmusic)\n* Docker Hub Project URL: [https://hub.docker.com/r/stefen2020/eventmusic](https://hub.docker.com/r/stefen2020/eventmusic)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EventMusic Producer: An Open-Source Application for Managing Music Event Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b09m4g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708924644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As a developer with a passion for music and open-source technologies, I recently created EventMusic Producer, a Dockerized application that reads data and outputs it to a Kafka topic, using Avro schemas for data serialization. This application seamlessly integrates with Kafka and Schema Registry to manage the flow of event data related to music event information.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;EventMusic Producer offers a comprehensive set of features for efficiently managing music event data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kafka Integration&lt;/strong&gt;: Sends event data to Kafka topics for real-time processing and analysis.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avro Serialization&lt;/strong&gt;: Utilizes Avro schemas to serialize event data, ensuring efficiency and data validation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Docker Support&lt;/strong&gt;: Fully Dockerized for easy deployment and scaling in any environment.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Sends event data in batches to optimize network usage and processing times.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: Easily configurable using environment variables for Kafka, Schema Registry, and other settings.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Event Types&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;EventMusic Producer supports three types of events:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Page View Events&lt;/strong&gt;: Generated when users browse a music-related website.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;{&lt;br/&gt;\n &amp;quot;ts&amp;quot;: 151302389,&lt;br/&gt;\n &amp;quot;sessionId&amp;quot;: &amp;quot;4301&amp;quot;,&lt;br/&gt;\n &amp;quot;page&amp;quot;: &amp;quot;/my-music&amp;quot;,&lt;br/&gt;\n &amp;quot;auth&amp;quot;: &amp;quot;Logged Out&amp;quot;,&lt;br/&gt;\n &amp;quot;method&amp;quot;: &amp;quot;GET&amp;quot;,&lt;br/&gt;\n &amp;quot;status&amp;quot;: &amp;quot;500&amp;quot;,&lt;br/&gt;\n &amp;quot;level&amp;quot;: &amp;quot;paid&amp;quot;,&lt;br/&gt;\n &amp;quot;itemInSession&amp;quot;: &amp;quot;40&amp;quot;,&lt;br/&gt;\n &amp;quot;city&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;Valentin&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;zip&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;11136&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;state&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;userAgent&amp;quot;: &amp;quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_7_9 rv:6.0; bn-IN) AppleWebKit/533.8.5 (KHTML, like Gecko) Version/4.0.2 Safari/533.8.5&amp;quot;,&lt;br/&gt;\n &amp;quot;lon&amp;quot;: &amp;quot;-30.419926&amp;quot;,&lt;br/&gt;\n &amp;quot;lat&amp;quot;: &amp;quot;-11.5486195&amp;quot;,&lt;br/&gt;\n &amp;quot;userId&amp;quot;: &amp;quot;57982&amp;quot;,&lt;br/&gt;\n &amp;quot;lastName&amp;quot;: &amp;quot;Watson&amp;quot;,&lt;br/&gt;\n &amp;quot;firstName&amp;quot;: &amp;quot;Courtney&amp;quot;,&lt;br/&gt;\n &amp;quot;gender&amp;quot;: &amp;quot;M&amp;quot;,&lt;br/&gt;\n &amp;quot;registration&amp;quot;: &amp;quot;337305197&amp;quot;,&lt;br/&gt;\n &amp;quot;artist&amp;quot;: &amp;quot;David Bowie&amp;quot;,&lt;br/&gt;\n &amp;quot;song&amp;quot;: &amp;quot;Let\u2019s Dance&amp;quot;,&lt;br/&gt;\n &amp;quot;duration&amp;quot;: &amp;quot;224.5126215638037&amp;quot;&lt;br/&gt;\n}&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Listen Events&lt;/strong&gt;: Generated when users listen to songs or albums.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;listen_events: {&lt;br/&gt;\n &amp;quot;artist&amp;quot;: &amp;quot;The Rolling Stones&amp;quot;,&lt;br/&gt;\n &amp;quot;song&amp;quot;: &amp;quot;(I Can\u2019t Get No) Satisfaction&amp;quot;,&lt;br/&gt;\n &amp;quot;duration&amp;quot;: &amp;quot;473.79469057974416&amp;quot;,&lt;br/&gt;\n &amp;quot;ts&amp;quot;: 6678921,&lt;br/&gt;\n &amp;quot;sessionid&amp;quot;: &amp;quot;8202&amp;quot;,&lt;br/&gt;\n &amp;quot;auth&amp;quot;: &amp;quot;Logged Out&amp;quot;,&lt;br/&gt;\n &amp;quot;level&amp;quot;: &amp;quot;free&amp;quot;,&lt;br/&gt;\n &amp;quot;itemInSession&amp;quot;: &amp;quot;18&amp;quot;,&lt;br/&gt;\n &amp;quot;city&amp;quot;: &amp;quot;New Sandrachester&amp;quot;,&lt;br/&gt;\n &amp;quot;zip&amp;quot;: &amp;quot;22955&amp;quot;,&lt;br/&gt;\n &amp;quot;state&amp;quot;: &amp;quot;MS&amp;quot;,&lt;br/&gt;\n &amp;quot;country&amp;quot;: &amp;quot;US&amp;quot;,&lt;br/&gt;\n &amp;quot;userAgent&amp;quot;: &amp;quot;Mozilla/5.0 (iPod; U; CPU iPhone OS 3_3 like Mac OS X; hi-IN) AppleWebKit/535.47.6 (KHTML, like Gecko) Version/3.0.5 Mobile/8B118 Safari/6535.47.6&amp;quot;,&lt;br/&gt;\n &amp;quot;lon&amp;quot;: &amp;quot;-99.431258&amp;quot;,&lt;br/&gt;\n &amp;quot;lat&amp;quot;: &amp;quot;75.0709225&amp;quot;,&lt;br/&gt;\n &amp;quot;userId&amp;quot;: &amp;quot;78902&amp;quot;,&lt;br/&gt;\n &amp;quot;lastName&amp;quot;: &amp;quot;Williams&amp;quot;,&lt;br/&gt;\n &amp;quot;firstName&amp;quot;: &amp;quot;Matthew&amp;quot;,&lt;br/&gt;\n &amp;quot;gender&amp;quot;: &amp;quot;M&amp;quot;,&lt;br/&gt;\n &amp;quot;registration&amp;quot;: &amp;quot;1584877204&amp;quot;&lt;br/&gt;\n}&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Authentication Events&lt;/strong&gt;: Generated when users log in or out of a music-related service.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;auth_events: {&lt;br/&gt;\n &amp;quot;ts&amp;quot;: 380291172,&lt;br/&gt;\n &amp;quot;sessionId&amp;quot;: &amp;quot;6616&amp;quot;,&lt;br/&gt;\n &amp;quot;level&amp;quot;: &amp;quot;paid&amp;quot;,&lt;br/&gt;\n &amp;quot;itemInSession&amp;quot;: &amp;quot;79&amp;quot;,&lt;br/&gt;\n &amp;quot;city&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;West Coletown&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;zip&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;03297&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;state&amp;quot;: {&lt;br/&gt;\n &amp;quot;string&amp;quot;: &amp;quot;MI&amp;quot;&lt;br/&gt;\n },&lt;br/&gt;\n &amp;quot;userAgent&amp;quot;: &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_5) AppleWebKit/533.1 (KHTML, like Gecko) Chrome/35.0.812.0 Safari/533.1&amp;quot;,&lt;br/&gt;\n &amp;quot;lon&amp;quot;: &amp;quot;-95.78613&amp;quot;,&lt;br/&gt;\n &amp;quot;lat&amp;quot;: &amp;quot;57.1778745&amp;quot;,&lt;br/&gt;\n &amp;quot;userId&amp;quot;: &amp;quot;91224&amp;quot;,&lt;br/&gt;\n &amp;quot;lastName&amp;quot;: &amp;quot;Thomas&amp;quot;,&lt;br/&gt;\n &amp;quot;firstName&amp;quot;: &amp;quot;Greg&amp;quot;,&lt;br/&gt;\n &amp;quot;gender&amp;quot;: &amp;quot;M&amp;quot;,&lt;br/&gt;\n &amp;quot;registration&amp;quot;: &amp;quot;341396702&amp;quot;,&lt;br/&gt;\n &amp;quot;success&amp;quot;: &amp;quot;False&amp;quot;&lt;br/&gt;\n}&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;EventMusic Producer provides numerous benefits for managing music event data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Easy Deployment and Scaling&lt;/strong&gt;: Docker support enables quick deployment and seamless scaling.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt;: Batch processing optimizes network usage and reduces processing times.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficient Data Validation&lt;/strong&gt;: Avro serialization ensures the integrity and validity of event data.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Flexibility and Customization&lt;/strong&gt;: Flexible configuration allows the application to be tailored to specific needs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;EventMusic Producer is a valuable tool for developers, data analysts, and music enthusiasts looking to manage and analyze music event data. Its seamless integration with Kafka, efficient Avro serialization, and flexible features make it an ideal solution for real-time applications and data analytics.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Source Code: &lt;a href=\"https://github.com/Stefen-Taime/eventmusic\"&gt;https://github.com/Stefen-Taime/eventmusic&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Docker Hub Project URL: &lt;a href=\"https://hub.docker.com/r/stefen2020/eventmusic\"&gt;https://hub.docker.com/r/stefen2020/eventmusic&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?auto=webp&amp;s=6ea41bbdb354b573a166bc6d38f2539259bb18f8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ffe54081a924754cf01bfb4a1ea623c0fdef551", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9b9e4e35db8930ef0ef27407a7d06aeb0ef32b4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=abe5294e0289bd8d9194da3a7d9b9540c85a6ea1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a759996a5ee8513cf72c32f3955bb7a6bf2e987e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d688168a4212cdb91be81f9805ccf457258ce95d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/5Mpf_5iEhexkrFK8aVpQgjJqregrCb26-zhpAoSDesk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e281991bee112258ea88fe5b1541a58234eea85a", "width": 1080, "height": 540}], "variants": {}, "id": "9OtvRzUFOuuWyX-6JGGTFrr2d7OU05-Ngkc80mkeQ7k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b09m4g", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b09m4g/eventmusic_producer_an_opensource_application_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b09m4g/eventmusic_producer_an_opensource_application_for/", "subreddit_subscribers": 163914, "created_utc": 1708924644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since I've never seen it mentioned here, I wanted to give it a shout\n\nhttps://dataninjago.com/\n\n", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataninjago is an incredibly useful blog for everything Spark related", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0r84s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708977992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since I&amp;#39;ve never seen it mentioned here, I wanted to give it a shout&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dataninjago.com/\"&gt;https://dataninjago.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?auto=webp&amp;s=39c2a5bd8776bf0e7b0144606e7543ecd4c7ddb9", "width": 180, "height": 180}, "resolutions": [{"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=135655fd0e3621c1a856d718292ab427bf80e75a", "width": 108, "height": 108}], "variants": {}, "id": "rWVpgP_Xt3xAzKbBbkWux1zFSEWLAUYM8N_v939mmQ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0r84s", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "subreddit_subscribers": 163914, "created_utc": 1708977992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll be transitioning positions from financial reporting where speed to market is more important than efficiency to IoT devices where efficiency is more important than speed to market. Some data sets range into the billions to tens of billions of records.\n\nThe stack consists of Azure and Snowflake, with source code handling API calls. I\u2019ve been reading up a bit on utilizing Go/Rust to handle the extract and load through continuous Functions for real time streaming and then DBT through a Logic App with functions as the runtime per DBT node (think tests and models).\n\nCurious to hear if other folks have advice for things to consider when working with IoT, larger data sets, and Azure (my experience is in AWS so it\u2019s a minor pivot).", "author_fullname": "t2_708ooj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Finance to IoT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0m2b3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708965814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll be transitioning positions from financial reporting where speed to market is more important than efficiency to IoT devices where efficiency is more important than speed to market. Some data sets range into the billions to tens of billions of records.&lt;/p&gt;\n\n&lt;p&gt;The stack consists of Azure and Snowflake, with source code handling API calls. I\u2019ve been reading up a bit on utilizing Go/Rust to handle the extract and load through continuous Functions for real time streaming and then DBT through a Logic App with functions as the runtime per DBT node (think tests and models).&lt;/p&gt;\n\n&lt;p&gt;Curious to hear if other folks have advice for things to consider when working with IoT, larger data sets, and Azure (my experience is in AWS so it\u2019s a minor pivot).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0m2b3", "is_robot_indexable": true, "report_reasons": null, "author": "ExistentialFajitas", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b0m2b3/from_finance_to_iot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0m2b3/from_finance_to_iot/", "subreddit_subscribers": 163914, "created_utc": 1708965814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We extract data using Talend from multiple tables in Oracle DB, Salesforce and load it in Redshift. There is no transformation in the process. Just extract and load. We perform the transformations once it is in redshift. We want to replace talend with another tool/technology and would like to know suggestions for this. I wish to not use low code tools for this because I believe it might hamper my learning/career. Which technology should I use if I want to the tick off the three things below :\n1. Well suited and less complex \n2. Good learning and skill development by getting exposed to state of the art DE/SWE tool/tech\n3. I want to have an option of moving to SWE so I want to use this opportunity for learning SWE skills which will help me market myself as an SWE once I am active in the job market ", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0bhh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708931474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We extract data using Talend from multiple tables in Oracle DB, Salesforce and load it in Redshift. There is no transformation in the process. Just extract and load. We perform the transformations once it is in redshift. We want to replace talend with another tool/technology and would like to know suggestions for this. I wish to not use low code tools for this because I believe it might hamper my learning/career. Which technology should I use if I want to the tick off the three things below :\n1. Well suited and less complex \n2. Good learning and skill development by getting exposed to state of the art DE/SWE tool/tech\n3. I want to have an option of moving to SWE so I want to use this opportunity for learning SWE skills which will help me market myself as an SWE once I am active in the job market &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0bhh1", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0bhh1/elt_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0bhh1/elt_options/", "subreddit_subscribers": 163914, "created_utc": 1708931474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone, \nBased in London I am a founding engineer in a early stage startup, we've been building a MVP for a year at my work (haven't sold and no real clients/users). It's a niche game changer (as per the demo-ers of product) in fintech for analysts specially. \n\nIt wasn't easy at first but my CEO and I together designed an ETL kind of system for that using all fancy tech(dagster, timescale cloud) . Streamlined the data investigation/QA process with python and started ingestion, it took a while (about 5 months ) just to get the process ready. \n\nNow for the last three months I have been manually cherry picking data to upload in the dB using  excel sheets from our source database (OECD/IMF and such). \n\nI am on a visa in UK so I cannot afford to not have a job (as in take a break and prepare/look for new job at my own pace) and I also need to find a sponsorship before my visa expires. The reason I am not feeling confident to apply for DE roles is that I have worked with big data framework like Hadoop and Spark at uni only and on my own pet project very little but haven't done so in a professional setting. \n\nI have been learning Data bricks/DBT one step at a time in my free time but haven't gotten the bigger pictureon their implication.Been reading \"Foundation of data engineering\" and \"Designing data intensive applications\". Basically as much I can after working 12hrs a day.\n\nSo bottom line, my question to all the nice people of this sub is would it be worth it for me to learn data modelling properly, sharpen my sql and learn the mordern datastack and apply for only DE job or should I rather just go and have a general approach where I do the job my skills can get my right away. \n\nIf you've read this far a word of advice would mean a lot. \n\nThank you", "author_fullname": "t2_cdqy9ph6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice/suggestion on my next switch as an Data Engineer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b07n9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708918215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone, \nBased in London I am a founding engineer in a early stage startup, we&amp;#39;ve been building a MVP for a year at my work (haven&amp;#39;t sold and no real clients/users). It&amp;#39;s a niche game changer (as per the demo-ers of product) in fintech for analysts specially. &lt;/p&gt;\n\n&lt;p&gt;It wasn&amp;#39;t easy at first but my CEO and I together designed an ETL kind of system for that using all fancy tech(dagster, timescale cloud) . Streamlined the data investigation/QA process with python and started ingestion, it took a while (about 5 months ) just to get the process ready. &lt;/p&gt;\n\n&lt;p&gt;Now for the last three months I have been manually cherry picking data to upload in the dB using  excel sheets from our source database (OECD/IMF and such). &lt;/p&gt;\n\n&lt;p&gt;I am on a visa in UK so I cannot afford to not have a job (as in take a break and prepare/look for new job at my own pace) and I also need to find a sponsorship before my visa expires. The reason I am not feeling confident to apply for DE roles is that I have worked with big data framework like Hadoop and Spark at uni only and on my own pet project very little but haven&amp;#39;t done so in a professional setting. &lt;/p&gt;\n\n&lt;p&gt;I have been learning Data bricks/DBT one step at a time in my free time but haven&amp;#39;t gotten the bigger pictureon their implication.Been reading &amp;quot;Foundation of data engineering&amp;quot; and &amp;quot;Designing data intensive applications&amp;quot;. Basically as much I can after working 12hrs a day.&lt;/p&gt;\n\n&lt;p&gt;So bottom line, my question to all the nice people of this sub is would it be worth it for me to learn data modelling properly, sharpen my sql and learn the mordern datastack and apply for only DE job or should I rather just go and have a general approach where I do the job my skills can get my right away. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve read this far a word of advice would mean a lot. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b07n9i", "is_robot_indexable": true, "report_reasons": null, "author": "miloplyat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b07n9i/looking_for_advicesuggestion_on_my_next_switch_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b07n9i/looking_for_advicesuggestion_on_my_next_switch_as/", "subreddit_subscribers": 163914, "created_utc": 1708918215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_paxxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient Non-Deterministic Sampling of Large BigQuery Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0si79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aO4E1W9053BUU_tzaP7abCmtLjZJ5zG5l7_GhV3BCfs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708980979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?auto=webp&amp;s=28328d547b21f8095cec278345241be98fa95b38", "width": 866, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=495b870085e41f24e1da2921cdb6ec99e9f4c272", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c458f751c6ec548295d153cc189050091bbf9984", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343387ac40b02dbc1f48ed92e3abf59970835fb8", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97da34b422f6db95192405571030235ac343ed88", "width": 640, "height": 480}], "variants": {}, "id": "yofiS0ASy7GpSFye6CASKDiG69VKiyUGIXYdS0FllMY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0si79", "is_robot_indexable": true, "report_reasons": null, "author": "mdixon1010", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0si79/efficient_nondeterministic_sampling_of_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "subreddit_subscribers": 163914, "created_utc": 1708980979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we've been able to build out ideas quickly but there aren't any real constraints between tables and I'm worried about the long-term maintenance efforts that could come up with this system. Right now we're relying on a lot of dbt tests to verify our assumptions on the whole system.", "author_fullname": "t2_653as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a DWH on S3/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b10kkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709001182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we&amp;#39;ve been able to build out ideas quickly but there aren&amp;#39;t any real constraints between tables and I&amp;#39;m worried about the long-term maintenance efforts that could come up with this system. Right now we&amp;#39;re relying on a lot of dbt tests to verify our assumptions on the whole system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b10kkd", "is_robot_indexable": true, "report_reasons": null, "author": "tedward27", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "subreddit_subscribers": 163914, "created_utc": 1709001182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.\n\nMy Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is \"expected behavior\" when using their \"intelligent cache\" technology on a private managed vnet.\n\nThat is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.\n\nPlease let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.\n", "author_fullname": "t2_6dfcntmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse reliability on private endpoints ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xukn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708993777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.&lt;/p&gt;\n\n&lt;p&gt;My Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is &amp;quot;expected behavior&amp;quot; when using their &amp;quot;intelligent cache&amp;quot; technology on a private managed vnet.&lt;/p&gt;\n\n&lt;p&gt;That is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0xukn", "is_robot_indexable": true, "report_reasons": null, "author": "SmallAd3697", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "subreddit_subscribers": 163914, "created_utc": 1708993777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. I want to dump my company's db to parquet files in S3. In my understanding, once it is dumped to parquet files it won't update again if there are changes in the db. \n\nWhat's the best way to update those parquet files with the latest data changes? Example, if a user is suddenly blocked. \n\nData is around 10GB", "author_fullname": "t2_4rpxclpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using MongoDB Data Federation to dump to parquet files, how do I keep them updated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0ixnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708958032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I want to dump my company&amp;#39;s db to parquet files in S3. In my understanding, once it is dumped to parquet files it won&amp;#39;t update again if there are changes in the db. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to update those parquet files with the latest data changes? Example, if a user is suddenly blocked. &lt;/p&gt;\n\n&lt;p&gt;Data is around 10GB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0ixnq", "is_robot_indexable": true, "report_reasons": null, "author": "RedBlueWhiteBlack", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0ixnq/using_mongodb_data_federation_to_dump_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0ixnq/using_mongodb_data_federation_to_dump_to_parquet/", "subreddit_subscribers": 163914, "created_utc": 1708958032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am running multiple flink jobs in one yarn session and all the logs are written in \"taskmanager.log\" and \"job manager.log\" files which makes it difficult to check logs of a specific job. Is there any way to create seperate log files for each flink job?", "author_fullname": "t2_rckks0bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache flink logs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b09p0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708924919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running multiple flink jobs in one yarn session and all the logs are written in &amp;quot;taskmanager.log&amp;quot; and &amp;quot;job manager.log&amp;quot; files which makes it difficult to check logs of a specific job. Is there any way to create seperate log files for each flink job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b09p0n", "is_robot_indexable": true, "report_reasons": null, "author": "TKMater", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b09p0n/apache_flink_logs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b09p0n/apache_flink_logs/", "subreddit_subscribers": 163914, "created_utc": 1708924919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My manager along with one of her HR buddies have had an informal conversation about what cert I should do as industry-standard in order to get me an in-role promotion from a data analyst to a data engineer\n\nThey have collectively agreed on the \"Certified Data Professional\", which is offered by the institute for certification of computing professionals, or ICCP, as part of its general database professional program.\n\n&amp;#x200B;\n\nI posted on this sub earlier regarding vendor-specific certs and looks like AWS is the better one to take, even though its the harder one! But alongside that, shall I agree to take this ICCP one also? All will be paid for by the company so money is not an issue.\n\n&amp;#x200B;\n\nAre there any udemy courses out there that walk me through this ICCP one? as their website seems abit old fashioned and I dont want to be stuck with just a big book to read from, need something more interactive!", "author_fullname": "t2_2btsrky1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Certified Data Professional cert, offered by ICCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0kypc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708963154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My manager along with one of her HR buddies have had an informal conversation about what cert I should do as industry-standard in order to get me an in-role promotion from a data analyst to a data engineer&lt;/p&gt;\n\n&lt;p&gt;They have collectively agreed on the &amp;quot;Certified Data Professional&amp;quot;, which is offered by the institute for certification of computing professionals, or ICCP, as part of its general database professional program.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I posted on this sub earlier regarding vendor-specific certs and looks like AWS is the better one to take, even though its the harder one! But alongside that, shall I agree to take this ICCP one also? All will be paid for by the company so money is not an issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are there any udemy courses out there that walk me through this ICCP one? as their website seems abit old fashioned and I dont want to be stuck with just a big book to read from, need something more interactive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b0kypc", "is_robot_indexable": true, "report_reasons": null, "author": "yungfilly", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0kypc/thoughts_on_certified_data_professional_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0kypc/thoughts_on_certified_data_professional_cert/", "subreddit_subscribers": 163914, "created_utc": 1708963154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will try to summarize as succinctly as I can. We're using datastage parallel and sequencer jobs to handle extract load and transform for our warehouse. We more or less learned as we went here with our processes. We used to have a Netezza on-premises and are now migrated to Netezza on Azure. Before we took for granted, just to get the job done, moving data from netezza, into a parallel job, then back to Netezza. Over time we shifted our processes so that all of the joins and, where possible, transformations are done with NZSQL code. Due to a lacking orchestration tool we're still running the data through the datastage server (on-prem) via parallel jobs that are called/scheduled with sequencer jobs. Its a pretty minor step from this point to edit the code a little further to change from a SELECT statement to an INSERT INTO. A good number of our tables are truncated before batch processing, we're simply loading data from the updated source data tables.   \n\n\nNow to my question, I could use a parallel job to execute the INSERT statement but only in a BEFORE or AFTER statement, which means i have to write a dummy query for the job to not fail. Something like a count of the table records, or even just 'select 1'. I've done this on some larger data jobs that we needed to regain performance on, but I am not a fan of losing some of the logging capability of datastage. If a BEFORE or AFTER statement succeeds or fails, thats all I get in the log, no error or return is captured from executing the statement. In Datastage there is an 'optimize' feature that is supposed to take the job design and write SQL code to push up or push down the processing to the source or target server. This is basically what we've done manually, but we want to take it a step further. Out of necessity, its all I have, I need to have datastage execute the SQL, but I don't need/want it to touch the data.   \n\n\nDoes anyone have experience with this? I am also open to hearing about non-datastage options that I can learn about for potential use in the future for our department. TYIA!", "author_fullname": "t2_ir7m9j5mo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datastage as Orchestration Tool? (Best Practices for non ETL data loading with datastage?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0kx5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708963047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will try to summarize as succinctly as I can. We&amp;#39;re using datastage parallel and sequencer jobs to handle extract load and transform for our warehouse. We more or less learned as we went here with our processes. We used to have a Netezza on-premises and are now migrated to Netezza on Azure. Before we took for granted, just to get the job done, moving data from netezza, into a parallel job, then back to Netezza. Over time we shifted our processes so that all of the joins and, where possible, transformations are done with NZSQL code. Due to a lacking orchestration tool we&amp;#39;re still running the data through the datastage server (on-prem) via parallel jobs that are called/scheduled with sequencer jobs. Its a pretty minor step from this point to edit the code a little further to change from a SELECT statement to an INSERT INTO. A good number of our tables are truncated before batch processing, we&amp;#39;re simply loading data from the updated source data tables.   &lt;/p&gt;\n\n&lt;p&gt;Now to my question, I could use a parallel job to execute the INSERT statement but only in a BEFORE or AFTER statement, which means i have to write a dummy query for the job to not fail. Something like a count of the table records, or even just &amp;#39;select 1&amp;#39;. I&amp;#39;ve done this on some larger data jobs that we needed to regain performance on, but I am not a fan of losing some of the logging capability of datastage. If a BEFORE or AFTER statement succeeds or fails, thats all I get in the log, no error or return is captured from executing the statement. In Datastage there is an &amp;#39;optimize&amp;#39; feature that is supposed to take the job design and write SQL code to push up or push down the processing to the source or target server. This is basically what we&amp;#39;ve done manually, but we want to take it a step further. Out of necessity, its all I have, I need to have datastage execute the SQL, but I don&amp;#39;t need/want it to touch the data.   &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with this? I am also open to hearing about non-datastage options that I can learn about for potential use in the future for our department. TYIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0kx5g", "is_robot_indexable": true, "report_reasons": null, "author": "I_Am_Jacks_Voice", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0kx5g/datastage_as_orchestration_tool_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0kx5g/datastage_as_orchestration_tool_best_practices/", "subreddit_subscribers": 163914, "created_utc": 1708963047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nSmall example: Let's say we need to ingest data from a Postgres DB server which has 10 DBs, and each DB having 10 tables.\n\nQuestion: How would one best organize the Debezium Kafka Connectors?\n\n* 1 Connector per DB\n* 1 Connecter for everything\n* 1 Connector per table\n* something else\n\nPlease and thank you", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium and Connector organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0ia9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708956271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;Small example: Let&amp;#39;s say we need to ingest data from a Postgres DB server which has 10 DBs, and each DB having 10 tables.&lt;/p&gt;\n\n&lt;p&gt;Question: How would one best organize the Debezium Kafka Connectors?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 Connector per DB&lt;/li&gt;\n&lt;li&gt;1 Connecter for everything&lt;/li&gt;\n&lt;li&gt;1 Connector per table&lt;/li&gt;\n&lt;li&gt;something else&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please and thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0ia9b", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0ia9b/debezium_and_connector_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0ia9b/debezium_and_connector_organization/", "subreddit_subscribers": 163914, "created_utc": 1708956271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all. I'm new to creating database. In my company (medium sized), data are stored in multiple Excel spreadsheets, ranging from 300 rows to as much as 20k rows. It's slowing down the file opening by a lot the more data is added.\n\nI understand Excel is never the suitable tool for database management. Is MS Access something I should use? My company also uses an SQL editor called Hue, but only certain user is allowed to use it due to the pricey license. We have about 20 users to use/query the data.\n\nI did some simple searches on the internet, it seems like MS Access is not a likable solution among many. How should I go about it in my case? Can Hue be connected with Excel or Access? Any recommendation is much appreciated.", "author_fullname": "t2_1757k1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Database with Limited tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0i9k7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708956212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. I&amp;#39;m new to creating database. In my company (medium sized), data are stored in multiple Excel spreadsheets, ranging from 300 rows to as much as 20k rows. It&amp;#39;s slowing down the file opening by a lot the more data is added.&lt;/p&gt;\n\n&lt;p&gt;I understand Excel is never the suitable tool for database management. Is MS Access something I should use? My company also uses an SQL editor called Hue, but only certain user is allowed to use it due to the pricey license. We have about 20 users to use/query the data.&lt;/p&gt;\n\n&lt;p&gt;I did some simple searches on the internet, it seems like MS Access is not a likable solution among many. How should I go about it in my case? Can Hue be connected with Excel or Access? Any recommendation is much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0i9k7", "is_robot_indexable": true, "report_reasons": null, "author": "imcrazyzzz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0i9k7/creating_database_with_limited_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0i9k7/creating_database_with_limited_tool/", "subreddit_subscribers": 163914, "created_utc": 1708956212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Introduction:** Hi everyone! This post is about OptScale software, which assists in cloud cost management, ML/AI task profiling and optimization, PaaS and SaaS instrumentation, and provides the following:  \n\n\n**MLOps capabilities:**\n\n* ML Leaderboards with candidates and qualifications\n* Dataset and model tracking and versioning\n* Run metrics and experiment tracker\n* Hypertuning integrated with Optuna\n* Training launcher\n* ML Model training profiler\n\n**FinOps and cloud cost optimization features:**\n\n* Optimal utilization of Reserved Instances, Savings Plans, and Spot Instances\n* Unused resource detection\n* R&amp;D resource power management and rightsizing\n* S3 duplicate object finder\n* Resource bottleneck identification\n* Optimal instance type and family selection\n* Databricks support\n* S3 and Redshift instrumentation\n* VM Power Schedules\n\n**GitHub link:** [https://github.com/hystax/optscale](https://github.com/hystax/optscale)\n\nWe\u2019d appreciate it if you give us a Star\n\n**Live demo:** [https://my.optscale.com/live-demo](https://my.optscale.com/live-demo)", "author_fullname": "t2_7s7idrlt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OptScale is an MLOps and FinOps OPEN SOURCE platform to run ML/AI or any other workloads with optimal performance and infrastructure cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0esry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708945059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; Hi everyone! This post is about OptScale software, which assists in cloud cost management, ML/AI task profiling and optimization, PaaS and SaaS instrumentation, and provides the following:  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MLOps capabilities:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ML Leaderboards with candidates and qualifications&lt;/li&gt;\n&lt;li&gt;Dataset and model tracking and versioning&lt;/li&gt;\n&lt;li&gt;Run metrics and experiment tracker&lt;/li&gt;\n&lt;li&gt;Hypertuning integrated with Optuna&lt;/li&gt;\n&lt;li&gt;Training launcher&lt;/li&gt;\n&lt;li&gt;ML Model training profiler&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;FinOps and cloud cost optimization features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Optimal utilization of Reserved Instances, Savings Plans, and Spot Instances&lt;/li&gt;\n&lt;li&gt;Unused resource detection&lt;/li&gt;\n&lt;li&gt;R&amp;amp;D resource power management and rightsizing&lt;/li&gt;\n&lt;li&gt;S3 duplicate object finder&lt;/li&gt;\n&lt;li&gt;Resource bottleneck identification&lt;/li&gt;\n&lt;li&gt;Optimal instance type and family selection&lt;/li&gt;\n&lt;li&gt;Databricks support&lt;/li&gt;\n&lt;li&gt;S3 and Redshift instrumentation&lt;/li&gt;\n&lt;li&gt;VM Power Schedules&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub link:&lt;/strong&gt; &lt;a href=\"https://github.com/hystax/optscale\"&gt;https://github.com/hystax/optscale&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We\u2019d appreciate it if you give us a Star&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href=\"https://my.optscale.com/live-demo\"&gt;https://my.optscale.com/live-demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?auto=webp&amp;s=1cd2d0ef465f7b3c1578b0e701c486cc29bfb870", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30a7bbfb76930802aae05698e2f3423517f25fa0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24f549f12fdad8048707da9ddf7e5bc59c362278", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd62aa0c5d58e1b26ba63f5a6a634007256f4434", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8912f31f34ae026bd10f68a68c244f80aff4e28e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9eb4d49ec0ccb24eadfa1f893aa4e317c31885d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jrs6WNrJ-XE_bYocPKTtBxF9EDMudBielThtT52BfIc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3352bb2b6fc109f7fe84f33e10d364d4c6ef03c", "width": 1080, "height": 540}], "variants": {}, "id": "KUR5VsM_gZWFOxHflOrqnHoIAdyDqXzEcNZMf7hbHuc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b0esry", "is_robot_indexable": true, "report_reasons": null, "author": "Hystax", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0esry/optscale_is_an_mlops_and_finops_open_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0esry/optscale_is_an_mlops_and_finops_open_source/", "subreddit_subscribers": 163914, "created_utc": 1708945059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here taken the Cloudera Data Engineering Certificate CDP-3002 exam?\n\nI'm going to take a Cloudera Certificate Exam (CDP Data Engineer Exam CDP-3002). I have already read the FAQ section, but I still have some questions about it. Please understand that I'm just a fresh graduate student from a 3rd world country, so the 333$ cost is very high for me. So I need to make sure I am fully prepared before paying for the exam.\n\n1. From payment, how long does it take until I receive an email with detailed information about the test?  \n2. From payment, how long does it take until the test starts?  \n3. The email will guide me in detail on the steps to start the exam, right?  \n4. I read through the FAQ section, is there a person called \"proctor\" who will instruct me on what to do before the exam?  \n5. I just need to install the Questionmark Secure software, right?\n\n6. There are only multiplier choice questions and no exercise questions (I have to code), right?\n\nSorry for all the questions, please understand for me because I'm very confused\u00a0right now. There isn't much thing on the internet about this certificate exam.", "author_fullname": "t2_xs4tsol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about Cloudera Data Engineering Certificate CDP-3002", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0bi4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708931547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here taken the Cloudera Data Engineering Certificate CDP-3002 exam?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to take a Cloudera Certificate Exam (CDP Data Engineer Exam CDP-3002). I have already read the FAQ section, but I still have some questions about it. Please understand that I&amp;#39;m just a fresh graduate student from a 3rd world country, so the 333$ cost is very high for me. So I need to make sure I am fully prepared before paying for the exam.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;From payment, how long does it take until I receive an email with detailed information about the test?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;From payment, how long does it take until the test starts?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;The email will guide me in detail on the steps to start the exam, right?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;I read through the FAQ section, is there a person called &amp;quot;proctor&amp;quot; who will instruct me on what to do before the exam?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I just need to install the Questionmark Secure software, right?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are only multiplier choice questions and no exercise questions (I have to code), right?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Sorry for all the questions, please understand for me because I&amp;#39;m very confused\u00a0right now. There isn&amp;#39;t much thing on the internet about this certificate exam.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b0bi4u", "is_robot_indexable": true, "report_reasons": null, "author": "quangbilly79", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0bi4u/question_about_cloudera_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0bi4u/question_about_cloudera_data_engineering/", "subreddit_subscribers": 163914, "created_utc": 1708931547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here are some of my higher-level tables:\n\n    CREATE TABLE workcenter_task (\n        id SERIAL PRIMARY KEY,\n        workcenter_id INT,\n        task_id INT\n    );\n    \n    CREATE TABLE space_task (\n        id SERIAL PRIMARY KEY,\n        space_id INT,\n        task_id INT\n    );\n    \n    CREATE TABLE workcenter_space (\n        id SERIAL PRIMARY KEY,\n        workcenter_id INT,\n        space_id INT,\n    \n        CONSTRAINT chk_space_task_workcenter \n            CHECK (NOT EXISTS (\n                SELECT 1\n                FROM space_task st\n                LEFT JOIN workcenter_task wt ON st.task_id = wt.task_id\n                WHERE st.space_id = workcenter_space.space_id\n                AND wt.workcenter_id != workcenter_space.workcenter_id\n            ))\n    );\n    \n\nNotice the \\`chk\\_space\\_task\\_workcenter\\` constraint. It is designed to make sure, on insert, that the newly inserted workcenter contains all tasks associated with the newly inserted space. If the space is assigned a task (within \\`space\\_task\\`) that the workcenter does not also have assigned to it (within \\`workcenter\\_task\\`), the insert should fail.\n\n&amp;#x200B;\n\nThis is great for maintaining integrity on insert, but it does very little to maintain integrity after that point. For example, a user could satisfy the requirements for insert but then delete the record in \\`workcenter\\_task\\` that allowed the insert in the first place. This is a violation of system integrity.\n\n&amp;#x200B;\n\nWould something like a partial index be usable for my desired outcome, such that a user can not make this type of violation? I want to avoid triggers because I understand they can get out of hand over time. Would this work : ?  \n\n\n    CREATE UNIQUE INDEX idx_space_task_workcenter\n    ON workcenter_space (space_id)\n    WHERE NOT EXISTS (\n        SELECT 1\n        FROM space_task st\n        LEFT JOIN workcenter_task wt ON st.task_id = wt.task_id\n        WHERE st.space_id = workcenter_space.space_id\n        AND wt.workcenter_id != workcenter_space.workcenter_id\n    );\n\nI'm not sure if this work be considered reliable and guarantee that users can not violate the relationship where a workcenter must always have the tasks associated with it that can be found in any one of its associated spaces.  \n\n\nCan someone levy in on this approach? Thanks! I am using postgres.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrity of Partial Indexes for Complex Relationships Between Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0772d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708917855.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708916831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here are some of my higher-level tables:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE workcenter_task (\n    id SERIAL PRIMARY KEY,\n    workcenter_id INT,\n    task_id INT\n);\n\nCREATE TABLE space_task (\n    id SERIAL PRIMARY KEY,\n    space_id INT,\n    task_id INT\n);\n\nCREATE TABLE workcenter_space (\n    id SERIAL PRIMARY KEY,\n    workcenter_id INT,\n    space_id INT,\n\n    CONSTRAINT chk_space_task_workcenter \n        CHECK (NOT EXISTS (\n            SELECT 1\n            FROM space_task st\n            LEFT JOIN workcenter_task wt ON st.task_id = wt.task_id\n            WHERE st.space_id = workcenter_space.space_id\n            AND wt.workcenter_id != workcenter_space.workcenter_id\n        ))\n);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Notice the `chk_space_task_workcenter` constraint. It is designed to make sure, on insert, that the newly inserted workcenter contains all tasks associated with the newly inserted space. If the space is assigned a task (within `space_task`) that the workcenter does not also have assigned to it (within `workcenter_task`), the insert should fail.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is great for maintaining integrity on insert, but it does very little to maintain integrity after that point. For example, a user could satisfy the requirements for insert but then delete the record in `workcenter_task` that allowed the insert in the first place. This is a violation of system integrity.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would something like a partial index be usable for my desired outcome, such that a user can not make this type of violation? I want to avoid triggers because I understand they can get out of hand over time. Would this work : ?  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE UNIQUE INDEX idx_space_task_workcenter\nON workcenter_space (space_id)\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM space_task st\n    LEFT JOIN workcenter_task wt ON st.task_id = wt.task_id\n    WHERE st.space_id = workcenter_space.space_id\n    AND wt.workcenter_id != workcenter_space.workcenter_id\n);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this work be considered reliable and guarantee that users can not violate the relationship where a workcenter must always have the tasks associated with it that can be found in any one of its associated spaces.  &lt;/p&gt;\n\n&lt;p&gt;Can someone levy in on this approach? Thanks! I am using postgres.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0772d", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0772d/integrity_of_partial_indexes_for_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0772d/integrity_of_partial_indexes_for_complex/", "subreddit_subscribers": 163914, "created_utc": 1708916831.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}