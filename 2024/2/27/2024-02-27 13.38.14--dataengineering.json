{"kind": "Listing", "data": {"after": "t3_1b0ia9b", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reaching a mental breaking point.\n\nI have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column 'ID' is in a predetermined set of ID's (roughly 135,000,000) . I've tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.\n\nI really don't know what to do at this point or if it is possible to make an efficient script to do this.", "author_fullname": "t2_ecc5gnt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read/Filter a 1.7 TB CSV File in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0up0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708986031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reaching a mental breaking point.&lt;/p&gt;\n\n&lt;p&gt;I have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column &amp;#39;ID&amp;#39; is in a predetermined set of ID&amp;#39;s (roughly 135,000,000) . I&amp;#39;ve tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.&lt;/p&gt;\n\n&lt;p&gt;I really don&amp;#39;t know what to do at this point or if it is possible to make an efficient script to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0up0e", "is_robot_indexable": true, "report_reasons": null, "author": "The-Salamander-Fan", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "subreddit_subscribers": 164010, "created_utc": 1708986031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Data Peeps,\n\nI was recently asked this question and failed miserably at it. Wanted to get people's opinion on how I can do better on it.   \n\n\nIngestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. \n\nProcessing: We can run a daily spark pipeline to update the playlist.   \n\n\nThey told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   \n\n\nI wasn't able to answer that. Can anyone please help me with how to design such things? ", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture: Generate Apple Music's Top 50 multiple playlists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xiyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708992919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Data Peeps,&lt;/p&gt;\n\n&lt;p&gt;I was recently asked this question and failed miserably at it. Wanted to get people&amp;#39;s opinion on how I can do better on it.   &lt;/p&gt;\n\n&lt;p&gt;Ingestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. &lt;/p&gt;\n\n&lt;p&gt;Processing: We can run a daily spark pipeline to update the playlist.   &lt;/p&gt;\n\n&lt;p&gt;They told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   &lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t able to answer that. Can anyone please help me with how to design such things? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0xiyh", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "subreddit_subscribers": 164010, "created_utc": 1708992919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8vv7mvb57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When marketing needs your help (heard through Hightouch)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0lgyz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/z6JK0URsKazrZJg_1gchcdXyEKI2mLBxUtYAHvvFXgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708964375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dnivh4m3gykc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?auto=webp&amp;s=5a0f919391c9cf55fccee5f9470f236e70ed34e9", "width": 500, "height": 558}, "resolutions": [{"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5d662aee614780ed8cdff0d32f83f7b693a78a3", "width": 108, "height": 120}, {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8bd1af3f1d325c442bee2d285ca13fdb1418076", "width": 216, "height": 241}, {"url": "https://preview.redd.it/dnivh4m3gykc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb4c6fc9c2668ccb4c4d43afc1c045d76c772f7f", "width": 320, "height": 357}], "variants": {}, "id": "-Bx4u7D20iRyvf7K1LxxOqyjpLczguKZv4s96kNoWWo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1b0lgyz", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Cartographer4232", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0lgyz/when_marketing_needs_your_help_heard_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dnivh4m3gykc1.jpeg", "subreddit_subscribers": 164010, "created_utc": 1708964375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we've been able to build out ideas quickly but there aren't any real constraints between tables and I'm worried about the long-term maintenance efforts that could come up with this system. Right now we're relying on a lot of dbt tests to verify our assumptions on the whole system.", "author_fullname": "t2_653as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a DWH on S3/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b10kkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709001182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we&amp;#39;ve been able to build out ideas quickly but there aren&amp;#39;t any real constraints between tables and I&amp;#39;m worried about the long-term maintenance efforts that could come up with this system. Right now we&amp;#39;re relying on a lot of dbt tests to verify our assumptions on the whole system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b10kkd", "is_robot_indexable": true, "report_reasons": null, "author": "tedward27", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "subreddit_subscribers": 164010, "created_utc": 1709001182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, ingestr is an open-source command-line application that allows ingesting &amp; copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)\n\nIt does a few things that make it the easiest alternative out there:\n\n&amp;#x200B;\n\n* \u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs\n* \u2795 incremental loading: create+replace, delete+insert, append\n* \ud83d\udc0d single-command installation: pip install ingestr\n\nWe built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.\n\nSome common use-cases ingestr solve are:\n\n&amp;#x200B;\n\n* Migrating data from legacy systems to modern databases for better analysis\n* Syncing data between your application's database and your analytics platform in batches or incrementally\n* Backing up your databases to ensure data safety\n* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases\n* Facilitating real-time data transfer for applications that require immediate updates\n\nWe\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0[https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)", "author_fullname": "t2_153gh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source CLI tool to ingest/copy data between any databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18mfk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709029438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, ingestr is an open-source command-line application that allows ingesting &amp;amp; copying data between two databases without any code: &lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It does a few things that make it the easiest alternative out there:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs&lt;/li&gt;\n&lt;li&gt;\u2795 incremental loading: create+replace, delete+insert, append&lt;/li&gt;\n&lt;li&gt;\ud83d\udc0d single-command installation: pip install ingestr&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.&lt;/p&gt;\n\n&lt;p&gt;Some common use-cases ingestr solve are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Migrating data from legacy systems to modern databases for better analysis&lt;/li&gt;\n&lt;li&gt;Syncing data between your application&amp;#39;s database and your analytics platform in batches or incrementally&lt;/li&gt;\n&lt;li&gt;Backing up your databases to ensure data safety&lt;/li&gt;\n&lt;li&gt;Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases&lt;/li&gt;\n&lt;li&gt;Facilitating real-time data transfer for applications that require immediate updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0&lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?auto=webp&amp;s=71ed11c21a77b630e601e5051f4772431462627b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04bda8eab9d0ae0614648424f7d053daef1a08e6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=49188a34f55d47bd66a68f0f88c20fb3ba364186", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08276d6a94f8f7be274a822eeb3a2826c0d29076", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da5f26047549b5b649fe8d98e70f833e5829aa1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b2d7565df4a4182a23ea538152715a64554bf04", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d57305cc005ba7fa2adfb13086d6f70916ef5c0", "width": 1080, "height": 540}], "variants": {}, "id": "XDsKdm5QbGo1ePzc9WAtlkw6WpXhnmXv5XfGicn_zpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b18mfk", "is_robot_indexable": true, "report_reasons": null, "author": "karakanb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "subreddit_subscribers": 164010, "created_utc": 1709029438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi team,\n\nhope you are crushing it this week.\n\nI have a newbie question here which i am sure have been asked many times as well and here goes.\n\nQuestion is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify's api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?\n\nWe are trying to understand if using an additional service like an integration tool justify the cost in the long run.\n\nThe other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.\n\nI am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook's api, twitter's api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?\n\n&amp;#x200B;\n\nthank you for your wisdom on this troubling thought.", "author_fullname": "t2_6mulsm33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using data integration tools like fivetran, airbyte etc vs building it from the ground up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b12old", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709007380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi team,&lt;/p&gt;\n\n&lt;p&gt;hope you are crushing it this week.&lt;/p&gt;\n\n&lt;p&gt;I have a newbie question here which i am sure have been asked many times as well and here goes.&lt;/p&gt;\n\n&lt;p&gt;Question is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify&amp;#39;s api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?&lt;/p&gt;\n\n&lt;p&gt;We are trying to understand if using an additional service like an integration tool justify the cost in the long run.&lt;/p&gt;\n\n&lt;p&gt;The other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.&lt;/p&gt;\n\n&lt;p&gt;I am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook&amp;#39;s api, twitter&amp;#39;s api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thank you for your wisdom on this troubling thought.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b12old", "is_robot_indexable": true, "report_reasons": null, "author": "simon_chia", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "subreddit_subscribers": 164010, "created_utc": 1709007380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.\n\nNow start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.\n\nWhich means it's main work is to perform **transform and load within DWH itself.**\n\nIs my understanding correct?", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DBT's main strength to help engineers execute transformation and load within data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b16o2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709021228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.&lt;/p&gt;\n\n&lt;p&gt;Now start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.&lt;/p&gt;\n\n&lt;p&gt;Which means it&amp;#39;s main work is to perform &lt;strong&gt;transform and load within DWH itself.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is my understanding correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b16o2d", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "subreddit_subscribers": 164010, "created_utc": 1709021228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since I've never seen it mentioned here, I wanted to give it a shout\n\nhttps://dataninjago.com/\n\n", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataninjago is an incredibly useful blog for everything Spark related", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0r84s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708977992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since I&amp;#39;ve never seen it mentioned here, I wanted to give it a shout&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dataninjago.com/\"&gt;https://dataninjago.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?auto=webp&amp;s=39c2a5bd8776bf0e7b0144606e7543ecd4c7ddb9", "width": 180, "height": 180}, "resolutions": [{"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=135655fd0e3621c1a856d718292ab427bf80e75a", "width": 108, "height": 108}], "variants": {}, "id": "rWVpgP_Xt3xAzKbBbkWux1zFSEWLAUYM8N_v939mmQ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0r84s", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "subreddit_subscribers": 164010, "created_utc": 1708977992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone \n", "author_fullname": "t2_84ztczxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to Dive Deeper into Data Modeling. Recommendations for In-Depth Learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0o4bb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/t6bucAzFX9Qv_76pKS6_eTpWtudZ6I3_jAXjfmSeHoQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708970629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1q4grlv2zykc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?auto=webp&amp;s=ad493fc45be45cf6330afbc6a218203f0a401e8c", "width": 700, "height": 394}, "resolutions": [{"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b02fb77805dc60307c2937b0bbef70bdc98d7d1", "width": 108, "height": 60}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a57ee70754f3c7aa563aaeebbc32f7297678a46", "width": 216, "height": 121}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c58659241bb8f3c8658c432e2e84698657b98a1", "width": 320, "height": 180}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=050eb410f54f8b7a8ea8a4b1799151bf0c72aafe", "width": 640, "height": 360}], "variants": {}, "id": "iwWyl6t2YUzpOC_MCfgLg7f7JQrgL8tB9oFp9H92Rnk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0o4bb", "is_robot_indexable": true, "report_reasons": null, "author": "chaachans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0o4bb/looking_to_dive_deeper_into_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1q4grlv2zykc1.jpeg", "subreddit_subscribers": 164010, "created_utc": 1708970629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll be transitioning positions from financial reporting where speed to market is more important than efficiency to IoT devices where efficiency is more important than speed to market. Some data sets range into the billions to tens of billions of records.\n\nThe stack consists of Azure and Snowflake, with source code handling API calls. I\u2019ve been reading up a bit on utilizing Go/Rust to handle the extract and load through continuous Functions for real time streaming and then DBT through a Logic App with functions as the runtime per DBT node (think tests and models).\n\nCurious to hear if other folks have advice for things to consider when working with IoT, larger data sets, and Azure (my experience is in AWS so it\u2019s a minor pivot).", "author_fullname": "t2_708ooj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Finance to IoT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0m2b3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708965814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll be transitioning positions from financial reporting where speed to market is more important than efficiency to IoT devices where efficiency is more important than speed to market. Some data sets range into the billions to tens of billions of records.&lt;/p&gt;\n\n&lt;p&gt;The stack consists of Azure and Snowflake, with source code handling API calls. I\u2019ve been reading up a bit on utilizing Go/Rust to handle the extract and load through continuous Functions for real time streaming and then DBT through a Logic App with functions as the runtime per DBT node (think tests and models).&lt;/p&gt;\n\n&lt;p&gt;Curious to hear if other folks have advice for things to consider when working with IoT, larger data sets, and Azure (my experience is in AWS so it\u2019s a minor pivot).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0m2b3", "is_robot_indexable": true, "report_reasons": null, "author": "ExistentialFajitas", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b0m2b3/from_finance_to_iot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0m2b3/from_finance_to_iot/", "subreddit_subscribers": 164010, "created_utc": 1708965814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am leaving my company at the end of the week. They leave me with \\~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  \n\n\nI have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y'all recommend for me to go for next? I'm up in the air about going \"tool-specific\" like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.\n\n&amp;#x200B;\n\nP.S. I will be doing personal projects on the side as well. I know it's foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.", "author_fullname": "t2_mkh2olbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Best\" Next DE Certification to get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b13uiy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709011093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am leaving my company at the end of the week. They leave me with ~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  &lt;/p&gt;\n\n&lt;p&gt;I have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp;amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y&amp;#39;all recommend for me to go for next? I&amp;#39;m up in the air about going &amp;quot;tool-specific&amp;quot; like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. I will be doing personal projects on the side as well. I know it&amp;#39;s foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b13uiy", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid-Second-8434", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "subreddit_subscribers": 164010, "created_utc": 1709011093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you love and hate about snowflake?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake, what do you like and dislike about it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1al5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709036624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you love and hate about snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1al5z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1al5z/snowflake_what_do_you_like_and_dislike_about_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1al5z/snowflake_what_do_you_like_and_dislike_about_it/", "subreddit_subscribers": 164010, "created_utc": 1709036624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience in AWS services related to data engineering. I've been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.\n\nWhen I'm trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I'm thinking of where it's a good idea to move my stack to azure. \n\nAppreciate your ideas on this", "author_fullname": "t2_6b96x89c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS or Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18xcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709030597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience in AWS services related to data engineering. I&amp;#39;ve been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.&lt;/p&gt;\n\n&lt;p&gt;When I&amp;#39;m trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I&amp;#39;m thinking of where it&amp;#39;s a good idea to move my stack to azure. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your ideas on this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b18xcc", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Buy-4947", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18xcc/aws_or_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18xcc/aws_or_azure/", "subreddit_subscribers": 164010, "created_utc": 1709030597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm running a Spark cluster on Azure Databricks. While notebooks in the browser work, I'd prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can't access certain JVM-dependent functions like `df.rdd.getNumPartitions()` but that's ok.\n\nIs it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I'd like to avoid.", "author_fullname": "t2_jkgy1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyway to run Scala code in notebooks using Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b11qvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709004562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a Spark cluster on Azure Databricks. While notebooks in the browser work, I&amp;#39;d prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can&amp;#39;t access certain JVM-dependent functions like &lt;code&gt;df.rdd.getNumPartitions()&lt;/code&gt; but that&amp;#39;s ok.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I&amp;#39;d like to avoid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b11qvo", "is_robot_indexable": true, "report_reasons": null, "author": "napsterv", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "subreddit_subscribers": 164010, "created_utc": 1709004562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.\n\nMy Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is \"expected behavior\" when using their \"intelligent cache\" technology on a private managed vnet.\n\nThat is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.\n\nPlease let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.\n", "author_fullname": "t2_6dfcntmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse reliability on private endpoints ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xukn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708993777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.&lt;/p&gt;\n\n&lt;p&gt;My Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is &amp;quot;expected behavior&amp;quot; when using their &amp;quot;intelligent cache&amp;quot; technology on a private managed vnet.&lt;/p&gt;\n\n&lt;p&gt;That is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0xukn", "is_robot_indexable": true, "report_reasons": null, "author": "SmallAd3697", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "subreddit_subscribers": 164010, "created_utc": 1708993777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_paxxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient Non-Deterministic Sampling of Large BigQuery Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0si79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aO4E1W9053BUU_tzaP7abCmtLjZJ5zG5l7_GhV3BCfs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708980979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?auto=webp&amp;s=28328d547b21f8095cec278345241be98fa95b38", "width": 866, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=495b870085e41f24e1da2921cdb6ec99e9f4c272", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c458f751c6ec548295d153cc189050091bbf9984", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343387ac40b02dbc1f48ed92e3abf59970835fb8", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97da34b422f6db95192405571030235ac343ed88", "width": 640, "height": 480}], "variants": {}, "id": "yofiS0ASy7GpSFye6CASKDiG69VKiyUGIXYdS0FllMY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0si79", "is_robot_indexable": true, "report_reasons": null, "author": "mdixon1010", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0si79/efficient_nondeterministic_sampling_of_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "subreddit_subscribers": 164010, "created_utc": 1708980979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will try to summarize as succinctly as I can. We're using datastage parallel and sequencer jobs to handle extract load and transform for our warehouse. We more or less learned as we went here with our processes. We used to have a Netezza on-premises and are now migrated to Netezza on Azure. Before we took for granted, just to get the job done, moving data from netezza, into a parallel job, then back to Netezza. Over time we shifted our processes so that all of the joins and, where possible, transformations are done with NZSQL code. Due to a lacking orchestration tool we're still running the data through the datastage server (on-prem) via parallel jobs that are called/scheduled with sequencer jobs. Its a pretty minor step from this point to edit the code a little further to change from a SELECT statement to an INSERT INTO. A good number of our tables are truncated before batch processing, we're simply loading data from the updated source data tables.   \n\n\nNow to my question, I could use a parallel job to execute the INSERT statement but only in a BEFORE or AFTER statement, which means i have to write a dummy query for the job to not fail. Something like a count of the table records, or even just 'select 1'. I've done this on some larger data jobs that we needed to regain performance on, but I am not a fan of losing some of the logging capability of datastage. If a BEFORE or AFTER statement succeeds or fails, thats all I get in the log, no error or return is captured from executing the statement. In Datastage there is an 'optimize' feature that is supposed to take the job design and write SQL code to push up or push down the processing to the source or target server. This is basically what we've done manually, but we want to take it a step further. Out of necessity, its all I have, I need to have datastage execute the SQL, but I don't need/want it to touch the data.   \n\n\nDoes anyone have experience with this? I am also open to hearing about non-datastage options that I can learn about for potential use in the future for our department. TYIA!", "author_fullname": "t2_ir7m9j5mo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datastage as Orchestration Tool? (Best Practices for non ETL data loading with datastage?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0kx5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708963047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will try to summarize as succinctly as I can. We&amp;#39;re using datastage parallel and sequencer jobs to handle extract load and transform for our warehouse. We more or less learned as we went here with our processes. We used to have a Netezza on-premises and are now migrated to Netezza on Azure. Before we took for granted, just to get the job done, moving data from netezza, into a parallel job, then back to Netezza. Over time we shifted our processes so that all of the joins and, where possible, transformations are done with NZSQL code. Due to a lacking orchestration tool we&amp;#39;re still running the data through the datastage server (on-prem) via parallel jobs that are called/scheduled with sequencer jobs. Its a pretty minor step from this point to edit the code a little further to change from a SELECT statement to an INSERT INTO. A good number of our tables are truncated before batch processing, we&amp;#39;re simply loading data from the updated source data tables.   &lt;/p&gt;\n\n&lt;p&gt;Now to my question, I could use a parallel job to execute the INSERT statement but only in a BEFORE or AFTER statement, which means i have to write a dummy query for the job to not fail. Something like a count of the table records, or even just &amp;#39;select 1&amp;#39;. I&amp;#39;ve done this on some larger data jobs that we needed to regain performance on, but I am not a fan of losing some of the logging capability of datastage. If a BEFORE or AFTER statement succeeds or fails, thats all I get in the log, no error or return is captured from executing the statement. In Datastage there is an &amp;#39;optimize&amp;#39; feature that is supposed to take the job design and write SQL code to push up or push down the processing to the source or target server. This is basically what we&amp;#39;ve done manually, but we want to take it a step further. Out of necessity, its all I have, I need to have datastage execute the SQL, but I don&amp;#39;t need/want it to touch the data.   &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with this? I am also open to hearing about non-datastage options that I can learn about for potential use in the future for our department. TYIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0kx5g", "is_robot_indexable": true, "report_reasons": null, "author": "I_Am_Jacks_Voice", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0kx5g/datastage_as_orchestration_tool_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0kx5g/datastage_as_orchestration_tool_best_practices/", "subreddit_subscribers": 164010, "created_utc": 1708963047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. I want to dump my company's db to parquet files in S3. In my understanding, once it is dumped to parquet files it won't update again if there are changes in the db. \n\nWhat's the best way to update those parquet files with the latest data changes? Example, if a user is suddenly blocked. \n\nData is around 10GB", "author_fullname": "t2_4rpxclpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using MongoDB Data Federation to dump to parquet files, how do I keep them updated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0ixnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708958032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I want to dump my company&amp;#39;s db to parquet files in S3. In my understanding, once it is dumped to parquet files it won&amp;#39;t update again if there are changes in the db. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to update those parquet files with the latest data changes? Example, if a user is suddenly blocked. &lt;/p&gt;\n\n&lt;p&gt;Data is around 10GB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0ixnq", "is_robot_indexable": true, "report_reasons": null, "author": "RedBlueWhiteBlack", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0ixnq/using_mongodb_data_federation_to_dump_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0ixnq/using_mongodb_data_federation_to_dump_to_parquet/", "subreddit_subscribers": 164010, "created_utc": 1708958032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I'm thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.", "author_fullname": "t2_h0hkehfsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion suggestion in azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1a3ni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709035013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I&amp;#39;m thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1a3ni", "is_robot_indexable": true, "report_reasons": null, "author": "timetravel_looper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "subreddit_subscribers": 164010, "created_utc": 1709035013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello everyone,\n\nWe're in the process of decommissioning Control-M, and we're exploring options to automate the migration process. Our current setup includes customized calendars and numerous cyclic jobs with custom schedules, along with some manual triggers.\n\nWe're wondering if there are any automated tools available that could help with this migration. If not, what would be the best approach to tackle this migration given our customized setup?\n\nAny advice, recommendations, or experiences with similar migrations would be greatly appreciated. Thank you in advance!", "author_fullname": "t2_tenfic98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Migrating from Control-M to Airflow - Automated Tool or Manual Approach?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18850", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709027824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re in the process of decommissioning Control-M, and we&amp;#39;re exploring options to automate the migration process. Our current setup includes customized calendars and numerous cyclic jobs with custom schedules, along with some manual triggers.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re wondering if there are any automated tools available that could help with this migration. If not, what would be the best approach to tackle this migration given our customized setup?&lt;/p&gt;\n\n&lt;p&gt;Any advice, recommendations, or experiences with similar migrations would be greatly appreciated. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b18850", "is_robot_indexable": true, "report_reasons": null, "author": "PreviousPositive9560", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18850/seeking_advice_migrating_from_controlm_to_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18850/seeking_advice_migrating_from_controlm_to_airflow/", "subreddit_subscribers": 164010, "created_utc": 1709027824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we can connect Microsoft 365 business central SQL Database!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b17ax0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709023867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b17ax0", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "subreddit_subscribers": 164010, "created_utc": 1709023867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting query timeouts and appropriate alerts for Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1b15yi4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oojNEsC-H3Ge4vQ7WjyWaiJvolq7cwbkK8qZJfrqjeQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709018452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?auto=webp&amp;s=83f5ede9df0770bc92fe9880edcb22ddc85750c3", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a9dcda060ee14aa03003fea8f51eb6e9e8411b8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a3acabcc936daa3d1ab0acb3f87eee4007c3f5", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebdb2e183c0fffcf7993fbcc10996816e0fb550d", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e75db1f02946562a52ac283ab5d47e5d80151b40", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9adda2cae88e7de944026539799f038d7d0de755", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=293187c6fe2dbf3bbcf054286fcbe7ee6a9470e8", "width": 1080, "height": 720}], "variants": {}, "id": "ZhjudSQ1egdhKR93zH-Y4GUy7HyAkIjZEYYCTBXg5fE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b15yi4", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b15yi4/setting_query_timeouts_and_appropriate_alerts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "subreddit_subscribers": 164010, "created_utc": 1709018452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anybody here have experience with designing huge enterprise-wide dimensional models? I have extensive experience in designing dimensional models on the data mart level for BI tools. I often see mentions of dimensional modeling being used at the core of a DWH for an enterprise-wide (whole business) model. I struggle with understanding how that is designed. \n\nIn my experience when talking about an enterprise-wide model we talk about a layer that contains cleansed data that all reporting, ML, and analytics use cases will be building on top of. That means that this model needs to be flexible (support many use cases, some of them not yet known) and is hard to change once designed (many dependencies). Without exact reporting requirements and the ability to easily change the model (e.g. grain of a fact table) later, how can a dimensional model be successful as an enterprise-wide layer? Thanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enterprise-wide dimensional model, does it work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b14do2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709012864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anybody here have experience with designing huge enterprise-wide dimensional models? I have extensive experience in designing dimensional models on the data mart level for BI tools. I often see mentions of dimensional modeling being used at the core of a DWH for an enterprise-wide (whole business) model. I struggle with understanding how that is designed. &lt;/p&gt;\n\n&lt;p&gt;In my experience when talking about an enterprise-wide model we talk about a layer that contains cleansed data that all reporting, ML, and analytics use cases will be building on top of. That means that this model needs to be flexible (support many use cases, some of them not yet known) and is hard to change once designed (many dependencies). Without exact reporting requirements and the ability to easily change the model (e.g. grain of a fact table) later, how can a dimensional model be successful as an enterprise-wide layer? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b14do2", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b14do2/enterprisewide_dimensional_model_does_it_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b14do2/enterprisewide_dimensional_model_does_it_work/", "subreddit_subscribers": 164010, "created_utc": 1709012864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My manager along with one of her HR buddies have had an informal conversation about what cert I should do as industry-standard in order to get me an in-role promotion from a data analyst to a data engineer\n\nThey have collectively agreed on the \"Certified Data Professional\", which is offered by the institute for certification of computing professionals, or ICCP, as part of its general database professional program.\n\n&amp;#x200B;\n\nI posted on this sub earlier regarding vendor-specific certs and looks like AWS is the better one to take, even though its the harder one! But alongside that, shall I agree to take this ICCP one also? All will be paid for by the company so money is not an issue.\n\n&amp;#x200B;\n\nAre there any udemy courses out there that walk me through this ICCP one? as their website seems abit old fashioned and I dont want to be stuck with just a big book to read from, need something more interactive!", "author_fullname": "t2_2btsrky1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Certified Data Professional cert, offered by ICCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0kypc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708963154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My manager along with one of her HR buddies have had an informal conversation about what cert I should do as industry-standard in order to get me an in-role promotion from a data analyst to a data engineer&lt;/p&gt;\n\n&lt;p&gt;They have collectively agreed on the &amp;quot;Certified Data Professional&amp;quot;, which is offered by the institute for certification of computing professionals, or ICCP, as part of its general database professional program.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I posted on this sub earlier regarding vendor-specific certs and looks like AWS is the better one to take, even though its the harder one! But alongside that, shall I agree to take this ICCP one also? All will be paid for by the company so money is not an issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are there any udemy courses out there that walk me through this ICCP one? as their website seems abit old fashioned and I dont want to be stuck with just a big book to read from, need something more interactive!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b0kypc", "is_robot_indexable": true, "report_reasons": null, "author": "yungfilly", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0kypc/thoughts_on_certified_data_professional_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0kypc/thoughts_on_certified_data_professional_cert/", "subreddit_subscribers": 164010, "created_utc": 1708963154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nSmall example: Let's say we need to ingest data from a Postgres DB server which has 10 DBs, and each DB having 10 tables.\n\nQuestion: How would one best organize the Debezium Kafka Connectors?\n\n* 1 Connector per DB\n* 1 Connecter for everything\n* 1 Connector per table\n* something else\n\nPlease and thank you", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium and Connector organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0ia9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708956271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;Small example: Let&amp;#39;s say we need to ingest data from a Postgres DB server which has 10 DBs, and each DB having 10 tables.&lt;/p&gt;\n\n&lt;p&gt;Question: How would one best organize the Debezium Kafka Connectors?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 Connector per DB&lt;/li&gt;\n&lt;li&gt;1 Connecter for everything&lt;/li&gt;\n&lt;li&gt;1 Connector per table&lt;/li&gt;\n&lt;li&gt;something else&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please and thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0ia9b", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0ia9b/debezium_and_connector_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0ia9b/debezium_and_connector_organization/", "subreddit_subscribers": 164010, "created_utc": 1708956271.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}