{"kind": "Listing", "data": {"after": "t3_1b1byww", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reaching a mental breaking point.\n\nI have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column 'ID' is in a predetermined set of ID's (roughly 135,000,000) . I've tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.\n\nI really don't know what to do at this point or if it is possible to make an efficient script to do this.", "author_fullname": "t2_ecc5gnt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read/Filter a 1.7 TB CSV File in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0up0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 90, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708986031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reaching a mental breaking point.&lt;/p&gt;\n\n&lt;p&gt;I have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column &amp;#39;ID&amp;#39; is in a predetermined set of ID&amp;#39;s (roughly 135,000,000) . I&amp;#39;ve tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.&lt;/p&gt;\n\n&lt;p&gt;I really don&amp;#39;t know what to do at this point or if it is possible to make an efficient script to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0up0e", "is_robot_indexable": true, "report_reasons": null, "author": "The-Salamander-Fan", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/", "subreddit_subscribers": 164075, "created_utc": 1708986031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Data Peeps,\n\nI was recently asked this question and failed miserably at it. Wanted to get people's opinion on how I can do better on it.   \n\n\nIngestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. \n\nProcessing: We can run a daily spark pipeline to update the playlist.   \n\n\nThey told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   \n\n\nI wasn't able to answer that. Can anyone please help me with how to design such things? ", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture: Generate Apple Music's Top 50 multiple playlists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xiyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708992919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Data Peeps,&lt;/p&gt;\n\n&lt;p&gt;I was recently asked this question and failed miserably at it. Wanted to get people&amp;#39;s opinion on how I can do better on it.   &lt;/p&gt;\n\n&lt;p&gt;Ingestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. &lt;/p&gt;\n\n&lt;p&gt;Processing: We can run a daily spark pipeline to update the playlist.   &lt;/p&gt;\n\n&lt;p&gt;They told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   &lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t able to answer that. Can anyone please help me with how to design such things? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0xiyh", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xiyh/data_architecture_generate_apple_musics_top_50/", "subreddit_subscribers": 164075, "created_utc": 1708992919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, ingestr is an open-source command-line application that allows ingesting &amp; copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)\n\nIt does a few things that make it the easiest alternative out there:\n\n&amp;#x200B;\n\n* \u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs\n* \u2795 incremental loading: create+replace, delete+insert, append\n* \ud83d\udc0d single-command installation: pip install ingestr\n\nWe built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.\n\nSome common use-cases ingestr solve are:\n\n&amp;#x200B;\n\n* Migrating data from legacy systems to modern databases for better analysis\n* Syncing data between your application's database and your analytics platform in batches or incrementally\n* Backing up your databases to ensure data safety\n* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases\n* Facilitating real-time data transfer for applications that require immediate updates\n\nWe\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0[https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)", "author_fullname": "t2_153gh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source CLI tool to ingest/copy data between any databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18mfk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709029438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, ingestr is an open-source command-line application that allows ingesting &amp;amp; copying data between two databases without any code: &lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It does a few things that make it the easiest alternative out there:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs&lt;/li&gt;\n&lt;li&gt;\u2795 incremental loading: create+replace, delete+insert, append&lt;/li&gt;\n&lt;li&gt;\ud83d\udc0d single-command installation: pip install ingestr&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.&lt;/p&gt;\n\n&lt;p&gt;Some common use-cases ingestr solve are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Migrating data from legacy systems to modern databases for better analysis&lt;/li&gt;\n&lt;li&gt;Syncing data between your application&amp;#39;s database and your analytics platform in batches or incrementally&lt;/li&gt;\n&lt;li&gt;Backing up your databases to ensure data safety&lt;/li&gt;\n&lt;li&gt;Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases&lt;/li&gt;\n&lt;li&gt;Facilitating real-time data transfer for applications that require immediate updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0&lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?auto=webp&amp;s=71ed11c21a77b630e601e5051f4772431462627b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04bda8eab9d0ae0614648424f7d053daef1a08e6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=49188a34f55d47bd66a68f0f88c20fb3ba364186", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08276d6a94f8f7be274a822eeb3a2826c0d29076", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da5f26047549b5b649fe8d98e70f833e5829aa1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b2d7565df4a4182a23ea538152715a64554bf04", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d57305cc005ba7fa2adfb13086d6f70916ef5c0", "width": 1080, "height": 540}], "variants": {}, "id": "XDsKdm5QbGo1ePzc9WAtlkw6WpXhnmXv5XfGicn_zpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b18mfk", "is_robot_indexable": true, "report_reasons": null, "author": "karakanb", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "subreddit_subscribers": 164075, "created_utc": 1709029438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we've been able to build out ideas quickly but there aren't any real constraints between tables and I'm worried about the long-term maintenance efforts that could come up with this system. Right now we're relying on a lot of dbt tests to verify our assumptions on the whole system.", "author_fullname": "t2_653as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a DWH on S3/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b10kkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709001182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working in an organization that has successfully pushed nearly all of its data to S3 on a regular and reliable schedule. We are building an SSOT using dbt and AWS Athena to stage the raw data then transform it into a dimensional model / star schema, which is also stored in S3. For now we are not running any kind of Redshift instance or other DWH system. Is there a point where this becomes a bad idea? Athena is really user-friendly and we&amp;#39;ve been able to build out ideas quickly but there aren&amp;#39;t any real constraints between tables and I&amp;#39;m worried about the long-term maintenance efforts that could come up with this system. Right now we&amp;#39;re relying on a lot of dbt tests to verify our assumptions on the whole system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b10kkd", "is_robot_indexable": true, "report_reasons": null, "author": "tedward27", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b10kkd/building_a_dwh_on_s3athena/", "subreddit_subscribers": 164075, "created_utc": 1709001182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Walmart saw spike in sales of beer during hurricanes.\n\nApparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)\n\nWhat\u2019s your favorite data insight story?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People buy beer during hurricanes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ckh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709042551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Walmart saw spike in sales of beer during hurricanes.&lt;/p&gt;\n\n&lt;p&gt;Apparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)&lt;/p&gt;\n\n&lt;p&gt;What\u2019s your favorite data insight story?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ckh8", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "subreddit_subscribers": 164075, "created_utc": 1709042551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi team,\n\nhope you are crushing it this week.\n\nI have a newbie question here which i am sure have been asked many times as well and here goes.\n\nQuestion is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify's api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?\n\nWe are trying to understand if using an additional service like an integration tool justify the cost in the long run.\n\nThe other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.\n\nI am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook's api, twitter's api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?\n\n&amp;#x200B;\n\nthank you for your wisdom on this troubling thought.", "author_fullname": "t2_6mulsm33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using data integration tools like fivetran, airbyte etc vs building it from the ground up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b12old", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709007380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi team,&lt;/p&gt;\n\n&lt;p&gt;hope you are crushing it this week.&lt;/p&gt;\n\n&lt;p&gt;I have a newbie question here which i am sure have been asked many times as well and here goes.&lt;/p&gt;\n\n&lt;p&gt;Question is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify&amp;#39;s api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?&lt;/p&gt;\n\n&lt;p&gt;We are trying to understand if using an additional service like an integration tool justify the cost in the long run.&lt;/p&gt;\n\n&lt;p&gt;The other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.&lt;/p&gt;\n\n&lt;p&gt;I am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook&amp;#39;s api, twitter&amp;#39;s api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thank you for your wisdom on this troubling thought.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b12old", "is_robot_indexable": true, "report_reasons": null, "author": "simon_chia", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "subreddit_subscribers": 164075, "created_utc": 1709007380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7yh1jlaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectation from junior engineer ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1f95l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d5AIo9QplSBYUvU60BHuhMPiUDvcXhSj9_24MbBuhkY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709049403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nyexsfsbh5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?auto=webp&amp;s=e0dc582c5437ab84712de4c1f5bf3dae4f548af4", "width": 1080, "height": 1136}, "resolutions": [{"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce47bc55d3a2cf60ad25cf0aa6a705f973d7a20", "width": 108, "height": 113}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90df9f4bf8c3189fea6ce58af72c64f77ea8d9b3", "width": 216, "height": 227}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0164bca5cbc2a79751460b58bfbf3c67c7bcd139", "width": 320, "height": 336}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=546933cf5dcd3d47afa647e7780de370cfe2b418", "width": 640, "height": 673}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eefd08a2fde80381bcb448ac8b88caad7739e8e2", "width": 960, "height": 1009}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=691d799253e5195fcf86563d672feaf51f7015cc", "width": 1080, "height": 1136}], "variants": {}, "id": "XTde57wjEcOWnaAgTdf11yPwKaQ4UtAouBLs8_9PIgk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1f95l", "is_robot_indexable": true, "report_reasons": null, "author": "Foot_Straight", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1f95l/expectation_from_junior_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nyexsfsbh5lc1.png", "subreddit_subscribers": 164075, "created_utc": 1709049403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.\n\nNow start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.\n\nWhich means it's main work is to perform **transform and load within DWH itself.**\n\nIs my understanding correct?", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DBT's main strength to help engineers execute transformation and load within data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b16o2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709021228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.&lt;/p&gt;\n\n&lt;p&gt;Now start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.&lt;/p&gt;\n\n&lt;p&gt;Which means it&amp;#39;s main work is to perform &lt;strong&gt;transform and load within DWH itself.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is my understanding correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b16o2d", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "subreddit_subscribers": 164075, "created_utc": 1709021228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am leaving my company at the end of the week. They leave me with \\~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  \n\n\nI have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y'all recommend for me to go for next? I'm up in the air about going \"tool-specific\" like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.\n\n&amp;#x200B;\n\nP.S. I will be doing personal projects on the side as well. I know it's foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.", "author_fullname": "t2_mkh2olbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Best\" Next DE Certification to get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b13uiy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709011093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am leaving my company at the end of the week. They leave me with ~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  &lt;/p&gt;\n\n&lt;p&gt;I have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp;amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y&amp;#39;all recommend for me to go for next? I&amp;#39;m up in the air about going &amp;quot;tool-specific&amp;quot; like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. I will be doing personal projects on the side as well. I know it&amp;#39;s foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b13uiy", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid-Second-8434", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "subreddit_subscribers": 164075, "created_utc": 1709011093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since I've never seen it mentioned here, I wanted to give it a shout\n\nhttps://dataninjago.com/\n\n", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataninjago is an incredibly useful blog for everything Spark related", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0r84s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708977992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since I&amp;#39;ve never seen it mentioned here, I wanted to give it a shout&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dataninjago.com/\"&gt;https://dataninjago.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?auto=webp&amp;s=39c2a5bd8776bf0e7b0144606e7543ecd4c7ddb9", "width": 180, "height": 180}, "resolutions": [{"url": "https://external-preview.redd.it/7C9vS_Q-wlxcLW7yRG33vWgAisTzfTEeA9lLZ0xHVQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=135655fd0e3621c1a856d718292ab427bf80e75a", "width": 108, "height": 108}], "variants": {}, "id": "rWVpgP_Xt3xAzKbBbkWux1zFSEWLAUYM8N_v939mmQ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0r84s", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0r84s/dataninjago_is_an_incredibly_useful_blog_for/", "subreddit_subscribers": 164075, "created_utc": 1708977992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone \n", "author_fullname": "t2_84ztczxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to Dive Deeper into Data Modeling. Recommendations for In-Depth Learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0o4bb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/t6bucAzFX9Qv_76pKS6_eTpWtudZ6I3_jAXjfmSeHoQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708970629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1q4grlv2zykc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?auto=webp&amp;s=ad493fc45be45cf6330afbc6a218203f0a401e8c", "width": 700, "height": 394}, "resolutions": [{"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b02fb77805dc60307c2937b0bbef70bdc98d7d1", "width": 108, "height": 60}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a57ee70754f3c7aa563aaeebbc32f7297678a46", "width": 216, "height": 121}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c58659241bb8f3c8658c432e2e84698657b98a1", "width": 320, "height": 180}, {"url": "https://preview.redd.it/1q4grlv2zykc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=050eb410f54f8b7a8ea8a4b1799151bf0c72aafe", "width": 640, "height": 360}], "variants": {}, "id": "iwWyl6t2YUzpOC_MCfgLg7f7JQrgL8tB9oFp9H92Rnk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b0o4bb", "is_robot_indexable": true, "report_reasons": null, "author": "chaachans", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0o4bb/looking_to_dive_deeper_into_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1q4grlv2zykc1.jpeg", "subreddit_subscribers": 164075, "created_utc": 1708970629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience in AWS services related to data engineering. I've been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.\n\nWhen I'm trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I'm thinking of where it's a good idea to move my stack to azure. \n\nAppreciate your ideas on this", "author_fullname": "t2_6b96x89c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS or Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18xcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709030597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience in AWS services related to data engineering. I&amp;#39;ve been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.&lt;/p&gt;\n\n&lt;p&gt;When I&amp;#39;m trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I&amp;#39;m thinking of where it&amp;#39;s a good idea to move my stack to azure. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your ideas on this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b18xcc", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Buy-4947", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18xcc/aws_or_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18xcc/aws_or_azure/", "subreddit_subscribers": 164075, "created_utc": 1709030597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5a55k9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the reasons why ADF is love/hate type of thing - a task that would be super easy in Python such as skipping a for loop block if a condition is not met, turns out into a creative hack that takes many hours to get right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1gmf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jA_LNyv0BCZbxetNQpaMxQenEHPQvRu3lH9TbA7DnTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709052668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rhfpex8nq5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?auto=webp&amp;s=f50af32a38a82268aaeacfbba92f41f97685fbbd", "width": 1515, "height": 861}, "resolutions": [{"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=587c1c97d4263e0f0c5c0831adac22d35e8db4c8", "width": 108, "height": 61}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d86464f6a9fe7ce768086594dc2b9a2048f8a4d3", "width": 216, "height": 122}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=036fa90d1e66d8966bfa8aaa01ece070c4b86e10", "width": 320, "height": 181}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed5695d954a3169c76fea2e650d7b011bf889162", "width": 640, "height": 363}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16d2b1fe9bf607f6f075b6267494c38d1eef469", "width": 960, "height": 545}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbb31ad543c4846964e2800599f9056ee9f3a7aa", "width": 1080, "height": 613}], "variants": {}, "id": "sK0y8seZr8DTVPJEbWf_dGHmEkWW2nm1w0iE7Iv3kH8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1gmf6", "is_robot_indexable": true, "report_reasons": null, "author": "koteikin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1gmf6/one_of_the_reasons_why_adf_is_lovehate_type_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rhfpex8nq5lc1.png", "subreddit_subscribers": 164075, "created_utc": 1709052668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm running a Spark cluster on Azure Databricks. While notebooks in the browser work, I'd prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can't access certain JVM-dependent functions like `df.rdd.getNumPartitions()` but that's ok.\n\nIs it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I'd like to avoid.", "author_fullname": "t2_jkgy1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyway to run Scala code in notebooks using Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b11qvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709004562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a Spark cluster on Azure Databricks. While notebooks in the browser work, I&amp;#39;d prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can&amp;#39;t access certain JVM-dependent functions like &lt;code&gt;df.rdd.getNumPartitions()&lt;/code&gt; but that&amp;#39;s ok.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I&amp;#39;d like to avoid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b11qvo", "is_robot_indexable": true, "report_reasons": null, "author": "napsterv", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "subreddit_subscribers": 164075, "created_utc": 1709004562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.\n\nMy Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is \"expected behavior\" when using their \"intelligent cache\" technology on a private managed vnet.\n\nThat is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.\n\nPlease let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.\n", "author_fullname": "t2_6dfcntmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse reliability on private endpoints ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0xukn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708993777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.&lt;/p&gt;\n\n&lt;p&gt;My Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is &amp;quot;expected behavior&amp;quot; when using their &amp;quot;intelligent cache&amp;quot; technology on a private managed vnet.&lt;/p&gt;\n\n&lt;p&gt;That is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b0xukn", "is_robot_indexable": true, "report_reasons": null, "author": "SmallAd3697", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b0xukn/synapse_reliability_on_private_endpoints/", "subreddit_subscribers": 164075, "created_utc": 1708993777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_paxxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient Non-Deterministic Sampling of Large BigQuery Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0si79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aO4E1W9053BUU_tzaP7abCmtLjZJ5zG5l7_GhV3BCfs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708980979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?auto=webp&amp;s=28328d547b21f8095cec278345241be98fa95b38", "width": 866, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=495b870085e41f24e1da2921cdb6ec99e9f4c272", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c458f751c6ec548295d153cc189050091bbf9984", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343387ac40b02dbc1f48ed92e3abf59970835fb8", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/S8rcWZpyDOv8ioeO5MnPyDtrUcCNCxLNGYqgl9Ka5wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97da34b422f6db95192405571030235ac343ed88", "width": 640, "height": 480}], "variants": {}, "id": "yofiS0ASy7GpSFye6CASKDiG69VKiyUGIXYdS0FllMY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b0si79", "is_robot_indexable": true, "report_reasons": null, "author": "mdixon1010", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b0si79/efficient_nondeterministic_sampling_of_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@matt.dixon1010/non-deterministic-repeatable-sampling-of-large-bigquery-tables-ddf18f56c898", "subreddit_subscribers": 164075, "created_utc": 1708980979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jnd3h0a4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharding Redis 101: Using Pixel Art To Explain The Basics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1el4v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1b1el4v", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/LHPXFmXwb3nzKUcnt9skH2TOZyJQEV5gUaa6WJQcnD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709047767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/OUNimTqgkZM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?auto=webp&amp;s=f5f162840cbbc19dfd9555c40dd3cdeeef61bf4c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=673d90c0d2d95230a6ca4c9c0beca2fa7c04fd89", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b23acaf341ff6bbb773b557d242b5681e0dddc3f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26b7b3baa6e01e47a2b7e29206f6652c230f02cd", "width": 320, "height": 240}], "variants": {}, "id": "WcjcUtEvWDXzwRXdZ5uUMo1KfN_fjy369prwUT8YECI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1el4v", "is_robot_indexable": true, "report_reasons": null, "author": "schematical", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1el4v/sharding_redis_101_using_pixel_art_to_explain_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/OUNimTqgkZM", "subreddit_subscribers": 164075, "created_utc": 1709047767.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to test a SQL database (its procedures and so on)?    \n\n\nAnd what is the best way to do it?", "author_fullname": "t2_lwhyoz1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way/tool for testing SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1e5lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709046689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to test a SQL database (its procedures and so on)?    &lt;/p&gt;\n\n&lt;p&gt;And what is the best way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1e5lm", "is_robot_indexable": true, "report_reasons": null, "author": "alexcrav", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "subreddit_subscribers": 164075, "created_utc": 1709046689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!! \n\nI\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.\n\nI\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.\n\nAny insights or suggestions would be greatly appreciated. Thank you!\n\nFeel free to let me know if you need any further adjustments or have any specific preferences!", "author_fullname": "t2_8fi5ln6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on backfilling airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1goj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.&lt;/p&gt;\n\n&lt;p&gt;Any insights or suggestions would be greatly appreciated. Thank you!&lt;/p&gt;\n\n&lt;p&gt;Feel free to let me know if you need any further adjustments or have any specific preferences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1goj4", "is_robot_indexable": true, "report_reasons": null, "author": "sailor_Moon_Pie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "subreddit_subscribers": 164075, "created_utc": 1709052808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello data engineers,\n\nI would love to invite you to follow Teradata's [Medium blog](https://medium.com/teradata) covering the latest trends and techniques in data engineering and data science. We're excited to share our expertise and resources with you. Happy learning!\n\n* **Comprehensive Tutorials:** Dive deep into end-to-end tutorials that not only guide you through various concepts but also provide practical experience.\n* **Dedicated Free Teradata Instance:** Gain hands-on experience with your Free Teradata instance. It's a fantastic opportunity to work with real-world tools in a controlled environment.\n* **Free Coding Environment:** Say goodbye to the hassle of setting up your environment. Teradata offers a free coding space on ClearScape Analytics Experience, making it easier for you to focus on learning and experimentation.\n* **Sample Data:** No need to worry about finding data for your projects. The blogs provides sample data that you can use to practice and enhance your skills.\n\n[https://medium.com/teradata](https://medium.com/teradata)\n\n**Latest topics include:**\n\n* **Data Mesh Architecture with dbt-core and Teradata Vantage\u2122:** In this article, dive into the process of implementing a data mesh architecture utilizing dbt-core alongside free and open-source plugins.\n* **Mastering In-Database Feature Engineering with Teradata:** Dive into the nuances of feature engineering within Teradata, a crucial skill for enhancing your predictive analytics, ML, and AI projects.\n* **Creating High-Quality Data for Trusted AI with In-Database Analytics:** The success of generative AI depends on data quality, especially as models and algorithms become more accessible. Teradata VantageCloud\u2019s provides a powerful, open, and connected AI/ML capabilities, which includes a comprehensive set of [in-database analytics capabilities](https://www.teradata.com/platform/clearscape-analytics/in-database) that simplify data preparation. In this article, we\u2019ll focus on cleaning and exploring data using ClearScape Analytics.\n* **Teradata Vantage\u2122 Destination Now Available on Airbyte Cloud:** Discover how to integrate Teradata Vantage with Airbyte Cloud, streamlining your data extraction and loading processes.\n* **GenAI: Introduction to Prompt Engineering and LangChain:** Explore generative AI, including prompt engineering and leveraging LLMs for querying databases in natural language.\n\n&amp;#x200B;", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering and Data Science Insights with Teradata's Developer Blog: Free Tutorials, Tools, and Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1gkvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data engineers,&lt;/p&gt;\n\n&lt;p&gt;I would love to invite you to follow Teradata&amp;#39;s &lt;a href=\"https://medium.com/teradata\"&gt;Medium blog&lt;/a&gt; covering the latest trends and techniques in data engineering and data science. We&amp;#39;re excited to share our expertise and resources with you. Happy learning!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Comprehensive Tutorials:&lt;/strong&gt; Dive deep into end-to-end tutorials that not only guide you through various concepts but also provide practical experience.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dedicated Free Teradata Instance:&lt;/strong&gt; Gain hands-on experience with your Free Teradata instance. It&amp;#39;s a fantastic opportunity to work with real-world tools in a controlled environment.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Free Coding Environment:&lt;/strong&gt; Say goodbye to the hassle of setting up your environment. Teradata offers a free coding space on ClearScape Analytics Experience, making it easier for you to focus on learning and experimentation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sample Data:&lt;/strong&gt; No need to worry about finding data for your projects. The blogs provides sample data that you can use to practice and enhance your skills.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/teradata\"&gt;https://medium.com/teradata&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Latest topics include:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Data Mesh Architecture with dbt-core and Teradata Vantage\u2122:&lt;/strong&gt; In this article, dive into the process of implementing a data mesh architecture utilizing dbt-core alongside free and open-source plugins.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mastering In-Database Feature Engineering with Teradata:&lt;/strong&gt; Dive into the nuances of feature engineering within Teradata, a crucial skill for enhancing your predictive analytics, ML, and AI projects.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Creating High-Quality Data for Trusted AI with In-Database Analytics:&lt;/strong&gt; The success of generative AI depends on data quality, especially as models and algorithms become more accessible. Teradata VantageCloud\u2019s provides a powerful, open, and connected AI/ML capabilities, which includes a comprehensive set of &lt;a href=\"https://www.teradata.com/platform/clearscape-analytics/in-database\"&gt;in-database analytics capabilities&lt;/a&gt; that simplify data preparation. In this article, we\u2019ll focus on cleaning and exploring data using ClearScape Analytics.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Teradata Vantage\u2122 Destination Now Available on Airbyte Cloud:&lt;/strong&gt; Discover how to integrate Teradata Vantage with Airbyte Cloud, streamlining your data extraction and loading processes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GenAI: Introduction to Prompt Engineering and LangChain:&lt;/strong&gt; Explore generative AI, including prompt engineering and leveraging LLMs for querying databases in natural language.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gwzo4xVBYtB7gVSN41V71w2DNjkP6yBAY7olBzE70io.jpg?auto=webp&amp;s=bbb0f594d1428b1bee56cbd76686a7e68d94be56", "width": 180, "height": 180}, "resolutions": [{"url": "https://external-preview.redd.it/Gwzo4xVBYtB7gVSN41V71w2DNjkP6yBAY7olBzE70io.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f9d5ac73180da396bc77178bb41e2fe0bc60c29", "width": 108, "height": 108}], "variants": {}, "id": "wWtEfLg1FBkPdnzjjDw4l4BMy4-A67x7UF_kdAo48G4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1gkvl", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1gkvl/data_engineering_and_data_science_insights_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1gkvl/data_engineering_and_data_science_insights_with/", "subreddit_subscribers": 164075, "created_utc": 1709052570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy company has an on premise server for both applications and file storage.  We pay a third party to professionally manage it, including backup and recovery.  We have begun shifting applications off the server into the cloud due to issues with concurrent user issues, which leaves our server only being used for shared file storage and data retention.\n\nNo one here is any kind of IT person - a few ideas are getting tossed around, such as moving our data to a professional Dropbox, or keeping our managed server for on-premise file storage, or moving our data to Google drive (we use Gsuite for business).  Our IT vendor suggested a move to the cloud (Azure).  \n\nI would really appreciate some advice.", "author_fullname": "t2_9xb4px07j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Advice on Data Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1ggok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My company has an on premise server for both applications and file storage.  We pay a third party to professionally manage it, including backup and recovery.  We have begun shifting applications off the server into the cloud due to issues with concurrent user issues, which leaves our server only being used for shared file storage and data retention.&lt;/p&gt;\n\n&lt;p&gt;No one here is any kind of IT person - a few ideas are getting tossed around, such as moving our data to a professional Dropbox, or keeping our managed server for on-premise file storage, or moving our data to Google drive (we use Gsuite for business).  Our IT vendor suggested a move to the cloud (Azure).  &lt;/p&gt;\n\n&lt;p&gt;I would really appreciate some advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1ggok", "is_robot_indexable": true, "report_reasons": null, "author": "AerialPatrol", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1ggok/need_advice_on_data_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ggok/need_advice_on_data_storage/", "subreddit_subscribers": 164075, "created_utc": 1709052296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data Engineers,\n\nI've been working on an Apache Beam pipeline with Dataflow to process and upload data into BigQuery. However, I'm encountering a scaling issue \u2013 after attempting to process more than 1,000 files at a time, I'm hitting memory errors.\n\nUpon further investigation, it seems like my pipeline is processing all stages in batches rather than streaming each file or element individually. I would like to configure my pipeline to stream each file individually and limit the number of files processed at a time to avoid memory issues.\n\nIf anyone has experience with Apache Beam and Dataflow, I would greatly appreciate any insights or tips on how to make my pipeline more efficient and scalable.\n\nHere's a simplified version of my current pipeline structure:\n\n    import apache_beam as beam\n    from apache_beam.options.pipeline_options import PipelineOptions\n    from apache_beam.io.gcp.bigquery import WriteToBigQuery\n    from apache_beam.runners import DirectRunner\n    from google.cloud import storage\n    \n    class GetFiles(beam.DoFn):\n    \n        def __call__(self, *args, **kwargs):\n            return self.process(*args, **kwargs)\n    \n        def process(self, element)\n            # Get the id from the element\n            file_path = element.get(\"id\")\n    \n            # Get the name of the gcp bucket\n            gcp_bucket_name = re.match(\"[REGEX HERE]\", file_path)\n            # Get the name of the gcp directory prefix\n            gcp_directory_prefix = re.search(\"[REGEX HERE]\", file_path)\n    \n            # Get the blobs associated with the file_path\n            storage_client = storage.Client()\n            bucket = storage_client.get_bucket(gcp_bucket_name)\n            blobs = bucket.list_blobs(prefix=gcp_directory_prefix)\n    \n            # For each blob, pass the id on to the next PTransform\n            for blob in blobs:\n                yield {\"id\": blob.id}\n    \n    class Extract(beam.DoFn):\n    \n        def __call__(self, *args, **kwargs):\n            return self.process(*args, **kwargs)\n    \n        def process(self, element):\n            # Get the name of the file\n            file_name = element.get(\"file_name\")\n            bucket = element.get(\"bucket\")\n    \n            # Get the data from GCS storage\n            storage_client = storage.Client()\n            bucket = storage_client.get_bucket(bucket)\n            blob = bucket.get_blob(file_name)\n    \n            blob_data = blob.download_as_string()\n    \n            # Get some data for the columns\n            json_data = json.loads(blob_data)\n    \n            # Get the id\n            id = json_data.get(\"id\")\n            \n            # Return the id with the blob data\n            return {\"id\": id, \"data\": blob_data}\n    \n    pipeline_options = PipelineOptions(\n        streaming=True\n    )\n    \n    schema = {\n        \"fields\": [\n            {\"name\": \"id\", \"type\": \"STRING\"},\n            {\"name\": \"data\", \"type\": \"JSON\"}\n        ]\n    }\n    \n    pipeline = beam.Pipeline(DirectRunner(), options=pipeline_options)\n    \n    extraction = ( pipeline\n        | \"Get file parameters\" &gt;&gt; beam.Create([\"bucket_name/path/to/directory\", \"bucket_name/path/to/directory\"])\n        | \"Get the list of files\" &gt;&gt; beam.ParDo(GetFiles())\n        | \"Extract the Json Data\" &gt;&gt; beam.ParDo(Extract())\n        | \"Upload the Data to BigQuery\" &gt;&gt; WriteToBigQuery(\n            table=\"project.dataset.table\",\n            schema=schema,\n            method=\"STREAMING_INSERTS\",\n            batch_size=1\n        )\n    )\n    \n    results = pipeline.run()\n    results.wait_until_finish()\n\n*Please ignore minor bugs, this code is only an example*\n\nThe workflow works, until I reach around 10,000 or more files, then I get an malloc error. I believe this is because my pipeline is attempting to process all the files at once, instead of streaming.\n\nAny suggestions, best practices, or sample code snippets would be immensely helpful. Thanks in advance for your expertise!\n\nI'm using Apache Beam with Python, in case that information is relevant.", "author_fullname": "t2_u1p3ntq1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Needed: Apache Beam/Dataflow Pipeline Scaling Issue with BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1g1w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709051625.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709051315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on an Apache Beam pipeline with Dataflow to process and upload data into BigQuery. However, I&amp;#39;m encountering a scaling issue \u2013 after attempting to process more than 1,000 files at a time, I&amp;#39;m hitting memory errors.&lt;/p&gt;\n\n&lt;p&gt;Upon further investigation, it seems like my pipeline is processing all stages in batches rather than streaming each file or element individually. I would like to configure my pipeline to stream each file individually and limit the number of files processed at a time to avoid memory issues.&lt;/p&gt;\n\n&lt;p&gt;If anyone has experience with Apache Beam and Dataflow, I would greatly appreciate any insights or tips on how to make my pipeline more efficient and scalable.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a simplified version of my current pipeline structure:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.io.gcp.bigquery import WriteToBigQuery\nfrom apache_beam.runners import DirectRunner\nfrom google.cloud import storage\n\nclass GetFiles(beam.DoFn):\n\n    def __call__(self, *args, **kwargs):\n        return self.process(*args, **kwargs)\n\n    def process(self, element)\n        # Get the id from the element\n        file_path = element.get(&amp;quot;id&amp;quot;)\n\n        # Get the name of the gcp bucket\n        gcp_bucket_name = re.match(&amp;quot;[REGEX HERE]&amp;quot;, file_path)\n        # Get the name of the gcp directory prefix\n        gcp_directory_prefix = re.search(&amp;quot;[REGEX HERE]&amp;quot;, file_path)\n\n        # Get the blobs associated with the file_path\n        storage_client = storage.Client()\n        bucket = storage_client.get_bucket(gcp_bucket_name)\n        blobs = bucket.list_blobs(prefix=gcp_directory_prefix)\n\n        # For each blob, pass the id on to the next PTransform\n        for blob in blobs:\n            yield {&amp;quot;id&amp;quot;: blob.id}\n\nclass Extract(beam.DoFn):\n\n    def __call__(self, *args, **kwargs):\n        return self.process(*args, **kwargs)\n\n    def process(self, element):\n        # Get the name of the file\n        file_name = element.get(&amp;quot;file_name&amp;quot;)\n        bucket = element.get(&amp;quot;bucket&amp;quot;)\n\n        # Get the data from GCS storage\n        storage_client = storage.Client()\n        bucket = storage_client.get_bucket(bucket)\n        blob = bucket.get_blob(file_name)\n\n        blob_data = blob.download_as_string()\n\n        # Get some data for the columns\n        json_data = json.loads(blob_data)\n\n        # Get the id\n        id = json_data.get(&amp;quot;id&amp;quot;)\n\n        # Return the id with the blob data\n        return {&amp;quot;id&amp;quot;: id, &amp;quot;data&amp;quot;: blob_data}\n\npipeline_options = PipelineOptions(\n    streaming=True\n)\n\nschema = {\n    &amp;quot;fields&amp;quot;: [\n        {&amp;quot;name&amp;quot;: &amp;quot;id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;},\n        {&amp;quot;name&amp;quot;: &amp;quot;data&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;JSON&amp;quot;}\n    ]\n}\n\npipeline = beam.Pipeline(DirectRunner(), options=pipeline_options)\n\nextraction = ( pipeline\n    | &amp;quot;Get file parameters&amp;quot; &amp;gt;&amp;gt; beam.Create([&amp;quot;bucket_name/path/to/directory&amp;quot;, &amp;quot;bucket_name/path/to/directory&amp;quot;])\n    | &amp;quot;Get the list of files&amp;quot; &amp;gt;&amp;gt; beam.ParDo(GetFiles())\n    | &amp;quot;Extract the Json Data&amp;quot; &amp;gt;&amp;gt; beam.ParDo(Extract())\n    | &amp;quot;Upload the Data to BigQuery&amp;quot; &amp;gt;&amp;gt; WriteToBigQuery(\n        table=&amp;quot;project.dataset.table&amp;quot;,\n        schema=schema,\n        method=&amp;quot;STREAMING_INSERTS&amp;quot;,\n        batch_size=1\n    )\n)\n\nresults = pipeline.run()\nresults.wait_until_finish()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Please ignore minor bugs, this code is only an example&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;The workflow works, until I reach around 10,000 or more files, then I get an malloc error. I believe this is because my pipeline is attempting to process all the files at once, instead of streaming.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions, best practices, or sample code snippets would be immensely helpful. Thanks in advance for your expertise!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Apache Beam with Python, in case that information is relevant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1g1w0", "is_robot_indexable": true, "report_reasons": null, "author": "More_Part_3365", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1g1w0/help_needed_apache_beamdataflow_pipeline_scaling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1g1w0/help_needed_apache_beamdataflow_pipeline_scaling/", "subreddit_subscribers": 164075, "created_utc": 1709051315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "By what i saw is not possible to add cluster directly on a existing table, but also can't do a create or replace adding the cluster, right now It seems like i would need to put the data in a temp table, delete the current and recreate getting the data from the temp table, but i wanted a better way since i need to do this for multiple tables that have some TBs each and i also didnt want down time/risk of missing data since there are processes that put data there and aren't in my control", "author_fullname": "t2_2hp6w7l7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is best way to add cluster in a pre-existing bigquery table ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1d94s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709044368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By what i saw is not possible to add cluster directly on a existing table, but also can&amp;#39;t do a create or replace adding the cluster, right now It seems like i would need to put the data in a temp table, delete the current and recreate getting the data from the temp table, but i wanted a better way since i need to do this for multiple tables that have some TBs each and i also didnt want down time/risk of missing data since there are processes that put data there and aren&amp;#39;t in my control&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1d94s", "is_robot_indexable": true, "report_reasons": null, "author": "Kolt_tmaker", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1d94s/what_is_best_way_to_add_cluster_in_a_preexisting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1d94s/what_is_best_way_to_add_cluster_in_a_preexisting/", "subreddit_subscribers": 164075, "created_utc": 1709044368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2m4jmqrn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability: The Next Frontier of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1c0iu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/92p5nuuO28iKEKJTddI8-afwtfhls7XSUT8vwmntpuM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709041001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?auto=webp&amp;s=23b87a3c3434a563f208b5ce9d60ae71cb25eb4c", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d6d3e6cebcee1463c51a65006b6bdaff91447ec", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=54235a755f7b3e0b34af63bd0bfd3e4306f5174b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe1bc4229063a081e7143bdd9c2c603eb10bba8c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64b707a5757ce60317484ebd1b65082e2efc0d31", "width": 640, "height": 336}], "variants": {}, "id": "JGhC9b9qhHNJ4eh7-qMHVcYJNdZfz1K-5VoSz-ZvhQY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1c0iu", "is_robot_indexable": true, "report_reasons": null, "author": "michaellyamm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1c0iu/data_observability_the_next_frontier_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "subreddit_subscribers": 164075, "created_utc": 1709041001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lw1vuj1gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Your Data Consultancy Might Be One Password Away from Disaster And 7 Ways To Fix It", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1byww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wGSkyvfT43b4Yve25V_aPZ2A5z8HTTxyO-BaUf2Mq-g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709040873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/why-your-data-consultancy-might-be-one-password-away-from-disaster-and-7-ways-to-fix-it/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?auto=webp&amp;s=e57d3b8c461698c7565e7ab66a9ad77fbae92bb7", "width": 2258, "height": 1528}, "resolutions": [{"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1000c9b5487d759bb6a5327802759702c2a24d1", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18c1462cab6b72ead48f2350f0612f4a7a4d87e5", "width": 216, "height": 146}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=54811d8c433a4566191e3acb3e6f9a8ab96e3110", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7a9970d6d5eb9abeab108c6c5043e8c08796506", "width": 640, "height": 433}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f09aa6bd7e54bd57c1d742ee8c8f8634b3be050c", "width": 960, "height": 649}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=906f8128fb1a75f1769203f3ac4244f3f87e7736", "width": 1080, "height": 730}], "variants": {}, "id": "R-cgpqSW0siFKko5wr6GGJDyV0tyhJdCGKizuk-hOhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1byww", "is_robot_indexable": true, "report_reasons": null, "author": "Distinct-Economics24", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1byww/why_your_data_consultancy_might_be_one_password/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/why-your-data-consultancy-might-be-one-password-away-from-disaster-and-7-ways-to-fix-it/", "subreddit_subscribers": 164075, "created_utc": 1709040873.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}