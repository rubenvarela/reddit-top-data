{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I got tired of reading job descriptions and searching for the keywords \"python\", \"data\" and \"pytorch\". So I made this notebook which can take just about any job board and a few CSS selectors and spits out a ranking far better than what the big aggregators can do. Maybe someone else will find it useful or want to collaborate? I'm deciding to take this minimal example public. Maybe it has commercial viability? Maybe someone here knows?\n\n&amp;#x200B;\n\n[Colab notebook](https://colab.research.google.com/gist/gbwiersum/b112a6534d71d8ae3c360f5a160cc1dc/job-board-scrapenscore.ipynb)\n\n&amp;#x200B;\n\nIt's also a demonstration of comparing arbitrarily long documents with true AI. I thought that was cool.\n\nIf you reaaaaly like it, maybe hire me? ", "author_fullname": "t2_43c8jfck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking like a Data Scientist in my job search. Making this tool public.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avx409", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708473395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got tired of reading job descriptions and searching for the keywords &amp;quot;python&amp;quot;, &amp;quot;data&amp;quot; and &amp;quot;pytorch&amp;quot;. So I made this notebook which can take just about any job board and a few CSS selectors and spits out a ranking far better than what the big aggregators can do. Maybe someone else will find it useful or want to collaborate? I&amp;#39;m deciding to take this minimal example public. Maybe it has commercial viability? Maybe someone here knows?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://colab.research.google.com/gist/gbwiersum/b112a6534d71d8ae3c360f5a160cc1dc/job-board-scrapenscore.ipynb\"&gt;Colab notebook&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also a demonstration of comparing arbitrarily long documents with true AI. I thought that was cool.&lt;/p&gt;\n\n&lt;p&gt;If you reaaaaly like it, maybe hire me? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?auto=webp&amp;s=ed5da41e2c4cee7a9e495c8291ecf5604f0e169d", "width": 260, "height": 260}, "resolutions": [{"url": "https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f34d2dfdbbfa7de0f1956f186fd8430ee96a1a55", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2817183828c9747b960cb2e55c59cfa41f4f9ded", "width": 216, "height": 216}], "variants": {}, "id": "nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "1avx409", "is_robot_indexable": true, "report_reasons": null, "author": "Biologistathome", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1avx409/thinking_like_a_data_scientist_in_my_job_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1avx409/thinking_like_a_data_scientist_in_my_job_search/", "subreddit_subscribers": 1356352, "created_utc": 1708473395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I start: 50ish and just one interview with the hiring manager that led to nothing. 3ish months.", "author_fullname": "t2_gzxyifokr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long did it take to get a DS or DA job and how many applications did you send?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw2itu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708488216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I start: 50ish and just one interview with the hiring manager that led to nothing. 3ish months.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1aw2itu", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Reporter-230", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1aw2itu/how_long_did_it_take_to_get_a_ds_or_da_job_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1aw2itu/how_long_did_it_take_to_get_a_ds_or_da_job_and/", "subreddit_subscribers": 1356352, "created_utc": 1708488216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I came accross this video by **Andrej Karpathy on** *Let's build the GPT Tokenizer* last night while browsing. Now I can clearly admit that this is way way above my current level of understanding but if someone undersatnds the projects that he descibes on youtube and can implement it to solve other problems (not just copy paste it), how \"hireable\" they are?\n\nMy apology if this is not the proper place to ask such question.", "author_fullname": "t2_7lqv45ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Was suggested to post it here for more indepth asnwer] If someone can understand and implement the entire projects or something of similar complexity that Andrej Karpathy does on his YouTube channel, how industry ready are they? And how hireable are they in current job market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awgwj8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708534285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came accross this video by &lt;strong&gt;Andrej Karpathy on&lt;/strong&gt; &lt;em&gt;Let&amp;#39;s build the GPT Tokenizer&lt;/em&gt; last night while browsing. Now I can clearly admit that this is way way above my current level of understanding but if someone undersatnds the projects that he descibes on youtube and can implement it to solve other problems (not just copy paste it), how &amp;quot;hireable&amp;quot; they are?&lt;/p&gt;\n\n&lt;p&gt;My apology if this is not the proper place to ask such question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awgwj8", "is_robot_indexable": true, "report_reasons": null, "author": "SmartPuppyy", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1awgwj8/was_suggested_to_post_it_here_for_more_indepth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1awgwj8/was_suggested_to_post_it_here_for_more_indepth/", "subreddit_subscribers": 1356352, "created_utc": 1708534285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey, guys! I need some help. Today at work, my manager give me the task of try to build a Decision Tree to analyse hierarchical groups and try to build some policys. But, the deal is, she mention some kind of Decision Tree of non binary splits that she used to build on SPSS. Do you guys know some kind of package on python and some resources about this? If someone know about it, would be great!! ", "author_fullname": "t2_ce6umqnmy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non binary split decision Trees\u2026?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avw4r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708470968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys! I need some help. Today at work, my manager give me the task of try to build a Decision Tree to analyse hierarchical groups and try to build some policys. But, the deal is, she mention some kind of Decision Tree of non binary splits that she used to build on SPSS. Do you guys know some kind of package on python and some resources about this? If someone know about it, would be great!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1avw4r0", "is_robot_indexable": true, "report_reasons": null, "author": "TioMir", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1avw4r0/non_binary_split_decision_trees/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1avw4r0/non_binary_split_decision_trees/", "subreddit_subscribers": 1356352, "created_utc": 1708470968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry in advance for the novel lol.\n\nHave a question about TS (or alternatives) for forecasting a target where the \"time\" index is just an ordered integer sequence denoting days since an employee was hired (\"tenure days\"), and where no data exists yet for the later tenure stages (new process, see notes below).\n\n We are trying to model the average productivity ramp-up period during the first year post-hire for manufacturing workers. For every worker, we have a daily record of the output produced each \"tenure day\", along with running cumulative totals. The cumulative output produced through Day X is our target. We also have predictors such as workplace type (Office/Remote), Team, etc.\n\nFor context, when you plot avg total throughput by tenure days,  the curve is approximately logarithmic, e.g. avg lifetime production = ln(tenure).\n\n**Busines Context**: This manufacturing process is new and we only have data through the first 8 months post-hire. Additionally, we have a cohort-style hiring cadence, so not all employees have the same amount of tenure, and our sample sizes decrease as we progress in \"tenure day time\"\n\n**The Ask*: Slead is trying to run a profitability analysis that requires avg expected lifetime throughput through the first 365 days post-hire\n\n**Question**: What are some possible options for forecasting avg expected total throughput for tenure day \"time points\" yet unseen? We don't have any data for months 9-12 as this is a new process and the longest tenured employee has ~8mon tenure. \n\n*Goal is not to forecast individual expected throughput for soecific employees, just to project the avg expected for future days*\n\nOpen to any suggestions! TIA", "author_fullname": "t2_vmxdbe7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting a target with a non-datetime index", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awdfzy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Statistics", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708525939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry in advance for the novel lol.&lt;/p&gt;\n\n&lt;p&gt;Have a question about TS (or alternatives) for forecasting a target where the &amp;quot;time&amp;quot; index is just an ordered integer sequence denoting days since an employee was hired (&amp;quot;tenure days&amp;quot;), and where no data exists yet for the later tenure stages (new process, see notes below).&lt;/p&gt;\n\n&lt;p&gt;We are trying to model the average productivity ramp-up period during the first year post-hire for manufacturing workers. For every worker, we have a daily record of the output produced each &amp;quot;tenure day&amp;quot;, along with running cumulative totals. The cumulative output produced through Day X is our target. We also have predictors such as workplace type (Office/Remote), Team, etc.&lt;/p&gt;\n\n&lt;p&gt;For context, when you plot avg total throughput by tenure days,  the curve is approximately logarithmic, e.g. avg lifetime production = ln(tenure).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Busines Context&lt;/strong&gt;: This manufacturing process is new and we only have data through the first 8 months post-hire. Additionally, we have a cohort-style hiring cadence, so not all employees have the same amount of tenure, and our sample sizes decrease as we progress in &amp;quot;tenure day time&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;The Ask&lt;/em&gt;: Slead is trying to run a profitability analysis that requires avg expected lifetime throughput through the first 365 days post-hire&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What are some possible options for forecasting avg expected total throughput for tenure day &amp;quot;time points&amp;quot; yet unseen? We don&amp;#39;t have any data for months 9-12 as this is a new process and the longest tenured employee has ~8mon tenure. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Goal is not to forecast individual expected throughput for soecific employees, just to project the avg expected for future days&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Open to any suggestions! TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "370e8fc0-70eb-11ee-b58a-86a96bfd3389", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "1awdfzy", "is_robot_indexable": true, "report_reasons": null, "author": "Effective-Piccolo973", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1awdfzy/forecasting_a_target_with_a_nondatetime_index/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1awdfzy/forecasting_a_target_with_a_nondatetime_index/", "subreddit_subscribers": 1356352, "created_utc": 1708525939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We (2 developers) made marimo\\[1\\] compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to [Pyodide](https://pyodide.org/).\n\nYou can try out the playground: [https://marimo.app/](https://marimo.app/). This has been a great tool for learning Python or educating others, as you can share snippets of code via the URL. For example, here is a notebook on [Bayes' Theorem](https://marimo.app/?code=JYWwDg9gTgLgBCAhlUEBQaD6mDmBTAOzykRjwBNMB3YGACzgF44AiABgDoAmDgVhbSIwYJgmSoOAQWEAKAJQY0AASFgOAYzwAbLTLrByeTOoiHGAFSgBXPAsMAzONhkgIcgFxo43hBA4hyGS8fEKgWcOCQ7wBiOAAhRABPPABnAHI4czo8aDwQDCifTCzgFLhgAjISdRhgADc8OAIIMgAjCAgAazgqRDKkQx7aBgBtJBRXAF09GBgwFPcAekWcYatWjQgQRfHUAFoyRG3d1zkAGjhEAnJyspHWvoo4CAJL17wADzAtK9JgF%2BejgSyXSmWyuRAcFaiTgAHESJU4ABlK6GKApF7TOizeZLRZUAkcRIQKwwdZ4TbbXowdR0AD8dUYAAkAFqwgDCEAAagAxADMAE06gBZABkWlKMEYAAUADIALwAjgB2UwgGD2ADSAEUqFxNQBJVraxVsKDygDqUEVq3sTM1nWlsIAjKKKoYPoxXVK%2BSk5JgOAVCvEkqkMvQclA8nAwFAIHUDKlLnATAQGgRgIR4L0YTAICnEFp1FYfmQ4BGY3GHq1gBKYIlIlEII5EHA6IlIBGUqU4Hh0-AACRMgdwVbp3sJwwETRwAcAUQHnkbIQAOiNlz5pTImXAVyADHA53JRCv7NUAN5bpnHrdz3f7m7XgC%2Bl5kR6fgeDu8mii-ACo-yyRoCCsEBiFIaAOAAsFgNA8C8ygW5y2ySsIGrWtaBhZsJyzMp5xHVFZ2HNtVEICocA3bw83wCMoAAbmQ0hbjOINCjXSiQivG833vA9r1EW9d3UIQ4GvT9gxXH8OIAoC4CnLYKggqAoL-DiDVeVwUngYSixLP4XhSC5aCQ7ItDAewS3LfMoyoFAywreT9wIJTZ1vOQCLKVionYr83IE7c5EEvc%2BOPABqOAtxXIgcFEm84B44KbiivAYuvFiv0kryQiofRaXKV4ySgV5hNeQsMShRpbNoMhSpSX8JPXXy32PZgguEkR%2BPCtqROS1K5HEtify-cIBBCfqtDwewYHkRso0KghFBUYQNG0LQ0AcJxMBcNxPBCMBMAYZhXA4KxgA4FIJTRGRODYC5nQuH5Wm0RgWAHLiBxYC46kLGxGE4e64C0vAwD%2BjhnQUPbMCMMdCAO0RjtO87LuIa6OFuuAAce57XqCh9Yo%2Br6frwUG%2BQuIGQf%2BiGfH26H6lh5oYDho6-ERi7EygIIvxuu6HsQJ6tEYMI3oSvHevxz64G%2BrRfs4UnAbICmwcbKnvDmqwipjKHcDpggDouGntfTTAGb1zW6EW1QVp0dbJs2mQDZh3W6H1rXHeNlpTf2ugPEbGnRC9jgpZsP8HZ1g7A6JuBwpkZ04D2M2I%2Bl2wQ9dsOTboRObEbB4QUwKMUhLeBmADoPGj-TXaaNjPS7gRYK9mvB5qhUMUjz1JC5dvALeWzQdD0RNjFMYnLBsOxbecVxO8N2HnYr6fdfTl3vd2nxjoCTngygewRo4kJoliaUqz5jDalSLLgwFEkC2Kl57GAHB1caCtYzQ4%2B60zMpsIrPtcKIgcLnnLxR8-83g3EAYlXc0VRIDnPoUc8x0%2BgHRgCAXQIwvZTzdrPUORt06TDkE%2BWBURZLfBaBVLQEAqCS1KFYQswB5RJmfkfGs78kxf2yCkRoP9KgpA4HADiO8xpeTVkVbuahe5rQ2s4VMWlrA1EwC-dCdZEjyPITAH2IRpEwFkYzBRb9MIqJaDNUIjd1YLQwEtMRq1%2B6GEHmYEetgbaOGcDnVIbcC5aBgBcSeFcMFhywanHBHt-HL0bGvQIHEt4CK-D4Z0vDZLJB0BQuAUYahXBwBNZJwN86-2-v2P%2BFxCK0DKMgPArYezC34swc8JcibuE4FwewT4BwDWiTwGC5ZSlaEyakgg6TGjxmID8YQ5Ehj0GQo0BJ5DKEvEaFGWMqQckoS4YOO8K52rQIKdcXehQimXCjGUvCXEhLkFIbjEKohqnhxruXbBM9M54DqdwRpzTtneD5HElChwukpJgGkjJ9AmInNSAQNI8ABlQCGeMuAkykk-L%2BbMrJCzuFQuWbOVZ6yxbDk2eQV5IRdklIOa5GQmLjxrJOfAM5SUoGVLgOeGOccE6l2PDcgJ9Mgn3MeQ0ppgZcUADkWjABnAC%2BAFYC6QjYY0AlZQKizi4ARa4s4%2BQjh7Iw1%2BzDMJEqPBcmmnLnm8MISEAA8kQHoSQrLIQqN0PmJJ4DAjDDBCESEKz2AgIkmgvT3BQt0eq%2BsRKdwQKPMqsoqrFGYVxdhfCoC-4kWEGRXpVkaLZEQjICsBLARQrhb0jJA45XnDkvURMNxoReqYSfLCjhI10FIhmXpuKmKFi6SmlCYr02pv2Z-Ctcqo0DiVf1XF6kLVlGEhwi4drQRAUdSkJIdVcU%2BWid4K8QDDwtV3GeRA6hzyvn4pS2KT4XyvnfE%2BE8a6N2bpqUnI9ZLSGXMrnc0ue7903t1Q%2Bi5LjW750Ls%2B2dQ152sAiII4x81RFWwkePLatyna%2BMCYzYJ6ifAbU0do%2BRpalEGOmnBnZ4BoDwCQHMVREoNhgA7Koy4ZRvgwEIeRjgiAPipBmhxWIAAlRA5BgBWAWBjC49h114D2CYchUBPVRnIBcCg%2BA%2BOuugJ61oScOJUbvg-KMMgFPdnoYwGQAA2M4Gm5AqyiA8Dh-sPEcEY3gHpfSZDXQuGwPNAMAb2HUC9GT67OgSzMy9HKtA8AS0LGAKtoMuC8D0yEQ6MZjOmfMxNSz6MbNL3uTzOADmXowqoG5xzLBPNkAlljAWLAmQsGCz4W9TsjMwBM2Z35Wa8Ab3nVZuAsXcXnpsOlX9EGrlExa-OpLLAvmfVxe5jL%2Bgsudeib5-znBeAja-Dll6c4%2BtfkK94Yr7sYOlfK5F6ruL7btaTtZ2zDKmt4Cg2ymD8XEvpd66J9LmXvMFLMuNvgHFFthbKzgYS8hqPkEoGAUgtIZAGYcV%2BKjb3EAfZY9937dA9DPeB%2B9-q4P5GQ5kMt9OMPjMg7B19xHNIofLe9nJ4zE18DXCMZvExGtYeg708Im%2BBAZFWDkd6staGWLmMtuIxxdsMPlCw7AMQEx8x9F8A3Juk9FDACccbI4RhMBMGYCwbASAKjYBYCvbwltrAEHkEAA).\n\n\\[1\\] marimo is open-source on GitHub: [https://github.com/marimo-team/marimo](https://github.com/marimo-team/marimo)", "author_fullname": "t2_jul3ckjm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "marimo-wasm: a reactive Python notebook in the browser", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awlq2x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708545618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We (2 developers) made marimo[1] compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to &lt;a href=\"https://pyodide.org/\"&gt;Pyodide&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;You can try out the playground: &lt;a href=\"https://marimo.app/\"&gt;https://marimo.app/&lt;/a&gt;. This has been a great tool for learning Python or educating others, as you can share snippets of code via the URL. For example, here is a notebook on &lt;a href=\"https://marimo.app/?code=JYWwDg9gTgLgBCAhlUEBQaD6mDmBTAOzykRjwBNMB3YGACzgF44AiABgDoAmDgVhbSIwYJgmSoOAQWEAKAJQY0AASFgOAYzwAbLTLrByeTOoiHGAFSgBXPAsMAzONhkgIcgFxo43hBA4hyGS8fEKgWcOCQ7wBiOAAhRABPPABnAHI4czo8aDwQDCifTCzgFLhgAjISdRhgADc8OAIIMgAjCAgAazgqRDKkQx7aBgBtJBRXAF09GBgwFPcAekWcYatWjQgQRfHUAFoyRG3d1zkAGjhEAnJyspHWvoo4CAJL17wADzAtK9JgF%2BejgSyXSmWyuRAcFaiTgAHESJU4ABlK6GKApF7TOizeZLRZUAkcRIQKwwdZ4TbbXowdR0AD8dUYAAkAFqwgDCEAAagAxADMAE06gBZABkWlKMEYAAUADIALwAjgB2UwgGD2ADSAEUqFxNQBJVraxVsKDygDqUEVq3sTM1nWlsIAjKKKoYPoxXVK%2BSk5JgOAVCvEkqkMvQclA8nAwFAIHUDKlLnATAQGgRgIR4L0YTAICnEFp1FYfmQ4BGY3GHq1gBKYIlIlEII5EHA6IlIBGUqU4Hh0-AACRMgdwVbp3sJwwETRwAcAUQHnkbIQAOiNlz5pTImXAVyADHA53JRCv7NUAN5bpnHrdz3f7m7XgC%2Bl5kR6fgeDu8mii-ACo-yyRoCCsEBiFIaAOAAsFgNA8C8ygW5y2ySsIGrWtaBhZsJyzMp5xHVFZ2HNtVEICocA3bw83wCMoAAbmQ0hbjOINCjXSiQivG833vA9r1EW9d3UIQ4GvT9gxXH8OIAoC4CnLYKggqAoL-DiDVeVwUngYSixLP4XhSC5aCQ7ItDAewS3LfMoyoFAywreT9wIJTZ1vOQCLKVionYr83IE7c5EEvc%2BOPABqOAtxXIgcFEm84B44KbiivAYuvFiv0kryQiofRaXKV4ySgV5hNeQsMShRpbNoMhSpSX8JPXXy32PZgguEkR%2BPCtqROS1K5HEtify-cIBBCfqtDwewYHkRso0KghFBUYQNG0LQ0AcJxMBcNxPBCMBMAYZhXA4KxgA4FIJTRGRODYC5nQuH5Wm0RgWAHLiBxYC46kLGxGE4e64C0vAwD%2BjhnQUPbMCMMdCAO0RjtO87LuIa6OFuuAAce57XqCh9Yo%2Br6frwUG%2BQuIGQf%2BiGfH26H6lh5oYDho6-ERi7EygIIvxuu6HsQJ6tEYMI3oSvHevxz64G%2BrRfs4UnAbICmwcbKnvDmqwipjKHcDpggDouGntfTTAGb1zW6EW1QVp0dbJs2mQDZh3W6H1rXHeNlpTf2ugPEbGnRC9jgpZsP8HZ1g7A6JuBwpkZ04D2M2I%2Bl2wQ9dsOTboRObEbB4QUwKMUhLeBmADoPGj-TXaaNjPS7gRYK9mvB5qhUMUjz1JC5dvALeWzQdD0RNjFMYnLBsOxbecVxO8N2HnYr6fdfTl3vd2nxjoCTngygewRo4kJoliaUqz5jDalSLLgwFEkC2Kl57GAHB1caCtYzQ4%2B60zMpsIrPtcKIgcLnnLxR8-83g3EAYlXc0VRIDnPoUc8x0%2BgHRgCAXQIwvZTzdrPUORt06TDkE%2BWBURZLfBaBVLQEAqCS1KFYQswB5RJmfkfGs78kxf2yCkRoP9KgpA4HADiO8xpeTVkVbuahe5rQ2s4VMWlrA1EwC-dCdZEjyPITAH2IRpEwFkYzBRb9MIqJaDNUIjd1YLQwEtMRq1%2B6GEHmYEetgbaOGcDnVIbcC5aBgBcSeFcMFhywanHBHt-HL0bGvQIHEt4CK-D4Z0vDZLJB0BQuAUYahXBwBNZJwN86-2-v2P%2BFxCK0DKMgPArYezC34swc8JcibuE4FwewT4BwDWiTwGC5ZSlaEyakgg6TGjxmID8YQ5Ehj0GQo0BJ5DKEvEaFGWMqQckoS4YOO8K52rQIKdcXehQimXCjGUvCXEhLkFIbjEKohqnhxruXbBM9M54DqdwRpzTtneD5HElChwukpJgGkjJ9AmInNSAQNI8ABlQCGeMuAkykk-L%2BbMrJCzuFQuWbOVZ6yxbDk2eQV5IRdklIOa5GQmLjxrJOfAM5SUoGVLgOeGOccE6l2PDcgJ9Mgn3MeQ0ppgZcUADkWjABnAC%2BAFYC6QjYY0AlZQKizi4ARa4s4%2BQjh7Iw1%2BzDMJEqPBcmmnLnm8MISEAA8kQHoSQrLIQqN0PmJJ4DAjDDBCESEKz2AgIkmgvT3BQt0eq%2BsRKdwQKPMqsoqrFGYVxdhfCoC-4kWEGRXpVkaLZEQjICsBLARQrhb0jJA45XnDkvURMNxoReqYSfLCjhI10FIhmXpuKmKFi6SmlCYr02pv2Z-Ctcqo0DiVf1XF6kLVlGEhwi4drQRAUdSkJIdVcU%2BWid4K8QDDwtV3GeRA6hzyvn4pS2KT4XyvnfE%2BE8a6N2bpqUnI9ZLSGXMrnc0ue7903t1Q%2Bi5LjW750Ls%2B2dQ152sAiII4x81RFWwkePLatyna%2BMCYzYJ6ifAbU0do%2BRpalEGOmnBnZ4BoDwCQHMVREoNhgA7Koy4ZRvgwEIeRjgiAPipBmhxWIAAlRA5BgBWAWBjC49h114D2CYchUBPVRnIBcCg%2BA%2BOuugJ61oScOJUbvg-KMMgFPdnoYwGQAA2M4Gm5AqyiA8Dh-sPEcEY3gHpfSZDXQuGwPNAMAb2HUC9GT67OgSzMy9HKtA8AS0LGAKtoMuC8D0yEQ6MZjOmfMxNSz6MbNL3uTzOADmXowqoG5xzLBPNkAlljAWLAmQsGCz4W9TsjMwBM2Z35Wa8Ab3nVZuAsXcXnpsOlX9EGrlExa-OpLLAvmfVxe5jL%2Bgsudeib5-znBeAja-Dll6c4%2BtfkK94Yr7sYOlfK5F6ruL7btaTtZ2zDKmt4Cg2ymD8XEvpd66J9LmXvMFLMuNvgHFFthbKzgYS8hqPkEoGAUgtIZAGYcV%2BKjb3EAfZY9937dA9DPeB%2B9-q4P5GQ5kMt9OMPjMg7B19xHNIofLe9nJ4zE18DXCMZvExGtYeg708Im%2BBAZFWDkd6staGWLmMtuIxxdsMPlCw7AMQEx8x9F8A3Juk9FDACccbI4RhMBMGYCwbASAKjYBYCvbwltrAEHkEAA\"&gt;Bayes&amp;#39; Theorem&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;[1] marimo is open-source on GitHub: &lt;a href=\"https://github.com/marimo-team/marimo\"&gt;https://github.com/marimo-team/marimo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1awlq2x", "is_robot_indexable": true, "report_reasons": null, "author": "mmmmmmyles", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1awlq2x/marimowasm_a_reactive_python_notebook_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1awlq2x/marimowasm_a_reactive_python_notebook_in_the/", "subreddit_subscribers": 1356352, "created_utc": 1708545618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Interesting paper I came across regarding statistical methodological innovations to address challenges in large scale online experimentation.\n\nhttps://arxiv.org/abs/2212.11366\n\n", "author_fullname": "t2_i69qgpqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenges with online controlled experiments ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awfm6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708531251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interesting paper I came across regarding statistical methodological innovations to address challenges in large scale online experimentation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2212.11366\"&gt;https://arxiv.org/abs/2212.11366&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1awfm6s", "is_robot_indexable": true, "report_reasons": null, "author": "AdFew4357", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1awfm6s/challenges_with_online_controlled_experiments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1awfm6s/challenges_with_online_controlled_experiments/", "subreddit_subscribers": 1356352, "created_utc": 1708531251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For open-source practitioners of Data-Centric AI (using AI to systematically improve your existing data):  I just released major updates to cleanlab, the\u00a0most popular software library for Data-Centric AI (with 8000 GitHub stars thanks to an amazing community).  \n\n\nFlawed data produces flawed AI, and real-world datasets have many flaws that are hard to catch manually.  With one line of Python code, you can run cleanlab on any dataset to automatically catch these flaws, and thus improve almost any ML model fit to this data.  Try it quickly to see why thousands of data scientists have adopted cleanlab\u2019s AI-based data quality algorithms to deploy more reliable ML.  \n\n\nToday\u2019s v2.6.0 release includes new capabilities like Data Valuation (via Data Shapely), detection of Underperforming Data Slices/Groups, and lots more.  I published a blogpost outlining new automated techniques this library provides to systematically increase the value your existing data.   \n\n\nBlogpost:  [https://cleanlab.ai/blog/cleanlab-2.6](https://cleanlab.ai/blog/cleanlab-2.6) \n\nGitHub repo:  [https://github.com/cleanlab/cleanlab](https://github.com/cleanlab/cleanlab)\n\n5min notebook tutorials:  [https://docs.cleanlab.ai/](https://docs.cleanlab.ai/)  \n\n\nI'd love to hear how you all doing data prep / exploratory data analysis in 2024?  \nMy view is you shouldn't do 100% of your data checking manually \u2013 also use automated algorithms like cleanlab offers to ensure you don\u2019t miss any problems (significantly improved coverage in terms of data flaws discovered and addressed).  The vision of Data-Centric AI is to use your trained ML models to help you find and fix dataset issues, which can allow to you subsequently train better versions of these models.", "author_fullname": "t2_5v7p3x0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using AI automation to help with data prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awjf6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708540198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For open-source practitioners of Data-Centric AI (using AI to systematically improve your existing data):  I just released major updates to cleanlab, the\u00a0most popular software library for Data-Centric AI (with 8000 GitHub stars thanks to an amazing community).  &lt;/p&gt;\n\n&lt;p&gt;Flawed data produces flawed AI, and real-world datasets have many flaws that are hard to catch manually.  With one line of Python code, you can run cleanlab on any dataset to automatically catch these flaws, and thus improve almost any ML model fit to this data.  Try it quickly to see why thousands of data scientists have adopted cleanlab\u2019s AI-based data quality algorithms to deploy more reliable ML.  &lt;/p&gt;\n\n&lt;p&gt;Today\u2019s v2.6.0 release includes new capabilities like Data Valuation (via Data Shapely), detection of Underperforming Data Slices/Groups, and lots more.  I published a blogpost outlining new automated techniques this library provides to systematically increase the value your existing data.   &lt;/p&gt;\n\n&lt;p&gt;Blogpost:  &lt;a href=\"https://cleanlab.ai/blog/cleanlab-2.6\"&gt;https://cleanlab.ai/blog/cleanlab-2.6&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;GitHub repo:  &lt;a href=\"https://github.com/cleanlab/cleanlab\"&gt;https://github.com/cleanlab/cleanlab&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;5min notebook tutorials:  &lt;a href=\"https://docs.cleanlab.ai/\"&gt;https://docs.cleanlab.ai/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear how you all doing data prep / exploratory data analysis in 2024?&lt;br/&gt;\nMy view is you shouldn&amp;#39;t do 100% of your data checking manually \u2013 also use automated algorithms like cleanlab offers to ensure you don\u2019t miss any problems (significantly improved coverage in terms of data flaws discovered and addressed).  The vision of Data-Centric AI is to use your trained ML models to help you find and fix dataset issues, which can allow to you subsequently train better versions of these models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?auto=webp&amp;s=76b269f80aed83cdba12887154819dec4688c156", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d769d3004d3f186cfd8f0070d6c18cd102cb89a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9a1f71d565f959f36171326b0eb6796cdd648d0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b95b4145117fa58951aad2c5bdc6356287c0162b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d817bb6d6835513053747a222fd97b50c6fe71d", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea17682e75005c2760d72a611fe32754eceeda36", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/jPz6eCa4tEkZMXOb8sleXHPqJpxG2RnvtaRB3sszX9Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a3d5e8f1c31739430ffb6adeb84fc7cef56e5eb", "width": 1080, "height": 607}], "variants": {}, "id": "LFx82LBCBUMu3pBjyrqStW_8vMsq7aQRaM7_TF1CFEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "1awjf6e", "is_robot_indexable": true, "report_reasons": null, "author": "jonas__m", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1awjf6e/using_ai_automation_to_help_with_data_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1awjf6e/using_ai_automation_to_help_with_data_prep/", "subreddit_subscribers": 1356352, "created_utc": 1708540198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm a student in Data Science field but my family is having a financial problem for last few years. I don't think I can afford the expensive tuition fee so I started to think that I gotta drop out. But I really have desire to this field and I don't wanna give up. Can anyone here give me some advices to build a career path in this situation? I have knowledge in Machine Learning and studying for the DP-100 certification. Can professional certs with some personal projects that involve advanced techniques, clear documents help? Please I really need help right now.  \n\n\nEdit: I don't think I'm having great knowledge that's enough for freelancing jobs but is it a good choice if I improve skills all by myself and find one when I'm ready. Actually, I'm ready to do the job for free in the first few months just to get an easier chance to work in real world working environment and get some experiment", "author_fullname": "t2_hxexp0pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I get in the field without the degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw28rk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.28, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708487583.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708487396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a student in Data Science field but my family is having a financial problem for last few years. I don&amp;#39;t think I can afford the expensive tuition fee so I started to think that I gotta drop out. But I really have desire to this field and I don&amp;#39;t wanna give up. Can anyone here give me some advices to build a career path in this situation? I have knowledge in Machine Learning and studying for the DP-100 certification. Can professional certs with some personal projects that involve advanced techniques, clear documents help? Please I really need help right now.  &lt;/p&gt;\n\n&lt;p&gt;Edit: I don&amp;#39;t think I&amp;#39;m having great knowledge that&amp;#39;s enough for freelancing jobs but is it a good choice if I improve skills all by myself and find one when I&amp;#39;m ready. Actually, I&amp;#39;m ready to do the job for free in the first few months just to get an easier chance to work in real world working environment and get some experiment&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aw28rk", "is_robot_indexable": true, "report_reasons": null, "author": "Avry_great", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1aw28rk/can_i_get_in_the_field_without_the_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1aw28rk/can_i_get_in_the_field_without_the_degree/", "subreddit_subscribers": 1356352, "created_utc": 1708487396.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}