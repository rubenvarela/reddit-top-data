{"kind": "Listing", "data": {"after": "t3_1aw762n", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_insol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT4 doing data analysis by writing and running python scripts, plotting charts and all. Experimental but promising. What should I test this on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_1aviw3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/exzs2wto2rjc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1650, "scrubber_media_url": "https://v.redd.it/exzs2wto2rjc1/DASH_96.mp4", "dash_url": "https://v.redd.it/exzs2wto2rjc1/DASHPlaylist.mpd?a=1711111698%2CNGUzNmQxYThmNWRhMWNiMjhlZTdkZmVkMjlhYmQ1NTc4YzUwYmQxMTFhMWU3MzIyMWNhZDYwMjk0YTZhNGQzNA%3D%3D&amp;v=1&amp;f=sd", "duration": 46, "hls_url": "https://v.redd.it/exzs2wto2rjc1/HLSPlaylist.m3u8?a=1711111698%2CMTNhNGJjYzM0OGQ1ZDM2Yzg2YTMyNjY1NWNiNjliMzZlZDFhODNmNmQ0ODFhNDcxN2VmZWJlOTk2MGYwYjc2NQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=140&amp;height=91&amp;crop=140:91,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ebccf20579d226c38c2116b71802d46eee47679e", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708439202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/exzs2wto2rjc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?format=pjpg&amp;auto=webp&amp;s=1fb2e5265231f37df29d7a4e4020d135ba3b7f99", "width": 3300, "height": 2160}, "resolutions": [{"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e16907b378901eda1532e9656d7402e458044bc", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=21f7802eec1a15dbb5abb21a3ef1e0fcb4f3a91b", "width": 216, "height": 141}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=73e77003a6c9af09eebf0ad00e2c8aab93e1bae0", "width": 320, "height": 209}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2e0290b717b2408478c7dd5e55c18faeb178182e", "width": 640, "height": 418}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=41394c4db0bfef20243ac38f87a8f57ce02b26e0", "width": 960, "height": 628}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c351cdb435bd43d3489021abca589eca15c5bb45", "width": 1080, "height": 706}], "variants": {}, "id": "MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1aviw3v", "is_robot_indexable": true, "report_reasons": null, "author": "ashpreetbedi", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aviw3v/gpt4_doing_data_analysis_by_writing_and_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/exzs2wto2rjc1", "subreddit_subscribers": 162497, "created_utc": 1708439202.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/exzs2wto2rjc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1650, "scrubber_media_url": "https://v.redd.it/exzs2wto2rjc1/DASH_96.mp4", "dash_url": "https://v.redd.it/exzs2wto2rjc1/DASHPlaylist.mpd?a=1711111698%2CNGUzNmQxYThmNWRhMWNiMjhlZTdkZmVkMjlhYmQ1NTc4YzUwYmQxMTFhMWU3MzIyMWNhZDYwMjk0YTZhNGQzNA%3D%3D&amp;v=1&amp;f=sd", "duration": 46, "hls_url": "https://v.redd.it/exzs2wto2rjc1/HLSPlaylist.m3u8?a=1711111698%2CMTNhNGJjYzM0OGQ1ZDM2Yzg2YTMyNjY1NWNiNjliMzZlZDFhODNmNmQ0ODFhNDcxN2VmZWJlOTk2MGYwYjc2NQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it's hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What was your biggest challenge getting into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avj2k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708439683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it&amp;#39;s hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avj2k2", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "subreddit_subscribers": 162497, "created_utc": 1708439683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\n**TLDR:** Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference &amp; modelling) package in Rust.\n\nWith DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.\n\nWe wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.\n\nLooking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated \ud83d\ude03)\n\nhttps://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;format=pjpg&amp;auto=webp&amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source DBT core alternative written in Rust (30x faster)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3kisasoefwjc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b268eb8ba9f4e906d5aff5f16f58945a88b06e0b"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af417f90677c04df9ae254e038c926930b7de974"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a7f7b68171bfd3c6c10a566df5c1775268f6d0f"}, {"y": 392, "x": 640, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=adc7e1478f6ee2a0e55cfebc81e89dd6e2801f3c"}, {"y": 588, "x": 960, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd118e81cb6c6ce854522191cb0693b7babc4a26"}, {"y": 662, "x": 1080, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38628b6d703ed6b02580f66bb9b5819575da209b"}], "s": {"y": 2216, "x": 3612, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;format=pjpg&amp;auto=webp&amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5"}, "id": "3kisasoefwjc1"}}, "name": "t3_1aw7368", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VZi9tcyvi9b-tgRfYixpPxqpV_kseg8ktYRf428RVvo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708503956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference &amp;amp; modelling) package in Rust.&lt;/p&gt;\n\n&lt;p&gt;With DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.&lt;/p&gt;\n\n&lt;p&gt;We wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated \ud83d\ude03)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5\"&gt;https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1aw7368", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw7368/open_source_dbt_core_alternative_written_in_rust/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw7368/open_source_dbt_core_alternative_written_in_rust/", "subreddit_subscribers": 162497, "created_utc": 1708503956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for a new role and every job on LinkedIn seems to have over 100 applicants. Is the field really this competitive? Seems incredibly difficult to find a job despite all of the articles I read touting the lack of supply of data engineers. Thoughts?", "author_fullname": "t2_d07hrm40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there too many data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw2se4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708489012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a new role and every job on LinkedIn seems to have over 100 applicants. Is the field really this competitive? Seems incredibly difficult to find a job despite all of the articles I read touting the lack of supply of data engineers. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw2se4", "is_robot_indexable": true, "report_reasons": null, "author": "bostinloyd", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw2se4/are_there_too_many_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw2se4/are_there_too_many_data_engineers/", "subreddit_subscribers": 162497, "created_utc": 1708489012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered \"only a few of those applicant are good\". I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? ", "author_fullname": "t2_13mcsa6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What divides good from average?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw4uaq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708495611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered &amp;quot;only a few of those applicant are good&amp;quot;. I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw4uaq", "is_robot_indexable": true, "report_reasons": null, "author": "Chediras", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "subreddit_subscribers": 162497, "created_utc": 1708495611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you or are you implementing a lakehouse?\n\nIf so what have been some of the biggest hurdles or things you\u2019ve learned?\n\nWhat were big decisions you were happy with or would do differently if you could start over.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most important parts of implementing a lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avhmkt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708435683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you or are you implementing a lakehouse?&lt;/p&gt;\n\n&lt;p&gt;If so what have been some of the biggest hurdles or things you\u2019ve learned?&lt;/p&gt;\n\n&lt;p&gt;What were big decisions you were happy with or would do differently if you could start over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avhmkt", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "subreddit_subscribers": 162497, "created_utc": 1708435683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, i just started my master's project and currently working with .parquet files that are \\~20GB each but i do not have much of an idea on how to work with those. I need to merge multiple of these files (each carrying different types of information) with a key that is not unique (so in each file the key that i am using appears in multiple different rows). When i try to do that with python-pandas, things quickly get out of hand with the memory and the system crashes. What would be the best way to achieve  the equivalent of 'inner' or 'left' merge of these .parquet files? Any help will be greatly appreciated :)", "author_fullname": "t2_a2ypvree", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging large .parquet files without a unique key efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avo2kw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708460079.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708451836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i just started my master&amp;#39;s project and currently working with .parquet files that are ~20GB each but i do not have much of an idea on how to work with those. I need to merge multiple of these files (each carrying different types of information) with a key that is not unique (so in each file the key that i am using appears in multiple different rows). When i try to do that with python-pandas, things quickly get out of hand with the memory and the system crashes. What would be the best way to achieve  the equivalent of &amp;#39;inner&amp;#39; or &amp;#39;left&amp;#39; merge of these .parquet files? Any help will be greatly appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avo2kw", "is_robot_indexable": true, "report_reasons": null, "author": "Martian_Onion", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avo2kw/merging_large_parquet_files_without_a_unique_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avo2kw/merging_large_parquet_files_without_a_unique_key/", "subreddit_subscribers": 162497, "created_utc": 1708451836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently working as a DE for a big insurance company in EU. My manager sees me (medior) fit for a more senior role within the company to coach and bring juniors/mediors to their next level (technical and soft skills). This is new for me, so I would appreciate some tips from DE leads and seniors.\n\n1. What are your tips and best practices to coach and bring juniors/mediors to their next level (technical/soft skills)?\n\n2. And, what should I be carefull with?\n\nThis would help me out a lot to get started with my next steps becoming a senior ;)\n", "author_fullname": "t2_344js9zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear seniors, how do you coach juniors/mediors to their next level?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avu2vf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708466035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as a DE for a big insurance company in EU. My manager sees me (medior) fit for a more senior role within the company to coach and bring juniors/mediors to their next level (technical and soft skills). This is new for me, so I would appreciate some tips from DE leads and seniors.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What are your tips and best practices to coach and bring juniors/mediors to their next level (technical/soft skills)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;And, what should I be carefull with?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This would help me out a lot to get started with my next steps becoming a senior ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avu2vf", "is_robot_indexable": true, "report_reasons": null, "author": "-SCYLLA-", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avu2vf/dear_seniors_how_do_you_coach_juniorsmediors_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avu2vf/dear_seniors_how_do_you_coach_juniorsmediors_to/", "subreddit_subscribers": 162497, "created_utc": 1708466035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n['](https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01)\n\n**Multiwoven** is built for **data teams** \\- *engineers*, *analysts*, *ops, etc*. - to easily prepare and sync data to business tools.\n\nWe\u2019re **open-source** (so you don\u2019t have to pay to move your own data to your own tools), **self-hosted** (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of **Multiwoven**, for your own needs)\n\n[**https://github.com/Multiwoven/multiwoven**](https://github.com/Multiwoven/multiwoven)\n\nStar it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) ", "author_fullname": "t2_rtrd3q97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched Multiwoven - an open-source Reverse ETL platform!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ec2bl2oberjc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe86d0b5ca53c1dd9e0ad579158f73eb36bdc3c"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62c60bf3aa7c430b5662e778781118fcdd5844f6"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=efc980e0a347d5d8e365eb1f775fef20d188f4cf"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3758c4feeef9986b3896ee00413668180b7d165b"}, {"y": 480, "x": 960, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3322c7e225a2a3d82e63ba5637c27f349d05bff9"}, {"y": 540, "x": 1080, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6701a1f01a4abff464adae7daa870e6d0422fbcb"}], "s": {"y": 1280, "x": 2560, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01"}, "id": "ec2bl2oberjc1"}}, "name": "t3_1avkgw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ic6Skc0tts236_84heUlo76cJufFHxdjWGqEdY50b1M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708443255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31fbcafee400f17cba29297c7360a598feacef01\"&gt;&amp;#39;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multiwoven&lt;/strong&gt; is built for &lt;strong&gt;data teams&lt;/strong&gt; - &lt;em&gt;engineers&lt;/em&gt;, &lt;em&gt;analysts&lt;/em&gt;, &lt;em&gt;ops, etc&lt;/em&gt;. - to easily prepare and sync data to business tools.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re &lt;strong&gt;open-source&lt;/strong&gt; (so you don\u2019t have to pay to move your own data to your own tools), &lt;strong&gt;self-hosted&lt;/strong&gt; (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of &lt;strong&gt;Multiwoven&lt;/strong&gt;, for your own needs)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;&lt;strong&gt;https://github.com/Multiwoven/multiwoven&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Star it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1avkgw5", "is_robot_indexable": true, "report_reasons": null, "author": "nagstler", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "subreddit_subscribers": 162497, "created_utc": 1708443255.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? ", "author_fullname": "t2_17a3gi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you understand by \u201eSQL Basics\u201d and \u201ePython Basics\u201d in CV, what exact skills would you expect from that person?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avi583", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708437174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avi583", "is_robot_indexable": true, "report_reasons": null, "author": "ItsGonnaBeGreatYear", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "subreddit_subscribers": 162497, "created_utc": 1708437174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a small website to help people brand new to Spark get started. The goal is to teach all of the basics in an afternoon, along with some opinionated style suggestions on how to write Spark code. However, before someone can get started though, they need somewhere to run their Spark code. \n\nI'm wondering what people found most helpful as they were getting started. Personally, I tend to prefer working locally with Docker, but I can see that being a small hurdle if folks aren't already familiar with running Docker (even though docker-compose simplifies this a lot IMO).\n\nWhat helped you with writing and testing out Spark when you were starting out?", "author_fullname": "t2_uyd9mc1b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you find it easier to learn Spark using a cloud notebook environment or locally with Docker?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avoont", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708453239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a small website to help people brand new to Spark get started. The goal is to teach all of the basics in an afternoon, along with some opinionated style suggestions on how to write Spark code. However, before someone can get started though, they need somewhere to run their Spark code. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what people found most helpful as they were getting started. Personally, I tend to prefer working locally with Docker, but I can see that being a small hurdle if folks aren&amp;#39;t already familiar with running Docker (even though docker-compose simplifies this a lot IMO).&lt;/p&gt;\n\n&lt;p&gt;What helped you with writing and testing out Spark when you were starting out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avoont", "is_robot_indexable": true, "report_reasons": null, "author": "zchtsk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avoont/do_you_find_it_easier_to_learn_spark_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avoont/do_you_find_it_easier_to_learn_spark_using_a/", "subreddit_subscribers": 162497, "created_utc": 1708453239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm about to embark on the most boring project of my life: updating product documentation \ud83e\udd23  \n\n\nAnd I need some inspiration before I get started.  \n\n\nCurious - of all the data products that you've used, which one has the best documentation?\n\nAspects of great documentation might include:  \n\\- Navigable  \n\\- Clear, concise (you're question gets answered quickly. Learning something new on the platform is straightforward)   \n\\- Companion tutorials  \n\n\n... Your input could save my sanity!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boring topic - Documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avrbd5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708459541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to embark on the most boring project of my life: updating product documentation \ud83e\udd23  &lt;/p&gt;\n\n&lt;p&gt;And I need some inspiration before I get started.  &lt;/p&gt;\n\n&lt;p&gt;Curious - of all the data products that you&amp;#39;ve used, which one has the best documentation?&lt;/p&gt;\n\n&lt;p&gt;Aspects of great documentation might include:&lt;br/&gt;\n- Navigable&lt;br/&gt;\n- Clear, concise (you&amp;#39;re question gets answered quickly. Learning something new on the platform is straightforward)&lt;br/&gt;\n- Companion tutorials  &lt;/p&gt;\n\n&lt;p&gt;... Your input could save my sanity!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avrbd5", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avrbd5/boring_topic_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avrbd5/boring_topic_documentation/", "subreddit_subscribers": 162497, "created_utc": 1708459541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nJust started my first data engineering position, but it\u2019s not at a company dealing with order/customer/deal data - the position has to do more with political and corporate data, which is very interesting, but it\u2019s much harder to figure out how to apply some of these data engineering concepts (like reading Kimball) without the same data types. For example, I\u2019m trying to set up a data warehouse from scratch for our data that\u2019s currently in PostgreSQL, but having a hard time figuring out where to start in terms of what to stage and how to go about dimensionally modeling this data. Does anybody have any tips for data engineering for atypical data? ", "author_fullname": "t2_2ivpdou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering for Non-Business Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avka8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708442797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Just started my first data engineering position, but it\u2019s not at a company dealing with order/customer/deal data - the position has to do more with political and corporate data, which is very interesting, but it\u2019s much harder to figure out how to apply some of these data engineering concepts (like reading Kimball) without the same data types. For example, I\u2019m trying to set up a data warehouse from scratch for our data that\u2019s currently in PostgreSQL, but having a hard time figuring out where to start in terms of what to stage and how to go about dimensionally modeling this data. Does anybody have any tips for data engineering for atypical data? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avka8p", "is_robot_indexable": true, "report_reasons": null, "author": "Butterhero_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avka8p/data_engineering_for_nonbusiness_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avka8p/data_engineering_for_nonbusiness_data/", "subreddit_subscribers": 162497, "created_utc": 1708442797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had started a blog series to guide people on the path I felt they should follow, to transition into Azure Data Engineer role as a way back of doing good to the community and help people in making their careers and providing for their family. The blog where I mentioned this received tremendous response which really motivated me to go for it.\n\nHowever that is where things changed. The first blog titled, structured way to get into DE, though received very good response on upvotes and shares, was filled with negative comments. Some questioned why only Azure, some questioned why I had given timelines, some questioned why am I focusing on only X number of things, etc etc. Very very few people had something positive to say. Still I ignored it as helping community was more important but the next two blogs, SQL and ADF had a drastic decrease in engagement. Almost 80% engagement was gone.\n\nThis brings me to the question, are the blogs still needed by the community, are they helping you in any way or it's something unnecessary and should be stopped?\n\nNote: If this ends up on the positive side, I will be needing at least 50 upvotes or shares (as karma farming comments will come so I am fine with just shares too) for the already published blogs (SQL and ADF) and the ones to follow from now. This way it can be ensured that people are engaging with the content and I am not spending time creating something nobody is reading :)\n\n[View Poll](https://www.reddit.com/poll/1avnq5y)", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The End?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avnq5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708455800.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708451004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had started a blog series to guide people on the path I felt they should follow, to transition into Azure Data Engineer role as a way back of doing good to the community and help people in making their careers and providing for their family. The blog where I mentioned this received tremendous response which really motivated me to go for it.&lt;/p&gt;\n\n&lt;p&gt;However that is where things changed. The first blog titled, structured way to get into DE, though received very good response on upvotes and shares, was filled with negative comments. Some questioned why only Azure, some questioned why I had given timelines, some questioned why am I focusing on only X number of things, etc etc. Very very few people had something positive to say. Still I ignored it as helping community was more important but the next two blogs, SQL and ADF had a drastic decrease in engagement. Almost 80% engagement was gone.&lt;/p&gt;\n\n&lt;p&gt;This brings me to the question, are the blogs still needed by the community, are they helping you in any way or it&amp;#39;s something unnecessary and should be stopped?&lt;/p&gt;\n\n&lt;p&gt;Note: If this ends up on the positive side, I will be needing at least 50 upvotes or shares (as karma farming comments will come so I am fine with just shares too) for the already published blogs (SQL and ADF) and the ones to follow from now. This way it can be ensured that people are engaging with the content and I am not spending time creating something nobody is reading :)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1avnq5y\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avnq5y", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1708537404984, "options": [{"text": "Continue with the blogs.", "id": "27157742"}, {"text": "Stop the blogs.", "id": "27157743"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 36, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avnq5y/the_end/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1avnq5y/the_end/", "subreddit_subscribers": 162497, "created_utc": 1708451004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "in the last days I saw a sessionization tool for ad tech events that was a domain specific language (not SQL) without any weird joins.\n\n&amp;#x200B;\n\nsadly now when I need it I cannot find it anymore. Does anyone know what it is and cand share a link? It was a specific database with its own query engine to simplify sessionization event queries ", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "simplified sessionization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avrl12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708460186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;in the last days I saw a sessionization tool for ad tech events that was a domain specific language (not SQL) without any weird joins.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;sadly now when I need it I cannot find it anymore. Does anyone know what it is and cand share a link? It was a specific database with its own query engine to simplify sessionization event queries &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1avrl12", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/1avrl12/simplified_sessionization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avrl12/simplified_sessionization/", "subreddit_subscribers": 162497, "created_utc": 1708460186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an experienced software architect, but relatively new to data engineering. We have an ML product, that receives IOT data from industrial processes.\n\nAt the moment, we use a near-realtime setup where data stages trigger one another through SQS queues, and we deal with small batches of data with AWS lambda.\n\n* Raw -&gt; staged: map data to a canonical format from incoming mqtt.\n* Staged -&gt; mapped: extract the features we care about.\n* Mapped -&gt; regularised: create dense data from sparse mapped data through interpolation.\n\nEach stage reads and writes from S3.\n\nThen using the regularised data we run some models:\n\n* Data status: identify potential sensor failures\n* Plant status: Give an overall health measure of the industrial process\n* Inference: Predict control values for the process given a known-healthy dataset and plant.\n\nAll this is _fine_ and we have the ability to run stages with Ray so that we can do historical batch processing, but I'm concerned about the complexity, and the delays that we incur at each stage. We're within our current SLOs, but we'd like to be able to run inference more frequently.\n\nWhat's the current state of the art for managing this kind of workflow? My biases are toward serverless options, but my hunch is that we're going to end up with some kind of stateful worker pattern.", "author_fullname": "t2_ay3xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting data pipelines for ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avgzow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708434554.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708433746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an experienced software architect, but relatively new to data engineering. We have an ML product, that receives IOT data from industrial processes.&lt;/p&gt;\n\n&lt;p&gt;At the moment, we use a near-realtime setup where data stages trigger one another through SQS queues, and we deal with small batches of data with AWS lambda.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Raw -&amp;gt; staged: map data to a canonical format from incoming mqtt.&lt;/li&gt;\n&lt;li&gt;Staged -&amp;gt; mapped: extract the features we care about.&lt;/li&gt;\n&lt;li&gt;Mapped -&amp;gt; regularised: create dense data from sparse mapped data through interpolation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Each stage reads and writes from S3.&lt;/p&gt;\n\n&lt;p&gt;Then using the regularised data we run some models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data status: identify potential sensor failures&lt;/li&gt;\n&lt;li&gt;Plant status: Give an overall health measure of the industrial process&lt;/li&gt;\n&lt;li&gt;Inference: Predict control values for the process given a known-healthy dataset and plant.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All this is &lt;em&gt;fine&lt;/em&gt; and we have the ability to run stages with Ray so that we can do historical batch processing, but I&amp;#39;m concerned about the complexity, and the delays that we incur at each stage. We&amp;#39;re within our current SLOs, but we&amp;#39;d like to be able to run inference more frequently.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the current state of the art for managing this kind of workflow? My biases are toward serverless options, but my hunch is that we&amp;#39;re going to end up with some kind of stateful worker pattern.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avgzow", "is_robot_indexable": true, "report_reasons": null, "author": "bobaduk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avgzow/architecting_data_pipelines_for_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avgzow/architecting_data_pipelines_for_ml/", "subreddit_subscribers": 162497, "created_utc": 1708433746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI'm a software developer diving in a new project in work for analytics in Azure and I need suggestions.  \nI have experience with airflow,sql, python and so on.  \nWe are going to create analytics solution for our customers.\n\nwe have raw data layer as parquet in azure blob storage. we use synapse to extract the data.  \n\n\nWe need to transform the data for other layers (silver,gold) and I find azure stack disappointing.  \n\n\n* Synapse and ADF are mostly no code solutions and I'm looking for something or combination that I can create scheduling and transform in code.  \n\n* for scheduling - synapse scheduling is lacking and airflow managed is very limited. ADF also seems no code.\n* databricks seems very pricey and more than we need.\n\n&amp;#x200B;\n\nfor the transformation - I know we need pyspark as we have a lot of data. this is also a code solution that satisfy me.\n\n&amp;#x200B;\n\nI see a PR with json file instead of code and I know it will slow development and won't be maintainable.  \n\n\nanyone has recommendation for stack that involve code ? coming from more software oriented I believe in more coding solutions as I worked with Apache NiFi and it was not maintainable.  \n\n\nThanks  \n", "author_fullname": "t2_atpck6ua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure stack for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw80uf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708507794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer diving in a new project in work for analytics in Azure and I need suggestions.&lt;br/&gt;\nI have experience with airflow,sql, python and so on.&lt;br/&gt;\nWe are going to create analytics solution for our customers.&lt;/p&gt;\n\n&lt;p&gt;we have raw data layer as parquet in azure blob storage. we use synapse to extract the data.  &lt;/p&gt;\n\n&lt;p&gt;We need to transform the data for other layers (silver,gold) and I find azure stack disappointing.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Synapse and ADF are mostly no code solutions and I&amp;#39;m looking for something or combination that I can create scheduling and transform in code.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;for scheduling - synapse scheduling is lacking and airflow managed is very limited. ADF also seems no code.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;databricks seems very pricey and more than we need.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;for the transformation - I know we need pyspark as we have a lot of data. this is also a code solution that satisfy me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I see a PR with json file instead of code and I know it will slow development and won&amp;#39;t be maintainable.  &lt;/p&gt;\n\n&lt;p&gt;anyone has recommendation for stack that involve code ? coming from more software oriented I believe in more coding solutions as I worked with Apache NiFi and it was not maintainable.  &lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw80uf", "is_robot_indexable": true, "report_reasons": null, "author": "pythondeveloper77", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw80uf/azure_stack_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw80uf/azure_stack_for_de/", "subreddit_subscribers": 162497, "created_utc": 1708507794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: \nI have a project coming up and I\u2019m being considered to play the  Data Architect for migration work and design. \n\nThe only issue is the tech stack is terrible for DE work \n- Powerapps (Dataverse, power automate etc) \n- Dynamics ( not sure this is still a thing) \n- Legacy Third party systems upstream sources  (Oracle , Java). No api confirmed.  No direct access. May have to write .net integrations \n\nA lot of this work seems to be glorified ETL. But not in a trivial sense. Tons of tables and likely a lot inconsistencies/clean up work. \n\nI maybe will get an Azure SQL Database, Data Factory and Logic Apps. Also expected to write integrations in .net to integrate the power apps ecosystem. Management is bought into low code and wants to avoid these tools, custom code etc. \n\nWhile I\u2019m excited by the title bump the effort seems doomed to fail just off the tech stack being a mess and the \u201clow code\u201d nonsense. The people who put this together are mostly concerned about the frontend and security. I didn\u2019t get to choose but I\u2019m thinking of pushing back and advocating for a proper azure architecture or at least fabric. Also suggesting Python. If I can\u2019t get the tooling I need to succeed this could be a real slog. \n\nMy question is \u2014 should I just take it in the chin and get the experience ? Has anyone done something similar (power through dumpster fire systems ) . There are zero online resources on this except from SharePoint devs and my prior work with power apps lets me know it\u2019s a clunky mess. \n\n", "author_fullname": "t2_ema0429e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take this Data Architect Role ? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw6nu9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708502266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: \nI have a project coming up and I\u2019m being considered to play the  Data Architect for migration work and design. &lt;/p&gt;\n\n&lt;p&gt;The only issue is the tech stack is terrible for DE work \n- Powerapps (Dataverse, power automate etc) \n- Dynamics ( not sure this is still a thing) \n- Legacy Third party systems upstream sources  (Oracle , Java). No api confirmed.  No direct access. May have to write .net integrations &lt;/p&gt;\n\n&lt;p&gt;A lot of this work seems to be glorified ETL. But not in a trivial sense. Tons of tables and likely a lot inconsistencies/clean up work. &lt;/p&gt;\n\n&lt;p&gt;I maybe will get an Azure SQL Database, Data Factory and Logic Apps. Also expected to write integrations in .net to integrate the power apps ecosystem. Management is bought into low code and wants to avoid these tools, custom code etc. &lt;/p&gt;\n\n&lt;p&gt;While I\u2019m excited by the title bump the effort seems doomed to fail just off the tech stack being a mess and the \u201clow code\u201d nonsense. The people who put this together are mostly concerned about the frontend and security. I didn\u2019t get to choose but I\u2019m thinking of pushing back and advocating for a proper azure architecture or at least fabric. Also suggesting Python. If I can\u2019t get the tooling I need to succeed this could be a real slog. &lt;/p&gt;\n\n&lt;p&gt;My question is \u2014 should I just take it in the chin and get the experience ? Has anyone done something similar (power through dumpster fire systems ) . There are zero online resources on this except from SharePoint devs and my prior work with power apps lets me know it\u2019s a clunky mess. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw6nu9", "is_robot_indexable": true, "report_reasons": null, "author": "IntentionThis441", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aw6nu9/should_i_take_this_data_architect_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw6nu9/should_i_take_this_data_architect_role/", "subreddit_subscribers": 162497, "created_utc": 1708502266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ez8d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review: Deciphering Data Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avne6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708450239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mullins.pro", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://mullins.pro/posts/deciphering_data_architectures/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avne6e", "is_robot_indexable": true, "report_reasons": null, "author": "Bazencourt", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avne6e/review_deciphering_data_architectures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://mullins.pro/posts/deciphering_data_architectures/", "subreddit_subscribers": 162497, "created_utc": 1708450239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Versioning, Cataloging, and Decommissioning Data Products", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1avh111", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gB_ZjJKC3197XhodvkkYWTDrjINSJ_rc0q8f7ibLLaQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708433867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?auto=webp&amp;s=b45c9b290ad8b6c7c2763d4ba3fc9f7d69a4d9d8", "width": 700, "height": 378}, "resolutions": [{"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e1fbb5dbacab22ce5c5bdb3a0ac1d16a6ee6be", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=226b1f43fa2918ebe8b0a3dd752f37c180ad5c67", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9a0ba30cb74687bc66030c20d31517a641216bf", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=133772bc0f8d87f8a49b52a21cd9f6da38b18adc", "width": 640, "height": 345}], "variants": {}, "id": "pJ8tPpgSlQhmlrsiqx50_VPVXrM4UCN658AcFCzyXbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avh111", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avh111/versioning_cataloging_and_decommissioning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "subreddit_subscribers": 162497, "created_utc": 1708433867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For data lake including rbacs for project folder, synapse, data factory, power bi. In past i've used yaml but this time they want in biceps. Does biceps will support or any limitation/complexity is there? What about synapse analytics iac deployment in bicep with roles, user creation etc and can we use sql script for schemas, objects creation? Advise synapse data artifacts deployment best approach/", "author_fullname": "t2_gw1qtave", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data analytics iac deployment through Azure bicep ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1awa35p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708515841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For data lake including rbacs for project folder, synapse, data factory, power bi. In past i&amp;#39;ve used yaml but this time they want in biceps. Does biceps will support or any limitation/complexity is there? What about synapse analytics iac deployment in bicep with roles, user creation etc and can we use sql script for schemas, objects creation? Advise synapse data artifacts deployment best approach/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awa35p", "is_robot_indexable": true, "report_reasons": null, "author": "efor007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awa35p/data_analytics_iac_deployment_through_azure_bicep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awa35p/data_analytics_iac_deployment_through_azure_bicep/", "subreddit_subscribers": 162497, "created_utc": 1708515841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to develop an in house data lineage application. I am able to get the data about relationships between tables but struggling with visualization of data. So far, I have tried using pyvis and d3 hierarchy trees. The problem I\u2019m facing in pyvis is flexibility to add on click node functions and problem with trees is that I cannot deal with multiple parent nodes(Since it is for hierarchical data). Has anybody worked on a similar project? If yes, then please help \n", "author_fullname": "t2_eb80kwgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lineage Solution: Snowflake, Django, D3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1awa1zz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708515711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to develop an in house data lineage application. I am able to get the data about relationships between tables but struggling with visualization of data. So far, I have tried using pyvis and d3 hierarchy trees. The problem I\u2019m facing in pyvis is flexibility to add on click node functions and problem with trees is that I cannot deal with multiple parent nodes(Since it is for hierarchical data). Has anybody worked on a similar project? If yes, then please help &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awa1zz", "is_robot_indexable": true, "report_reasons": null, "author": "No-Caregiver-1204", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awa1zz/data_lineage_solution_snowflake_django_d3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awa1zz/data_lineage_solution_snowflake_django_d3/", "subreddit_subscribers": 162497, "created_utc": 1708515711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building up tables and shema in my data warehouse (let's say Redshift). And this build-up process is in very early stage right now.\n\nand no matter how long I give a careful thought on how to set the schema right, at some point I have to change schema (add or subtract columns) .\n\nIt's like this:\n\n\\--------------------------------------------------------------------------------------\n\n|date              | base\\_trade\\_volume | quote\\_trade\\_volume |\n\n| 2023-01-02 | 200,000                       | 400,000,000               |\n\n| 2023-01-03 | 300,000                       | 450,000,000               |\n\n|    ....               |            ....                     |        ...                           |\n\n\\--------------------------------------------------------------------------------------\n\nThis is a table called 'user\\_trade\\_statistics'. and is the very first version of it.\n\n&amp;#x200B;\n\nand all of sudden, there's a need for 'trade\\_count' column. so I have to reprocess whole data to add new column to existing table.\n\n\\-------------------------------------------------------------------------------------------------------------\n\n|date              | base\\_trade\\_volume | quote\\_trade\\_volume |. trade\\_count\n\n| 2023-01-02 | 200,000                       | 400,000,000               |.  120\n\n| 2023-01-03 | 300,000                       | 450,000,000               |.  130\n\n|    ....               |            ....                     |        ...                           |.  .......\n\n\\-------------------------------------------------------------------------------------------------------------\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe problem here is I have to use lots of resources every time to reprocess for the changing schema. and this is going to happen quite frequently until the warehouse gets mature.\n\nI just want to know how you guys these kind of problems.\n\n&amp;#x200B;\n\np.s. This problem has nothing to do with 'slow changing dimension(SCD)'\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_h4q1lnfou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deal with frequently changing schema in early stage of building data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aw9rlc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708515750.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708514657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building up tables and shema in my data warehouse (let&amp;#39;s say Redshift). And this build-up process is in very early stage right now.&lt;/p&gt;\n\n&lt;p&gt;and no matter how long I give a careful thought on how to set the schema right, at some point I have to change schema (add or subtract columns) .&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;--------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;|date              | base_trade_volume | quote_trade_volume |&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-02 | 200,000                       | 400,000,000               |&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-03 | 300,000                       | 450,000,000               |&lt;/p&gt;\n\n&lt;p&gt;|    ....               |            ....                     |        ...                           |&lt;/p&gt;\n\n&lt;p&gt;--------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;This is a table called &amp;#39;user_trade_statistics&amp;#39;. and is the very first version of it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and all of sudden, there&amp;#39;s a need for &amp;#39;trade_count&amp;#39; column. so I have to reprocess whole data to add new column to existing table.&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;|date              | base_trade_volume | quote_trade_volume |. trade_count&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-02 | 200,000                       | 400,000,000               |.  120&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-03 | 300,000                       | 450,000,000               |.  130&lt;/p&gt;\n\n&lt;p&gt;|    ....               |            ....                     |        ...                           |.  .......&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The problem here is I have to use lots of resources every time to reprocess for the changing schema. and this is going to happen quite frequently until the warehouse gets mature.&lt;/p&gt;\n\n&lt;p&gt;I just want to know how you guys these kind of problems.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;p.s. This problem has nothing to do with &amp;#39;slow changing dimension(SCD)&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw9rlc", "is_robot_indexable": true, "report_reasons": null, "author": "DonkeyThin8833", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw9rlc/how_do_you_deal_with_frequently_changing_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw9rlc/how_do_you_deal_with_frequently_changing_schema/", "subreddit_subscribers": 162497, "created_utc": 1708514657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\nI am looking for practical walkthrough on how, step-by-step, one does normalization and implements a star schema data warehouse using Python or SQL. For such cases, people mostly throwing books and theory at you, Ive read those but looking for real implementation. In the past I was on fairly small project, where I would get everything in one table (which later stays as fact), create synthetic keys there, and then strip down distinct values along with keys to dim table with qualitative attributes as a view but I think thats not how every other, bigger project goes so I wonder. Is there any video/tutorial like that, or my approach seem correct for most of projects", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dimensional modelling implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aw9q6b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708515205.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708514508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello\nI am looking for practical walkthrough on how, step-by-step, one does normalization and implements a star schema data warehouse using Python or SQL. For such cases, people mostly throwing books and theory at you, Ive read those but looking for real implementation. In the past I was on fairly small project, where I would get everything in one table (which later stays as fact), create synthetic keys there, and then strip down distinct values along with keys to dim table with qualitative attributes as a view but I think thats not how every other, bigger project goes so I wonder. Is there any video/tutorial like that, or my approach seem correct for most of projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw9q6b", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw9q6b/dimensional_modelling_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw9q6b/dimensional_modelling_implementation/", "subreddit_subscribers": 162497, "created_utc": 1708514508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a bit stuck and need some input from my fellow reddit engineers. :D\n\nWe are redesigning our (central) data platform and I'm trying to identify database candidates that we could use to serve req/rep-style workloads related to personalization and \"data apps\". I looked at the obvious candidates - Firebolt, Druid, MongoDB, Postgres - but neither convinced me as the \"obviously right choice\". Are there others or am I missing something?\n\nThe characteristics that I need are:\n\n* **Throughput-optimized ingest**: We currently produce around 12TB/day of clickstream data and a couple GB/day of transaction and marketing data. I can filter this down in our stream layer (Kafka+Flink) but I still expect &gt;10M updates/inserts a day if not more.\n* **low-latency point queries**: Queries will ask for data on a specific userID in order to personalize user experience, trigger custom offers, etc. This is user-facing so it should have low latency to not affect UX negatively. Currently there is no latency SLA, but lower is certainly better.\n* **rollup/aggregation**: We expect queries to ask for aggregates, e.g., number of orders for this userID over the last 30 days, number of product/page views in the current session, etc.\n\nFor context: *Our current setup is segment based. We have a batch pipeline that spins at regular intervals, computes statistics for each segment using a warehouse (redshift) and then sends the updates to an OLTP database (postgres) for serving. We would like to change the grain from segment-level to user-level and are now checking what implications this would have for our platform/infra.*\n\n&amp;#x200B;", "author_fullname": "t2_frdb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for personalization and \"data apps\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw762n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708504284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit stuck and need some input from my fellow reddit engineers. :D&lt;/p&gt;\n\n&lt;p&gt;We are redesigning our (central) data platform and I&amp;#39;m trying to identify database candidates that we could use to serve req/rep-style workloads related to personalization and &amp;quot;data apps&amp;quot;. I looked at the obvious candidates - Firebolt, Druid, MongoDB, Postgres - but neither convinced me as the &amp;quot;obviously right choice&amp;quot;. Are there others or am I missing something?&lt;/p&gt;\n\n&lt;p&gt;The characteristics that I need are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Throughput-optimized ingest&lt;/strong&gt;: We currently produce around 12TB/day of clickstream data and a couple GB/day of transaction and marketing data. I can filter this down in our stream layer (Kafka+Flink) but I still expect &amp;gt;10M updates/inserts a day if not more.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;low-latency point queries&lt;/strong&gt;: Queries will ask for data on a specific userID in order to personalize user experience, trigger custom offers, etc. This is user-facing so it should have low latency to not affect UX negatively. Currently there is no latency SLA, but lower is certainly better.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;rollup/aggregation&lt;/strong&gt;: We expect queries to ask for aggregates, e.g., number of orders for this userID over the last 30 days, number of product/page views in the current session, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For context: &lt;em&gt;Our current setup is segment based. We have a batch pipeline that spins at regular intervals, computes statistics for each segment using a warehouse (redshift) and then sends the updates to an OLTP database (postgres) for serving. We would like to change the grain from segment-level to user-level and are now checking what implications this would have for our platform/infra.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aw762n", "is_robot_indexable": true, "report_reasons": null, "author": "FirefoxMetzger", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw762n/choosing_a_database_for_personalization_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw762n/choosing_a_database_for_personalization_and_data/", "subreddit_subscribers": 162497, "created_utc": 1708504284.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}