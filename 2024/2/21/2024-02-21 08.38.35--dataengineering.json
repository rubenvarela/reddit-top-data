{"kind": "Listing", "data": {"after": "t3_1avitns", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_insol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT4 doing data analysis by writing and running python scripts, plotting charts and all. Experimental but promising. What should I test this on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_1aviw3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/exzs2wto2rjc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1650, "scrubber_media_url": "https://v.redd.it/exzs2wto2rjc1/DASH_96.mp4", "dash_url": "https://v.redd.it/exzs2wto2rjc1/DASHPlaylist.mpd?a=1711096714%2COTM1MGZjNjgyZWJiYTFiMzUwOWNkMzZjMGFiNWViOGE5MzFiZTg3NmVkMWJmZTk0MzgwM2JkYTQ2NzQ4MzA0NA%3D%3D&amp;v=1&amp;f=sd", "duration": 46, "hls_url": "https://v.redd.it/exzs2wto2rjc1/HLSPlaylist.m3u8?a=1711096714%2CMDY1OWE3NDcxZmI4OTMxNjgzZTcwYTU3NDU4MjhlZTYyNzUxMDkyYjYxMzU2NzRlNDFkODUxYmQ4ZDcxM2YwZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=140&amp;height=91&amp;crop=140:91,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ebccf20579d226c38c2116b71802d46eee47679e", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708439202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/exzs2wto2rjc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?format=pjpg&amp;auto=webp&amp;s=1fb2e5265231f37df29d7a4e4020d135ba3b7f99", "width": 3300, "height": 2160}, "resolutions": [{"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e16907b378901eda1532e9656d7402e458044bc", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=21f7802eec1a15dbb5abb21a3ef1e0fcb4f3a91b", "width": 216, "height": 141}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=73e77003a6c9af09eebf0ad00e2c8aab93e1bae0", "width": 320, "height": 209}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2e0290b717b2408478c7dd5e55c18faeb178182e", "width": 640, "height": 418}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=41394c4db0bfef20243ac38f87a8f57ce02b26e0", "width": 960, "height": 628}, {"url": "https://external-preview.redd.it/MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c351cdb435bd43d3489021abca589eca15c5bb45", "width": 1080, "height": 706}], "variants": {}, "id": "MHoydG9qeXYycmpjMfKJo1qonaDCKzYpMmI9_LtVBQ9fPJZx3DsjULNQjr4G"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1aviw3v", "is_robot_indexable": true, "report_reasons": null, "author": "ashpreetbedi", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aviw3v/gpt4_doing_data_analysis_by_writing_and_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/exzs2wto2rjc1", "subreddit_subscribers": 162451, "created_utc": 1708439202.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/exzs2wto2rjc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1650, "scrubber_media_url": "https://v.redd.it/exzs2wto2rjc1/DASH_96.mp4", "dash_url": "https://v.redd.it/exzs2wto2rjc1/DASHPlaylist.mpd?a=1711096714%2COTM1MGZjNjgyZWJiYTFiMzUwOWNkMzZjMGFiNWViOGE5MzFiZTg3NmVkMWJmZTk0MzgwM2JkYTQ2NzQ4MzA0NA%3D%3D&amp;v=1&amp;f=sd", "duration": 46, "hls_url": "https://v.redd.it/exzs2wto2rjc1/HLSPlaylist.m3u8?a=1711096714%2CMDY1OWE3NDcxZmI4OTMxNjgzZTcwYTU3NDU4MjhlZTYyNzUxMDkyYjYxMzU2NzRlNDFkODUxYmQ4ZDcxM2YwZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it's hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What was your biggest challenge getting into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avj2k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708439683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it&amp;#39;s hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avj2k2", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "subreddit_subscribers": 162451, "created_utc": 1708439683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for a new role and every job on LinkedIn seems to have over 100 applicants. Is the field really this competitive? Seems incredibly difficult to find a job despite all of the articles I read touting the lack of supply of data engineers. Thoughts?", "author_fullname": "t2_d07hrm40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there too many data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw2se4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708489012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a new role and every job on LinkedIn seems to have over 100 applicants. Is the field really this competitive? Seems incredibly difficult to find a job despite all of the articles I read touting the lack of supply of data engineers. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw2se4", "is_robot_indexable": true, "report_reasons": null, "author": "bostinloyd", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw2se4/are_there_too_many_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw2se4/are_there_too_many_data_engineers/", "subreddit_subscribers": 162451, "created_utc": 1708489012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you or are you implementing a lakehouse?\n\nIf so what have been some of the biggest hurdles or things you\u2019ve learned?\n\nWhat were big decisions you were happy with or would do differently if you could start over.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most important parts of implementing a lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avhmkt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708435683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you or are you implementing a lakehouse?&lt;/p&gt;\n\n&lt;p&gt;If so what have been some of the biggest hurdles or things you\u2019ve learned?&lt;/p&gt;\n\n&lt;p&gt;What were big decisions you were happy with or would do differently if you could start over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avhmkt", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "subreddit_subscribers": 162451, "created_utc": 1708435683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, i just started my master's project and currently working with .parquet files that are \\~20GB each but i do not have much of an idea on how to work with those. I need to merge multiple of these files (each carrying different types of information) with a key that is not unique (so in each file the key that i am using appears in multiple different rows). When i try to do that with python-pandas, things quickly get out of hand with the memory and the system crashes. What would be the best way to achieve  the equivalent of 'inner' or 'left' merge of these .parquet files? Any help will be greatly appreciated :)", "author_fullname": "t2_a2ypvree", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging large .parquet files without a unique key efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avo2kw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708460079.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708451836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i just started my master&amp;#39;s project and currently working with .parquet files that are ~20GB each but i do not have much of an idea on how to work with those. I need to merge multiple of these files (each carrying different types of information) with a key that is not unique (so in each file the key that i am using appears in multiple different rows). When i try to do that with python-pandas, things quickly get out of hand with the memory and the system crashes. What would be the best way to achieve  the equivalent of &amp;#39;inner&amp;#39; or &amp;#39;left&amp;#39; merge of these .parquet files? Any help will be greatly appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avo2kw", "is_robot_indexable": true, "report_reasons": null, "author": "Martian_Onion", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avo2kw/merging_large_parquet_files_without_a_unique_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avo2kw/merging_large_parquet_files_without_a_unique_key/", "subreddit_subscribers": 162451, "created_utc": 1708451836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently working as a DE for a big insurance company in EU. My manager sees me (medior) fit for a more senior role within the company to coach and bring juniors/mediors to their next level (technical and soft skills). This is new for me, so I would appreciate some tips from DE leads and seniors.\n\n1. What are your tips and best practices to coach and bring juniors/mediors to their next level (technical/soft skills)?\n\n2. And, what should I be carefull with?\n\nThis would help me out a lot to get started with my next steps becoming a senior ;)\n", "author_fullname": "t2_344js9zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear seniors, how do you coach juniors/mediors to their next level?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avu2vf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708466035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as a DE for a big insurance company in EU. My manager sees me (medior) fit for a more senior role within the company to coach and bring juniors/mediors to their next level (technical and soft skills). This is new for me, so I would appreciate some tips from DE leads and seniors.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What are your tips and best practices to coach and bring juniors/mediors to their next level (technical/soft skills)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;And, what should I be carefull with?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This would help me out a lot to get started with my next steps becoming a senior ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avu2vf", "is_robot_indexable": true, "report_reasons": null, "author": "-SCYLLA-", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avu2vf/dear_seniors_how_do_you_coach_juniorsmediors_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avu2vf/dear_seniors_how_do_you_coach_juniorsmediors_to/", "subreddit_subscribers": 162451, "created_utc": 1708466035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered \"only a few of those applicant are good\". I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? ", "author_fullname": "t2_13mcsa6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What divides good from average?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw4uaq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708495611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered &amp;quot;only a few of those applicant are good&amp;quot;. I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw4uaq", "is_robot_indexable": true, "report_reasons": null, "author": "Chediras", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "subreddit_subscribers": 162451, "created_utc": 1708495611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? ", "author_fullname": "t2_17a3gi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you understand by \u201eSQL Basics\u201d and \u201ePython Basics\u201d in CV, what exact skills would you expect from that person?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avi583", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708437174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avi583", "is_robot_indexable": true, "report_reasons": null, "author": "ItsGonnaBeGreatYear", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "subreddit_subscribers": 162451, "created_utc": 1708437174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n['](https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01)\n\n**Multiwoven** is built for **data teams** \\- *engineers*, *analysts*, *ops, etc*. - to easily prepare and sync data to business tools.\n\nWe\u2019re **open-source** (so you don\u2019t have to pay to move your own data to your own tools), **self-hosted** (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of **Multiwoven**, for your own needs)\n\n[**https://github.com/Multiwoven/multiwoven**](https://github.com/Multiwoven/multiwoven)\n\nStar it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) ", "author_fullname": "t2_rtrd3q97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched Multiwoven - an open-source Reverse ETL platform!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ec2bl2oberjc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe86d0b5ca53c1dd9e0ad579158f73eb36bdc3c"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62c60bf3aa7c430b5662e778781118fcdd5844f6"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=efc980e0a347d5d8e365eb1f775fef20d188f4cf"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3758c4feeef9986b3896ee00413668180b7d165b"}, {"y": 480, "x": 960, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3322c7e225a2a3d82e63ba5637c27f349d05bff9"}, {"y": 540, "x": 1080, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6701a1f01a4abff464adae7daa870e6d0422fbcb"}], "s": {"y": 1280, "x": 2560, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01"}, "id": "ec2bl2oberjc1"}}, "name": "t3_1avkgw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ic6Skc0tts236_84heUlo76cJufFHxdjWGqEdY50b1M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708443255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31fbcafee400f17cba29297c7360a598feacef01\"&gt;&amp;#39;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multiwoven&lt;/strong&gt; is built for &lt;strong&gt;data teams&lt;/strong&gt; - &lt;em&gt;engineers&lt;/em&gt;, &lt;em&gt;analysts&lt;/em&gt;, &lt;em&gt;ops, etc&lt;/em&gt;. - to easily prepare and sync data to business tools.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re &lt;strong&gt;open-source&lt;/strong&gt; (so you don\u2019t have to pay to move your own data to your own tools), &lt;strong&gt;self-hosted&lt;/strong&gt; (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of &lt;strong&gt;Multiwoven&lt;/strong&gt;, for your own needs)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;&lt;strong&gt;https://github.com/Multiwoven/multiwoven&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Star it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1avkgw5", "is_robot_indexable": true, "report_reasons": null, "author": "nagstler", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "subreddit_subscribers": 162451, "created_utc": 1708443255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a small website to help people brand new to Spark get started. The goal is to teach all of the basics in an afternoon, along with some opinionated style suggestions on how to write Spark code. However, before someone can get started though, they need somewhere to run their Spark code. \n\nI'm wondering what people found most helpful as they were getting started. Personally, I tend to prefer working locally with Docker, but I can see that being a small hurdle if folks aren't already familiar with running Docker (even though docker-compose simplifies this a lot IMO).\n\nWhat helped you with writing and testing out Spark when you were starting out?", "author_fullname": "t2_uyd9mc1b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you find it easier to learn Spark using a cloud notebook environment or locally with Docker?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avoont", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708453239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a small website to help people brand new to Spark get started. The goal is to teach all of the basics in an afternoon, along with some opinionated style suggestions on how to write Spark code. However, before someone can get started though, they need somewhere to run their Spark code. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what people found most helpful as they were getting started. Personally, I tend to prefer working locally with Docker, but I can see that being a small hurdle if folks aren&amp;#39;t already familiar with running Docker (even though docker-compose simplifies this a lot IMO).&lt;/p&gt;\n\n&lt;p&gt;What helped you with writing and testing out Spark when you were starting out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avoont", "is_robot_indexable": true, "report_reasons": null, "author": "zchtsk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avoont/do_you_find_it_easier_to_learn_spark_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avoont/do_you_find_it_easier_to_learn_spark_using_a/", "subreddit_subscribers": 162451, "created_utc": 1708453239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nSo, I am currently working on a problem where I have to find duplicates in my data. The data has columns C1, C2, C3 and T. T stores timestamps. The received data is not sorted by timestamp. A duplicate is defined as when columns C1, C2 and C3 have same value but column T should have time difference of less than 120 seconds. The duplicates should be returned in groups.\n\nE.g.\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xyx\", C2:\"aba\", C3: \"ded\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:01.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:01:05.000\"`\n\n`C1:\"aaa\", C2:\"bbb\", C3: \"ccc\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:02:10.000\"`\n\nThe output for this would be:\n\n`[[C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:01:05.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:02:10.000\"]`\n\n`[C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:01.000\"]]`\n\nMy current solution for small data does not use Pandas or Spark and works by sorting the data over T column, then grouping them on (C1, C2, C3) and removing any duplicates which do not uphold the time difference criteria. And finally,, sorting the discovered duplicate groups over T again.\n\nHow would I approach this problem when I have say 1 billion rows? Should I use Spark or process with pandas in batches? I am not able to get an intuition of how sorting and grouping would work when the data is distributed or processed in batches?\n\n", "author_fullname": "t2_bmxda7ioc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach this problem for very large datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1averfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708430799.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708425818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;So, I am currently working on a problem where I have to find duplicates in my data. The data has columns C1, C2, C3 and T. T stores timestamps. The received data is not sorted by timestamp. A duplicate is defined as when columns C1, C2 and C3 have same value but column T should have time difference of less than 120 seconds. The duplicates should be returned in groups.&lt;/p&gt;\n\n&lt;p&gt;E.g.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyx&amp;quot;, C2:&amp;quot;aba&amp;quot;, C3: &amp;quot;ded&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:01.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:01:05.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;aaa&amp;quot;, C2:&amp;quot;bbb&amp;quot;, C3: &amp;quot;ccc&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:02:10.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The output for this would be:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[[C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:01:05.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:02:10.000&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:01.000&amp;quot;]]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My current solution for small data does not use Pandas or Spark and works by sorting the data over T column, then grouping them on (C1, C2, C3) and removing any duplicates which do not uphold the time difference criteria. And finally,, sorting the discovered duplicate groups over T again.&lt;/p&gt;\n\n&lt;p&gt;How would I approach this problem when I have say 1 billion rows? Should I use Spark or process with pandas in batches? I am not able to get an intuition of how sorting and grouping would work when the data is distributed or processed in batches?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1averfs", "is_robot_indexable": true, "report_reasons": null, "author": "Even_Work_7995", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1averfs/how_would_you_approach_this_problem_for_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1averfs/how_would_you_approach_this_problem_for_very/", "subreddit_subscribers": 162451, "created_utc": 1708425818.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm about to embark on the most boring project of my life: updating product documentation \ud83e\udd23  \n\n\nAnd I need some inspiration before I get started.  \n\n\nCurious - of all the data products that you've used, which one has the best documentation?\n\nAspects of great documentation might include:  \n\\- Navigable  \n\\- Clear, concise (you're question gets answered quickly. Learning something new on the platform is straightforward)   \n\\- Companion tutorials  \n\n\n... Your input could save my sanity!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boring topic - Documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avrbd5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708459541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to embark on the most boring project of my life: updating product documentation \ud83e\udd23  &lt;/p&gt;\n\n&lt;p&gt;And I need some inspiration before I get started.  &lt;/p&gt;\n\n&lt;p&gt;Curious - of all the data products that you&amp;#39;ve used, which one has the best documentation?&lt;/p&gt;\n\n&lt;p&gt;Aspects of great documentation might include:&lt;br/&gt;\n- Navigable&lt;br/&gt;\n- Clear, concise (you&amp;#39;re question gets answered quickly. Learning something new on the platform is straightforward)&lt;br/&gt;\n- Companion tutorials  &lt;/p&gt;\n\n&lt;p&gt;... Your input could save my sanity!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avrbd5", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avrbd5/boring_topic_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avrbd5/boring_topic_documentation/", "subreddit_subscribers": 162451, "created_utc": 1708459541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nJust started my first data engineering position, but it\u2019s not at a company dealing with order/customer/deal data - the position has to do more with political and corporate data, which is very interesting, but it\u2019s much harder to figure out how to apply some of these data engineering concepts (like reading Kimball) without the same data types. For example, I\u2019m trying to set up a data warehouse from scratch for our data that\u2019s currently in PostgreSQL, but having a hard time figuring out where to start in terms of what to stage and how to go about dimensionally modeling this data. Does anybody have any tips for data engineering for atypical data? ", "author_fullname": "t2_2ivpdou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering for Non-Business Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avka8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708442797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Just started my first data engineering position, but it\u2019s not at a company dealing with order/customer/deal data - the position has to do more with political and corporate data, which is very interesting, but it\u2019s much harder to figure out how to apply some of these data engineering concepts (like reading Kimball) without the same data types. For example, I\u2019m trying to set up a data warehouse from scratch for our data that\u2019s currently in PostgreSQL, but having a hard time figuring out where to start in terms of what to stage and how to go about dimensionally modeling this data. Does anybody have any tips for data engineering for atypical data? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avka8p", "is_robot_indexable": true, "report_reasons": null, "author": "Butterhero_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avka8p/data_engineering_for_nonbusiness_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avka8p/data_engineering_for_nonbusiness_data/", "subreddit_subscribers": 162451, "created_utc": 1708442797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had started a blog series to guide people on the path I felt they should follow, to transition into Azure Data Engineer role as a way back of doing good to the community and help people in making their careers and providing for their family. The blog where I mentioned this received tremendous response which really motivated me to go for it.\n\nHowever that is where things changed. The first blog titled, structured way to get into DE, though received very good response on upvotes and shares, was filled with negative comments. Some questioned why only Azure, some questioned why I had given timelines, some questioned why am I focusing on only X number of things, etc etc. Very very few people had something positive to say. Still I ignored it as helping community was more important but the next two blogs, SQL and ADF had a drastic decrease in engagement. Almost 80% engagement was gone.\n\nThis brings me to the question, are the blogs still needed by the community, are they helping you in any way or it's something unnecessary and should be stopped?\n\nNote: If this ends up on the positive side, I will be needing at least 50 upvotes or shares (as karma farming comments will come so I am fine with just shares too) for the already published blogs (SQL and ADF) and the ones to follow from now. This way it can be ensured that people are engaging with the content and I am not spending time creating something nobody is reading :)\n\n[View Poll](https://www.reddit.com/poll/1avnq5y)", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The End?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avnq5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708455800.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708451004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had started a blog series to guide people on the path I felt they should follow, to transition into Azure Data Engineer role as a way back of doing good to the community and help people in making their careers and providing for their family. The blog where I mentioned this received tremendous response which really motivated me to go for it.&lt;/p&gt;\n\n&lt;p&gt;However that is where things changed. The first blog titled, structured way to get into DE, though received very good response on upvotes and shares, was filled with negative comments. Some questioned why only Azure, some questioned why I had given timelines, some questioned why am I focusing on only X number of things, etc etc. Very very few people had something positive to say. Still I ignored it as helping community was more important but the next two blogs, SQL and ADF had a drastic decrease in engagement. Almost 80% engagement was gone.&lt;/p&gt;\n\n&lt;p&gt;This brings me to the question, are the blogs still needed by the community, are they helping you in any way or it&amp;#39;s something unnecessary and should be stopped?&lt;/p&gt;\n\n&lt;p&gt;Note: If this ends up on the positive side, I will be needing at least 50 upvotes or shares (as karma farming comments will come so I am fine with just shares too) for the already published blogs (SQL and ADF) and the ones to follow from now. This way it can be ensured that people are engaging with the content and I am not spending time creating something nobody is reading :)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1avnq5y\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avnq5y", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1708537404984, "options": [{"text": "Continue with the blogs.", "id": "27157742"}, {"text": "Stop the blogs.", "id": "27157743"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 35, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avnq5y/the_end/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1avnq5y/the_end/", "subreddit_subscribers": 162451, "created_utc": 1708451004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interesting writeup from Grab on the evolution of their architecture and what drove it, also covers the tool stack\n\n[https://engineering.grab.com/attribution-platform](https://engineering.grab.com/attribution-platform)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ad-attribution toolstack and architecture evolution Kappa -&gt; Lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ave5me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708423386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interesting writeup from Grab on the evolution of their architecture and what drove it, also covers the tool stack&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://engineering.grab.com/attribution-platform\"&gt;https://engineering.grab.com/attribution-platform&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?auto=webp&amp;s=d7f3d72292945fabad7e425d6dac8116bc8d6e8e", "width": 820, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb2280b5de657ab5aa7ccf3f8db3ffe8fa589c64", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9eb75e1d7b5d9afd7cc95933569c8da0e34d9cbb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f45878784b7396c1a4b9f8a18f3451d77b8aaabc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b9b9331c902ce00b15be8ab905704d531385cf7", "width": 640, "height": 320}], "variants": {}, "id": "NcKcYtUrZALKPPo2SThdpWDUuQIC8J5ZFrY4ANzdEiA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ave5me", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ave5me/adattribution_toolstack_and_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ave5me/adattribution_toolstack_and_architecture/", "subreddit_subscribers": 162451, "created_utc": 1708423386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "in the last days I saw a sessionization tool for ad tech events that was a domain specific language (not SQL) without any weird joins.\n\n&amp;#x200B;\n\nsadly now when I need it I cannot find it anymore. Does anyone know what it is and cand share a link? It was a specific database with its own query engine to simplify sessionization event queries ", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "simplified sessionization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avrl12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708460186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;in the last days I saw a sessionization tool for ad tech events that was a domain specific language (not SQL) without any weird joins.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;sadly now when I need it I cannot find it anymore. Does anyone know what it is and cand share a link? It was a specific database with its own query engine to simplify sessionization event queries &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1avrl12", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/1avrl12/simplified_sessionization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avrl12/simplified_sessionization/", "subreddit_subscribers": 162451, "created_utc": 1708460186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an experienced software architect, but relatively new to data engineering. We have an ML product, that receives IOT data from industrial processes.\n\nAt the moment, we use a near-realtime setup where data stages trigger one another through SQS queues, and we deal with small batches of data with AWS lambda.\n\n* Raw -&gt; staged: map data to a canonical format from incoming mqtt.\n* Staged -&gt; mapped: extract the features we care about.\n* Mapped -&gt; regularised: create dense data from sparse mapped data through interpolation.\n\nEach stage reads and writes from S3.\n\nThen using the regularised data we run some models:\n\n* Data status: identify potential sensor failures\n* Plant status: Give an overall health measure of the industrial process\n* Inference: Predict control values for the process given a known-healthy dataset and plant.\n\nAll this is _fine_ and we have the ability to run stages with Ray so that we can do historical batch processing, but I'm concerned about the complexity, and the delays that we incur at each stage. We're within our current SLOs, but we'd like to be able to run inference more frequently.\n\nWhat's the current state of the art for managing this kind of workflow? My biases are toward serverless options, but my hunch is that we're going to end up with some kind of stateful worker pattern.", "author_fullname": "t2_ay3xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting data pipelines for ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avgzow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708434554.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708433746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an experienced software architect, but relatively new to data engineering. We have an ML product, that receives IOT data from industrial processes.&lt;/p&gt;\n\n&lt;p&gt;At the moment, we use a near-realtime setup where data stages trigger one another through SQS queues, and we deal with small batches of data with AWS lambda.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Raw -&amp;gt; staged: map data to a canonical format from incoming mqtt.&lt;/li&gt;\n&lt;li&gt;Staged -&amp;gt; mapped: extract the features we care about.&lt;/li&gt;\n&lt;li&gt;Mapped -&amp;gt; regularised: create dense data from sparse mapped data through interpolation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Each stage reads and writes from S3.&lt;/p&gt;\n\n&lt;p&gt;Then using the regularised data we run some models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data status: identify potential sensor failures&lt;/li&gt;\n&lt;li&gt;Plant status: Give an overall health measure of the industrial process&lt;/li&gt;\n&lt;li&gt;Inference: Predict control values for the process given a known-healthy dataset and plant.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All this is &lt;em&gt;fine&lt;/em&gt; and we have the ability to run stages with Ray so that we can do historical batch processing, but I&amp;#39;m concerned about the complexity, and the delays that we incur at each stage. We&amp;#39;re within our current SLOs, but we&amp;#39;d like to be able to run inference more frequently.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the current state of the art for managing this kind of workflow? My biases are toward serverless options, but my hunch is that we&amp;#39;re going to end up with some kind of stateful worker pattern.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avgzow", "is_robot_indexable": true, "report_reasons": null, "author": "bobaduk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avgzow/architecting_data_pipelines_for_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avgzow/architecting_data_pipelines_for_ml/", "subreddit_subscribers": 162451, "created_utc": 1708433746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks!   \n[https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)  \n\n\n I'm Subin, co-founder at Multiwoven .Multiwoven is a OSS reverse ETL platform that helps dev &amp; data teams to sync data from databases to business tools. Multiwoven is built using Ruby on Rails . Our data sync orchestration is built on top of Temporal using temporal-ruby SDK.I would greatly appreciate any feedback. Our codebase is available at Github. Please star us to get updates.", "author_fullname": "t2_9bibppzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiwoven - Open-source reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avew2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708426332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks!&lt;br/&gt;\n&lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;https://github.com/Multiwoven/multiwoven&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Subin, co-founder at Multiwoven .Multiwoven is a OSS reverse ETL platform that helps dev &amp;amp; data teams to sync data from databases to business tools. Multiwoven is built using Ruby on Rails . Our data sync orchestration is built on top of Temporal using temporal-ruby SDK.I would greatly appreciate any feedback. Our codebase is available at Github. Please star us to get updates.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?auto=webp&amp;s=7bee046b00e43a80d51ec06b5b03fab8fe50e8a6", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c820e531bd69e73f8ab0a6ae0beacd99c2b46eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c80be54ff0c4e74ab2f7726675fcebd3a845b46", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ac265f606906d37f473536da1c0f1402f557f04", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f31b53a48c8d26b2abdbd514797bd69ea9539bcf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d338fcd1ab24aecad9b1dcf39d5649f7cff2a29", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3be250d32cd9b8a61ac25ca44aa1a3ab73cb70a", "width": 1080, "height": 540}], "variants": {}, "id": "KXdCRFA3PFw6uZLyGfHzBPQBTSPbN50XvrsQE2m5iOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1avew2o", "is_robot_indexable": true, "report_reasons": null, "author": "BobcatOutrageous2152", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avew2o/multiwoven_opensource_reverse_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avew2o/multiwoven_opensource_reverse_etl/", "subreddit_subscribers": 162451, "created_utc": 1708426332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ez8d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review: Deciphering Data Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avne6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708450239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mullins.pro", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://mullins.pro/posts/deciphering_data_architectures/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avne6e", "is_robot_indexable": true, "report_reasons": null, "author": "Bazencourt", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avne6e/review_deciphering_data_architectures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://mullins.pro/posts/deciphering_data_architectures/", "subreddit_subscribers": 162451, "created_utc": 1708450239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Versioning, Cataloging, and Decommissioning Data Products", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1avh111", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gB_ZjJKC3197XhodvkkYWTDrjINSJ_rc0q8f7ibLLaQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708433867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?auto=webp&amp;s=b45c9b290ad8b6c7c2763d4ba3fc9f7d69a4d9d8", "width": 700, "height": 378}, "resolutions": [{"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e1fbb5dbacab22ce5c5bdb3a0ac1d16a6ee6be", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=226b1f43fa2918ebe8b0a3dd752f37c180ad5c67", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9a0ba30cb74687bc66030c20d31517a641216bf", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=133772bc0f8d87f8a49b52a21cd9f6da38b18adc", "width": 640, "height": 345}], "variants": {}, "id": "pJ8tPpgSlQhmlrsiqx50_VPVXrM4UCN658AcFCzyXbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avh111", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avh111/versioning_cataloging_and_decommissioning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "subreddit_subscribers": 162451, "created_utc": 1708433867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: \nI have a project coming up and I\u2019m being considered to play the  Data Architect for migration work and design. \n\nThe only issue is the tech stack is terrible for DE work \n- Powerapps (Dataverse, power automate etc) \n- Dynamics ( not sure this is still a thing) \n- Legacy Third party systems upstream sources  (Oracle , Java). No api confirmed.  No direct access. May have to write .net integrations \n\nA lot of this work seems to be glorified ETL. But not in a trivial sense. Tons of tables and likely a lot inconsistencies/clean up work. \n\nI maybe will get an Azure SQL Database, Data Factory and Logic Apps. Also expected to write integrations in .net to integrate the power apps ecosystem. Management is bought into low code and wants to avoid these tools, custom code etc. \n\nWhile I\u2019m excited by the title bump the effort seems doomed to fail just off the tech stack being a mess and the \u201clow code\u201d nonsense. The people who put this together are mostly concerned about the frontend and security. I didn\u2019t get to choose but I\u2019m thinking of pushing back and advocating for a proper azure architecture or at least fabric. Also suggesting Python. If I can\u2019t get the tooling I need to succeed this could be a real slog. \n\nMy question is \u2014 should I just take it in the chin and get the experience ? Has anyone done something similar (power through dumpster fire systems ) . There are zero online resources on this except from SharePoint devs and my prior work with power apps lets me know it\u2019s a clunky mess. \n\n", "author_fullname": "t2_ema0429e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take this Data Architect Role ? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aw6nu9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708502266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: \nI have a project coming up and I\u2019m being considered to play the  Data Architect for migration work and design. &lt;/p&gt;\n\n&lt;p&gt;The only issue is the tech stack is terrible for DE work \n- Powerapps (Dataverse, power automate etc) \n- Dynamics ( not sure this is still a thing) \n- Legacy Third party systems upstream sources  (Oracle , Java). No api confirmed.  No direct access. May have to write .net integrations &lt;/p&gt;\n\n&lt;p&gt;A lot of this work seems to be glorified ETL. But not in a trivial sense. Tons of tables and likely a lot inconsistencies/clean up work. &lt;/p&gt;\n\n&lt;p&gt;I maybe will get an Azure SQL Database, Data Factory and Logic Apps. Also expected to write integrations in .net to integrate the power apps ecosystem. Management is bought into low code and wants to avoid these tools, custom code etc. &lt;/p&gt;\n\n&lt;p&gt;While I\u2019m excited by the title bump the effort seems doomed to fail just off the tech stack being a mess and the \u201clow code\u201d nonsense. The people who put this together are mostly concerned about the frontend and security. I didn\u2019t get to choose but I\u2019m thinking of pushing back and advocating for a proper azure architecture or at least fabric. Also suggesting Python. If I can\u2019t get the tooling I need to succeed this could be a real slog. &lt;/p&gt;\n\n&lt;p&gt;My question is \u2014 should I just take it in the chin and get the experience ? Has anyone done something similar (power through dumpster fire systems ) . There are zero online resources on this except from SharePoint devs and my prior work with power apps lets me know it\u2019s a clunky mess. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw6nu9", "is_robot_indexable": true, "report_reasons": null, "author": "IntentionThis441", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1aw6nu9/should_i_take_this_data_architect_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw6nu9/should_i_take_this_data_architect_role/", "subreddit_subscribers": 162451, "created_utc": 1708502266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone,\n\nI'd like to try and get a more formal/proper data engineering skillset put together for future employment prospects and I am not really sure where to start. I've been doing data analysis/management for almost a decade now in some capacity or another but it has always been attached to the business side of things, not the IT side.\n\nMy company is in the middle of some merger/acquisition stuff so training, POs for new tools etc aren't happening anytime soon, though we do have a GCP instance which I am playing around with.\n\nHere is where I am from a skillset perspective:\n\nI manage our department's Azure Cloud db (our formal IT data warehouse has a 6 month wait time on new ETL)\n\nI run a from-scratch Python script that eats files from FTP, oneDrive &amp; S3 buckets, does some basic transformations, loads to staging tables in the Azure DB, and triggers a MERGE proc for incremental loads of data.\n\nThe Python scripts also take address info from raw sources and cleanse them using the USPS API to try and get some form of standardization.\n\nThe Python scripts are all housed on an Azure VM and run a few times a day as a Windows scheduler task\n\nI've started the process of migrating this to a GCP big query dataset since I can take advantage of some premade data from the wider company instead of reinventing the wheel. \n\nAll of this data is managed, partitioned, clustered, indexed by me, along with schema maps and documentation (which no one views, but I like to have it)\n\nAt the end of all of this, my actual JOB is to take this data and build Tableau dashboards/reports and Power BI Reports for end users per their request of the day (they will look at the dashboard once to survive the next meeting and never again)\n\nAll in all, the gig is totally self-taught and feels VERY taped together. I've been reading this sub for direction but I get a little overwhelmed with all the nomenclature/products etc. I'd like to find a good path to learn on that I can find value in at work, while also formalizing the skillset I have now.\n\nAny help or guidance is appreciated!\n\n&amp;#x200B;", "author_fullname": "t2_crbnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to formalize my skillset?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avkvrm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708444297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to try and get a more formal/proper data engineering skillset put together for future employment prospects and I am not really sure where to start. I&amp;#39;ve been doing data analysis/management for almost a decade now in some capacity or another but it has always been attached to the business side of things, not the IT side.&lt;/p&gt;\n\n&lt;p&gt;My company is in the middle of some merger/acquisition stuff so training, POs for new tools etc aren&amp;#39;t happening anytime soon, though we do have a GCP instance which I am playing around with.&lt;/p&gt;\n\n&lt;p&gt;Here is where I am from a skillset perspective:&lt;/p&gt;\n\n&lt;p&gt;I manage our department&amp;#39;s Azure Cloud db (our formal IT data warehouse has a 6 month wait time on new ETL)&lt;/p&gt;\n\n&lt;p&gt;I run a from-scratch Python script that eats files from FTP, oneDrive &amp;amp; S3 buckets, does some basic transformations, loads to staging tables in the Azure DB, and triggers a MERGE proc for incremental loads of data.&lt;/p&gt;\n\n&lt;p&gt;The Python scripts also take address info from raw sources and cleanse them using the USPS API to try and get some form of standardization.&lt;/p&gt;\n\n&lt;p&gt;The Python scripts are all housed on an Azure VM and run a few times a day as a Windows scheduler task&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve started the process of migrating this to a GCP big query dataset since I can take advantage of some premade data from the wider company instead of reinventing the wheel. &lt;/p&gt;\n\n&lt;p&gt;All of this data is managed, partitioned, clustered, indexed by me, along with schema maps and documentation (which no one views, but I like to have it)&lt;/p&gt;\n\n&lt;p&gt;At the end of all of this, my actual JOB is to take this data and build Tableau dashboards/reports and Power BI Reports for end users per their request of the day (they will look at the dashboard once to survive the next meeting and never again)&lt;/p&gt;\n\n&lt;p&gt;All in all, the gig is totally self-taught and feels VERY taped together. I&amp;#39;ve been reading this sub for direction but I get a little overwhelmed with all the nomenclature/products etc. I&amp;#39;d like to find a good path to learn on that I can find value in at work, while also formalizing the skillset I have now.&lt;/p&gt;\n\n&lt;p&gt;Any help or guidance is appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avkvrm", "is_robot_indexable": true, "report_reasons": null, "author": "puttyarrowbro", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avkvrm/how_to_formalize_my_skillset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkvrm/how_to_formalize_my_skillset/", "subreddit_subscribers": 162451, "created_utc": 1708444297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, right now I have some DA role where I utilize pandas for data wrangling/analysis, however I would like to do a transition into more cloud/DE role in the future. \nTherefore, I was looking for something that I could learn on the job, which could help me with the transition later on. I found that there are alternatives to pandas, e.g. polars or some more sql-like packages. Which would be the best choice actually to work in if I would like to change job in the future?", "author_fullname": "t2_2oo6rj79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best packages for data manipulation/engineering in python - transition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avkppv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708443872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, right now I have some DA role where I utilize pandas for data wrangling/analysis, however I would like to do a transition into more cloud/DE role in the future. \nTherefore, I was looking for something that I could learn on the job, which could help me with the transition later on. I found that there are alternatives to pandas, e.g. polars or some more sql-like packages. Which would be the best choice actually to work in if I would like to change job in the future?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avkppv", "is_robot_indexable": true, "report_reasons": null, "author": "Omnetfh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avkppv/best_packages_for_data_manipulationengineering_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkppv/best_packages_for_data_manipulationengineering_in/", "subreddit_subscribers": 162451, "created_utc": 1708443872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering that most of the code in my workflows is asynchronous, I am looking for an orchestration tool that can effectively handle this. I\u2019ve been considering Airflow, but to run an async DAG in Airflow requires more configuration. Could someone provide a comparison of these tools in terms of their support for asynchronous execution and their suitability for building DAGs? ", "author_fullname": "t2_4ci4znsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avkkuw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708443537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering that most of the code in my workflows is asynchronous, I am looking for an orchestration tool that can effectively handle this. I\u2019ve been considering Airflow, but to run an async DAG in Airflow requires more configuration. Could someone provide a comparison of these tools in terms of their support for asynchronous execution and their suitability for building DAGs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avkkuw", "is_robot_indexable": true, "report_reasons": null, "author": "PhotojournalistOk882", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1avkkuw/orchestration_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkkuw/orchestration_tool/", "subreddit_subscribers": 162451, "created_utc": 1708443537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI need some help from this amazing community as I am completely lost\u2026\n\nI built a data warehouse to feed a huge dashboard for a client. They have 2 data sources: MySQL with everything related to transactions, orders, customers, etc. so everything \\*revenue\\* related, and an API for everything related to advertising spend. They don\u2019t have a huge amount of data (about 30G). I did a simple star schema with transactions as the grain, dashboard works, everybody happy.\n\nThe issue is that they are now introducing a new tool to manage all transactions and payments and will be deprecating the MySQL database. They started using this new tool last month for about 10% of their transactions and the rollout will be complete in about 5 months. There will be no data transfer between the 2 tools.\n\nExtraction for this new source is easy but\u2026 The structure of the transactions, orders and customers are obviously not the same at all. I have no idea how to rebuild the entire model\u2026 I\u2019m leaning towards building 2 separate star schemas, one with MySQL + Advertising spend and the other with New Tool + Advertising spend, then try my best to build a central table that links both fact tables, but I\u2019m not sure it\u2019s the right way to do this.\n\nI\u2019ve looked everywhere for ressources about companies onboarding new tools, having different sources for the same grain etc. Maybe I didn\u2019t look properly or maybe that\u2019s uncommon. Either way, I\u2019m lost. Any ideas?", "author_fullname": "t2_ulewnzmld", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to build a model with 2 data sources with the same grain?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avitns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708439017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I need some help from this amazing community as I am completely lost\u2026&lt;/p&gt;\n\n&lt;p&gt;I built a data warehouse to feed a huge dashboard for a client. They have 2 data sources: MySQL with everything related to transactions, orders, customers, etc. so everything *revenue* related, and an API for everything related to advertising spend. They don\u2019t have a huge amount of data (about 30G). I did a simple star schema with transactions as the grain, dashboard works, everybody happy.&lt;/p&gt;\n\n&lt;p&gt;The issue is that they are now introducing a new tool to manage all transactions and payments and will be deprecating the MySQL database. They started using this new tool last month for about 10% of their transactions and the rollout will be complete in about 5 months. There will be no data transfer between the 2 tools.&lt;/p&gt;\n\n&lt;p&gt;Extraction for this new source is easy but\u2026 The structure of the transactions, orders and customers are obviously not the same at all. I have no idea how to rebuild the entire model\u2026 I\u2019m leaning towards building 2 separate star schemas, one with MySQL + Advertising spend and the other with New Tool + Advertising spend, then try my best to build a central table that links both fact tables, but I\u2019m not sure it\u2019s the right way to do this.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked everywhere for ressources about companies onboarding new tools, having different sources for the same grain etc. Maybe I didn\u2019t look properly or maybe that\u2019s uncommon. Either way, I\u2019m lost. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avitns", "is_robot_indexable": true, "report_reasons": null, "author": "volcanicseagull", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avitns/how_to_build_a_model_with_2_data_sources_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avitns/how_to_build_a_model_with_2_data_sources_with_the/", "subreddit_subscribers": 162451, "created_utc": 1708439017.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}