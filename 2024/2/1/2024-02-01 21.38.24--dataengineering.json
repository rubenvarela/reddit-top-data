{"kind": "Listing", "data": {"after": "t3_1agjvds", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?", "author_fullname": "t2_5bpuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a flight this weekend, which do I read first?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1aggfae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fkp7Vi_KUwsffn0f-CQp6ImM1t_tLY7rbFUtNaCXdoo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706808494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?auto=webp&amp;s=0bb941bc45a50426758995677a5b66ade92d4487", "width": 4284, "height": 4284}, "resolutions": [{"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c142ee9e88145d514baf92c943cb4782a02fe1fd", "width": 108, "height": 108}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dd4cfeb0dea340a652f5cbedc437c8af3b7d383", "width": 216, "height": 216}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=799a87e4b938ce32637470e436697ccef761995b", "width": 320, "height": 320}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d4f5e17cc504cc6fceb83bcc7598f1a0e9acde7", "width": 640, "height": 640}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87b404df2c257e27d4cd12c14dc6a1815c8d8229", "width": 960, "height": 960}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e5d546b223a59bbb785a129044ec15075d289f4", "width": 1080, "height": 1080}], "variants": {}, "id": "RB47IfFEb96kxJx90GTTv2lUGE-jR20k6IwvMCivNjY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aggfae", "is_robot_indexable": true, "report_reasons": null, "author": "cheanerman", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aggfae/got_a_flight_this_weekend_which_do_i_read_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "subreddit_subscribers": 157522, "created_utc": 1706808494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far", "author_fullname": "t2_es1pfwhmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Hireable ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afxium", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706747301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afxium", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Series-2277", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "subreddit_subscribers": 157522, "created_utc": 1706747301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "#### Background\n\nMy company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. \n\n#### Question\nWhich library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? \n\n#### More Contexts\nOn the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. \n\nPreviously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. \n\n#### TLDR\nSo which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Portable and Cleanest Way to Integrate BigQuery with Python Dataframe Manipulation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afzpri", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706754014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706753532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h4&gt;Background&lt;/h4&gt;\n\n&lt;p&gt;My company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. &lt;/p&gt;\n\n&lt;h4&gt;Question&lt;/h4&gt;\n\n&lt;p&gt;Which library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? &lt;/p&gt;\n\n&lt;h4&gt;More Contexts&lt;/h4&gt;\n\n&lt;p&gt;On the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. &lt;/p&gt;\n\n&lt;p&gt;Previously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. &lt;/p&gt;\n\n&lt;h4&gt;TLDR&lt;/h4&gt;\n\n&lt;p&gt;So which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afzpri", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "subreddit_subscribers": 157522, "created_utc": 1706753532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does any company or data engineers really using Dremio?", "author_fullname": "t2_fta9agm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3wih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does any company or data engineers really using Dremio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3wih", "is_robot_indexable": true, "report_reasons": null, "author": "luqmancrit69", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "subreddit_subscribers": 157522, "created_utc": 1706766620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?\n\nI see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don't use python for transforming data, there will be tons of replies questioning why not.\n\nHow are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).\n\nmaybe you just clean the data in python from ingestion -&gt; raw -&gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.\n\nI see so many ways to do this, and wondering how you all would approach this considering the constraints.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python-Only Data Pipeline Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afwt5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706745393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?&lt;/p&gt;\n\n&lt;p&gt;I see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don&amp;#39;t use python for transforming data, there will be tons of replies questioning why not.&lt;/p&gt;\n\n&lt;p&gt;How are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).&lt;/p&gt;\n\n&lt;p&gt;maybe you just clean the data in python from ingestion -&amp;gt; raw -&amp;gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.&lt;/p&gt;\n\n&lt;p&gt;I see so many ways to do this, and wondering how you all would approach this considering the constraints.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afwt5i", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "subreddit_subscribers": 157522, "created_utc": 1706745393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey Data Engineering community,\n\nI'm currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I'm using SQL PostgreSQL to fetch products by type. I have a 'products' table with intrinsic information like title, URL, image URL, etc., and sub-tables such as 'books' and 'movies,' each containing specific fields for authors, editors, and movie details.\n\nHowever, I'm facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.\n\nThese products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.\n\nI've heard suggestions that a NoSQL database could be a good fit for this use case. As I'm working exclusively with Python, I'd appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.\n\nThanks a lot for your valuable insights!", "author_fullname": "t2_3mvaeyfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-commerce Products storing (SQL / no sql )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag7kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706781655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineering community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I&amp;#39;m using SQL PostgreSQL to fetch products by type. I have a &amp;#39;products&amp;#39; table with intrinsic information like title, URL, image URL, etc., and sub-tables such as &amp;#39;books&amp;#39; and &amp;#39;movies,&amp;#39; each containing specific fields for authors, editors, and movie details.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.&lt;/p&gt;\n\n&lt;p&gt;These products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard suggestions that a NoSQL database could be a good fit for this use case. As I&amp;#39;m working exclusively with Python, I&amp;#39;d appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your valuable insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ag7kh1", "is_robot_indexable": true, "report_reasons": null, "author": "Saa3dLfachil", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "subreddit_subscribers": 157522, "created_utc": 1706781655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spend the last few months using dbt  to model and analyze historical NBA data sets. [The project](https://github.com/paradime-io/paradime-dbt-nba-data-challenge) has been so fun that I'm releasing it to data folks as a competition!\n\nIn this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!\n\nHere's how it works:\n\nUpon registration, Participants will gain access to:  \n\ud83d\udc49 Paradime for SQL &amp; dbt\u2122 development.  \n\u2744\ufe0f Snowflake for computing and storage.  \n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.  \n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023\n\nFrom there, participants will create insightful analyses and visualizations, and submit them for a chance to win! \n\nIf you're curious, learn more below!\n\n[https://www.paradime.io/dbt-data-modeling-challenge-nba-edition](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt\u2122 data modeling Challenge - NBA Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agj7rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706815476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spend the last few months using dbt  to model and analyze historical NBA data sets. &lt;a href=\"https://github.com/paradime-io/paradime-dbt-nba-data-challenge\"&gt;The project&lt;/a&gt; has been so fun that I&amp;#39;m releasing it to data folks as a competition!&lt;/p&gt;\n\n&lt;p&gt;In this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;p&gt;Upon registration, Participants will gain access to:&lt;br/&gt;\n\ud83d\udc49 Paradime for SQL &amp;amp; dbt\u2122 development.&lt;br/&gt;\n\u2744\ufe0f Snowflake for computing and storage.&lt;br/&gt;\n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.&lt;br/&gt;\n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023&lt;/p&gt;\n\n&lt;p&gt;From there, participants will create insightful analyses and visualizations, and submit them for a chance to win! &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, learn more below!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;https://www.paradime.io/dbt-data-modeling-challenge-nba-edition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?auto=webp&amp;s=41fdb09a495622cce5c1f05123e1900fbad6458f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0bfcb7fbab608cc41fe9498b564e93d489c6b27", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a40dbe345268ba354342895cc4ca9607f43f87be", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=785b1c83ea7863ff2de6c16a16e12de4c8f1e39b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed0083d4fe841bfb13478442010389f368cc1d31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2aeaaee50e003c299bde24ad70d7ca810d0e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8f90192aba6736b970c5fc77523839360fd4610", "width": 1080, "height": 540}], "variants": {}, "id": "vW2rNRLOsGuCMv-C4hnHQbNq81LUjPoWMDxQDE6o6jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agj7rh", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "subreddit_subscribers": 157522, "created_utc": 1706815476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.\n\nIt\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c\n\nThe behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f\n\nIm calling it SQRUFF :)", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Portable SQL Linter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1agijyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S-kVxl0yQ0zf6MOW3PZbmZsw0mFsesa5vKjPHCqg0WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706813857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;The behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f&lt;/p&gt;\n\n&lt;p&gt;Im calling it SQRUFF :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/quarylabs/sqruff", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?auto=webp&amp;s=87a0b3113d8a8e8a4fad2901b55e98449c340d4a", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e09230469f5ffa3e0cd2651e18d860042ba4e457", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f7839ea5bdb3e5e564f2e5afa389d6990bfb89", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fda65169a18d5a214619e0464afdb2852107fa47", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57c64448d9e4256e89bfc92a1e03627f1e1c4050", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cde8788c99759e9bf6399fa46456d0e851962e6", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c224395a06778cde7bd43e5c28b6fab324950f9", "width": 1080, "height": 1080}], "variants": {}, "id": "yi1H-2cV1KrS4mf3uf9SP2lDbruuU_3Qx50MFRPXxz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agijyw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agijyw/open_source_portable_sql_linter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/quarylabs/sqruff", "subreddit_subscribers": 157522, "created_utc": 1706813857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.\n\nThe problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I'm thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our \"data warehouse\", and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.\n\nIs this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don't really have that much data in the OLTP database right now, it's about 140k rows.\n\n&amp;#x200B;\n\nEdit: I doesn't necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives for data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agbigy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706795682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.&lt;/p&gt;\n\n&lt;p&gt;The problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I&amp;#39;m thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our &amp;quot;data warehouse&amp;quot;, and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.&lt;/p&gt;\n\n&lt;p&gt;Is this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don&amp;#39;t really have that much data in the OLTP database right now, it&amp;#39;s about 140k rows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I doesn&amp;#39;t necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agbigy", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "subreddit_subscribers": 157522, "created_utc": 1706795682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is fundamental to databases like Clickhouse, Pinot or Druid that the same can't be achieved using MPP?   \nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I'm thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?  \nI want an understanding of why MPP's don't fit well for user analytical use cases?", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Does MPP's not support high QPS, low latency queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag4ehz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706768368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is fundamental to databases like Clickhouse, Pinot or Druid that the same can&amp;#39;t be achieved using MPP?&lt;br/&gt;\nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I&amp;#39;m thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?&lt;br/&gt;\nI want an understanding of why MPP&amp;#39;s don&amp;#39;t fit well for user analytical use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag4ehz", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "subreddit_subscribers": 157522, "created_utc": 1706768368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.   \nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  \n\n\nI took Alan Rodrigues course in udemy and Ramesh retnasamy's ADF course .  \n[https://www.udemy.com/course/data-engineering-on-microsoft-azure/](https://www.udemy.com/course/data-engineering-on-microsoft-azure/)  \n[https://www.udemy.com/course/learn-azure-data-factory-from-scratch/](https://www.udemy.com/course/learn-azure-data-factory-from-scratch/)\n\nAnd following Tybul on azure youtube play list.   \n[https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg](https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg)  \n\n\nPlease share any study materials or notes for new syllabus. \n\n&amp;#x200B;", "author_fullname": "t2_pdssduhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP -203 Exam study materials help..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agiyq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706814845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.&lt;br/&gt;\nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  &lt;/p&gt;\n\n&lt;p&gt;I took Alan Rodrigues course in udemy and Ramesh retnasamy&amp;#39;s ADF course .&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/data-engineering-on-microsoft-azure/\"&gt;https://www.udemy.com/course/data-engineering-on-microsoft-azure/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/learn-azure-data-factory-from-scratch/\"&gt;https://www.udemy.com/course/learn-azure-data-factory-from-scratch/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And following Tybul on azure youtube play list.&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg\"&gt;https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Please share any study materials or notes for new syllabus. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agiyq1", "is_robot_indexable": true, "report_reasons": null, "author": "bmkmanojkumar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "subreddit_subscribers": 157522, "created_utc": 1706814845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data \u201ccontract\u201d actual implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agkong", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706819160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agkong", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "subreddit_subscribers": 157522, "created_utc": 1706819160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Feb 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n", "t3_1agfqy9"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1706806830.354, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agfqy9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706806830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agfqy9", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "subreddit_subscribers": 157522, "created_utc": 1706806830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you, people, approach new projects?\n\nIn my experience, most data and analytics engineers do the following:\n\n1. Collect all the information they need initially\n2. Start digging and working in the background\n3. Occasionally ask additional questions\n4. Show everything when ready\n\nI don't like that approach. In my opinion, there are too many risks.\n\nInstead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bold Projects vs MVPs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3rhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you, people, approach new projects?&lt;/p&gt;\n\n&lt;p&gt;In my experience, most data and analytics engineers do the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Collect all the information they need initially&lt;/li&gt;\n&lt;li&gt;Start digging and working in the background&lt;/li&gt;\n&lt;li&gt;Occasionally ask additional questions&lt;/li&gt;\n&lt;li&gt;Show everything when ready&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I don&amp;#39;t like that approach. In my opinion, there are too many risks.&lt;/p&gt;\n\n&lt;p&gt;Instead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3rhm", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "subreddit_subscribers": 157522, "created_utc": 1706766104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don't want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.", "author_fullname": "t2_dla93s85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharing data with non-technical teams for review and approval", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag0ir4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706755856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don&amp;#39;t want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag0ir4", "is_robot_indexable": true, "report_reasons": null, "author": "FooFighter_V", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "subreddit_subscribers": 157522, "created_utc": 1706755856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How was the switch for you, and what steps did you take to make the switch?", "author_fullname": "t2_55l23zh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here switched to DE from Technical Project Management/Product Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aglecn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706820952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How was the switch for you, and what steps did you take to make the switch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aglecn", "is_robot_indexable": true, "report_reasons": null, "author": "egg_boi56", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "subreddit_subscribers": 157522, "created_utc": 1706820952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database \u201cAPI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agifx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agifx8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agifx8/database_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agifx8/database_api/", "subreddit_subscribers": 157522, "created_utc": 1706813572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a tax transformation group at a large accounting firm, where we do data transformation, data analytics and process automation for tax departments.\n\nTax departments are not generally tech savvy, and a lot of our work is just replacing awful, manual monthly excel processes with low-code solutions, generally using Alteryx or Microsoft's Power Platform (PowerQuery, PowerApps, etc.) \n\nMost of my bosses and staff are accountants who learned tech concepts along the way, and most of my clients are tax accountants with very limited tech skills. \n\nOur most 'sophisticated' clients are using tools like Alteryx, Microsoft SQL databases and REST APIs to push/pull data from software systems.  \n\nThere's a lot of resistance to using \"code\" because clients don't understand it, and staff turnover is constant. It's \"easier\" to teach people to maintain, review and update low code solutions. \n\nThis usually works fine due to low volume, but occasionally cracks begin to show and I can tell we are doing things in a way that isn't ideal. \n\nFor example, I've got a project right now where I need to process 100m rows of data and store the resulting 50m records somewhere it can be accessed by a PowerBI dashboard in the client's tenant. \n\nI could process that data quickly in batches with Alteryx, but it seems like they want to use Power Platform since the client may already have licenses, so I might have to try to process it using a PowerApps Dataflow, which is basically cloud-based PowerQuery. \n\nI don't think the client has any actual databases, so we may have to store the outputs in a Microsoft Dataverse table, but I'm having trouble even assessing the potential storage cost and performance of storing 50m+ records in a dataverse table and interacting with the data via PowerBI. I could also potentially store a giant CSV on SharePoint and try to open that through PowerBI, but I'm not sure if that would even work. \n\nI don't think these issues are specific to my firm either, there is a whole industry of \"tax technology\" and \"transformation\" that seem to basically be doing data engineering work inefficiently, using suboptimal tools. \n\nDoes anyone have recommendations on how we can do things in more sophisticated ways that are more in line with data engineering best practices? \n\nI'd love to understand the \"right\" way to do some of these tasks, and how I can potentially educate firm leadership and steer things in a better direction. \n\nThank you for any advice you may have!", "author_fullname": "t2_a1kvwds9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I help make things better? (Accounting firm data work)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agi84m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a tax transformation group at a large accounting firm, where we do data transformation, data analytics and process automation for tax departments.&lt;/p&gt;\n\n&lt;p&gt;Tax departments are not generally tech savvy, and a lot of our work is just replacing awful, manual monthly excel processes with low-code solutions, generally using Alteryx or Microsoft&amp;#39;s Power Platform (PowerQuery, PowerApps, etc.) &lt;/p&gt;\n\n&lt;p&gt;Most of my bosses and staff are accountants who learned tech concepts along the way, and most of my clients are tax accountants with very limited tech skills. &lt;/p&gt;\n\n&lt;p&gt;Our most &amp;#39;sophisticated&amp;#39; clients are using tools like Alteryx, Microsoft SQL databases and REST APIs to push/pull data from software systems.  &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of resistance to using &amp;quot;code&amp;quot; because clients don&amp;#39;t understand it, and staff turnover is constant. It&amp;#39;s &amp;quot;easier&amp;quot; to teach people to maintain, review and update low code solutions. &lt;/p&gt;\n\n&lt;p&gt;This usually works fine due to low volume, but occasionally cracks begin to show and I can tell we are doing things in a way that isn&amp;#39;t ideal. &lt;/p&gt;\n\n&lt;p&gt;For example, I&amp;#39;ve got a project right now where I need to process 100m rows of data and store the resulting 50m records somewhere it can be accessed by a PowerBI dashboard in the client&amp;#39;s tenant. &lt;/p&gt;\n\n&lt;p&gt;I could process that data quickly in batches with Alteryx, but it seems like they want to use Power Platform since the client may already have licenses, so I might have to try to process it using a PowerApps Dataflow, which is basically cloud-based PowerQuery. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think the client has any actual databases, so we may have to store the outputs in a Microsoft Dataverse table, but I&amp;#39;m having trouble even assessing the potential storage cost and performance of storing 50m+ records in a dataverse table and interacting with the data via PowerBI. I could also potentially store a giant CSV on SharePoint and try to open that through PowerBI, but I&amp;#39;m not sure if that would even work. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think these issues are specific to my firm either, there is a whole industry of &amp;quot;tax technology&amp;quot; and &amp;quot;transformation&amp;quot; that seem to basically be doing data engineering work inefficiently, using suboptimal tools. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have recommendations on how we can do things in more sophisticated ways that are more in line with data engineering best practices? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to understand the &amp;quot;right&amp;quot; way to do some of these tasks, and how I can potentially educate firm leadership and steer things in a better direction. &lt;/p&gt;\n\n&lt;p&gt;Thank you for any advice you may have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agi84m", "is_robot_indexable": true, "report_reasons": null, "author": "comments_egg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agi84m/how_can_i_help_make_things_better_accounting_firm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agi84m/how_can_i_help_make_things_better_accounting_firm/", "subreddit_subscribers": 157522, "created_utc": 1706813042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. [https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs](https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs)", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Five Useful Queries to Get BigQuery Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agh025", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706809949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. &lt;a href=\"https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs\"&gt;https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?auto=webp&amp;s=2c376e4c51790abdb77e784f486631d6e609bf89", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=624487499dff65b0038a6c79404f8cb230d1a7fc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=957574fe4817ae688d5220549723b418efbd73b2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a83e577e94738c1cb9f73585da0e1bb35b288f7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11affefba0bc6c9c3308849bfea98546e488beb4", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a74bcd4439a79a120286cee87d6cf904a597e79b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56bf0012cdb3fb9322b7eb47cdf976897786096b", "width": 1080, "height": 567}], "variants": {}, "id": "KzLetsP8vZqCBImB3BNcFmeWS5rzEuEkGvHPTrzwTtw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agh025", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "subreddit_subscribers": 157522, "created_utc": 1706809949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm coming up on 5 yoe in the data space, as an analytics engineer for 3 years and a DE for the last 2, and have jumped between a few start-ups in HealthCare and Fintech. Ive been at my current gig just over a year, I'm doing well and the company wants to promote me to a senior DE position soon. The thing is I'm a strong individual contributor, not so strong at deligating to or managing others, and kind of dread being sucked into the corporate culture if I continue up the ladder. \n\nThis has made me start thinking about how I can position myself to grow my career while maintaining my current autonomy. My initial thoughts are contracting and starting my own business, not something I have ever done, but thought it might be an avenue to explore. I also thought about stacking a couple of jobs, my current role only requires a few focused hours a day and pulling in double my salary sounds great. Again, not something I have ever done, and not sure if employers are cool with this or if I'll be lying all the time.\n\nI guess I'm looking to know what my options are and how to choose the best route for me. Thanks in advance for your feedback!", "author_fullname": "t2_1nvzg4ji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Next steps for an individual contributor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aggfov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706808520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m coming up on 5 yoe in the data space, as an analytics engineer for 3 years and a DE for the last 2, and have jumped between a few start-ups in HealthCare and Fintech. Ive been at my current gig just over a year, I&amp;#39;m doing well and the company wants to promote me to a senior DE position soon. The thing is I&amp;#39;m a strong individual contributor, not so strong at deligating to or managing others, and kind of dread being sucked into the corporate culture if I continue up the ladder. &lt;/p&gt;\n\n&lt;p&gt;This has made me start thinking about how I can position myself to grow my career while maintaining my current autonomy. My initial thoughts are contracting and starting my own business, not something I have ever done, but thought it might be an avenue to explore. I also thought about stacking a couple of jobs, my current role only requires a few focused hours a day and pulling in double my salary sounds great. Again, not something I have ever done, and not sure if employers are cool with this or if I&amp;#39;ll be lying all the time.&lt;/p&gt;\n\n&lt;p&gt;I guess I&amp;#39;m looking to know what my options are and how to choose the best route for me. Thanks in advance for your feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aggfov", "is_robot_indexable": true, "report_reasons": null, "author": "data-punk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aggfov/next_steps_for_an_individual_contributor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aggfov/next_steps_for_an_individual_contributor/", "subreddit_subscribers": 157522, "created_utc": 1706808520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source and the Data Lakehouse: Apache Arrow, Apache Iceberg, Nessie and Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agce9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1706798072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1agce9z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agce9z/open_source_and_the_data_lakehouse_apache_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "subreddit_subscribers": 157522, "created_utc": 1706798072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn't find anything resembling what I wanted to get. I want to get metadata rather than the data itself.", "author_fullname": "t2_mekatj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to extract data sources from a PBI dashboard in .pbix?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag9anf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706788577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn&amp;#39;t find anything resembling what I wanted to get. I want to get metadata rather than the data itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag9anf", "is_robot_indexable": true, "report_reasons": null, "author": "thiophosgene", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "subreddit_subscribers": 157522, "created_utc": 1706788577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don\u2019t know if this is the correct place to post this but here goes.\n\nI\u2019ve been storing large amounts of data in txt files and whenever it needs something it splits the whole file by line using .split(\u201c\\n\u201d) and reads the file with fs.readFile. What programs/methods should I use to speed up processing speeds without buying actual servers. Also I\u2019m aware that this is a bad way of storing data I just haven\u2019t researched it that much", "author_fullname": "t2_reg4lo4zv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to store amd retrieve data using node js?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag0wb4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706756972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don\u2019t know if this is the correct place to post this but here goes.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been storing large amounts of data in txt files and whenever it needs something it splits the whole file by line using .split(\u201c\\n\u201d) and reads the file with fs.readFile. What programs/methods should I use to speed up processing speeds without buying actual servers. Also I\u2019m aware that this is a bad way of storing data I just haven\u2019t researched it that much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag0wb4", "is_robot_indexable": true, "report_reasons": null, "author": "ArmiliteRifle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag0wb4/whats_the_best_way_to_store_amd_retrieve_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag0wb4/whats_the_best_way_to_store_amd_retrieve_data/", "subreddit_subscribers": 157522, "created_utc": 1706756972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nBefore digging in let\u2019s state my background:\n\n1. I was Software Engineer for almost 2 years in an agile team where I contributed to analysis, development, reviewing and deployment.\n2. The last year I am working as a Data Scientist but it\u2019s more like AI Engineer where we use Azure and SQL server. However, the department is new thus, we did not really deployed something to production yet but we\u2019re coming there. The thing is that currently I do not even think that I could use this experience for later, but it\u2019s not a discussion for this post.\n\nBeing on both sides, I think that would suit me better to work as a Data Engineer as I think I\u2019m better and more productive at giving technical solutions regarding databases etc than thinking of AI algorithms in terms of making our approach go that extra mile and I also see that for AI Tech Leads a PhD is necessary while in Data Engineering it\u2019s not. Also AI Engineering in industry currently it\u2019s just ChatGPT prompt engineering, thus I do not think it\u2019s worth it much.\n\nHowever, for some reason when I discuss with recruiters they are like I said something bad but it\u2019s just my genuine opinion.\n\nThe question I want to ask is that provided that I\u2019ll change jobs after at least 2-3 years, is it worth it to invest in courses, personal projects etc. in order to pursue a career in Data Emgineering or should I focus on my current position like MLOps? My main concern is whether I can find a job at Mid-Senior level as a Data Engineer without having any DE professional experience, but only my personal projects.", "author_fullname": "t2_fc2rgdx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I pursue Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agka5d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706818160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Before digging in let\u2019s state my background:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was Software Engineer for almost 2 years in an agile team where I contributed to analysis, development, reviewing and deployment.&lt;/li&gt;\n&lt;li&gt;The last year I am working as a Data Scientist but it\u2019s more like AI Engineer where we use Azure and SQL server. However, the department is new thus, we did not really deployed something to production yet but we\u2019re coming there. The thing is that currently I do not even think that I could use this experience for later, but it\u2019s not a discussion for this post.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Being on both sides, I think that would suit me better to work as a Data Engineer as I think I\u2019m better and more productive at giving technical solutions regarding databases etc than thinking of AI algorithms in terms of making our approach go that extra mile and I also see that for AI Tech Leads a PhD is necessary while in Data Engineering it\u2019s not. Also AI Engineering in industry currently it\u2019s just ChatGPT prompt engineering, thus I do not think it\u2019s worth it much.&lt;/p&gt;\n\n&lt;p&gt;However, for some reason when I discuss with recruiters they are like I said something bad but it\u2019s just my genuine opinion.&lt;/p&gt;\n\n&lt;p&gt;The question I want to ask is that provided that I\u2019ll change jobs after at least 2-3 years, is it worth it to invest in courses, personal projects etc. in order to pursue a career in Data Emgineering or should I focus on my current position like MLOps? My main concern is whether I can find a job at Mid-Senior level as a Data Engineer without having any DE professional experience, but only my personal projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1agka5d", "is_robot_indexable": true, "report_reasons": null, "author": "Capital-Ganache8631", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agka5d/should_i_pursue_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agka5d/should_i_pursue_data_engineering/", "subreddit_subscribers": 157522, "created_utc": 1706818160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of the questions here are about technical skills and tools. \nBut I just wanted to ask what are some ways to improve non technical skills especially communication.\nDo you recommend any books one should read to be a good at communication?\nOr is there anything else.\nLooking for some good insights.\nThanks", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving non technical skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agjvds", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706817152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of the questions here are about technical skills and tools. \nBut I just wanted to ask what are some ways to improve non technical skills especially communication.\nDo you recommend any books one should read to be a good at communication?\nOr is there anything else.\nLooking for some good insights.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agjvds", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agjvds/improving_non_technical_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agjvds/improving_non_technical_skills/", "subreddit_subscribers": 157522, "created_utc": 1706817152.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}