{"kind": "Listing", "data": {"after": "t3_1afqsf5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.", "author_fullname": "t2_a0i580op", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern data pipeline for on premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8ts7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706674252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af8ts7", "is_robot_indexable": true, "report_reasons": null, "author": "lezzgooooo", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8ts7/modern_data_pipeline_for_on_premise/", "subreddit_subscribers": 157294, "created_utc": 1706674252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit : Thank you guys, you're all awesome and helpful, this was absolutely my first post here and you guys provided valuable and curated tips and tricks. Thank you. \n\nMy manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.\n\nSorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.\n\n&amp;#x200B;\n\nThank you so much.", "author_fullname": "t2_43bjqfmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python and Spark Scala", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aff8gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706745882.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706698474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit : Thank you guys, you&amp;#39;re all awesome and helpful, this was absolutely my first post here and you guys provided valuable and curated tips and tricks. Thank you. &lt;/p&gt;\n\n&lt;p&gt;My manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.&lt;/p&gt;\n\n&lt;p&gt;Sorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aff8gu", "is_robot_indexable": true, "report_reasons": null, "author": "WadieXkiller", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aff8gu/python_and_spark_scala/", "subreddit_subscribers": 157294, "created_utc": 1706698474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Dagster Believes About Data Platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1afnd1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1HfcUuzrMpvBBY94JElFNYso1NQQ4RrLwGz_rUBvKb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706722293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?auto=webp&amp;s=94dcf8ce7019ce3e221a84088531c82d39c59478", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eea7cd5c0c3002c13b2398269a37e06cf911e3ae", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6982f3cf54d8a8754e64ee7e2c4bc544d1a5422c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74ed1228f650dddfb6ed29eb136a6b8f73ada0ac", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=80a10915705eaf3527c7d89450431f653a590f8d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fbe788ae92611b87791c735d2d71eb2e00e28b5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=006b1460ef750708d063f2e3c7670fde92270524", "width": 1080, "height": 567}], "variants": {}, "id": "9BPh1HBe8dpb9hwolf4ivC5UxBl9ds4I31wiMA6yM0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afnd1d", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afnd1d/what_dagster_believes_about_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "subreddit_subscribers": 157294, "created_utc": 1706722293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.\n\n[Link](https://rr43.net/posts/2024/1/Dremel/)", "author_fullname": "t2_dbozei2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My first blog about DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afakqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanted to try writing a blogpost for a while, finally got the motivation and time to write it. Would appreciate any constructive feedback on it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://rr43.net/posts/2024/1/Dremel/\"&gt;Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afakqy", "is_robot_indexable": true, "report_reasons": null, "author": "InstitutionalizedSon", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afakqy/my_first_blog_about_de/", "subreddit_subscribers": 157294, "created_utc": 1706679855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite Python Data Processing Libraries? (PyArrow, Pandas, Polars, DuckDB, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflut4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706718628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aflut4", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "subreddit_subscribers": 157294, "created_utc": 1706718628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seeing that a lot of DE positions seem to be shifting towards analytics and layoffs are ongoing I want to stay competitive. I'm interested in backend engineering to avoid low code tools and SAAS solutions that have found their way into DE. I'm also considering platform engineering, but only because it seems more accessible. \n\nI'm planning to pick a language and build projects in my spare time, but realistically this won't get me above entry level and that's not a pay cut I can afford to take.\n\nI work for a big company and switching departments is not an easy process, but something I will consider in the future. Perhaps I should give consulting a shot? I'm not a fan of start ups so that's not an option. \n\nHow did others approach a similar career change?", "author_fullname": "t2_vsf3ige5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to retrain as SWE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afrpib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706732779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing that a lot of DE positions seem to be shifting towards analytics and layoffs are ongoing I want to stay competitive. I&amp;#39;m interested in backend engineering to avoid low code tools and SAAS solutions that have found their way into DE. I&amp;#39;m also considering platform engineering, but only because it seems more accessible. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to pick a language and build projects in my spare time, but realistically this won&amp;#39;t get me above entry level and that&amp;#39;s not a pay cut I can afford to take.&lt;/p&gt;\n\n&lt;p&gt;I work for a big company and switching departments is not an easy process, but something I will consider in the future. Perhaps I should give consulting a shot? I&amp;#39;m not a fan of start ups so that&amp;#39;s not an option. &lt;/p&gt;\n\n&lt;p&gt;How did others approach a similar career change?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1afrpib", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated_Ad_1108", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afrpib/best_way_to_retrain_as_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afrpib/best_way_to_retrain_as_swe/", "subreddit_subscribers": 157294, "created_utc": 1706732779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are converting a database load based to DBT. The old value chain was often like this:\n\n1. Insert new base data into table X \n2. Run multiple update statement or insert statement on table X\n\nThis was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). \n\nI feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.\n\nDo you try to organize models in folders in such a way that make that closely related models are kept together? \n\nIs a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7hbs1ihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organize code in DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afj9o0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706711826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are converting a database load based to DBT. The old value chain was often like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert new base data into table X &lt;/li&gt;\n&lt;li&gt;Run multiple update statement or insert statement on table X&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This was done at regular intervals each year by the means of stored procedures. All these updates and inserts have to be replaced with models that increases the number of models/tables compared with the old data base (which only have the final tables). &lt;/p&gt;\n\n&lt;p&gt;I feel that this is becoming messy very fast. Even if DBT keeps track of the linage, the DAG have become so large that it is difficult to get a good overview. Much scrolling in the DAG html view is needed.&lt;/p&gt;\n\n&lt;p&gt;Do you try to organize models in folders in such a way that make that closely related models are kept together? &lt;/p&gt;\n\n&lt;p&gt;Is a stupid idea to put numbers on folders/models in order to get a idea of the relative order between models?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afj9o0", "is_robot_indexable": true, "report_reasons": null, "author": "Wise-Ad-7492", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afj9o0/organize_code_in_dbt/", "subreddit_subscribers": 157294, "created_utc": 1706711826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_un9ertyb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 simple software engineering habits that transformed my productivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1afe5ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4kP-FUOwxhBUYC9OE08WdSJCvzdPdX92Eb4bizlVL74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706693946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "read.engineerscodex.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?auto=webp&amp;s=5871f0d7487438c46b01eebb7041b2f5a2ba7527", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdec1cf8f05f2821e71a81d57b8b1b702bc44ebf", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e5627d5bdcdcaa244366390da0a789cad4ee8c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ebe3ed6fb441eabc7f8c3f2481e9ead91151a20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a92ae3e201c4437dc18cbe46538d166170fc37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=371233137e9fb0406471aba4201351f9bf459b61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jFIuUEXCUZiN4K05QzfG924liJb35SyGDZ9cYW5k8T0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32f7bba1c8cf00ab2b15dfe0088d3570c3d8f56c", "width": 1080, "height": 540}], "variants": {}, "id": "jv2y0q2UCecenbfBaCYYmlJZojw4Z73RqFv9jW4FMmA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afe5ug", "is_robot_indexable": true, "report_reasons": null, "author": "boyrot37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afe5ug/4_simple_software_engineering_habits_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://read.engineerscodex.com/p/simple-software-engineering-habits?utm_source=%252Fbrowse%252Ftechnology&amp;utm_medium=reader2&amp;ref=dailydev", "subreddit_subscribers": 157294, "created_utc": 1706693946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have a &gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. ", "author_fullname": "t2_vos56utz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions for real-time data refreshes with large tables (&gt;1TB) in Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afacpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706679112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have a &amp;gt;1TB table containing billions of rows (updated via a data loader from a third party). This data is updated three times a day, and rows are added to the table. We currently use a materialized view to generate a subset of the data that we actually care about. However, we wanted to get real-time data refreshes without waiting for the materialized view to update. We were exploring Airbyte, but their CDC approach requires replicating the table, and we were looking to avoid having 2 copies of a 1 TB table. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afacpu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Accountant9659", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afacpu/looking_for_suggestions_for_realtime_data/", "subreddit_subscribers": 157294, "created_utc": 1706679112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just had a code challenge with meta. There were 5 python and 5 sql questions. I answered 3 questions correctly in both sections and ran out of time before even attempting the 4th. \n\nI misunderstood the 3rd sql questions so I kinda spent too much time trying to debug\n\nIs there a good chance I won\u2019t hear back? This is for L5 or L6", "author_fullname": "t2_nhotjt8ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meta code challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afwjt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706744711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just had a code challenge with meta. There were 5 python and 5 sql questions. I answered 3 questions correctly in both sections and ran out of time before even attempting the 4th. &lt;/p&gt;\n\n&lt;p&gt;I misunderstood the 3rd sql questions so I kinda spent too much time trying to debug&lt;/p&gt;\n\n&lt;p&gt;Is there a good chance I won\u2019t hear back? This is for L5 or L6&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1afwjt9", "is_robot_indexable": true, "report_reasons": null, "author": "Complex_Counter_6407", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afwjt9/meta_code_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afwjt9/meta_code_challenge/", "subreddit_subscribers": 157294, "created_utc": 1706744711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of **30 private VM servers and baremetal servers with GPU**. I\u2019m hoping to find a solution that can help me **spin up a dynamic instance like ECS**, **spin dev and prod Jupyter Lab notebooks**, have a MLflow and dagster setup on these instances and **manage an execution queue for GPU-based servers**.\n\nwe dont have a very large team to manage and support kubernetees installation, but I'm open to discuss if this is the only way. \n\nIf anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.\n\nThank you in advance!\n\nI hope this helps! Let me know if you have any other questions.", "author_fullname": "t2_4uziix4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions on a solution to orchestrate corporate infrastructure of 30 private VM servers and baremetal servers with GPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af6zgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for suggestions on a solution that can help me orchestrate the corporate infrastructure of &lt;strong&gt;30 private VM servers and baremetal servers with GPU&lt;/strong&gt;. I\u2019m hoping to find a solution that can help me &lt;strong&gt;spin up a dynamic instance like ECS&lt;/strong&gt;, &lt;strong&gt;spin dev and prod Jupyter Lab notebooks&lt;/strong&gt;, have a MLflow and dagster setup on these instances and &lt;strong&gt;manage an execution queue for GPU-based servers&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;we dont have a very large team to manage and support kubernetees installation, but I&amp;#39;m open to discuss if this is the only way. &lt;/p&gt;\n\n&lt;p&gt;If anyone has any experience with similar requirements or can suggest a solution, I\u2019d really appreciate it.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;I hope this helps! Let me know if you have any other questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1af6zgk", "is_robot_indexable": true, "report_reasons": null, "author": "mvishruth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af6zgk/looking_for_suggestions_on_a_solution_to/", "subreddit_subscribers": 157294, "created_utc": 1706668736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. \n\nSo when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.\n\nFor running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.\n\nSo is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?", "author_fullname": "t2_txl4izdo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aforig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706725682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. &lt;/p&gt;\n\n&lt;p&gt;So when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.&lt;/p&gt;\n\n&lt;p&gt;For running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.&lt;/p&gt;\n\n&lt;p&gt;So is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aforig", "is_robot_indexable": true, "report_reasons": null, "author": "__1l0__", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "subreddit_subscribers": 157294, "created_utc": 1706725682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi guys,\n\nI recently came across this announce of databricks (see link below) and I became really interested in becoming certified as a databricks Generative AI Engineer.\n\nI guess they have not yet posted any info in regards of the dates when the certification will become available, but they mention (correct me if I am wrong) that a beta version is already available once you finish the learning pathway for this specialization.\n\nThis looks like maybe they put the full-fledged certification in the second quarter of the year?. What do you think?.\n\n[https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification](https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification)", "author_fullname": "t2_oonyiqxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Databricks certification and learning pathway for Generative AI Engineering.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afls0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706718428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I recently came across this announce of databricks (see link below) and I became really interested in becoming certified as a databricks Generative AI Engineer.&lt;/p&gt;\n\n&lt;p&gt;I guess they have not yet posted any info in regards of the dates when the certification will become available, but they mention (correct me if I am wrong) that a beta version is already available once you finish the learning pathway for this specialization.&lt;/p&gt;\n\n&lt;p&gt;This looks like maybe they put the full-fledged certification in the second quarter of the year?. What do you think?.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification\"&gt;https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?auto=webp&amp;s=3b62e7f25e67dfcc535b34d650d9b8cfae8f582b", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdbdfd1f295bd67ba31b6556e5253c85e7f4f97d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fcd0a9fb3dd5aa169617f1d1d7bb39a27b7a4676", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30a0d7c33d06c0990f97c5f13441a3bfee56c3aa", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bb49290ec20b840662b7e554ecd7d60dbc7c06c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fcb833595d8aad2af89c50c13d0cf36556955a9", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aedf47ac9db802230ce034ee4917568441966237", "width": 1080, "height": 565}], "variants": {}, "id": "JRDcv3JjHYyHvFgK73Z7LiAs-njaiY6mjeH-kcrGeMc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1afls0t", "is_robot_indexable": true, "report_reasons": null, "author": "Due_Percentage447", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afls0t/new_databricks_certification_and_learning_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afls0t/new_databricks_certification_and_learning_pathway/", "subreddit_subscribers": 157294, "created_utc": 1706718428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don't really see the point of using this extensive paid tool for basically doing just simple API and database calls. \n\nIs there a better tool/method for automating and managing a copy activity easily? (in Azure)", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Near realtime copy ELT with other tool than ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflaae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706717177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don&amp;#39;t really see the point of using this extensive paid tool for basically doing just simple API and database calls. &lt;/p&gt;\n\n&lt;p&gt;Is there a better tool/method for automating and managing a copy activity easily? (in Azure)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aflaae", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "subreddit_subscribers": 157294, "created_utc": 1706717177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I've managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?", "author_fullname": "t2_hae00nzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift performance optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afcicb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706686938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with a massive 14 billion row dataset in Redshift for sales analytics reporting: I&amp;#39;ve managed to optimize query times using sort keys and distribution keys, but as the dataset is continuously growing and currently spans three years of data, what are other effective strategies or methods you would recommend for further optimizing read performance on such a large and expanding dataset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afcicb", "is_robot_indexable": true, "report_reasons": null, "author": "New-Statistician-155", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afcicb/redshift_performance_optimization/", "subreddit_subscribers": 157294, "created_utc": 1706686938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Redshift Enthusiasts,\n\nI'm encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.\n\nHere's the scenario: I'm in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it's taking quite a bit of time to complete.\n\nIn an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.\n\nHowever, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.\n\nI've pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?\n\nI'd greatly appreciate any insights, tips, or experiences you've had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?\n\nLooking forward to your thoughts and expertise!\n\nCheers!", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Troubleshooting Redshift COPY Command Performance: Need Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af8onh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706673814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redshift Enthusiasts,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m encountering a puzzling situation with the Redshift COPY command and could really use some insights from the community.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the scenario: I&amp;#39;m in the midst of copying a substantial 2 GB CSV file from S3 into Redshift as part of a full load operation. Naturally, given the size of the file, it&amp;#39;s taking quite a bit of time to complete.&lt;/p&gt;\n\n&lt;p&gt;In an effort to expedite the process, I decided to scale up my Redshift cluster. Initially, I was operating on a single dc2.xlarge node, and I thought increasing it to a cluster with two dc2.xlarge nodes would significantly improve the load time.&lt;/p&gt;\n\n&lt;p&gt;However, to my surprise, the two-node cluster seems to be taking even longer compared to the single-node setup. This unexpected turn has left me scratching my head, wondering what could be causing this slowdown.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pondered potential reasons behind this counterintuitive outcome. Could it be an issue with the distribution keys, data skew, or perhaps something else entirely?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d greatly appreciate any insights, tips, or experiences you&amp;#39;ve had with similar situations. What could be causing the two-node cluster to lag behind its single-node counterpart in terms of load times?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your thoughts and expertise!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1af8onh", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af8onh/troubleshooting_redshift_copy_command_performance/", "subreddit_subscribers": 157294, "created_utc": 1706673814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\njust looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have 'dimCustomerId', 'CustomerId', 'CustomerName' as fields. The 'dimCustomerId' is an auto incrementing ID. The dimCustomerId is what is used in my fact table. \n\n  \nWhat is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use 'CustomerId'. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? \n\nWould it be suitable to have a dimCustomer table with 'CustomerId' as the PK and 'CustomerName' as the field, removing the dimCustomerId altogether? I'm not looking to actually do this, just for my own learning. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dimensional table ids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af7s1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706671079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;just looking for some clarification on best practices for dimensional and fact tables. At my work, we have a number of dimensional tables. Say we are looking at the dimCustomers table. I have &amp;#39;dimCustomerId&amp;#39;, &amp;#39;CustomerId&amp;#39;, &amp;#39;CustomerName&amp;#39; as fields. The &amp;#39;dimCustomerId&amp;#39; is an auto incrementing ID. The dimCustomerId is what is used in my fact table. &lt;/p&gt;\n\n&lt;p&gt;What is the point in having this dimCustomerId? Everywhere else in our business including our lakehouse, we simply use &amp;#39;CustomerId&amp;#39;. When we load data to our fact table, we read in the dimCustomers table, use CustomerId to find the dimCustomerId, and then store dimCustomerId in the fact table. It seems like an unnecessary, round about way since we get the dimCustomerId using CustomerId, so why not just use CustomerId? &lt;/p&gt;\n\n&lt;p&gt;Would it be suitable to have a dimCustomer table with &amp;#39;CustomerId&amp;#39; as the PK and &amp;#39;CustomerName&amp;#39; as the field, removing the dimCustomerId altogether? I&amp;#39;m not looking to actually do this, just for my own learning. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af7s1s", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af7s1s/dimensional_table_ids/", "subreddit_subscribers": 157294, "created_utc": 1706671079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Folks aren't too happy with Synapse for multiple reasons; one is that we can't get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.\n\nThis is our data flow  Dataverse --&gt; Synapse Link --- &gt; Datalake Storage Gen 2 ---&gt; Synapse Analytics serverless SQL Endpoint ----&gt; Synapse Pipelines -----&gt; Upsert data and schema evolution to Azure SQL Server -----&gt; Snaplogic ------&gt; AWS Redshift.\n\nI am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I've tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it's not supported.\n\nAny suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it's worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.", "author_fullname": "t2_5yj82gi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Platform suggestions to migrate off of Azure Synapse Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af70gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706668813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks aren&amp;#39;t too happy with Synapse for multiple reasons; one is that we can&amp;#39;t get it running on a private endpoint, so port 1433 sits on the internet for the Serverless SQL Pool.  Apparently, this is also the case with Fabric, although Fabric uses managed identities whereas Synapse has a SQL Auth method turned on by default.&lt;/p&gt;\n\n&lt;p&gt;This is our data flow  Dataverse --&amp;gt; Synapse Link --- &amp;gt; Datalake Storage Gen 2 ---&amp;gt; Synapse Analytics serverless SQL Endpoint ----&amp;gt; Synapse Pipelines -----&amp;gt; Upsert data and schema evolution to Azure SQL Server -----&amp;gt; Snaplogic ------&amp;gt; AWS Redshift.&lt;/p&gt;\n\n&lt;p&gt;I am the build owner for the architecture up to Azure SQL Server and want to come up with a secure alternative (no open port 1433 to the internet).  The solution needs to do schema evolution from the D365 Rest endpoint as well as update all changed data every 5 to 10 minutes. The current system does all that with minimal issues.  I&amp;#39;ve tried pursuing a private endpoint for the Synapse SQL endpoint and was told by Microsoft it&amp;#39;s not supported.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are much appreciated. I have VMS on premise at my current disposal and will eventually get them in Azure when we finish a hardened image. For what it&amp;#39;s worth we also use BigTable and VErtex at GCP and Snaplogic and Redshift on AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af70gd", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Cry_6841", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af70gd/looking_for_platform_suggestions_to_migrate_off/", "subreddit_subscribers": 157294, "created_utc": 1706668813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far", "author_fullname": "t2_es1pfwhmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Hireable ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afxium", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706747301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afxium", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Series-2277", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "subreddit_subscribers": 157294, "created_utc": 1706747301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?\n\nI see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don't use python for transforming data, there will be tons of replies questioning why not.\n\nHow are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).\n\nmaybe you just clean the data in python from ingestion -&gt; raw -&gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.\n\nI see so many ways to do this, and wondering how you all would approach this considering the constraints.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python-Only Data Pipeline Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afwt5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706745393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?&lt;/p&gt;\n\n&lt;p&gt;I see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don&amp;#39;t use python for transforming data, there will be tons of replies questioning why not.&lt;/p&gt;\n\n&lt;p&gt;How are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).&lt;/p&gt;\n\n&lt;p&gt;maybe you just clean the data in python from ingestion -&amp;gt; raw -&amp;gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.&lt;/p&gt;\n\n&lt;p&gt;I see so many ways to do this, and wondering how you all would approach this considering the constraints.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afwt5i", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "subreddit_subscribers": 157294, "created_utc": 1706745393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on DE team that directly supports a DS team. Our tech stack can be summarized as:\n\n* airbyte populates clickhouse\n* dbt drives transformation in clickhouse\n* dagster orchestrates everything, airbyte dbt and everything before, in between, and after\n\nAside from metrics created with dbt, we often have final deliverables that need to be sent to customers via email, sftp, etc. These deliverables are often the results of pure sql, or python+sql. As the number of deliverables increases, so does the number of custom code solutions to send them where they need to go.\n\nI've done my best to separate report logic from delivery. The scientists/analysts have their own repo where logic is implemented, thus they can commit/merge as needed. The Dagster repo then installs their repo as a python package, and imports and runs their code.\n\nDoes anyone have any recommendation for managing this growth? Preferably in a way that is easy to extend and generic? Thanks!", "author_fullname": "t2_7ch0sgxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing deliverables (reports sent via email, sftp, etc)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afsbue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706734700.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706734270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on DE team that directly supports a DS team. Our tech stack can be summarized as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;airbyte populates clickhouse&lt;/li&gt;\n&lt;li&gt;dbt drives transformation in clickhouse&lt;/li&gt;\n&lt;li&gt;dagster orchestrates everything, airbyte dbt and everything before, in between, and after&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Aside from metrics created with dbt, we often have final deliverables that need to be sent to customers via email, sftp, etc. These deliverables are often the results of pure sql, or python+sql. As the number of deliverables increases, so does the number of custom code solutions to send them where they need to go.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done my best to separate report logic from delivery. The scientists/analysts have their own repo where logic is implemented, thus they can commit/merge as needed. The Dagster repo then installs their repo as a python package, and imports and runs their code.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any recommendation for managing this growth? Preferably in a way that is easy to extend and generic? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afsbue", "is_robot_indexable": true, "report_reasons": null, "author": "coreytrevorlahey69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afsbue/managing_deliverables_reports_sent_via_email_sftp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afsbue/managing_deliverables_reports_sent_via_email_sftp/", "subreddit_subscribers": 157294, "created_utc": 1706734270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI am exploring options to build data mart to support some financial dashboards and I would love to know your opinions on this. Sorry for the lengthy post\n\nWe receive data from 2 ERP systems and a custom web and desktop applications. the current state of analytics platform is there are ADF pipelines which dump data into Azure SQL so basically it is replica of the different sources. We are BI team of 3 people and every one builds their own reporting views with lot of joins, when creating power bi reports so no body owns anything in the Azure SQL database and there is lot of rework being done for each dashboard due to the lack of common model . \n\n&amp;#x200B;\n\nAt least our finance team is now coming up with some well defined requirements for couple of new dashboards. So we are thinking of creating a data mart for finance team and thinking of choosing the right architecture. Please correct me if any of my points are dumb, I am in early stage of my BI career and don't have much experience in choosing the architecture. below are the things that I am considering\n\n**1**. I am planning to dump source data  to ADLS(there are some large xml files that we need to parse so I am thinking using ADLS might reduce cots) and perform all the transformations using spark notebooks in synapse and write as delta tables(synapse server less sql pool) and have these delta tables serve as data mart with dimensions and fact tables ready to be imported to power bi.\n\n**issues that I am seeing :** \n\n1.can I use the delta tables to build dimensionally modeled data mart. I think we can not create relationships between tables ,implement incremental surrogate keys also how difficult is it to implement SCDs?\n\n2.If we want any adhoc reports, I dont see any easy way to just do some joins. To over come this do i need to create another serverless sql pool between adls and my data mart which serves as replica of the source tables? if that is the case am i over complicating things by having too many layers?\n\n3. Also using serverless sql pool we can  not create stored procs and do some traditional tsql things\n\n&amp;#x200B;\n\n**2.**  how about dedicated sql pools? but I am not sure if i should really use it because of the costs. my entire data would be around 5 tb with daily data about 2-3 gb. \n\n**3.** I want to leverage adls to minimize storage costs but also want my data mart to have traditional t sql features to perform dimensional modelling. so instead of using Azure sql as staging layer can i use synapse server less sql pool and the have Azure SQL Db as data mart. if so is there any way to write from the delta tables to Azure SQL?\n\n**4**. should I even consider using ADLS? or should I do the traditional way of having staging area and a data mart both in a Azure SQL db?\n\n ", "author_fullname": "t2_7s9ezasyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help in choosing Synapse server less sql pool as data mart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1af6al2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706666697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am exploring options to build data mart to support some financial dashboards and I would love to know your opinions on this. Sorry for the lengthy post&lt;/p&gt;\n\n&lt;p&gt;We receive data from 2 ERP systems and a custom web and desktop applications. the current state of analytics platform is there are ADF pipelines which dump data into Azure SQL so basically it is replica of the different sources. We are BI team of 3 people and every one builds their own reporting views with lot of joins, when creating power bi reports so no body owns anything in the Azure SQL database and there is lot of rework being done for each dashboard due to the lack of common model . &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;At least our finance team is now coming up with some well defined requirements for couple of new dashboards. So we are thinking of creating a data mart for finance team and thinking of choosing the right architecture. Please correct me if any of my points are dumb, I am in early stage of my BI career and don&amp;#39;t have much experience in choosing the architecture. below are the things that I am considering&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. I am planning to dump source data  to ADLS(there are some large xml files that we need to parse so I am thinking using ADLS might reduce cots) and perform all the transformations using spark notebooks in synapse and write as delta tables(synapse server less sql pool) and have these delta tables serve as data mart with dimensions and fact tables ready to be imported to power bi.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;issues that I am seeing :&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;1.can I use the delta tables to build dimensionally modeled data mart. I think we can not create relationships between tables ,implement incremental surrogate keys also how difficult is it to implement SCDs?&lt;/p&gt;\n\n&lt;p&gt;2.If we want any adhoc reports, I dont see any easy way to just do some joins. To over come this do i need to create another serverless sql pool between adls and my data mart which serves as replica of the source tables? if that is the case am i over complicating things by having too many layers?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Also using serverless sql pool we can  not create stored procs and do some traditional tsql things&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;  how about dedicated sql pools? but I am not sure if i should really use it because of the costs. my entire data would be around 5 tb with daily data about 2-3 gb. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; I want to leverage adls to minimize storage costs but also want my data mart to have traditional t sql features to perform dimensional modelling. so instead of using Azure sql as staging layer can i use synapse server less sql pool and the have Azure SQL Db as data mart. if so is there any way to write from the delta tables to Azure SQL?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. should I even consider using ADLS? or should I do the traditional way of having staging area and a data mart both in a Azure SQL db?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1af6al2", "is_robot_indexable": true, "report_reasons": null, "author": "Soft_Manufacturer314", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1af6al2/need_help_in_choosing_synapse_server_less_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1af6al2/need_help_in_choosing_synapse_server_less_sql/", "subreddit_subscribers": 157294, "created_utc": 1706666697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'll be recruiting a senior data engineer in a couple of months and need to design a test for the interview.\n\nDid anyone have any examples of what they have been through during the interview process or something they have set themselves?  Just looking for a bit of inspiration!\n\nThanks", "author_fullname": "t2_674ibeps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior data engineer interview test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1afxacb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706746653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be recruiting a senior data engineer in a couple of months and need to design a test for the interview.&lt;/p&gt;\n\n&lt;p&gt;Did anyone have any examples of what they have been through during the interview process or something they have set themselves?  Just looking for a bit of inspiration!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1afxacb", "is_robot_indexable": true, "report_reasons": null, "author": "atrifleamused", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afxacb/senior_data_engineer_interview_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afxacb/senior_data_engineer_interview_test/", "subreddit_subscribers": 157294, "created_utc": 1706746653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi! i\u2019m wondering if anyone has any advice / best practices on designing a data model for a 2-sided business (uber, airbnb, doordash, stubhub) where users can be buyers, sellers, both or none. \n\nhow would users get materialized into tables? how would the business model be properly reflected?", "author_fullname": "t2_tu15i208", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Marketplace Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afrtle", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706733048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi! i\u2019m wondering if anyone has any advice / best practices on designing a data model for a 2-sided business (uber, airbnb, doordash, stubhub) where users can be buyers, sellers, both or none. &lt;/p&gt;\n\n&lt;p&gt;how would users get materialized into tables? how would the business model be properly reflected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1afrtle", "is_robot_indexable": true, "report_reasons": null, "author": "Final-Praline-275", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afrtle/marketplace_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afrtle/marketplace_data_model/", "subreddit_subscribers": 157294, "created_utc": 1706733048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know good ressources to get insights into famous products t architectures ?", "author_fullname": "t2_adlmr9gsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insight on famous techs tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afqsf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706730576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know good ressources to get insights into famous products t architectures ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afqsf5", "is_robot_indexable": true, "report_reasons": null, "author": "tech_curious27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "subreddit_subscribers": 157294, "created_utc": 1706730576.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}