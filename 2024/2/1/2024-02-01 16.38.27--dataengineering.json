{"kind": "Listing", "data": {"after": "t3_1afqsf5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Dagster Believes About Data Platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1afnd1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1HfcUuzrMpvBBY94JElFNYso1NQQ4RrLwGz_rUBvKb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706722293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?auto=webp&amp;s=94dcf8ce7019ce3e221a84088531c82d39c59478", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eea7cd5c0c3002c13b2398269a37e06cf911e3ae", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6982f3cf54d8a8754e64ee7e2c4bc544d1a5422c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74ed1228f650dddfb6ed29eb136a6b8f73ada0ac", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=80a10915705eaf3527c7d89450431f653a590f8d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fbe788ae92611b87791c735d2d71eb2e00e28b5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/lSg3RztVkw9k9cNY0mz5t02hS4bQaxvuRFQqCKWbYfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=006b1460ef750708d063f2e3c7670fde92270524", "width": 1080, "height": 567}], "variants": {}, "id": "9BPh1HBe8dpb9hwolf4ivC5UxBl9ds4I31wiMA6yM0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1afnd1d", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afnd1d/what_dagster_believes_about_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/what-dagster-believes-about-data-platforms", "subreddit_subscribers": 157442, "created_utc": 1706722293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far", "author_fullname": "t2_es1pfwhmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Hireable ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afxium", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706747301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afxium", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Series-2277", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afxium/most_hireable_etl_tools/", "subreddit_subscribers": 157442, "created_utc": 1706747301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite Python Data Processing Libraries? (PyArrow, Pandas, Polars, DuckDB, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflut4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706718628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What libraries do use most when writing ad-hoc data scripts, why? Any lesser known libraries you found very useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aflut4", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflut4/favorite_python_data_processing_libraries_pyarrow/", "subreddit_subscribers": 157442, "created_utc": 1706718628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just had a code challenge with meta. There were 5 python and 5 sql questions. I answered 3 questions correctly in both sections and ran out of time before even attempting the 4th. \n\nI misunderstood the 3rd sql questions so I kinda spent too much time trying to debug\n\nIs there a good chance I won\u2019t hear back? This is for L5 or L6", "author_fullname": "t2_nhotjt8ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meta code challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afwjt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706744711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just had a code challenge with meta. There were 5 python and 5 sql questions. I answered 3 questions correctly in both sections and ran out of time before even attempting the 4th. &lt;/p&gt;\n\n&lt;p&gt;I misunderstood the 3rd sql questions so I kinda spent too much time trying to debug&lt;/p&gt;\n\n&lt;p&gt;Is there a good chance I won\u2019t hear back? This is for L5 or L6&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1afwjt9", "is_robot_indexable": true, "report_reasons": null, "author": "Complex_Counter_6407", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afwjt9/meta_code_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afwjt9/meta_code_challenge/", "subreddit_subscribers": 157442, "created_utc": 1706744711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seeing that a lot of DE positions seem to be shifting towards analytics and layoffs are ongoing I want to stay competitive. I'm interested in backend engineering to avoid low code tools and SAAS solutions that have found their way into DE. I'm also considering platform engineering, but only because it seems more accessible. \n\nI'm planning to pick a language and build projects in my spare time, but realistically this won't get me above entry level and that's not a pay cut I can afford to take.\n\nI work for a big company and switching departments is not an easy process, but something I will consider in the future. Perhaps I should give consulting a shot? I'm not a fan of start ups so that's not an option. \n\nHow did others approach a similar career change?", "author_fullname": "t2_vsf3ige5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to retrain as SWE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afrpib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706732779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing that a lot of DE positions seem to be shifting towards analytics and layoffs are ongoing I want to stay competitive. I&amp;#39;m interested in backend engineering to avoid low code tools and SAAS solutions that have found their way into DE. I&amp;#39;m also considering platform engineering, but only because it seems more accessible. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to pick a language and build projects in my spare time, but realistically this won&amp;#39;t get me above entry level and that&amp;#39;s not a pay cut I can afford to take.&lt;/p&gt;\n\n&lt;p&gt;I work for a big company and switching departments is not an easy process, but something I will consider in the future. Perhaps I should give consulting a shot? I&amp;#39;m not a fan of start ups so that&amp;#39;s not an option. &lt;/p&gt;\n\n&lt;p&gt;How did others approach a similar career change?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1afrpib", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated_Ad_1108", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afrpib/best_way_to_retrain_as_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afrpib/best_way_to_retrain_as_swe/", "subreddit_subscribers": 157442, "created_utc": 1706732779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "#### Background\n\nMy company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. \n\n#### Question\nWhich library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? \n\n#### More Contexts\nOn the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. \n\nPreviously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. \n\n#### TLDR\nSo which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Portable and Cleanest Way to Integrate BigQuery with Python Dataframe Manipulation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afzpri", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706754014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706753532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h4&gt;Background&lt;/h4&gt;\n\n&lt;p&gt;My company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. &lt;/p&gt;\n\n&lt;h4&gt;Question&lt;/h4&gt;\n\n&lt;p&gt;Which library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? &lt;/p&gt;\n\n&lt;h4&gt;More Contexts&lt;/h4&gt;\n\n&lt;p&gt;On the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. &lt;/p&gt;\n\n&lt;p&gt;Previously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. &lt;/p&gt;\n\n&lt;h4&gt;TLDR&lt;/h4&gt;\n\n&lt;p&gt;So which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afzpri", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "subreddit_subscribers": 157442, "created_utc": 1706753532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does any company or data engineers really using Dremio?", "author_fullname": "t2_fta9agm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3wih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does any company or data engineers really using Dremio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3wih", "is_robot_indexable": true, "report_reasons": null, "author": "luqmancrit69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "subreddit_subscribers": 157442, "created_utc": 1706766620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?\n\nI see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don't use python for transforming data, there will be tons of replies questioning why not.\n\nHow are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).\n\nmaybe you just clean the data in python from ingestion -&gt; raw -&gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.\n\nI see so many ways to do this, and wondering how you all would approach this considering the constraints.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python-Only Data Pipeline Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afwt5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706745393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say all you had access to was an EC2 instance, python, S3, and Redshift Spectrum for serving S3 data to Power BI. How would you build it?&lt;/p&gt;\n\n&lt;p&gt;I see so many conflicting views on using python to transform data (excluding Pyspark of course). Pandas is there, but at what cost? Best alternatives would be Polars, Dask, and DuckDB. Hell, I see some of you guys saying you just use plain vanilla python, so my question on that is: what data structure are you using, and are you really writing all of the transformations from scratch? For every person who says don&amp;#39;t use python for transforming data, there will be tons of replies questioning why not.&lt;/p&gt;\n\n&lt;p&gt;How are you approaching design patterns for defining the table transformations? Would you use a template method with repeatable logic for reading and writing files, while having an abstract transformation method to be defined in a subclass dedicated to some table where you can write the table-specific transformations using (pandas/dask/duckdb/vanilla python).&lt;/p&gt;\n\n&lt;p&gt;maybe you just clean the data in python from ingestion -&amp;gt; raw -&amp;gt; cleansed. But then to actually perform the transformations you use external tables with redshift spectrum and create views. But now you have logic in two places.&lt;/p&gt;\n\n&lt;p&gt;I see so many ways to do this, and wondering how you all would approach this considering the constraints.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afwt5i", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afwt5i/pythononly_data_pipeline_suggestions/", "subreddit_subscribers": 157442, "created_utc": 1706745393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don't really see the point of using this extensive paid tool for basically doing just simple API and database calls. \n\nIs there a better tool/method for automating and managing a copy activity easily? (in Azure)", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Near realtime copy ELT with other tool than ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aflaae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706717177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually develop data platforms that periodically update using ADF for copying data from the source system to a data lake or data warehouse, and dbt for building the models. I have a client that wants near realtime refreshes. I think with ADF this would create a lot of costs, and in general I don&amp;#39;t really see the point of using this extensive paid tool for basically doing just simple API and database calls. &lt;/p&gt;\n\n&lt;p&gt;Is there a better tool/method for automating and managing a copy activity easily? (in Azure)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aflaae", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aflaae/near_realtime_copy_elt_with_other_tool_than_adf/", "subreddit_subscribers": 157442, "created_utc": 1706717177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey Data Engineering community,\n\nI'm currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I'm using SQL PostgreSQL to fetch products by type. I have a 'products' table with intrinsic information like title, URL, image URL, etc., and sub-tables such as 'books' and 'movies,' each containing specific fields for authors, editors, and movie details.\n\nHowever, I'm facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.\n\nThese products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.\n\nI've heard suggestions that a NoSQL database could be a good fit for this use case. As I'm working exclusively with Python, I'd appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.\n\nThanks a lot for your valuable insights!", "author_fullname": "t2_3mvaeyfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-commerce Products storing (SQL / no sql )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag7kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706781655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineering community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I&amp;#39;m using SQL PostgreSQL to fetch products by type. I have a &amp;#39;products&amp;#39; table with intrinsic information like title, URL, image URL, etc., and sub-tables such as &amp;#39;books&amp;#39; and &amp;#39;movies,&amp;#39; each containing specific fields for authors, editors, and movie details.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.&lt;/p&gt;\n\n&lt;p&gt;These products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard suggestions that a NoSQL database could be a good fit for this use case. As I&amp;#39;m working exclusively with Python, I&amp;#39;d appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your valuable insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ag7kh1", "is_robot_indexable": true, "report_reasons": null, "author": "Saa3dLfachil", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "subreddit_subscribers": 157442, "created_utc": 1706781655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. \n\nSo when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.\n\nFor running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.\n\nSo is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?", "author_fullname": "t2_txl4izdo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aforig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706725682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So currently at my company we are using snowflake, Python and DBT for data pipeline. We do not have used any orchestration tools for orchestrating. Instead we have 2 AWS EC2 instance and we have scheduled CRON jobs in these instances to process the data pipeline. &lt;/p&gt;\n\n&lt;p&gt;So when number of clients increased we had to process multiple clients parallelly so we thought of using AWS Fargate service. So what we did was we implemented Circle CI for creating container image of our pipeline and pushing it to AWS ECR service. And when ever we had to process multiple clients in parallel we created multiple ECS tasks and process it in fargate.&lt;/p&gt;\n\n&lt;p&gt;For running Fargate we have not used IAC tools like Terraform or cloud formation instead we have written python script using botto3 to create Fargate instance.&lt;/p&gt;\n\n&lt;p&gt;So is the above process CI/CD? From what i have researched CI/CD is mainly for removing server downtime and to automate pushing code with integration testing and unit testing  into multiple staging environments. It can be useful in software engineering but how can this be useful for data pipeline that has batch processing. I mean in our pipeline we already do data quality checks using dbt test feature. So what kind of tests can be implemented in our CI/CD pipeline(Currenlty we dont have any tests in CICD) we just build and push to ECR. Can you give me an example or how you are using CI CD for data pipeline? Or any tutorial or article so i can practice and be clear about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aforig", "is_robot_indexable": true, "report_reasons": null, "author": "__1l0__", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aforig/cicd_for_data_engineering/", "subreddit_subscribers": 157442, "created_utc": 1706725682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi guys,\n\nI recently came across this announce of databricks (see link below) and I became really interested in becoming certified as a databricks Generative AI Engineer.\n\nI guess they have not yet posted any info in regards of the dates when the certification will become available, but they mention (correct me if I am wrong) that a beta version is already available once you finish the learning pathway for this specialization.\n\nThis looks like maybe they put the full-fledged certification in the second quarter of the year?. What do you think?.\n\n[https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification](https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification)", "author_fullname": "t2_oonyiqxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Databricks certification and learning pathway for Generative AI Engineering.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afls0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706718428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I recently came across this announce of databricks (see link below) and I became really interested in becoming certified as a databricks Generative AI Engineer.&lt;/p&gt;\n\n&lt;p&gt;I guess they have not yet posted any info in regards of the dates when the certification will become available, but they mention (correct me if I am wrong) that a beta version is already available once you finish the learning pathway for this specialization.&lt;/p&gt;\n\n&lt;p&gt;This looks like maybe they put the full-fledged certification in the second quarter of the year?. What do you think?.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification\"&gt;https://www.databricks.com/blog/databricks-announces-industrys-first-generative-ai-engineer-learning-pathway-and-certification&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?auto=webp&amp;s=3b62e7f25e67dfcc535b34d650d9b8cfae8f582b", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdbdfd1f295bd67ba31b6556e5253c85e7f4f97d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fcd0a9fb3dd5aa169617f1d1d7bb39a27b7a4676", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30a0d7c33d06c0990f97c5f13441a3bfee56c3aa", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bb49290ec20b840662b7e554ecd7d60dbc7c06c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fcb833595d8aad2af89c50c13d0cf36556955a9", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_1tGejed1Bhx9fLosCkemVbjz1IIZpnjGp9owBI-8Y8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aedf47ac9db802230ce034ee4917568441966237", "width": 1080, "height": 565}], "variants": {}, "id": "JRDcv3JjHYyHvFgK73Z7LiAs-njaiY6mjeH-kcrGeMc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1afls0t", "is_robot_indexable": true, "report_reasons": null, "author": "Due_Percentage447", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afls0t/new_databricks_certification_and_learning_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afls0t/new_databricks_certification_and_learning_pathway/", "subreddit_subscribers": 157442, "created_utc": 1706718428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you, people, approach new projects?\n\nIn my experience, most data and analytics engineers do the following:\n\n1. Collect all the information they need initially\n2. Start digging and working in the background\n3. Occasionally ask additional questions\n4. Show everything when ready\n\nI don't like that approach. In my opinion, there are too many risks.\n\nInstead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bold Projects vs MVPs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3rhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you, people, approach new projects?&lt;/p&gt;\n\n&lt;p&gt;In my experience, most data and analytics engineers do the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Collect all the information they need initially&lt;/li&gt;\n&lt;li&gt;Start digging and working in the background&lt;/li&gt;\n&lt;li&gt;Occasionally ask additional questions&lt;/li&gt;\n&lt;li&gt;Show everything when ready&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I don&amp;#39;t like that approach. In my opinion, there are too many risks.&lt;/p&gt;\n\n&lt;p&gt;Instead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3rhm", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "subreddit_subscribers": 157442, "created_utc": 1706766104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don't want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.", "author_fullname": "t2_dla93s85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharing data with non-technical teams for review and approval", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag0ir4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706755856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don&amp;#39;t want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag0ir4", "is_robot_indexable": true, "report_reasons": null, "author": "FooFighter_V", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "subreddit_subscribers": 157442, "created_utc": 1706755856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on DE team that directly supports a DS team. Our tech stack can be summarized as:\n\n* airbyte populates clickhouse\n* dbt drives transformation in clickhouse\n* dagster orchestrates everything, airbyte dbt and everything before, in between, and after\n\nAside from metrics created with dbt, we often have final deliverables that need to be sent to customers via email, sftp, etc. These deliverables are often the results of pure sql, or python+sql. As the number of deliverables increases, so does the number of custom code solutions to send them where they need to go.\n\nI've done my best to separate report logic from delivery. The scientists/analysts have their own repo where logic is implemented, thus they can commit/merge as needed. The Dagster repo then installs their repo as a python package, and imports and runs their code.\n\nDoes anyone have any recommendation for managing this growth? Preferably in a way that is easy to extend and generic? Thanks!", "author_fullname": "t2_7ch0sgxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing deliverables (reports sent via email, sftp, etc)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afsbue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706734700.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706734270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on DE team that directly supports a DS team. Our tech stack can be summarized as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;airbyte populates clickhouse&lt;/li&gt;\n&lt;li&gt;dbt drives transformation in clickhouse&lt;/li&gt;\n&lt;li&gt;dagster orchestrates everything, airbyte dbt and everything before, in between, and after&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Aside from metrics created with dbt, we often have final deliverables that need to be sent to customers via email, sftp, etc. These deliverables are often the results of pure sql, or python+sql. As the number of deliverables increases, so does the number of custom code solutions to send them where they need to go.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done my best to separate report logic from delivery. The scientists/analysts have their own repo where logic is implemented, thus they can commit/merge as needed. The Dagster repo then installs their repo as a python package, and imports and runs their code.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any recommendation for managing this growth? Preferably in a way that is easy to extend and generic? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afsbue", "is_robot_indexable": true, "report_reasons": null, "author": "coreytrevorlahey69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afsbue/managing_deliverables_reports_sent_via_email_sftp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afsbue/managing_deliverables_reports_sent_via_email_sftp/", "subreddit_subscribers": 157442, "created_utc": 1706734270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reaching out to inquire if anyone would be available to answer a few questions regarding their job as a data engineer. I am currently working on a senior project and am in search of insightful sources. Your expertise would be immensely valuable.", "author_fullname": "t2_mq5pw09i9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agdgu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706800983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reaching out to inquire if anyone would be available to answer a few questions regarding their job as a data engineer. I am currently working on a senior project and am in search of insightful sources. Your expertise would be immensely valuable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1agdgu5", "is_robot_indexable": true, "report_reasons": null, "author": "HavenReign882", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agdgu5/data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agdgu5/data_engineer_interview/", "subreddit_subscribers": 157442, "created_utc": 1706800983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source and the Data Lakehouse: Apache Arrow, Apache Iceberg, Nessie and Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agce9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1706798072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1agce9z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agce9z/open_source_and_the_data_lakehouse_apache_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "subreddit_subscribers": 157442, "created_utc": 1706798072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.\n\nThe problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I'm thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our \"data warehouse\", and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.\n\nIs this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don't really have that much data in the OLTP database right now, it's about 140k rows.\n\n&amp;#x200B;\n\nEdit: I doesn't necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives for data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agbigy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706795682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.&lt;/p&gt;\n\n&lt;p&gt;The problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I&amp;#39;m thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our &amp;quot;data warehouse&amp;quot;, and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.&lt;/p&gt;\n\n&lt;p&gt;Is this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don&amp;#39;t really have that much data in the OLTP database right now, it&amp;#39;s about 140k rows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I doesn&amp;#39;t necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agbigy", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "subreddit_subscribers": 157442, "created_utc": 1706795682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Inverted Index Accelerates Text Searches by 40 Times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag9d5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aSzTPtPR4Bnf-RenDuLOONd0s8fx2zJtjNC5F-6Ou40.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706788830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8cgFwMcQP6e_xw4EOZsFJEALXdN4GxHUgeRa0pSo4Xs.jpg?auto=webp&amp;s=13f22b6464f35cae33eb8922b6d83cf46dcf67c0", "width": 900, "height": 384}, "resolutions": [{"url": "https://external-preview.redd.it/8cgFwMcQP6e_xw4EOZsFJEALXdN4GxHUgeRa0pSo4Xs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=238c1387bb220208e6e3b184f37b11f5c2646cde", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/8cgFwMcQP6e_xw4EOZsFJEALXdN4GxHUgeRa0pSo4Xs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4664d64ff1360bc0a6cfe68a2d79769ffdcbc59e", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/8cgFwMcQP6e_xw4EOZsFJEALXdN4GxHUgeRa0pSo4Xs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5772f7783614b66c087c645550f0f2d3e5b8962a", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/8cgFwMcQP6e_xw4EOZsFJEALXdN4GxHUgeRa0pSo4Xs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=88e1b42d2b04759034f83a0c24be03895eb1946f", "width": 640, "height": 273}], "variants": {}, "id": "iOhf2y7JEafksFS9aUU2chajDqumhQJ3zMwnCjqQ9Qs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ag9d5p", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag9d5p/how_inverted_index_accelerates_text_searches_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris", "subreddit_subscribers": 157442, "created_utc": 1706788830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn't find anything resembling what I wanted to get. I want to get metadata rather than the data itself.", "author_fullname": "t2_mekatj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to extract data sources from a PBI dashboard in .pbix?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag9anf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706788577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn&amp;#39;t find anything resembling what I wanted to get. I want to get metadata rather than the data itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag9anf", "is_robot_indexable": true, "report_reasons": null, "author": "thiophosgene", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "subreddit_subscribers": 157442, "created_utc": 1706788577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is fundamental to databases like Clickhouse, Pinot or Druid that the same can't be achieved using MPP?   \nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I'm thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?  \nI want an understanding of why MPP's don't fit well for user analytical use cases?", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Does MPP's not support high QPS, low latency queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag4ehz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706768368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is fundamental to databases like Clickhouse, Pinot or Druid that the same can&amp;#39;t be achieved using MPP?&lt;br/&gt;\nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I&amp;#39;m thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?&lt;br/&gt;\nI want an understanding of why MPP&amp;#39;s don&amp;#39;t fit well for user analytical use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag4ehz", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "subreddit_subscribers": 157442, "created_utc": 1706768368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some free time and wanted to build up my profile/portfolio for Data Engineering jobs. So I decided to do a personal project but for the life of me can't figure out a fun one to do. I came up with/was inspired to implement the following idea:\n\n* Twitter is scraped for data regarding a specific keyword for a certain time period.\n* Data is then stored in mongoDB (for data persistence)\n* This data would then be given to a service that conducts sentiment analysis. I'm thinking of utilizing LLMs for this task as I have dipped my toes in it. I know doing sentiment analysis on such large amounts of data would incur rate limits or massive costs but I'm just spit-balling here. I could alternatively use machine learning models for this. \n* Either way before doing the analysis, data would have to be cleaned (Pyspark?)\n* As a result, I would have sentiments on a per-day-basis regarding that keyword which I could show in a nice graph on a front-end service. (This same service would be used to take in that keyword in the first place)\n* Idk whether in-between services, Kafka could be utilized since this isn't real time.\n\nI know this might have terrible logic that's why I'm posting here (advice?). But also, the other problem is that someone told me doing Twitter Sentiment Analysis is run-of-the-mill and its equivalent to \\`Titanic.csv\\` for Data Science learning. So this project might not stand out and I don't know what to do :/\n\nAny advice would be greatly appreciated as to what I can do with this or something else I can work with.", "author_fullname": "t2_4w6ebksa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help figuring out a personal project for portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3efm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706764859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some free time and wanted to build up my profile/portfolio for Data Engineering jobs. So I decided to do a personal project but for the life of me can&amp;#39;t figure out a fun one to do. I came up with/was inspired to implement the following idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Twitter is scraped for data regarding a specific keyword for a certain time period.&lt;/li&gt;\n&lt;li&gt;Data is then stored in mongoDB (for data persistence)&lt;/li&gt;\n&lt;li&gt;This data would then be given to a service that conducts sentiment analysis. I&amp;#39;m thinking of utilizing LLMs for this task as I have dipped my toes in it. I know doing sentiment analysis on such large amounts of data would incur rate limits or massive costs but I&amp;#39;m just spit-balling here. I could alternatively use machine learning models for this. &lt;/li&gt;\n&lt;li&gt;Either way before doing the analysis, data would have to be cleaned (Pyspark?)&lt;/li&gt;\n&lt;li&gt;As a result, I would have sentiments on a per-day-basis regarding that keyword which I could show in a nice graph on a front-end service. (This same service would be used to take in that keyword in the first place)&lt;/li&gt;\n&lt;li&gt;Idk whether in-between services, Kafka could be utilized since this isn&amp;#39;t real time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I know this might have terrible logic that&amp;#39;s why I&amp;#39;m posting here (advice?). But also, the other problem is that someone told me doing Twitter Sentiment Analysis is run-of-the-mill and its equivalent to `Titanic.csv` for Data Science learning. So this project might not stand out and I don&amp;#39;t know what to do :/&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated as to what I can do with this or something else I can work with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3efm", "is_robot_indexable": true, "report_reasons": null, "author": "SAAD_3XK", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag3efm/need_help_figuring_out_a_personal_project_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3efm/need_help_figuring_out_a_personal_project_for/", "subreddit_subscribers": 157442, "created_utc": 1706764859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don\u2019t know if this is the correct place to post this but here goes.\n\nI\u2019ve been storing large amounts of data in txt files and whenever it needs something it splits the whole file by line using .split(\u201c\\n\u201d) and reads the file with fs.readFile. What programs/methods should I use to speed up processing speeds without buying actual servers. Also I\u2019m aware that this is a bad way of storing data I just haven\u2019t researched it that much", "author_fullname": "t2_reg4lo4zv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to store amd retrieve data using node js?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag0wb4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706756972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don\u2019t know if this is the correct place to post this but here goes.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been storing large amounts of data in txt files and whenever it needs something it splits the whole file by line using .split(\u201c\\n\u201d) and reads the file with fs.readFile. What programs/methods should I use to speed up processing speeds without buying actual servers. Also I\u2019m aware that this is a bad way of storing data I just haven\u2019t researched it that much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag0wb4", "is_robot_indexable": true, "report_reasons": null, "author": "ArmiliteRifle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag0wb4/whats_the_best_way_to_store_amd_retrieve_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag0wb4/whats_the_best_way_to_store_amd_retrieve_data/", "subreddit_subscribers": 157442, "created_utc": 1706756972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi! i\u2019m wondering if anyone has any advice / best practices on designing a data model for a 2-sided business (uber, airbnb, doordash, stubhub) where users can be buyers, sellers, both or none. \n\nhow would users get materialized into tables? how would the business model be properly reflected?", "author_fullname": "t2_tu15i208", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Marketplace Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afrtle", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706733048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi! i\u2019m wondering if anyone has any advice / best practices on designing a data model for a 2-sided business (uber, airbnb, doordash, stubhub) where users can be buyers, sellers, both or none. &lt;/p&gt;\n\n&lt;p&gt;how would users get materialized into tables? how would the business model be properly reflected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1afrtle", "is_robot_indexable": true, "report_reasons": null, "author": "Final-Praline-275", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afrtle/marketplace_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afrtle/marketplace_data_model/", "subreddit_subscribers": 157442, "created_utc": 1706733048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know good ressources to get insights into famous products t architectures ?", "author_fullname": "t2_adlmr9gsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insight on famous techs tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afqsf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706730576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know good ressources to get insights into famous products t architectures ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1afqsf5", "is_robot_indexable": true, "report_reasons": null, "author": "tech_curious27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afqsf5/insight_on_famous_techs_tech/", "subreddit_subscribers": 157442, "created_utc": 1706730576.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}