{"kind": "Listing", "data": {"after": "t3_1aj5dgd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been using DBT working for a startup company for a few years &amp; think its an amazing tool for anyone who works with SQL databases.  I'm starting to see how DBT could be used as a backend for a full headless BI system, if you leverages things like defining your data as a Semantic Model. \n\nI know DBT offers a lot, like [Source Yamls](https://docs.getdbt.com/docs/build/sources), [Model Property Yamls](https://docs.getdbt.com/reference/model-properties), [Semantic Models](https://docs.getdbt.com/docs/build/semantic-models), [Exposures](https://docs.getdbt.com/docs/build/exposures), etc.   \n\n\n1. For those that you use, how do you maintain them at scale? Who manages the creation and editing of model data like descriptions and such?\n2. Why don't you use the other ones? is it becuase its too tedious to maintain at scale? \n3. Anyone doing anything custom with Tags or Groups? User authentication or Row-Level-Security? \n\nSecretly here I'm trying to validate a SAAS idea. How many other people even use DBT? Let-alone the numerious use cases and special files you need to maintain. I've always thought  a UI where you could successfully CRUD (Create, Read, Update, Delete) all your DBT assets would be great. As I learn more about software development, I see this as an opportunity. Wondering if anyone else has pain points with DBT and if so what are they?", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People who use DBT (Data Build Tool) - what are you using it for? Do you use ALL the components like Tests, Semantic Models, Exposures, ...etc?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj3fac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707093490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using DBT working for a startup company for a few years &amp;amp; think its an amazing tool for anyone who works with SQL databases.  I&amp;#39;m starting to see how DBT could be used as a backend for a full headless BI system, if you leverages things like defining your data as a Semantic Model. &lt;/p&gt;\n\n&lt;p&gt;I know DBT offers a lot, like &lt;a href=\"https://docs.getdbt.com/docs/build/sources\"&gt;Source Yamls&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/reference/model-properties\"&gt;Model Property Yamls&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/docs/build/semantic-models\"&gt;Semantic Models&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/docs/build/exposures\"&gt;Exposures&lt;/a&gt;, etc.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For those that you use, how do you maintain them at scale? Who manages the creation and editing of model data like descriptions and such?&lt;/li&gt;\n&lt;li&gt;Why don&amp;#39;t you use the other ones? is it becuase its too tedious to maintain at scale? &lt;/li&gt;\n&lt;li&gt;Anyone doing anything custom with Tags or Groups? User authentication or Row-Level-Security? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Secretly here I&amp;#39;m trying to validate a SAAS idea. How many other people even use DBT? Let-alone the numerious use cases and special files you need to maintain. I&amp;#39;ve always thought  a UI where you could successfully CRUD (Create, Read, Update, Delete) all your DBT assets would be great. As I learn more about software development, I see this as an opportunity. Wondering if anyone else has pain points with DBT and if so what are they?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj3fac", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/", "subreddit_subscribers": 158499, "created_utc": 1707093490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alternative title: What exactly is the point of debezium?\n\nRight now I have a postgres database in which my own code sends all changes to certain tables and publishes the changes to kafka. This is is the rough flow:\n\n1. Insert/update/delete data from certain tables usings `returning *` as appropriate.\n2. Create a custom json payload describing what happened.\n3. Send that payload to a Kafka topic.\n\nThe current setup has worked more or less fine, but it's a bit of a hassle and can get more complicated with different kinds of actions (e.g. upserts). Also it has the obvious downside that I actively need to do this for whatever tables I care about instead of having it happen magically.\n\nA better approach would be something that uses postgres replication to send out the data. An option I've looked into is using wal2json ( https://github.com/eulerto/wal2json ) and then sending the output straight to Kafka after that. Of course this makes me wonder if I should instead just move all the way away from my custom work and just use something off the shelf. Google has eventually led me to debezium. Is that something I should seriously consider? Honestly it looks to me to be quite confusing. I've looked at the output from wal2json and that already feels like enough. I personally don't even need to use something like Kafka connect and don't mind sending the changes in using a regular publication to Kafka (that's already simpler that what I've been doing until now), but am I just being naive? Am I fool to skip debezium and tie things together myself?", "author_fullname": "t2_f6kbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use debezium or reinvent the wheel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajf01q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707134954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alternative title: What exactly is the point of debezium?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a postgres database in which my own code sends all changes to certain tables and publishes the changes to kafka. This is is the rough flow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert/update/delete data from certain tables usings &lt;code&gt;returning *&lt;/code&gt; as appropriate.&lt;/li&gt;\n&lt;li&gt;Create a custom json payload describing what happened.&lt;/li&gt;\n&lt;li&gt;Send that payload to a Kafka topic.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The current setup has worked more or less fine, but it&amp;#39;s a bit of a hassle and can get more complicated with different kinds of actions (e.g. upserts). Also it has the obvious downside that I actively need to do this for whatever tables I care about instead of having it happen magically.&lt;/p&gt;\n\n&lt;p&gt;A better approach would be something that uses postgres replication to send out the data. An option I&amp;#39;ve looked into is using wal2json ( &lt;a href=\"https://github.com/eulerto/wal2json\"&gt;https://github.com/eulerto/wal2json&lt;/a&gt; ) and then sending the output straight to Kafka after that. Of course this makes me wonder if I should instead just move all the way away from my custom work and just use something off the shelf. Google has eventually led me to debezium. Is that something I should seriously consider? Honestly it looks to me to be quite confusing. I&amp;#39;ve looked at the output from wal2json and that already feels like enough. I personally don&amp;#39;t even need to use something like Kafka connect and don&amp;#39;t mind sending the changes in using a regular publication to Kafka (that&amp;#39;s already simpler that what I&amp;#39;ve been doing until now), but am I just being naive? Am I fool to skip debezium and tie things together myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?auto=webp&amp;s=43601c671c6cca7248f3360f807500661f2ca84b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67f5e0b319d668c71bf1426b14510f187d5d7067", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25bd7a73f37202a42636e7d7ae74e54650f6b75b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2336d105e1717a3cfd01046bf4f2507be44e4c9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a02875797173ca04355e598604a6caa35a27f533", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=772a75332d16afc3ac814cdb5a5a2b43a01af790", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a1d58f5c0fa146441a97baf245586602c5714cf", "width": 1080, "height": 540}], "variants": {}, "id": "B4qAodveQeFwIQVpvTspEI_OVM_nNQVrpVn-TeCZtEg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajf01q", "is_robot_indexable": true, "report_reasons": null, "author": "ApproximateIdentity", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajf01q/use_debezium_or_reinvent_the_wheel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajf01q/use_debezium_or_reinvent_the_wheel/", "subreddit_subscribers": 158499, "created_utc": 1707134954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I don't have much experience in data engineering, so I'm here to ask for advice. I am working on a project which consists of building a dashboard for the IT department of a bank. the dashboard should present information from log data. Log data includes security vulnerabilities, issues reported by the help desk, and logs showing who is working on those issues. The data includes information such as description of issues, when they are reported, which device is affected.... Data is provided via an internal API (I don't believe it provides real-time data streaming). I want to create a data pipeline that extracts this data, transforms it, loads it into a database, and then creates a dashboard from it. Normally this pipeline should run once a day. so I think an ETL should work fine. I was thinking of using Python and Pandas to perform ETL since the data is not very large.\nThe challenge is that alongside this ETL (which should be scheduled to run once a day), I want to achieve this functionality: If a critical issue is reported (server is down, high risk security vulnerability , ...) The IT department must be notified immediately (via the dashboard). How to implement such a pipeline. The data pipeline and dashboard must be deployed internally (no cloud services). Can you help me choose the right tools and give me some tips for designing this pipeline. THANKS.", "author_fullname": "t2_54yh79tez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me design a data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajanxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707117083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I don&amp;#39;t have much experience in data engineering, so I&amp;#39;m here to ask for advice. I am working on a project which consists of building a dashboard for the IT department of a bank. the dashboard should present information from log data. Log data includes security vulnerabilities, issues reported by the help desk, and logs showing who is working on those issues. The data includes information such as description of issues, when they are reported, which device is affected.... Data is provided via an internal API (I don&amp;#39;t believe it provides real-time data streaming). I want to create a data pipeline that extracts this data, transforms it, loads it into a database, and then creates a dashboard from it. Normally this pipeline should run once a day. so I think an ETL should work fine. I was thinking of using Python and Pandas to perform ETL since the data is not very large.\nThe challenge is that alongside this ETL (which should be scheduled to run once a day), I want to achieve this functionality: If a critical issue is reported (server is down, high risk security vulnerability , ...) The IT department must be notified immediately (via the dashboard). How to implement such a pipeline. The data pipeline and dashboard must be deployed internally (no cloud services). Can you help me choose the right tools and give me some tips for designing this pipeline. THANKS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajanxk", "is_robot_indexable": true, "report_reasons": null, "author": "3Ammar404", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajanxk/help_me_design_a_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajanxk/help_me_design_a_data_pipeline/", "subreddit_subscribers": 158499, "created_utc": 1707117083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "People who work(ed) with finance teams, can you please tell me what problems you solve(d) for them and the solutions you've built ?\nI'd love your inputs and hear your stories !\nThanks\n\nMine is:\nBeen \"servicing\" finance teams in banks and SMEs for the past couple of years - I often build pipelines to ingest their CSV's (bank statements, accounting extracts), display bank accounts and transactions, reconcile accounting entries to product database events, calculate financial metrics (ARR/MRR, cohort views etc) and build lightweight visulisations (with option to export back to excel). Often times the biggest issue is teaching them how to self-serve and finding out where the data they need lives.", "author_fullname": "t2_ilw3j5ci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE for Finance teams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aitre5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707069043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who work(ed) with finance teams, can you please tell me what problems you solve(d) for them and the solutions you&amp;#39;ve built ?\nI&amp;#39;d love your inputs and hear your stories !\nThanks&lt;/p&gt;\n\n&lt;p&gt;Mine is:\nBeen &amp;quot;servicing&amp;quot; finance teams in banks and SMEs for the past couple of years - I often build pipelines to ingest their CSV&amp;#39;s (bank statements, accounting extracts), display bank accounts and transactions, reconcile accounting entries to product database events, calculate financial metrics (ARR/MRR, cohort views etc) and build lightweight visulisations (with option to export back to excel). Often times the biggest issue is teaching them how to self-serve and finding out where the data they need lives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aitre5", "is_robot_indexable": true, "report_reasons": null, "author": "GiacomoLeopardi6", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aitre5/de_for_finance_teams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aitre5/de_for_finance_teams/", "subreddit_subscribers": 158499, "created_utc": 1707069043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today, I have a relatively small set of data living on S3 in parquet format with a basic hive partitioning format on year, month, day, and hour covering 3 years with data associated to a time and ID.  Data is produced on an hourly basis. A common query is \"give me the latest data\" which fits well into this partition scheme. However another common use case is \"give me all the data for this ID over several months/years\". This requires reading thousands of objects which isn't terribly expensive or slow, but not exactly ideal either. Better would be if I could have a second set of data partitioned less granular on time and taking into account the ID of the data, updated either continually or on a set schedule.\n\nI have been reading about Apache Iceberg, Hudi, and DeltaLake but all seem pretty rigid in their partitioning scheme. Obviously I could roll my own with a little work, but I would like to fit into an existing ecosystem. Is there something in Iceberg or another format that can handle this type of layout?", "author_fullname": "t2_39r4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "multi partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj2j4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707091043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, I have a relatively small set of data living on S3 in parquet format with a basic hive partitioning format on year, month, day, and hour covering 3 years with data associated to a time and ID.  Data is produced on an hourly basis. A common query is &amp;quot;give me the latest data&amp;quot; which fits well into this partition scheme. However another common use case is &amp;quot;give me all the data for this ID over several months/years&amp;quot;. This requires reading thousands of objects which isn&amp;#39;t terribly expensive or slow, but not exactly ideal either. Better would be if I could have a second set of data partitioned less granular on time and taking into account the ID of the data, updated either continually or on a set schedule.&lt;/p&gt;\n\n&lt;p&gt;I have been reading about Apache Iceberg, Hudi, and DeltaLake but all seem pretty rigid in their partitioning scheme. Obviously I could roll my own with a little work, but I would like to fit into an existing ecosystem. Is there something in Iceberg or another format that can handle this type of layout?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aj2j4f", "is_robot_indexable": true, "report_reasons": null, "author": "evilryry", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj2j4f/multi_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj2j4f/multi_partitioning/", "subreddit_subscribers": 158499, "created_utc": 1707091043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nI\u2019m a few weeks into my first data engineering gig and really enjoying it.  I\u2019m learning so much and getting the chance to work on really cool things.\n\nOne thing I was hoping to get peoples\u2019 thoughts on.  My manager doesn\u2019t know much about my work, what it will take to do it or how long it will take.  It\u2019s not in his domain.  I\u2019m mostly self-learning and doing things on my own.  I\u2019m a hard worker and am confident that I will do a good job.  But I know that I need to come across to him as competent, make him know that I am achieving and make him aware of my achievements, basically manage his expectations and uphold my reputation as a good worker.  This, in particular because he doesn\u2019t have a point of reference into how well I am doing from the outside; mostly it will come from what I inform him, and of what I tell him is possible.  \n\nI know people may find it repugnant to think about things like this, the \u2018optics\u2019, I was wondering if anyone might have advice on how to manage this.  How to come across well.  How to keep a good reputation for a manager who may not have insight into the nitty gritty details of what my actual work may consist of.  I\u2019ve been working long hours to \u2018be doing as much as possible\u2019 to come across as competent, but I\u2019m realizing it might be more efficient in the long run to work more regular hours and then just manage my reputation more effectively.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maintaining a Reputation of Competence with my Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj0b4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707085175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a few weeks into my first data engineering gig and really enjoying it.  I\u2019m learning so much and getting the chance to work on really cool things.&lt;/p&gt;\n\n&lt;p&gt;One thing I was hoping to get peoples\u2019 thoughts on.  My manager doesn\u2019t know much about my work, what it will take to do it or how long it will take.  It\u2019s not in his domain.  I\u2019m mostly self-learning and doing things on my own.  I\u2019m a hard worker and am confident that I will do a good job.  But I know that I need to come across to him as competent, make him know that I am achieving and make him aware of my achievements, basically manage his expectations and uphold my reputation as a good worker.  This, in particular because he doesn\u2019t have a point of reference into how well I am doing from the outside; mostly it will come from what I inform him, and of what I tell him is possible.  &lt;/p&gt;\n\n&lt;p&gt;I know people may find it repugnant to think about things like this, the \u2018optics\u2019, I was wondering if anyone might have advice on how to manage this.  How to come across well.  How to keep a good reputation for a manager who may not have insight into the nitty gritty details of what my actual work may consist of.  I\u2019ve been working long hours to \u2018be doing as much as possible\u2019 to come across as competent, but I\u2019m realizing it might be more efficient in the long run to work more regular hours and then just manage my reputation more effectively.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj0b4n", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj0b4n/maintaining_a_reputation_of_competence_with_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj0b4n/maintaining_a_reputation_of_competence_with_my/", "subreddit_subscribers": 158499, "created_utc": 1707085175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jboiz9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Between a Streaming Database and a Stream Processing Framework in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajfeae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05zVvmxAitSOjhR5DLpwTDVEOvyC5VqPCjAenG-1FEU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707136299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "learn.glassflow.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://learn.glassflow.dev/blog/articles/choosing-between-a-streaming-database-and-a-stream-processing-framework-in-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?auto=webp&amp;s=38d16cb2cdd17d4596f2f6d399fb2a357c905601", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a971d3b1b3f7d7ce2606900c292994adb8666a1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e6bf4ef25d2663869d8e324444d234841bfa1bc", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1041bf37e9ef71d655d43ca5359ac224ac7dd7a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=18c5b4041e6bb531d5062ea233a810821f70e8c9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d66da090a45917586b43b0b3caeae6f7185e729d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa7ce18f1da053bbc07010c06fdf17da21f54aa0", "width": 1080, "height": 540}], "variants": {}, "id": "UquCtzVxSiovsutvPzya49pQXfcdwUIPnz3w2HP1VXo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajfeae", "is_robot_indexable": true, "report_reasons": null, "author": "bumurzokov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajfeae/choosing_between_a_streaming_database_and_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://learn.glassflow.dev/blog/articles/choosing-between-a-streaming-database-and-a-stream-processing-framework-in-python", "subreddit_subscribers": 158499, "created_utc": 1707136299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi colleagues!\n\nQuestion:\n\nHave you encountered a single-node processing engine (akin to DuckDB, Polars, Pandas) that can work with a metastore (Hive/Glue/Unity catalog) instead of raw file system?\n\nBackground:\n\nI am investigating single-node data processing engines (DuckDB and Polars) as an alternative to pushing every workload to a Spark cluster. As I am looking at benchmarks it makes sense to run smaller workloads (let's say 10-100 GB) on a single-machine-centric engine, and only use Spark when the workload needs to be pushed to a cluster.\n\nDistributed systems are great, but when a company only needs to process a small amount of data outside of the main batch we either run into cost issues (cluster is idling) or incur the overheads of spark on demand (Spark pool or Glue will take minutes to start processing 100 row file). \n\nAt the same time I prefer to manage my files as tables through a metastore (I believe this makes more sense for access control and ease of use for non-technical users).\n\nI don't think running a single-node Spark cluster makes sense (again, judging by benchmarks DuckDB and Polars win) as not every job will grow to a cluster-worthy size, and even when it does migrating SQL or Dataframe-based logic to Spark seems to be trivial.\n\nThis is a summary of my thoughts on running a hybrid system:\n\n* Pros\n   * Faster runtime for smaller workloads\n   * Simplified job development\n   * Maybe we will never need Spark\n   * Lower cost (even if a cluster exists, it does not have to run for every workload)\n* Cons\n   * May require migration if the data size grows \n   * Additional components in the data platform increase the complexity\n\nI'd be happy to discuss my assumptions in the comments.", "author_fullname": "t2_f8lq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A single-node processing engine with a metastore connection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajdk5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707129501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi colleagues!&lt;/p&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;Have you encountered a single-node processing engine (akin to DuckDB, Polars, Pandas) that can work with a metastore (Hive/Glue/Unity catalog) instead of raw file system?&lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;I am investigating single-node data processing engines (DuckDB and Polars) as an alternative to pushing every workload to a Spark cluster. As I am looking at benchmarks it makes sense to run smaller workloads (let&amp;#39;s say 10-100 GB) on a single-machine-centric engine, and only use Spark when the workload needs to be pushed to a cluster.&lt;/p&gt;\n\n&lt;p&gt;Distributed systems are great, but when a company only needs to process a small amount of data outside of the main batch we either run into cost issues (cluster is idling) or incur the overheads of spark on demand (Spark pool or Glue will take minutes to start processing 100 row file). &lt;/p&gt;\n\n&lt;p&gt;At the same time I prefer to manage my files as tables through a metastore (I believe this makes more sense for access control and ease of use for non-technical users).&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think running a single-node Spark cluster makes sense (again, judging by benchmarks DuckDB and Polars win) as not every job will grow to a cluster-worthy size, and even when it does migrating SQL or Dataframe-based logic to Spark seems to be trivial.&lt;/p&gt;\n\n&lt;p&gt;This is a summary of my thoughts on running a hybrid system:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pros\n\n&lt;ul&gt;\n&lt;li&gt;Faster runtime for smaller workloads&lt;/li&gt;\n&lt;li&gt;Simplified job development&lt;/li&gt;\n&lt;li&gt;Maybe we will never need Spark&lt;/li&gt;\n&lt;li&gt;Lower cost (even if a cluster exists, it does not have to run for every workload)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Cons\n\n&lt;ul&gt;\n&lt;li&gt;May require migration if the data size grows &lt;/li&gt;\n&lt;li&gt;Additional components in the data platform increase the complexity&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d be happy to discuss my assumptions in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajdk5y", "is_robot_indexable": true, "report_reasons": null, "author": "kirilscherbach", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajdk5y/a_singlenode_processing_engine_with_a_metastore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajdk5y/a_singlenode_processing_engine_with_a_metastore/", "subreddit_subscribers": 158499, "created_utc": 1707129501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have observed previous discussions in this group regarding the subpar data quality originating from Pendo. Personally, I encountered challenges when attempting to link feature usage with the customer utilizing it.\nTo address this issue, our engineers integrated tracking directly into the software. However, executing this task correctly is intricate and requires ongoing complexity management. This includes meeting requirements for handling streaming (real-time) and deduplication of events, implementing native in-product queries, and providing continuous engineering support.\nI'm keen to learn from others in this group who have developed similar in-house systems for tracking product usage natively. \n\nIf you have experience building such a system, could you please share insights into the system you developed? Specifically, I'm interested in understanding the infrastructure set-up, data movement, monitoring, recovery, and any other relevant details involved in this process.", "author_fullname": "t2_acwc2csg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In-House Solutions for Product Usage Tracking in Response to Pendo's Limitations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajhy8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707143735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have observed previous discussions in this group regarding the subpar data quality originating from Pendo. Personally, I encountered challenges when attempting to link feature usage with the customer utilizing it.\nTo address this issue, our engineers integrated tracking directly into the software. However, executing this task correctly is intricate and requires ongoing complexity management. This includes meeting requirements for handling streaming (real-time) and deduplication of events, implementing native in-product queries, and providing continuous engineering support.\nI&amp;#39;m keen to learn from others in this group who have developed similar in-house systems for tracking product usage natively. &lt;/p&gt;\n\n&lt;p&gt;If you have experience building such a system, could you please share insights into the system you developed? Specifically, I&amp;#39;m interested in understanding the infrastructure set-up, data movement, monitoring, recovery, and any other relevant details involved in this process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajhy8f", "is_robot_indexable": true, "report_reasons": null, "author": "No_Way_1569", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajhy8f/inhouse_solutions_for_product_usage_tracking_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajhy8f/inhouse_solutions_for_product_usage_tracking_in/", "subreddit_subscribers": 158499, "created_utc": 1707143735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m392e2dd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dirty Data Destroys Dependability: Respecting your customers\u2019 privacy with a data clean room", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajfzn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bNPwwA6_m4zokWNPt_D5ys0qFDkRQGJ59Eqsyffvn-k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707138176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "segment.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://segment.com/blog/respect-customer-privacy-with-a-data-clean-room/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?auto=webp&amp;s=ee522541dbf4d1ed891e78e82ad75034a01774a3", "width": 2990, "height": 1640}, "resolutions": [{"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b573256ed860e2d27391fea413928e97503f0a2b", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=402be54526784751ae10351da38551a9c4d74346", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0e12890321d240c9e624e691a84b3f733a9c57a", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14419dcf321f665b3c32879cce59586c2fc07e89", "width": 640, "height": 351}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5908e7ec1637f95d3428874885b62dff0b38b37", "width": 960, "height": 526}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8615c442df116ff37f44be1dc157f13b77c7be5e", "width": 1080, "height": 592}], "variants": {}, "id": "j9MFOxs_SqxMy5ZGbfRX-WrppWWG92LmHhHg5_lV_1I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajfzn6", "is_robot_indexable": true, "report_reasons": null, "author": "sunher444", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajfzn6/dirty_data_destroys_dependability_respecting_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://segment.com/blog/respect-customer-privacy-with-a-data-clean-room/", "subreddit_subscribers": 158499, "created_utc": 1707138176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nWe are trying to build awareness for Information and Data Governance in my company.\n\nDoes anyone have experience in using short (2 - 3 mins)  humorous training/compliance videos for Data Governance topics, such as the videos that Mimecast produce for Cyber Security to build awareness/knowledge..\n\nIs there a company like Mimecast which specialises in this type of videos\n\nThank you", "author_fullname": "t2_tj09r5oxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Governance Training /Awareness Videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj7tli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707106966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;We are trying to build awareness for Information and Data Governance in my company.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience in using short (2 - 3 mins)  humorous training/compliance videos for Data Governance topics, such as the videos that Mimecast produce for Cyber Security to build awareness/knowledge..&lt;/p&gt;\n\n&lt;p&gt;Is there a company like Mimecast which specialises in this type of videos&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj7tli", "is_robot_indexable": true, "report_reasons": null, "author": "TTwins1983", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj7tli/data_governance_training_awareness_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj7tli/data_governance_training_awareness_videos/", "subreddit_subscribers": 158499, "created_utc": 1707106966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently stumbled across this whitepaper, [Introduction to the SQLBI Methodology](https://www.sqlbi.com/wp-content/uploads/Introduction-to-SQLBI-Methodology-draft-1.0.pdf), and think it's an under-appreciated gem. It's over 15 years old and has a heavy focus on Microsoft technologies; however, while the specific technology stack may not be relevant to your job today, the underlying principles most certainly are. As a side note, the authors are not native English speakers, so some of the wording can be strange. But the content is S-tier.\n\nIf you're wondering whether to read \"Fundamentals of Data Engineering\" or take Zach Wilson's data engineering course to uplevel your skillset, I highly recommend you first check out this resource.", "author_fullname": "t2_c9g6hufqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Concise guide to building a data warehouse and BI platform from 2008", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ajkxsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707151394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently stumbled across this whitepaper, &lt;a href=\"https://www.sqlbi.com/wp-content/uploads/Introduction-to-SQLBI-Methodology-draft-1.0.pdf\"&gt;Introduction to the SQLBI Methodology&lt;/a&gt;, and think it&amp;#39;s an under-appreciated gem. It&amp;#39;s over 15 years old and has a heavy focus on Microsoft technologies; however, while the specific technology stack may not be relevant to your job today, the underlying principles most certainly are. As a side note, the authors are not native English speakers, so some of the wording can be strange. But the content is S-tier.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re wondering whether to read &amp;quot;Fundamentals of Data Engineering&amp;quot; or take Zach Wilson&amp;#39;s data engineering course to uplevel your skillset, I highly recommend you first check out this resource.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajkxsz", "is_robot_indexable": true, "report_reasons": null, "author": "carlineng_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ajkxsz/concise_guide_to_building_a_data_warehouse_and_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajkxsz/concise_guide_to_building_a_data_warehouse_and_bi/", "subreddit_subscribers": 158499, "created_utc": 1707151394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Contracts For Real? Or Just More Snake Oil?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": true, "name": "t3_1ajkler", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6b3zou_2jsC6FNxOxJMhQrvPZ8dEQlPUKRGQjO1dsgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707150561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/are-data-contracts-for-real", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?auto=webp&amp;s=3e1b050fccac006516a1a2c332eb6255b319220b", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8601c0ed11b0d368d4581f81d516ab8fff076557", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db94a83d7bfe8775bc034375ec1bb552b1227737", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc8d30b84348d00bc0492d6ec5a3612280f30e6a", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9590c4cc70149cc60627292a255205f2c7f576a2", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e9e0d05c2cba238cc12d8629a015d76cf192d10", "width": 960, "height": 562}], "variants": {}, "id": "fg1n0IgkWneo7yi4bF4A_kaeSju7jPEgYsY6gRK-JKI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajkler", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajkler/are_data_contracts_for_real_or_just_more_snake_oil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/are-data-contracts-for-real", "subreddit_subscribers": 158499, "created_utc": 1707150561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically title. Looking to get more hands-on experience with processing large volumes of data. I know this is incredibly vague and \"large volumes of data\" is entirely subjective, but I'm looking for a data API that will, without upgrading to a paid tier, produce millions of records at a time for a data engineering project. \n\nAny suggestions welcome :)", "author_fullname": "t2_am2gg89e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some truly open source APIs that can produce actually large volumes of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ajjzez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707149035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically title. Looking to get more hands-on experience with processing large volumes of data. I know this is incredibly vague and &amp;quot;large volumes of data&amp;quot; is entirely subjective, but I&amp;#39;m looking for a data API that will, without upgrading to a paid tier, produce millions of records at a time for a data engineering project. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajjzez", "is_robot_indexable": true, "report_reasons": null, "author": "Whiskeystring", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajjzez/what_are_some_truly_open_source_apis_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajjzez/what_are_some_truly_open_source_apis_that_can/", "subreddit_subscribers": 158499, "created_utc": 1707149035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building a data tool to help you collect and analyze data from multiple sources. Some more key features include pre-built and custom metrics, AI-assisted querying of DB, alerts, built-in, and bring your own data sources.\n\nWhat am I missing? Need help \ud83d\ude4f", "author_fullname": "t2_1upujjf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much data is too much data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajj95k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707147189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building a data tool to help you collect and analyze data from multiple sources. Some more key features include pre-built and custom metrics, AI-assisted querying of DB, alerts, built-in, and bring your own data sources.&lt;/p&gt;\n\n&lt;p&gt;What am I missing? Need help \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajj95k", "is_robot_indexable": true, "report_reasons": null, "author": "thehungryindian", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajj95k/how_much_data_is_too_much_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajj95k/how_much_data_is_too_much_data/", "subreddit_subscribers": 158499, "created_utc": 1707147189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_paxxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Guide] Leveraging Parameterized Shell Commands in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajh664", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cX1WKijmn6VSOutNgMyWC1tKxu6OFhm7dweKbFG-PLE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707141611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@matt.dixon1010/databricks-parameterized-shell-commands-745e035f6b2a", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?auto=webp&amp;s=0e1d1aca8c064c92ee9ad4d19dc540ed974087c5", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3120c06fdc9cd9db98ea438a775a329815635c15", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24f5c7748e9bd62849f22298bf3ab49f1b1dca1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f7baeb6780d9ce5f803ea42f5fa7d491d1a75bb", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=585ab825620578412bd869578f13ec75d4e92c94", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84880571083e4b255a8df04e85289f22671f57c0", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5660ce4c780c635a1a7aec16226a90c04b0b498", "width": 1080, "height": 810}], "variants": {}, "id": "IUI2fkZV2rKZsQyiZANnhjeGgEPz7Tjcj8G-mM8-ytY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajh664", "is_robot_indexable": true, "report_reasons": null, "author": "mdixon1010", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajh664/guide_leveraging_parameterized_shell_commands_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@matt.dixon1010/databricks-parameterized-shell-commands-745e035f6b2a", "subreddit_subscribers": 158499, "created_utc": 1707141611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,  \n\n\nHope you are doing well. I'm crafting a pipeline for data scientists to train models based on google analytics data. As i work on the architecture i was wondering, whats your experience with it? Do you base tables more on events &amp; co? What's the most efficient way to go on with this .", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt/dataform and GA4", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajds70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707130402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,  &lt;/p&gt;\n\n&lt;p&gt;Hope you are doing well. I&amp;#39;m crafting a pipeline for data scientists to train models based on google analytics data. As i work on the architecture i was wondering, whats your experience with it? Do you base tables more on events &amp;amp; co? What&amp;#39;s the most efficient way to go on with this .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajds70", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajds70/dbtdataform_and_ga4/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajds70/dbtdataform_and_ga4/", "subreddit_subscribers": 158499, "created_utc": 1707130402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone, I need help with Great Expectations. When using GX OSS to validate data on an MSSQL database, how does GX handle it? Does it pull data from the table for validation, or does it create a SQL query to validate directly on the MSSQL database? ", "author_fullname": "t2_2q3ifruw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great Expectation validate data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajc102", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707122837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I need help with Great Expectations. When using GX OSS to validate data on an MSSQL database, how does GX handle it? Does it pull data from the table for validation, or does it create a SQL query to validate directly on the MSSQL database? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajc102", "is_robot_indexable": true, "report_reasons": null, "author": "IllogicalAmbassador", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajc102/great_expectation_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajc102/great_expectation_validate_data/", "subreddit_subscribers": 158499, "created_utc": 1707122837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I am trying to set a parameter for a SQL stored procedure in a ForEach loop of ADF. My code works fine: \n\n    @item().properties[1].value\n\n I'm not a fan of hard-coding that \\[1\\] in, though, in case someone changes the column order. The data source I am pulling from is out of my control. Is there something I can do in ADF to select the row by key?  Something like: \n\n    @item().properties[item.properties.key==\"key\"].value", "author_fullname": "t2_tcne0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajbkbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707120794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to set a parameter for a SQL stored procedure in a ForEach loop of ADF. My code works fine: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@item().properties[1].value\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m not a fan of hard-coding that [1] in, though, in case someone changes the column order. The data source I am pulling from is out of my control. Is there something I can do in ADF to select the row by key?  Something like: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@item().properties[item.properties.key==&amp;quot;key&amp;quot;].value\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajbkbo", "is_robot_indexable": true, "report_reasons": null, "author": "Karsticles", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajbkbo/azure_data_factory_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajbkbo/azure_data_factory_question/", "subreddit_subscribers": 158499, "created_utc": 1707120794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been trying out Copilot in Microsoft Fabric Notebooks and so far it seems to be working quite well. Time to time I have encountered some minor bugs with it. Have you already tried Copilot in Notebooks and what are your thoughts and experiences with it?\n\n  \nI created a short demo about Copilot's Magic Commands in Fabric Notebooks.  \nFeel free to watch it if you are interested in Copilot Magic in Microsoft Fabric.\n\n  \nLink to the video:  \n[https://youtu.be/4hh1e6oy2A4](https://youtu.be/4hh1e6oy2A4)", "author_fullname": "t2_u9bnox3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copilot in Microsoft Fabric Notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajbglk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707120355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying out Copilot in Microsoft Fabric Notebooks and so far it seems to be working quite well. Time to time I have encountered some minor bugs with it. Have you already tried Copilot in Notebooks and what are your thoughts and experiences with it?&lt;/p&gt;\n\n&lt;p&gt;I created a short demo about Copilot&amp;#39;s Magic Commands in Fabric Notebooks.&lt;br/&gt;\nFeel free to watch it if you are interested in Copilot Magic in Microsoft Fabric.&lt;/p&gt;\n\n&lt;p&gt;Link to the video:&lt;br/&gt;\n&lt;a href=\"https://youtu.be/4hh1e6oy2A4\"&gt;https://youtu.be/4hh1e6oy2A4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xN9bdfE5_gYR5qS1KqyUbVa-EI-q-5RlgCdUGy929Dc.jpg?auto=webp&amp;s=9e87c938b682bccafe6f64549c8540e5d55afcb2", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/xN9bdfE5_gYR5qS1KqyUbVa-EI-q-5RlgCdUGy929Dc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4e98373693b80a98c7290bde135823c23426d6e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/xN9bdfE5_gYR5qS1KqyUbVa-EI-q-5RlgCdUGy929Dc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b6a7922e3f3f201f8bb9b5e0459225fef74dd9d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/xN9bdfE5_gYR5qS1KqyUbVa-EI-q-5RlgCdUGy929Dc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76f5349fa3865ba48076c4627ba3216e9dd98a50", "width": 320, "height": 240}], "variants": {}, "id": "7k1pLaEVcE5beOBnrx_gJO9gBMfaqBsPGxlnx2-SvtY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajbglk", "is_robot_indexable": true, "report_reasons": null, "author": "aleks1ck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajbglk/copilot_in_microsoft_fabric_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajbglk/copilot_in_microsoft_fabric_notebooks/", "subreddit_subscribers": 158499, "created_utc": 1707120355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So the problem statement is like this. I am using a for loop which runs from current_date()-15 to current_date(). This loopdate is then passed to notebook in which some aggregations are taking place between the date range loopdate-45 to loopdate. I want to optimize this code and possibly remove the dependency of the for loop.Any ideas on how I dan approach this situation.", "author_fullname": "t2_5a9iwwnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimize Loop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajaus6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707117882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So the problem statement is like this. I am using a for loop which runs from current_date()-15 to current_date(). This loopdate is then passed to notebook in which some aggregations are taking place between the date range loopdate-45 to loopdate. I want to optimize this code and possibly remove the dependency of the for loop.Any ideas on how I dan approach this situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajaus6", "is_robot_indexable": true, "report_reasons": null, "author": "willywonka786", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajaus6/optimize_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajaus6/optimize_loop/", "subreddit_subscribers": 158499, "created_utc": 1707117882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work as a data engineer building and maintaining pipelines for internal business teams. I've only worked in one organisation since graduating a few years ago. For previous projects, typically the lead data engineer gathered user requirements from the business team and translated those requirements for the data engineers. If anybody within the team had questions about what they were building, they could talk to the business team directly for further clarification.\n\nUp until this point, we've never had business analysts working with us.\n\nFor the current project, the responsibility of gathering and translating user requirements are now with business analysts. This seems to be creating more work than before. They don't seem to have much of a technical background, so there's questions they don't ask (which we would) and we often get told to do things (e.g., implement some transformation logic) which doesn't actually solve the real business issue.\n\nDo data teams often make use of business analysts for data engineering projects? Most sources seem to highlight \"requirements gathering\" as a core skill of a data engineer. I feel like it's creating a lot of confusion and additional work here. I also haven't worked anywhere else before so my experience is limited.", "author_fullname": "t2_tjdatpl4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the role of a business analyst on data engineering projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj7b8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707105335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work as a data engineer building and maintaining pipelines for internal business teams. I&amp;#39;ve only worked in one organisation since graduating a few years ago. For previous projects, typically the lead data engineer gathered user requirements from the business team and translated those requirements for the data engineers. If anybody within the team had questions about what they were building, they could talk to the business team directly for further clarification.&lt;/p&gt;\n\n&lt;p&gt;Up until this point, we&amp;#39;ve never had business analysts working with us.&lt;/p&gt;\n\n&lt;p&gt;For the current project, the responsibility of gathering and translating user requirements are now with business analysts. This seems to be creating more work than before. They don&amp;#39;t seem to have much of a technical background, so there&amp;#39;s questions they don&amp;#39;t ask (which we would) and we often get told to do things (e.g., implement some transformation logic) which doesn&amp;#39;t actually solve the real business issue.&lt;/p&gt;\n\n&lt;p&gt;Do data teams often make use of business analysts for data engineering projects? Most sources seem to highlight &amp;quot;requirements gathering&amp;quot; as a core skill of a data engineer. I feel like it&amp;#39;s creating a lot of confusion and additional work here. I also haven&amp;#39;t worked anywhere else before so my experience is limited.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj7b8f", "is_robot_indexable": true, "report_reasons": null, "author": "Acrobatic-Divide-173", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj7b8f/what_is_the_role_of_a_business_analyst_on_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj7b8f/what_is_the_role_of_a_business_analyst_on_data/", "subreddit_subscribers": 158499, "created_utc": 1707105335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to make a dashboard from data created in a notebook but there are times where there is no data in a pyspark dataframe. When this happens, the visualization doesn't show up in the dashboard anymore. If this happens, I would want it to show an empty dataframe visualization or a count that shows 0 instead of the visualization completely disappearing. Is there a way to get around this?  \n\n\nEG \n\n&amp;#x200B;\n\ndisplay(df) when the df is empty, the display won't show and the visualization in the notebook dashboard doesn't show up either.", "author_fullname": "t2_j4kzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks dashboard from notebook with empty dataframes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajb97b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707119532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a dashboard from data created in a notebook but there are times where there is no data in a pyspark dataframe. When this happens, the visualization doesn&amp;#39;t show up in the dashboard anymore. If this happens, I would want it to show an empty dataframe visualization or a count that shows 0 instead of the visualization completely disappearing. Is there a way to get around this?  &lt;/p&gt;\n\n&lt;p&gt;EG &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;display(df) when the df is empty, the display won&amp;#39;t show and the visualization in the notebook dashboard doesn&amp;#39;t show up either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajb97b", "is_robot_indexable": true, "report_reasons": null, "author": "bongdong42O", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajb97b/databricks_dashboard_from_notebook_with_empty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajb97b/databricks_dashboard_from_notebook_with_empty/", "subreddit_subscribers": 158499, "created_utc": 1707119532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there are tools out there to simulate a virtual network on a single PC, where you can section off subnets and play with their ip tables, launch VM instances inside each subnet and run docker inside the VMs?\n\nI want to experiment with some network architectures but don\u2019t want to deploy stuff out on the cloud right away.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to simulate a VPC with VMs and all that jazz?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aixdix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707077891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there are tools out there to simulate a virtual network on a single PC, where you can section off subnets and play with their ip tables, launch VM instances inside each subnet and run docker inside the VMs?&lt;/p&gt;\n\n&lt;p&gt;I want to experiment with some network architectures but don\u2019t want to deploy stuff out on the cloud right away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aixdix", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aixdix/any_tools_to_simulate_a_vpc_with_vms_and_all_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aixdix/any_tools_to_simulate_a_vpc_with_vms_and_all_that/", "subreddit_subscribers": 158499, "created_utc": 1707077891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "(Note: I don't name here the job title of the job description, as it would be very easy to identify the company. But the job title doesn't cleary state a data engineering role.)\n\nAccording to the job description, the main responsibility of the function is to manage a data catalog. The role has to make sure that the data is well-structured and findable, and has to maintain contact with stakeholders of the catalog and be communicative.\nThe applier is required to have skills in knowledge management (e.g. Data modeling, ontologies, UML) and has preferably experience with tools like Datahub or Collibra. And the candidate should be interested in technologies like data lake / data mesh / data warehouse.\n\nWhat data engineer role is this?\n\nAnd some background of mine: I am educated in information science and have therefore experience in relational databases, sql, data modelling, basics of programming, and so on. However I never learned the skills of data scientists or data analysts, even though I was always interested in that field.\n\nFor several years, I worked as a records manager and product owner, and I am not enjoying it so much, due to the business-nature of the activities and high level of stress. I prefer technical and focused tasks much more, and the countless meetings are killing me.\nI don't mind talking with people, and I actually enjoy giving supports to user, but it needs to be in moderation.\nI am considering applying for this job as they are looking for someone with my background.\n\nAccording to the job description, what could I expect? Is it more a tech-oriented activity, or is it more of a management-oriented activity with countless calls and non-stop customer-interaction? And what can I expect regarding stress level, is this field usually very demanding or enjoyable? And could this be a good entry point into the field of data engineering?\n\nThanks a lot.", "author_fullname": "t2_sxmanq59n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What role according to job description is this? And what can I expect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj5dgd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707102164.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707099190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Note: I don&amp;#39;t name here the job title of the job description, as it would be very easy to identify the company. But the job title doesn&amp;#39;t cleary state a data engineering role.)&lt;/p&gt;\n\n&lt;p&gt;According to the job description, the main responsibility of the function is to manage a data catalog. The role has to make sure that the data is well-structured and findable, and has to maintain contact with stakeholders of the catalog and be communicative.\nThe applier is required to have skills in knowledge management (e.g. Data modeling, ontologies, UML) and has preferably experience with tools like Datahub or Collibra. And the candidate should be interested in technologies like data lake / data mesh / data warehouse.&lt;/p&gt;\n\n&lt;p&gt;What data engineer role is this?&lt;/p&gt;\n\n&lt;p&gt;And some background of mine: I am educated in information science and have therefore experience in relational databases, sql, data modelling, basics of programming, and so on. However I never learned the skills of data scientists or data analysts, even though I was always interested in that field.&lt;/p&gt;\n\n&lt;p&gt;For several years, I worked as a records manager and product owner, and I am not enjoying it so much, due to the business-nature of the activities and high level of stress. I prefer technical and focused tasks much more, and the countless meetings are killing me.\nI don&amp;#39;t mind talking with people, and I actually enjoy giving supports to user, but it needs to be in moderation.\nI am considering applying for this job as they are looking for someone with my background.&lt;/p&gt;\n\n&lt;p&gt;According to the job description, what could I expect? Is it more a tech-oriented activity, or is it more of a management-oriented activity with countless calls and non-stop customer-interaction? And what can I expect regarding stress level, is this field usually very demanding or enjoyable? And could this be a good entry point into the field of data engineering?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aj5dgd", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable-Twist-405", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj5dgd/what_role_according_to_job_description_is_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj5dgd/what_role_according_to_job_description_is_this/", "subreddit_subscribers": 158499, "created_utc": 1707099190.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}