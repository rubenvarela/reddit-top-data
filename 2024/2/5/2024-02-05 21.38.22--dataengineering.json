{"kind": "Listing", "data": {"after": "t3_1ajbkbo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a data analyst who just signed his first offer letter for an DE role. My manager told me that I won\u2019t be a \u201ctrue\u201d data engineer until about a year in, he said the first year will be a lot of learning and that a lot of my job will be closely supervised so don\u2019t feel like Im being micromanaged but once he feels comfortable i will be able to handle projects on my own. Is this normal?", "author_fullname": "t2_hfgr9xjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally landed my first DE role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajnqf4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707157922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a data analyst who just signed his first offer letter for an DE role. My manager told me that I won\u2019t be a \u201ctrue\u201d data engineer until about a year in, he said the first year will be a lot of learning and that a lot of my job will be closely supervised so don\u2019t feel like Im being micromanaged but once he feels comfortable i will be able to handle projects on my own. Is this normal?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajnqf4", "is_robot_indexable": true, "report_reasons": null, "author": "tofin02", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajnqf4/finally_landed_my_first_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajnqf4/finally_landed_my_first_de_role/", "subreddit_subscribers": 158550, "created_utc": 1707157922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been using DBT working for a startup company for a few years &amp; think its an amazing tool for anyone who works with SQL databases.  I'm starting to see how DBT could be used as a backend for a full headless BI system, if you leverages things like defining your data as a Semantic Model. \n\nI know DBT offers a lot, like [Source Yamls](https://docs.getdbt.com/docs/build/sources), [Model Property Yamls](https://docs.getdbt.com/reference/model-properties), [Semantic Models](https://docs.getdbt.com/docs/build/semantic-models), [Exposures](https://docs.getdbt.com/docs/build/exposures), etc.   \n\n\n1. For those that you use, how do you maintain them at scale? Who manages the creation and editing of model data like descriptions and such?\n2. Why don't you use the other ones? is it becuase its too tedious to maintain at scale? \n3. Anyone doing anything custom with Tags or Groups? User authentication or Row-Level-Security? \n\nSecretly here I'm trying to validate a SAAS idea. How many other people even use DBT? Let-alone the numerious use cases and special files you need to maintain. I've always thought  a UI where you could successfully CRUD (Create, Read, Update, Delete) all your DBT assets would be great. As I learn more about software development, I see this as an opportunity. Wondering if anyone else has pain points with DBT and if so what are they?", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People who use DBT (Data Build Tool) - what are you using it for? Do you use ALL the components like Tests, Semantic Models, Exposures, ...etc?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj3fac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707093490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using DBT working for a startup company for a few years &amp;amp; think its an amazing tool for anyone who works with SQL databases.  I&amp;#39;m starting to see how DBT could be used as a backend for a full headless BI system, if you leverages things like defining your data as a Semantic Model. &lt;/p&gt;\n\n&lt;p&gt;I know DBT offers a lot, like &lt;a href=\"https://docs.getdbt.com/docs/build/sources\"&gt;Source Yamls&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/reference/model-properties\"&gt;Model Property Yamls&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/docs/build/semantic-models\"&gt;Semantic Models&lt;/a&gt;, &lt;a href=\"https://docs.getdbt.com/docs/build/exposures\"&gt;Exposures&lt;/a&gt;, etc.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For those that you use, how do you maintain them at scale? Who manages the creation and editing of model data like descriptions and such?&lt;/li&gt;\n&lt;li&gt;Why don&amp;#39;t you use the other ones? is it becuase its too tedious to maintain at scale? &lt;/li&gt;\n&lt;li&gt;Anyone doing anything custom with Tags or Groups? User authentication or Row-Level-Security? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Secretly here I&amp;#39;m trying to validate a SAAS idea. How many other people even use DBT? Let-alone the numerious use cases and special files you need to maintain. I&amp;#39;ve always thought  a UI where you could successfully CRUD (Create, Read, Update, Delete) all your DBT assets would be great. As I learn more about software development, I see this as an opportunity. Wondering if anyone else has pain points with DBT and if so what are they?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj3fac", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/", "subreddit_subscribers": 158550, "created_utc": 1707093490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alternative title: What exactly is the point of debezium?\n\nRight now I have a postgres database in which my own code sends all changes to certain tables and publishes the changes to kafka. This is is the rough flow:\n\n1. Insert/update/delete data from certain tables usings `returning *` as appropriate.\n2. Create a custom json payload describing what happened.\n3. Send that payload to a Kafka topic.\n\nThe current setup has worked more or less fine, but it's a bit of a hassle and can get more complicated with different kinds of actions (e.g. upserts). Also it has the obvious downside that I actively need to do this for whatever tables I care about instead of having it happen magically.\n\nA better approach would be something that uses postgres replication to send out the data. An option I've looked into is using wal2json ( https://github.com/eulerto/wal2json ) and then sending the output straight to Kafka after that. Of course this makes me wonder if I should instead just move all the way away from my custom work and just use something off the shelf. Google has eventually led me to debezium. Is that something I should seriously consider? Honestly it looks to me to be quite confusing. I've looked at the output from wal2json and that already feels like enough. I personally don't even need to use something like Kafka connect and don't mind sending the changes in using a regular publication to Kafka (that's already simpler that what I've been doing until now), but am I just being naive? Am I fool to skip debezium and tie things together myself?", "author_fullname": "t2_f6kbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use debezium or reinvent the wheel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajf01q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707134954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alternative title: What exactly is the point of debezium?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a postgres database in which my own code sends all changes to certain tables and publishes the changes to kafka. This is is the rough flow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert/update/delete data from certain tables usings &lt;code&gt;returning *&lt;/code&gt; as appropriate.&lt;/li&gt;\n&lt;li&gt;Create a custom json payload describing what happened.&lt;/li&gt;\n&lt;li&gt;Send that payload to a Kafka topic.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The current setup has worked more or less fine, but it&amp;#39;s a bit of a hassle and can get more complicated with different kinds of actions (e.g. upserts). Also it has the obvious downside that I actively need to do this for whatever tables I care about instead of having it happen magically.&lt;/p&gt;\n\n&lt;p&gt;A better approach would be something that uses postgres replication to send out the data. An option I&amp;#39;ve looked into is using wal2json ( &lt;a href=\"https://github.com/eulerto/wal2json\"&gt;https://github.com/eulerto/wal2json&lt;/a&gt; ) and then sending the output straight to Kafka after that. Of course this makes me wonder if I should instead just move all the way away from my custom work and just use something off the shelf. Google has eventually led me to debezium. Is that something I should seriously consider? Honestly it looks to me to be quite confusing. I&amp;#39;ve looked at the output from wal2json and that already feels like enough. I personally don&amp;#39;t even need to use something like Kafka connect and don&amp;#39;t mind sending the changes in using a regular publication to Kafka (that&amp;#39;s already simpler that what I&amp;#39;ve been doing until now), but am I just being naive? Am I fool to skip debezium and tie things together myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?auto=webp&amp;s=43601c671c6cca7248f3360f807500661f2ca84b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67f5e0b319d668c71bf1426b14510f187d5d7067", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25bd7a73f37202a42636e7d7ae74e54650f6b75b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2336d105e1717a3cfd01046bf4f2507be44e4c9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a02875797173ca04355e598604a6caa35a27f533", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=772a75332d16afc3ac814cdb5a5a2b43a01af790", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/2Z2GgTVjLlh6ESs4FYzXYC2HKv1_CcZLc33OnVrAl8c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a1d58f5c0fa146441a97baf245586602c5714cf", "width": 1080, "height": 540}], "variants": {}, "id": "B4qAodveQeFwIQVpvTspEI_OVM_nNQVrpVn-TeCZtEg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajf01q", "is_robot_indexable": true, "report_reasons": null, "author": "ApproximateIdentity", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajf01q/use_debezium_or_reinvent_the_wheel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajf01q/use_debezium_or_reinvent_the_wheel/", "subreddit_subscribers": 158550, "created_utc": 1707134954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I don't have much experience in data engineering, so I'm here to ask for advice. I am working on a project which consists of building a dashboard for the IT department of a bank. the dashboard should present information from log data. Log data includes security vulnerabilities, issues reported by the help desk, and logs showing who is working on those issues. The data includes information such as description of issues, when they are reported, which device is affected.... Data is provided via an internal API (I don't believe it provides real-time data streaming). I want to create a data pipeline that extracts this data, transforms it, loads it into a database, and then creates a dashboard from it. Normally this pipeline should run once a day. so I think an ETL should work fine. I was thinking of using Python and Pandas to perform ETL since the data is not very large.\nThe challenge is that alongside this ETL (which should be scheduled to run once a day), I want to achieve this functionality: If a critical issue is reported (server is down, high risk security vulnerability , ...) The IT department must be notified immediately (via the dashboard). How to implement such a pipeline. The data pipeline and dashboard must be deployed internally (no cloud services). Can you help me choose the right tools and give me some tips for designing this pipeline. THANKS.", "author_fullname": "t2_54yh79tez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me design a data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajanxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707117083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I don&amp;#39;t have much experience in data engineering, so I&amp;#39;m here to ask for advice. I am working on a project which consists of building a dashboard for the IT department of a bank. the dashboard should present information from log data. Log data includes security vulnerabilities, issues reported by the help desk, and logs showing who is working on those issues. The data includes information such as description of issues, when they are reported, which device is affected.... Data is provided via an internal API (I don&amp;#39;t believe it provides real-time data streaming). I want to create a data pipeline that extracts this data, transforms it, loads it into a database, and then creates a dashboard from it. Normally this pipeline should run once a day. so I think an ETL should work fine. I was thinking of using Python and Pandas to perform ETL since the data is not very large.\nThe challenge is that alongside this ETL (which should be scheduled to run once a day), I want to achieve this functionality: If a critical issue is reported (server is down, high risk security vulnerability , ...) The IT department must be notified immediately (via the dashboard). How to implement such a pipeline. The data pipeline and dashboard must be deployed internally (no cloud services). Can you help me choose the right tools and give me some tips for designing this pipeline. THANKS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajanxk", "is_robot_indexable": true, "report_reasons": null, "author": "3Ammar404", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajanxk/help_me_design_a_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajanxk/help_me_design_a_data_pipeline/", "subreddit_subscribers": 158550, "created_utc": 1707117083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not necessarily a complaint post.\n\nCompanies are obviously allowed to interview however they want and ask whatever they want. I\u2019m a senior level DE and my background was perfect for what they wanted. \n\nFor context, I got asked LC 84 Largest Rectangle in a Histogram. I\u2019ll admit my LC knowledge is not great. I\u2019ve been working on it but this one is beyond where I\u2019m at right now. But I do think it\u2019s a little funny that this particular question was asked. \n\nLeetcodes like 84 really make me question my intelligence sometimes. I could\u2019ve looked at that problem for 3 hours and I might not have even been able to brute force it. Even the stack answer doesn\u2019t make sense after seeing it, let alone the dynamic programming solutions.", "author_fullname": "t2_5eb4n7lng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got a LC Hard in an interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ajq0td", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707163411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not necessarily a complaint post.&lt;/p&gt;\n\n&lt;p&gt;Companies are obviously allowed to interview however they want and ask whatever they want. I\u2019m a senior level DE and my background was perfect for what they wanted. &lt;/p&gt;\n\n&lt;p&gt;For context, I got asked LC 84 Largest Rectangle in a Histogram. I\u2019ll admit my LC knowledge is not great. I\u2019ve been working on it but this one is beyond where I\u2019m at right now. But I do think it\u2019s a little funny that this particular question was asked. &lt;/p&gt;\n\n&lt;p&gt;Leetcodes like 84 really make me question my intelligence sometimes. I could\u2019ve looked at that problem for 3 hours and I might not have even been able to brute force it. Even the stack answer doesn\u2019t make sense after seeing it, let alone the dynamic programming solutions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ajq0td", "is_robot_indexable": true, "report_reasons": null, "author": "JustASimpleDE", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajq0td/just_got_a_lc_hard_in_an_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajq0td/just_got_a_lc_hard_in_an_interview/", "subreddit_subscribers": 158550, "created_utc": 1707163411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically title. Looking to get more hands-on experience with processing large volumes of data. I know this is incredibly vague and \"large volumes of data\" is entirely subjective, but I'm looking for a data API that will, without upgrading to a paid tier, produce millions of records at a time for a data engineering project. \n\nAny suggestions welcome :)", "author_fullname": "t2_am2gg89e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some truly open source APIs that can produce actually large volumes of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajjzez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707149035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically title. Looking to get more hands-on experience with processing large volumes of data. I know this is incredibly vague and &amp;quot;large volumes of data&amp;quot; is entirely subjective, but I&amp;#39;m looking for a data API that will, without upgrading to a paid tier, produce millions of records at a time for a data engineering project. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajjzez", "is_robot_indexable": true, "report_reasons": null, "author": "Whiskeystring", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajjzez/what_are_some_truly_open_source_apis_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajjzez/what_are_some_truly_open_source_apis_that_can/", "subreddit_subscribers": 158550, "created_utc": 1707149035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jboiz9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Between a Streaming Database and a Stream Processing Framework in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajfeae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05zVvmxAitSOjhR5DLpwTDVEOvyC5VqPCjAenG-1FEU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707136299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "learn.glassflow.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://learn.glassflow.dev/blog/articles/choosing-between-a-streaming-database-and-a-stream-processing-framework-in-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?auto=webp&amp;s=38d16cb2cdd17d4596f2f6d399fb2a357c905601", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a971d3b1b3f7d7ce2606900c292994adb8666a1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e6bf4ef25d2663869d8e324444d234841bfa1bc", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1041bf37e9ef71d655d43ca5359ac224ac7dd7a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=18c5b4041e6bb531d5062ea233a810821f70e8c9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d66da090a45917586b43b0b3caeae6f7185e729d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/eGA7hD6UggHjzcpInPCSCXUJ_ywiMThJrWautQmQxz0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa7ce18f1da053bbc07010c06fdf17da21f54aa0", "width": 1080, "height": 540}], "variants": {}, "id": "UquCtzVxSiovsutvPzya49pQXfcdwUIPnz3w2HP1VXo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajfeae", "is_robot_indexable": true, "report_reasons": null, "author": "bumurzokov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajfeae/choosing_between_a_streaming_database_and_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://learn.glassflow.dev/blog/articles/choosing-between-a-streaming-database-and-a-stream-processing-framework-in-python", "subreddit_subscribers": 158550, "created_utc": 1707136299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today, I have a relatively small set of data living on S3 in parquet format with a basic hive partitioning format on year, month, day, and hour covering 3 years with data associated to a time and ID.  Data is produced on an hourly basis. A common query is \"give me the latest data\" which fits well into this partition scheme. However another common use case is \"give me all the data for this ID over several months/years\". This requires reading thousands of objects which isn't terribly expensive or slow, but not exactly ideal either. Better would be if I could have a second set of data partitioned less granular on time and taking into account the ID of the data, updated either continually or on a set schedule.\n\nI have been reading about Apache Iceberg, Hudi, and DeltaLake but all seem pretty rigid in their partitioning scheme. Obviously I could roll my own with a little work, but I would like to fit into an existing ecosystem. Is there something in Iceberg or another format that can handle this type of layout?", "author_fullname": "t2_39r4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "multi partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj2j4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707091043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, I have a relatively small set of data living on S3 in parquet format with a basic hive partitioning format on year, month, day, and hour covering 3 years with data associated to a time and ID.  Data is produced on an hourly basis. A common query is &amp;quot;give me the latest data&amp;quot; which fits well into this partition scheme. However another common use case is &amp;quot;give me all the data for this ID over several months/years&amp;quot;. This requires reading thousands of objects which isn&amp;#39;t terribly expensive or slow, but not exactly ideal either. Better would be if I could have a second set of data partitioned less granular on time and taking into account the ID of the data, updated either continually or on a set schedule.&lt;/p&gt;\n\n&lt;p&gt;I have been reading about Apache Iceberg, Hudi, and DeltaLake but all seem pretty rigid in their partitioning scheme. Obviously I could roll my own with a little work, but I would like to fit into an existing ecosystem. Is there something in Iceberg or another format that can handle this type of layout?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aj2j4f", "is_robot_indexable": true, "report_reasons": null, "author": "evilryry", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj2j4f/multi_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj2j4f/multi_partitioning/", "subreddit_subscribers": 158550, "created_utc": 1707091043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nI\u2019m a few weeks into my first data engineering gig and really enjoying it.  I\u2019m learning so much and getting the chance to work on really cool things.\n\nOne thing I was hoping to get peoples\u2019 thoughts on.  My manager doesn\u2019t know much about my work, what it will take to do it or how long it will take.  It\u2019s not in his domain.  I\u2019m mostly self-learning and doing things on my own.  I\u2019m a hard worker and am confident that I will do a good job.  But I know that I need to come across to him as competent, make him know that I am achieving and make him aware of my achievements, basically manage his expectations and uphold my reputation as a good worker.  This, in particular because he doesn\u2019t have a point of reference into how well I am doing from the outside; mostly it will come from what I inform him, and of what I tell him is possible.  \n\nI know people may find it repugnant to think about things like this, the \u2018optics\u2019, I was wondering if anyone might have advice on how to manage this.  How to come across well.  How to keep a good reputation for a manager who may not have insight into the nitty gritty details of what my actual work may consist of.  I\u2019ve been working long hours to \u2018be doing as much as possible\u2019 to come across as competent, but I\u2019m realizing it might be more efficient in the long run to work more regular hours and then just manage my reputation more effectively.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maintaining a Reputation of Competence with my Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj0b4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707085175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a few weeks into my first data engineering gig and really enjoying it.  I\u2019m learning so much and getting the chance to work on really cool things.&lt;/p&gt;\n\n&lt;p&gt;One thing I was hoping to get peoples\u2019 thoughts on.  My manager doesn\u2019t know much about my work, what it will take to do it or how long it will take.  It\u2019s not in his domain.  I\u2019m mostly self-learning and doing things on my own.  I\u2019m a hard worker and am confident that I will do a good job.  But I know that I need to come across to him as competent, make him know that I am achieving and make him aware of my achievements, basically manage his expectations and uphold my reputation as a good worker.  This, in particular because he doesn\u2019t have a point of reference into how well I am doing from the outside; mostly it will come from what I inform him, and of what I tell him is possible.  &lt;/p&gt;\n\n&lt;p&gt;I know people may find it repugnant to think about things like this, the \u2018optics\u2019, I was wondering if anyone might have advice on how to manage this.  How to come across well.  How to keep a good reputation for a manager who may not have insight into the nitty gritty details of what my actual work may consist of.  I\u2019ve been working long hours to \u2018be doing as much as possible\u2019 to come across as competent, but I\u2019m realizing it might be more efficient in the long run to work more regular hours and then just manage my reputation more effectively.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj0b4n", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj0b4n/maintaining_a_reputation_of_competence_with_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj0b4n/maintaining_a_reputation_of_competence_with_my/", "subreddit_subscribers": 158550, "created_utc": 1707085175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi colleagues!\n\nQuestion:\n\nHave you encountered a single-node processing engine (akin to DuckDB, Polars, Pandas) that can work with a metastore (Hive/Glue/Unity catalog) instead of raw file system?\n\nBackground:\n\nI am investigating single-node data processing engines (DuckDB and Polars) as an alternative to pushing every workload to a Spark cluster. As I am looking at benchmarks it makes sense to run smaller workloads (let's say 10-100 GB) on a single-machine-centric engine, and only use Spark when the workload needs to be pushed to a cluster.\n\nDistributed systems are great, but when a company only needs to process a small amount of data outside of the main batch we either run into cost issues (cluster is idling) or incur the overheads of spark on demand (Spark pool or Glue will take minutes to start processing 100 row file). \n\nAt the same time I prefer to manage my files as tables through a metastore (I believe this makes more sense for access control and ease of use for non-technical users).\n\nI don't think running a single-node Spark cluster makes sense (again, judging by benchmarks DuckDB and Polars win) as not every job will grow to a cluster-worthy size, and even when it does migrating SQL or Dataframe-based logic to Spark seems to be trivial.\n\nThis is a summary of my thoughts on running a hybrid system:\n\n* Pros\n   * Faster runtime for smaller workloads\n   * Simplified job development\n   * Maybe we will never need Spark\n   * Lower cost (even if a cluster exists, it does not have to run for every workload)\n* Cons\n   * May require migration if the data size grows \n   * Additional components in the data platform increase the complexity\n\nI'd be happy to discuss my assumptions in the comments.", "author_fullname": "t2_f8lq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A single-node processing engine with a metastore connection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajdk5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707129501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi colleagues!&lt;/p&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;Have you encountered a single-node processing engine (akin to DuckDB, Polars, Pandas) that can work with a metastore (Hive/Glue/Unity catalog) instead of raw file system?&lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;I am investigating single-node data processing engines (DuckDB and Polars) as an alternative to pushing every workload to a Spark cluster. As I am looking at benchmarks it makes sense to run smaller workloads (let&amp;#39;s say 10-100 GB) on a single-machine-centric engine, and only use Spark when the workload needs to be pushed to a cluster.&lt;/p&gt;\n\n&lt;p&gt;Distributed systems are great, but when a company only needs to process a small amount of data outside of the main batch we either run into cost issues (cluster is idling) or incur the overheads of spark on demand (Spark pool or Glue will take minutes to start processing 100 row file). &lt;/p&gt;\n\n&lt;p&gt;At the same time I prefer to manage my files as tables through a metastore (I believe this makes more sense for access control and ease of use for non-technical users).&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think running a single-node Spark cluster makes sense (again, judging by benchmarks DuckDB and Polars win) as not every job will grow to a cluster-worthy size, and even when it does migrating SQL or Dataframe-based logic to Spark seems to be trivial.&lt;/p&gt;\n\n&lt;p&gt;This is a summary of my thoughts on running a hybrid system:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pros\n\n&lt;ul&gt;\n&lt;li&gt;Faster runtime for smaller workloads&lt;/li&gt;\n&lt;li&gt;Simplified job development&lt;/li&gt;\n&lt;li&gt;Maybe we will never need Spark&lt;/li&gt;\n&lt;li&gt;Lower cost (even if a cluster exists, it does not have to run for every workload)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Cons\n\n&lt;ul&gt;\n&lt;li&gt;May require migration if the data size grows &lt;/li&gt;\n&lt;li&gt;Additional components in the data platform increase the complexity&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d be happy to discuss my assumptions in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajdk5y", "is_robot_indexable": true, "report_reasons": null, "author": "kirilscherbach", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajdk5y/a_singlenode_processing_engine_with_a_metastore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajdk5y/a_singlenode_processing_engine_with_a_metastore/", "subreddit_subscribers": 158550, "created_utc": 1707129501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not sure whether I should be going for Mid or Sr level DE positions.\n\nI only really have about 2 years of DE experience, but 10 years total in data. The first 2 years I did some DE work with legacy tools (SQL Server, SSIS, Talend, heavy data modeling, migrations) but mostly BI and reporting. For the next 6 years I pursued BIE (reporting and server admin) and became a team lead but became complacent and stagnated.   \nTwo years ago I switched internally to an actual DE team and bumped down from lead but got to keep my title. I am neck deep in Python, Snowflake, Terraform, GitlabCI, AWS, Prefect, Fivetran, and dbt. Our team lead has taught so much about proper SDLC (he comes from the SWE side)\n\nBecause of my background in the BI / DB side, I was put in charge of our Snowflake account as well as data lineage / observability testing.\n\nAm I a senior? I'm a super quick learner (that's something I've realized I do better than most) but missed the whole big data wave (Spark, Hadoop, etc...). I don't want to be disingenuous and get a job over my head, I still have a lot to learn.\n\n&amp;#x200B;", "author_fullname": "t2_qhsi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mid vs Senior positions. Is it a blurry line? Where do I fit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajlt1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707153433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure whether I should be going for Mid or Sr level DE positions.&lt;/p&gt;\n\n&lt;p&gt;I only really have about 2 years of DE experience, but 10 years total in data. The first 2 years I did some DE work with legacy tools (SQL Server, SSIS, Talend, heavy data modeling, migrations) but mostly BI and reporting. For the next 6 years I pursued BIE (reporting and server admin) and became a team lead but became complacent and stagnated.&lt;br/&gt;\nTwo years ago I switched internally to an actual DE team and bumped down from lead but got to keep my title. I am neck deep in Python, Snowflake, Terraform, GitlabCI, AWS, Prefect, Fivetran, and dbt. Our team lead has taught so much about proper SDLC (he comes from the SWE side)&lt;/p&gt;\n\n&lt;p&gt;Because of my background in the BI / DB side, I was put in charge of our Snowflake account as well as data lineage / observability testing.&lt;/p&gt;\n\n&lt;p&gt;Am I a senior? I&amp;#39;m a super quick learner (that&amp;#39;s something I&amp;#39;ve realized I do better than most) but missed the whole big data wave (Spark, Hadoop, etc...). I don&amp;#39;t want to be disingenuous and get a job over my head, I still have a lot to learn.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ajlt1t", "is_robot_indexable": true, "report_reasons": null, "author": "NoUsernames1eft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajlt1t/mid_vs_senior_positions_is_it_a_blurry_line_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajlt1t/mid_vs_senior_positions_is_it_a_blurry_line_where/", "subreddit_subscribers": 158550, "created_utc": 1707153433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently stumbled across this whitepaper, [Introduction to the SQLBI Methodology](https://www.sqlbi.com/wp-content/uploads/Introduction-to-SQLBI-Methodology-draft-1.0.pdf), and think it's an under-appreciated gem. It's over 15 years old and has a heavy focus on Microsoft technologies; however, while the specific technology stack may not be relevant to your job today, the underlying principles most certainly are. As a side note, the authors are not native English speakers, so some of the wording can be strange. But the content is S-tier.\n\nIf you're wondering whether to read \"Fundamentals of Data Engineering\" or take Zach Wilson's data engineering course to uplevel your skillset, I highly recommend you first check out this resource.", "author_fullname": "t2_c9g6hufqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Concise guide to building a data warehouse and BI platform from 2008", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajkxsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707151394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently stumbled across this whitepaper, &lt;a href=\"https://www.sqlbi.com/wp-content/uploads/Introduction-to-SQLBI-Methodology-draft-1.0.pdf\"&gt;Introduction to the SQLBI Methodology&lt;/a&gt;, and think it&amp;#39;s an under-appreciated gem. It&amp;#39;s over 15 years old and has a heavy focus on Microsoft technologies; however, while the specific technology stack may not be relevant to your job today, the underlying principles most certainly are. As a side note, the authors are not native English speakers, so some of the wording can be strange. But the content is S-tier.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re wondering whether to read &amp;quot;Fundamentals of Data Engineering&amp;quot; or take Zach Wilson&amp;#39;s data engineering course to uplevel your skillset, I highly recommend you first check out this resource.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajkxsz", "is_robot_indexable": true, "report_reasons": null, "author": "carlineng_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ajkxsz/concise_guide_to_building_a_data_warehouse_and_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajkxsz/concise_guide_to_building_a_data_warehouse_and_bi/", "subreddit_subscribers": 158550, "created_utc": 1707151394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have observed previous discussions in this group regarding the subpar data quality originating from Pendo. Personally, I encountered challenges when attempting to link feature usage with the customer utilizing it.\nTo address this issue, our engineers integrated tracking directly into the software. However, executing this task correctly is intricate and requires ongoing complexity management. This includes meeting requirements for handling streaming (real-time) and deduplication of events, implementing native in-product queries, and providing continuous engineering support.\nI'm keen to learn from others in this group who have developed similar in-house systems for tracking product usage natively. \n\nIf you have experience building such a system, could you please share insights into the system you developed? Specifically, I'm interested in understanding the infrastructure set-up, data movement, monitoring, recovery, and any other relevant details involved in this process.", "author_fullname": "t2_acwc2csg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In-House Solutions for Product Usage Tracking in Response to Pendo's Limitations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajhy8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707143735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have observed previous discussions in this group regarding the subpar data quality originating from Pendo. Personally, I encountered challenges when attempting to link feature usage with the customer utilizing it.\nTo address this issue, our engineers integrated tracking directly into the software. However, executing this task correctly is intricate and requires ongoing complexity management. This includes meeting requirements for handling streaming (real-time) and deduplication of events, implementing native in-product queries, and providing continuous engineering support.\nI&amp;#39;m keen to learn from others in this group who have developed similar in-house systems for tracking product usage natively. &lt;/p&gt;\n\n&lt;p&gt;If you have experience building such a system, could you please share insights into the system you developed? Specifically, I&amp;#39;m interested in understanding the infrastructure set-up, data movement, monitoring, recovery, and any other relevant details involved in this process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajhy8f", "is_robot_indexable": true, "report_reasons": null, "author": "No_Way_1569", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajhy8f/inhouse_solutions_for_product_usage_tracking_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajhy8f/inhouse_solutions_for_product_usage_tracking_in/", "subreddit_subscribers": 158550, "created_utc": 1707143735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m392e2dd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dirty Data Destroys Dependability: Respecting your customers\u2019 privacy with a data clean room", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajfzn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bNPwwA6_m4zokWNPt_D5ys0qFDkRQGJ59Eqsyffvn-k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707138176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "segment.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://segment.com/blog/respect-customer-privacy-with-a-data-clean-room/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?auto=webp&amp;s=ee522541dbf4d1ed891e78e82ad75034a01774a3", "width": 2990, "height": 1640}, "resolutions": [{"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b573256ed860e2d27391fea413928e97503f0a2b", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=402be54526784751ae10351da38551a9c4d74346", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0e12890321d240c9e624e691a84b3f733a9c57a", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14419dcf321f665b3c32879cce59586c2fc07e89", "width": 640, "height": 351}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5908e7ec1637f95d3428874885b62dff0b38b37", "width": 960, "height": 526}, {"url": "https://external-preview.redd.it/kbgxtjKzaaPLBGQKkUBrVAHGwAyJ5OPqfK1tZY-YmYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8615c442df116ff37f44be1dc157f13b77c7be5e", "width": 1080, "height": 592}], "variants": {}, "id": "j9MFOxs_SqxMy5ZGbfRX-WrppWWG92LmHhHg5_lV_1I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajfzn6", "is_robot_indexable": true, "report_reasons": null, "author": "sunher444", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajfzn6/dirty_data_destroys_dependability_respecting_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://segment.com/blog/respect-customer-privacy-with-a-data-clean-room/", "subreddit_subscribers": 158550, "created_utc": 1707138176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nWe are trying to build awareness for Information and Data Governance in my company.\n\nDoes anyone have experience in using short (2 - 3 mins)  humorous training/compliance videos for Data Governance topics, such as the videos that Mimecast produce for Cyber Security to build awareness/knowledge..\n\nIs there a company like Mimecast which specialises in this type of videos\n\nThank you", "author_fullname": "t2_tj09r5oxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Governance Training /Awareness Videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aj7tli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707106966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;We are trying to build awareness for Information and Data Governance in my company.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience in using short (2 - 3 mins)  humorous training/compliance videos for Data Governance topics, such as the videos that Mimecast produce for Cyber Security to build awareness/knowledge..&lt;/p&gt;\n\n&lt;p&gt;Is there a company like Mimecast which specialises in this type of videos&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aj7tli", "is_robot_indexable": true, "report_reasons": null, "author": "TTwins1983", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aj7tli/data_governance_training_awareness_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aj7tli/data_governance_training_awareness_videos/", "subreddit_subscribers": 158550, "created_utc": 1707106966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you were entry to mid-level and you find that these firms passed you over during your initial job search, are you more likely to look for roles at these companies further down the line?\n\nIf the US job market is flooded with capable people with FAANG experience and/or 5+ years experience, from layoffs who aren't able to find jobs either; are these people more likely to pool together and form businesses? What are the implications of outsourcing to the degree that I'm seeing for the US labor market and data engineering overall?", "author_fullname": "t2_lp2ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you all think of smaller more boutique firms and businesses outsourcing their open roles to Bengaluru/Bangalore, India?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajopph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707160275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you were entry to mid-level and you find that these firms passed you over during your initial job search, are you more likely to look for roles at these companies further down the line?&lt;/p&gt;\n\n&lt;p&gt;If the US job market is flooded with capable people with FAANG experience and/or 5+ years experience, from layoffs who aren&amp;#39;t able to find jobs either; are these people more likely to pool together and form businesses? What are the implications of outsourcing to the degree that I&amp;#39;m seeing for the US labor market and data engineering overall?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajopph", "is_robot_indexable": true, "report_reasons": null, "author": "mrbrucel33", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajopph/what_do_you_all_think_of_smaller_more_boutique/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajopph/what_do_you_all_think_of_smaller_more_boutique/", "subreddit_subscribers": 158550, "created_utc": 1707160275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You can get various types of data like emails, addresses, different types of sentences like explanations, definitions and more. It depends what you have in your types dataset which I want to keep growing. So I could have weather status for example warm, sunny, cold. I want to be so that Instead of people having to figure out what variables are holding what data you can simply grab it because it's a semantic.\n\nI personally use it with a large language model to quickly get specific personal information such as my phone number, urls and more. The data is then extracted by the tool concisely and the put into my clipboard ready to be pasted. \n\nYou can download tool [here](https://github.com/1stRadiant/MAI-Extract)\n\nPlease let me know what you guys think whether you think this would be useful to you and how. Also what similar tool are there that you know of, I am a software engineer and I thought this could be great for data engineers. \n\nThank you in advance\ud83d\ude4f\ud83c\udffe", "author_fullname": "t2_453a35jn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have created data extraction to use with LLMs and different data sources.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajnusp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707158222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can get various types of data like emails, addresses, different types of sentences like explanations, definitions and more. It depends what you have in your types dataset which I want to keep growing. So I could have weather status for example warm, sunny, cold. I want to be so that Instead of people having to figure out what variables are holding what data you can simply grab it because it&amp;#39;s a semantic.&lt;/p&gt;\n\n&lt;p&gt;I personally use it with a large language model to quickly get specific personal information such as my phone number, urls and more. The data is then extracted by the tool concisely and the put into my clipboard ready to be pasted. &lt;/p&gt;\n\n&lt;p&gt;You can download tool &lt;a href=\"https://github.com/1stRadiant/MAI-Extract\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please let me know what you guys think whether you think this would be useful to you and how. Also what similar tool are there that you know of, I am a software engineer and I thought this could be great for data engineers. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance\ud83d\ude4f\ud83c\udffe&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?auto=webp&amp;s=5a736a054fa2bc72f638fd7f8ed7007a122dd571", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c58e37a254b22bb27dbc1253bfe35db24dc69b1d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=30338d72012d9ae20b9fa0d2b7f19816121fd77d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7bb20e4a8c8cc16f5c8360f348fad90e90cce00", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b67ad4f217e42296a6a45331451680513fb8b060", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d89fcaf541702ab7ba47e5600110c767a1219091", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/v7WMngCbAezRS1-ej8d-GQ_Gh3Yy0Q7Z-uF-x5ZWmlk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=032607ae75f8c3e5ff4a50a5c33c37d864157e3d", "width": 1080, "height": 540}], "variants": {}, "id": "upz_Nyj1581Ww3A-V6dH0TVD8GsdrfJr0iFZ7yHeSa8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ajnusp", "is_robot_indexable": true, "report_reasons": null, "author": "Alert-Estimate", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajnusp/i_have_created_data_extraction_to_use_with_llms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajnusp/i_have_created_data_extraction_to_use_with_llms/", "subreddit_subscribers": 158550, "created_utc": 1707158222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've 5 years of IT experience on infrastructure log analytics and search domain. I would like change my path towards data science and thought of starting with Google cloud data engineer certification as I have hands-on with GCP. \nPlease suggest career prospects with GCP on data science in India or any other better approaches.", "author_fullname": "t2_73ivrtusu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Data Engineer Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajniqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707157410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve 5 years of IT experience on infrastructure log analytics and search domain. I would like change my path towards data science and thought of starting with Google cloud data engineer certification as I have hands-on with GCP. \nPlease suggest career prospects with GCP on data science in India or any other better approaches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ajniqj", "is_robot_indexable": true, "report_reasons": null, "author": "thestrangemonk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajniqj/google_cloud_data_engineer_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajniqj/google_cloud_data_engineer_certification/", "subreddit_subscribers": 158550, "created_utc": 1707157410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just wanted to share a new project I\u2019ve been working on. This project aims to take medical claims billing data from employees in the state of Texas, model it, and implement with dbt. My main focus for this project was mainly learning how to use MDS tools. Any feedback on how I can improve this project is much appreciated.\n\nLink: https://github.com/seacevedo/texas_claims_billing", "author_fullname": "t2_3iljgzjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling Texas Medical Claims Billing Data and implementing with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajm8iw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707154427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to share a new project I\u2019ve been working on. This project aims to take medical claims billing data from employees in the state of Texas, model it, and implement with dbt. My main focus for this project was mainly learning how to use MDS tools. Any feedback on how I can improve this project is much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://github.com/seacevedo/texas_claims_billing\"&gt;https://github.com/seacevedo/texas_claims_billing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?auto=webp&amp;s=de9444c4f330b62144f9854d232c30637594844e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b27917d212eaa0289d76f6bc088a7c6184725826", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d50a206a40569d06337f624d636f770057b3b3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c99ef2453fb4b6306f7865e466866d2dbc863e7e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d089ec832dc342500784c145c9ad24a78e88f5e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=21a67eb9d3792d74eca887e0797ea18ea050d7b1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/yyh-XrhUHR6CFcvkn8vE5EVNfTkCFLwPuLFaoOh4BDQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e35355bcaaa24969f8886eaae995c3c35a421f0b", "width": 1080, "height": 540}], "variants": {}, "id": "KhK-rEYqUcLQfqiUy7njafUlDnJVNXuGOEcBnohfesc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ajm8iw", "is_robot_indexable": true, "report_reasons": null, "author": "bass581", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajm8iw/modeling_texas_medical_claims_billing_data_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajm8iw/modeling_texas_medical_claims_billing_data_and/", "subreddit_subscribers": 158550, "created_utc": 1707154427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Contracts For Real? Or Just More Snake Oil?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajkler", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6b3zou_2jsC6FNxOxJMhQrvPZ8dEQlPUKRGQjO1dsgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707150561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/are-data-contracts-for-real", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?auto=webp&amp;s=3e1b050fccac006516a1a2c332eb6255b319220b", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8601c0ed11b0d368d4581f81d516ab8fff076557", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db94a83d7bfe8775bc034375ec1bb552b1227737", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc8d30b84348d00bc0492d6ec5a3612280f30e6a", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9590c4cc70149cc60627292a255205f2c7f576a2", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/0FpmGtn67IUMjEY7-9C8qcznGHMVu0LUff8f8Xf58W4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e9e0d05c2cba238cc12d8629a015d76cf192d10", "width": 960, "height": 562}], "variants": {}, "id": "fg1n0IgkWneo7yi4bF4A_kaeSju7jPEgYsY6gRK-JKI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajkler", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajkler/are_data_contracts_for_real_or_just_more_snake_oil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/are-data-contracts-for-real", "subreddit_subscribers": 158550, "created_utc": 1707150561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building a data tool to help you collect and analyze data from multiple sources. Some more key features include pre-built and custom metrics, AI-assisted querying of DB, alerts, built-in, and bring your own data sources.\n\nWhat am I missing? Need help \ud83d\ude4f", "author_fullname": "t2_1upujjf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much data is too much data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajj95k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707147189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building a data tool to help you collect and analyze data from multiple sources. Some more key features include pre-built and custom metrics, AI-assisted querying of DB, alerts, built-in, and bring your own data sources.&lt;/p&gt;\n\n&lt;p&gt;What am I missing? Need help \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajj95k", "is_robot_indexable": true, "report_reasons": null, "author": "thehungryindian", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajj95k/how_much_data_is_too_much_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajj95k/how_much_data_is_too_much_data/", "subreddit_subscribers": 158550, "created_utc": 1707147189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_paxxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Guide] Leveraging Parameterized Shell Commands in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajh664", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cX1WKijmn6VSOutNgMyWC1tKxu6OFhm7dweKbFG-PLE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707141611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@matt.dixon1010/databricks-parameterized-shell-commands-745e035f6b2a", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?auto=webp&amp;s=0e1d1aca8c064c92ee9ad4d19dc540ed974087c5", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3120c06fdc9cd9db98ea438a775a329815635c15", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24f5c7748e9bd62849f22298bf3ab49f1b1dca1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f7baeb6780d9ce5f803ea42f5fa7d491d1a75bb", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=585ab825620578412bd869578f13ec75d4e92c94", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84880571083e4b255a8df04e85289f22671f57c0", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/y1y97EZL0LU9yXyyZ02IzGfEP64fNXuWwl8Wm-Uuwho.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5660ce4c780c635a1a7aec16226a90c04b0b498", "width": 1080, "height": 810}], "variants": {}, "id": "IUI2fkZV2rKZsQyiZANnhjeGgEPz7Tjcj8G-mM8-ytY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ajh664", "is_robot_indexable": true, "report_reasons": null, "author": "mdixon1010", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajh664/guide_leveraging_parameterized_shell_commands_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@matt.dixon1010/databricks-parameterized-shell-commands-745e035f6b2a", "subreddit_subscribers": 158550, "created_utc": 1707141611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,  \n\n\nHope you are doing well. I'm crafting a pipeline for data scientists to train models based on google analytics data. As i work on the architecture i was wondering, whats your experience with it? Do you base tables more on events &amp; co? What's the most efficient way to go on with this .", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt/dataform and GA4", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajds70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707130402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,  &lt;/p&gt;\n\n&lt;p&gt;Hope you are doing well. I&amp;#39;m crafting a pipeline for data scientists to train models based on google analytics data. As i work on the architecture i was wondering, whats your experience with it? Do you base tables more on events &amp;amp; co? What&amp;#39;s the most efficient way to go on with this .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ajds70", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajds70/dbtdataform_and_ga4/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajds70/dbtdataform_and_ga4/", "subreddit_subscribers": 158550, "created_utc": 1707130402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone, I need help with Great Expectations. When using GX OSS to validate data on an MSSQL database, how does GX handle it? Does it pull data from the table for validation, or does it create a SQL query to validate directly on the MSSQL database? ", "author_fullname": "t2_2q3ifruw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great Expectation validate data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajc102", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707122837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I need help with Great Expectations. When using GX OSS to validate data on an MSSQL database, how does GX handle it? Does it pull data from the table for validation, or does it create a SQL query to validate directly on the MSSQL database? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajc102", "is_robot_indexable": true, "report_reasons": null, "author": "IllogicalAmbassador", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajc102/great_expectation_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajc102/great_expectation_validate_data/", "subreddit_subscribers": 158550, "created_utc": 1707122837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I am trying to set a parameter for a SQL stored procedure in a ForEach loop of ADF. My code works fine: \n\n    @item().properties[1].value\n\n I'm not a fan of hard-coding that \\[1\\] in, though, in case someone changes the column order. The data source I am pulling from is out of my control. Is there something I can do in ADF to select the row by key?  Something like: \n\n    @item().properties[item.properties.key==\"key\"].value", "author_fullname": "t2_tcne0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ajbkbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707120794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to set a parameter for a SQL stored procedure in a ForEach loop of ADF. My code works fine: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@item().properties[1].value\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m not a fan of hard-coding that [1] in, though, in case someone changes the column order. The data source I am pulling from is out of my control. Is there something I can do in ADF to select the row by key?  Something like: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@item().properties[item.properties.key==&amp;quot;key&amp;quot;].value\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ajbkbo", "is_robot_indexable": true, "report_reasons": null, "author": "Karsticles", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ajbkbo/azure_data_factory_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ajbkbo/azure_data_factory_question/", "subreddit_subscribers": 158550, "created_utc": 1707120794.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}