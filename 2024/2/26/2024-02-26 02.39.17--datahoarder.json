{"kind": "Listing", "data": {"after": "t3_1b04dwo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those of you worried about not being able to play DivX/XviD AVI files in the future, please enjoy this screenshot of Kodi 20.4 playing a 20+ year old 160x120 RealMedia video. You don't appreciate how amazing FFMPEG is.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1azphye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 297, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 297, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k7NHtsX5zmn0xyCLmu2SzYLP1fcFr-Hdcb19-xxtQrA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708872198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yusjnrd1uqkc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?auto=webp&amp;s=eb0ccf81b97157c9a0462b7a46c38d253e72e5a3", "width": 3840, "height": 2160}, "resolutions": [{"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3203b1a3197b61f6a87a22f68898e9d09ffb12df", "width": 108, "height": 60}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc4bfe9138044832da7bab68f55a4ba62c4ab9b8", "width": 216, "height": 121}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f83f02327240f5d3c9bc663fa627bcb5aa92b36", "width": 320, "height": 180}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94f8d8600af239eee51bcf180b4845ab9b6c2a37", "width": 640, "height": 360}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=032cf24d07dfbb5b460a6e948a9e5c948977585c", "width": 960, "height": 540}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ec1e1669aa0d7024fd0751c57ccb8d038585841", "width": 1080, "height": 607}], "variants": {}, "id": "5Io6ryMNxvF535M3V7_qZkOZMvFgyw8QrPp5qz5Rf9U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1azphye", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azphye/for_those_of_you_worried_about_not_being_able_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yusjnrd1uqkc1.png", "subreddit_subscribers": 734526, "created_utc": 1708872198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/ to the article: ", "author_fullname": "t2_24c0105r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correct me if I'm wrong, but isn't 1 petabyte actually 1000 terabytes? This article is saying 1 petabyte relates to 200 terabytes. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwdqw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 163, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 163, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RDVJCAV-WCeBrpOuwnPFHiOg6U74nmJ02bi6Hlea_YU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708889045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/\"&gt;https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/&lt;/a&gt; to the article: &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/oomiusxh8skc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?auto=webp&amp;s=35e9a295883cc8bf230b8c4956e2c67508d187fa", "width": 1080, "height": 6345}, "resolutions": [{"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58e16eeb4cfc419e17002b33b7208a4487623c42", "width": 108, "height": 216}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed2602d5c297100f276e902d8321037c80ec9a82", "width": 216, "height": 432}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e4b86221b31c7e2181aaadb0a8be9b445a4306a", "width": 320, "height": 640}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b76bfbd9140cc231996cb3616467325630c8a55c", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7b0bd1dc99435934117e38917448ccb6492f652", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a70f4322be2ecce917ab21991362747019f259a0", "width": 1080, "height": 2160}], "variants": {}, "id": "RbG64Tsu-Ka0QgRPSuO_KBjJrZopolGt3sqyDi8LeLI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azwdqw", "is_robot_indexable": true, "report_reasons": null, "author": "licidil95", "discussion_type": null, "num_comments": 56, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azwdqw/correct_me_if_im_wrong_but_isnt_1_petabyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/oomiusxh8skc1.jpeg", "subreddit_subscribers": 734526, "created_utc": 1708889045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "continue\n\n* [5,719,123 subtitles from opensubtitles.org](https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/) \\- subs 1 to 9180517\n* [opensubtitles.org dump - 1 million subtitles - 23 GB](https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/) \\- subs 9180519 to 9521948\n\n## opensubtitles.org.dump.9500000.to.9599999\n\nTODO i will add this part in about 10 days. now its 85% complete\n\n## opensubtitles.org.dump.9600000.to.9699999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;dn=opensubtitles.org.dump.9600000.to.9699999\n\n## opensubtitles.org.dump.9700000.to.9799999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;dn=opensubtitles.org.dump.9700000.to.9799999\n\n## download from github\n\nNOTE i will remove these files from github in some weeks, to keep the repo size below 10GB\n\n`ln` = create hardlinks\n\n    git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n    \n    mkdir opensubtitles.org.dump.9600000.to.9699999\n    ln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n      opensubtitles.org.dump.9600000.to.9699999\n    \n    mkdir opensubtitles.org.dump.9700000.to.9799999\n    ln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n      opensubtitles.org.dump.9700000.to.9799999\n\n## download from archive.org\n\nTODO upload to archive.org for long term storage\n\n## scraper\n\n[https://github.com/milahu/opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n\nmy latest version is still unreleased. it is based on my [aiohttp\\_chromium](https://github.com/milahu/aiohttp_chromium) to bypass cloudflare\n\ni have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com\n\n## problem of trust\n\none problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files\n\n## subtitles server\n\nTODO create a subtitles server to make this usable for thin clients (video players)\n\n* the biggest challenge is the database size of about 150GB\n* use metadata from subtitles\\_all.txt.gz from [https://dl.opensubtitles.org/addons/export/](https://dl.opensubtitles.org/addons/export/) \\- see also `subtitles_all.txt.gz-parse.py` in [opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n* map movie filename to imdb id to subtitles - see also `get-subs.py`\n* map movie filename to movie name to subtitles\n* recode to utf8 - see also `repack.py`\n* remove ads - see also `opensubtitles-ads.txt` and `find_ads.py`\n* maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay", "author_fullname": "t2_naq15kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "subtitles from opensubtitles.org - subs 9500000 to 9799999", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azqwa4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;continue&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/\"&gt;5,719,123 subtitles from opensubtitles.org&lt;/a&gt; - subs 1 to 9180517&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/\"&gt;opensubtitles.org dump - 1 million subtitles - 23 GB&lt;/a&gt; - subs 9180519 to 9521948&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9500000.to.9599999&lt;/h2&gt;\n\n&lt;p&gt;TODO i will add this part in about 10 days. now its 85% complete&lt;/p&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9600000.to.9699999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;amp;dn=opensubtitles.org.dump.9600000.to.9699999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9700000.to.9799999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;amp;dn=opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from github&lt;/h2&gt;\n\n&lt;p&gt;NOTE i will remove these files from github in some weeks, to keep the repo size below 10GB&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ln&lt;/code&gt; = create hardlinks&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n\nmkdir opensubtitles.org.dump.9600000.to.9699999\nln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n  opensubtitles.org.dump.9600000.to.9699999\n\nmkdir opensubtitles.org.dump.9700000.to.9799999\nln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n  opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from archive.org&lt;/h2&gt;\n\n&lt;p&gt;TODO upload to archive.org for long term storage&lt;/p&gt;\n\n&lt;h2&gt;scraper&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;https://github.com/milahu/opensubtitles-scraper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;my latest version is still unreleased. it is based on my &lt;a href=\"https://github.com/milahu/aiohttp_chromium\"&gt;aiohttp_chromium&lt;/a&gt; to bypass cloudflare&lt;/p&gt;\n\n&lt;p&gt;i have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com&lt;/p&gt;\n\n&lt;h2&gt;problem of trust&lt;/h2&gt;\n\n&lt;p&gt;one problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files&lt;/p&gt;\n\n&lt;h2&gt;subtitles server&lt;/h2&gt;\n\n&lt;p&gt;TODO create a subtitles server to make this usable for thin clients (video players)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the biggest challenge is the database size of about 150GB&lt;/li&gt;\n&lt;li&gt;use metadata from subtitles_all.txt.gz from &lt;a href=\"https://dl.opensubtitles.org/addons/export/\"&gt;https://dl.opensubtitles.org/addons/export/&lt;/a&gt; - see also &lt;code&gt;subtitles_all.txt.gz-parse.py&lt;/code&gt; in &lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;opensubtitles-scraper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to imdb id to subtitles - see also &lt;code&gt;get-subs.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to movie name to subtitles&lt;/li&gt;\n&lt;li&gt;recode to utf8 - see also &lt;code&gt;repack.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;remove ads - see also &lt;code&gt;opensubtitles-ads.txt&lt;/code&gt; and &lt;code&gt;find_ads.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?auto=webp&amp;s=4addee391a8fcdd79d9dc8bb3be80840cfbb2292", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6d87b10c172c2af78696cd0a9017ecffcb12898", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f65d0b1b68d620588e0a5da27334b745e70c8dc", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50392b9b700a2ed49029d9fc031570a4f0e6318b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5df9dc84a8f8e25e12eef950e3fe12459f8c712", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e752c9a858437424fd7d6df3cb09459a0923e91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=261adcbc7bfcb31a1aae5ed22a391bedcf898e0b", "width": 1080, "height": 540}], "variants": {}, "id": "xzPnR4eWjwiQ6_oTfaBERSRsup0-vUB8Zvj-2miVx3M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqwa4", "is_robot_indexable": true, "report_reasons": null, "author": "milahu2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "subreddit_subscribers": 734526, "created_utc": 1708875847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8oh1uiuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder to check and test your new drives before using", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_1b02paj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/r_WXnARmvY9_c0PZ6naDDz4czq0VRbEYrx_w_fMm95c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708904235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/c6meddkihtkc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?auto=webp&amp;s=f2db730e68b6736626f5e21e068bd05d958b46ab", "width": 1598, "height": 830}, "resolutions": [{"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01d5e7188598f8a9520ce9802c5067ddb1684308", "width": 108, "height": 56}, {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1ebcb5d359af0d99d1200daea9570576c271657", "width": 216, "height": 112}, {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=760d591d41b9c839b2e4725c1e2024023cd1d40f", "width": 320, "height": 166}, {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=49c69a6995c81ba9b047e2576f1df7b02e5b8b9f", "width": 640, "height": 332}, {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f58abb6359b155d1ab80a58e32bc773e29d028bd", "width": 960, "height": 498}, {"url": "https://preview.redd.it/c6meddkihtkc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd46cd322630756b7c38035f1763829298304da6", "width": 1080, "height": 560}], "variants": {}, "id": "pFtUq4qI1K04RuUhmUW2Q_uHCaRFzSewGMDpqMgtL54"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b02paj", "is_robot_indexable": true, "report_reasons": null, "author": "EMC2DATA592", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b02paj/reminder_to_check_and_test_your_new_drives_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/c6meddkihtkc1.jpeg", "subreddit_subscribers": 734526, "created_utc": 1708904235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8nd1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I think Google Drive does not like Stablebit Drivepool... Is this a known issue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "name": "t3_1azw154", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YYV1XJoUQO-m-Wsyn40f53rydCFd1DDPPTJoc3wvGmQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708888210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/i61o4c4w5skc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?auto=webp&amp;s=97b96e3f2993b01ae7e76def3205a8e6fd2c6fc0", "width": 1695, "height": 1271}, "resolutions": [{"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43e90b906e7518f7f4055d6862693fdffd7dbee1", "width": 108, "height": 80}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d13ce242077bc065a6e17b70ec6d257322eb3b4d", "width": 216, "height": 161}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc85c967e1950d5ab636c37a2526989cf526ee27", "width": 320, "height": 239}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e46c38f654a913cc4754921fe9561a58124e3fa6", "width": 640, "height": 479}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=793d28a666a910e9c154665967807279ed313994", "width": 960, "height": 719}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b2990d780870fb20cb637db75c632d7648da39a", "width": 1080, "height": 809}], "variants": {}, "id": "AtCgh6Mhawd7HhppuIDmeqPGPJigARGYgTcqhgdXPJU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azw154", "is_robot_indexable": true, "report_reasons": null, "author": "clavicon", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azw154/i_think_google_drive_does_not_like_stablebit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/i61o4c4w5skc1.jpeg", "subreddit_subscribers": 734526, "created_utc": 1708888210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.\n\nAll I did was drag and drop in the windows explorer.\n\nIf I want to routinely back up my drive every few months, should I be using an application?\n\nI feel like dragging and dropping again after the first transfer is suboptimal. \n\nTIA.", "author_fullname": "t2_iaosze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys do your routine offline backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgjh5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708840359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.&lt;/p&gt;\n\n&lt;p&gt;All I did was drag and drop in the windows explorer.&lt;/p&gt;\n\n&lt;p&gt;If I want to routinely back up my drive every few months, should I be using an application?&lt;/p&gt;\n\n&lt;p&gt;I feel like dragging and dropping again after the first transfer is suboptimal. &lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgjh5", "is_robot_indexable": true, "report_reasons": null, "author": "TryTurningItOffAgain", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "subreddit_subscribers": 734526, "created_utc": 1708840359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know that the best way is to preserve the original without re-encoding. But I'm actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don't even wanna know about Realmedia compatibility today.", "author_fullname": "t2_1e60y1rd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have videos in divx and xvid codec. Should I encode them to modern standards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkv62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708856980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that the best way is to preserve the original without re-encoding. But I&amp;#39;m actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don&amp;#39;t even wanna know about Realmedia compatibility today.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkv62", "is_robot_indexable": true, "report_reasons": null, "author": "skylinestar1986", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "subreddit_subscribers": 734526, "created_utc": 1708856980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I\u2019ve got a ton of drives, and lots of project backups from the last 15+ years. I\u2019m talking many many terabytes across multiple drives. Lots of these backups have duplicate folders, some of those duplicates may or may not have a few unique files or folders in them. And some of the drives may have corrupted files (when copying files from old drives to new ones sometimes Windows freezes up on certain files, so I don\u2019t know if they are corrupt or what\u2026)\n\nI know.. I regret not backing things up properly all these years. It\u2019s all haphazard and disorganized\n\nSo I\u2019m looking for the best way to somehow consolidate all these folders and files onto one or more drives, skipping the duplicate and corrupt files, so I have everything in one place (that I can then backup properly)\n\nI\u2019m on Windows 10. What would be my best course of action?\n\nThank you!", "author_fullname": "t2_49mql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consolidate multiple drives with duplicate and (maybe) corrupt files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azt8rl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708883081.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708881577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve got a ton of drives, and lots of project backups from the last 15+ years. I\u2019m talking many many terabytes across multiple drives. Lots of these backups have duplicate folders, some of those duplicates may or may not have a few unique files or folders in them. And some of the drives may have corrupted files (when copying files from old drives to new ones sometimes Windows freezes up on certain files, so I don\u2019t know if they are corrupt or what\u2026)&lt;/p&gt;\n\n&lt;p&gt;I know.. I regret not backing things up properly all these years. It\u2019s all haphazard and disorganized&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m looking for the best way to somehow consolidate all these folders and files onto one or more drives, skipping the duplicate and corrupt files, so I have everything in one place (that I can then backup properly)&lt;/p&gt;\n\n&lt;p&gt;I\u2019m on Windows 10. What would be my best course of action?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azt8rl", "is_robot_indexable": true, "report_reasons": null, "author": "un-sub", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azt8rl/consolidate_multiple_drives_with_duplicate_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azt8rl/consolidate_multiple_drives_with_duplicate_and/", "subreddit_subscribers": 734526, "created_utc": 1708881577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently trying to downsize and no longer pay the fee for my monthly 2TB Google Drive plan. I moved and am now currently/temporarily stuck on a data plan. I am not the owner of the house/person who pays for the internet plan, so it's not optional to get off it until I move out much later this year. My GDrive is currently sitting at about 1.75TB and would much rather store it physically and quit paying for a service I can't really use anyways. We're on a 1.25TB cap so downloading it all in one go really isn't an option. I could theoretically do it over time but I'd really prefer to just avoid the headache of that. Is there a service that'll do this for me or am I better of just sticking with the bill for now?\n\nSorry if this is a stupid question, as much as I'm more of a hoarder than the typical PC owner, I really haven't delved in to this stuff much before.", "author_fullname": "t2_20duihb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Service that will download my Google Drive to a hard disk then mail it to me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azlleh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708859777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently trying to downsize and no longer pay the fee for my monthly 2TB Google Drive plan. I moved and am now currently/temporarily stuck on a data plan. I am not the owner of the house/person who pays for the internet plan, so it&amp;#39;s not optional to get off it until I move out much later this year. My GDrive is currently sitting at about 1.75TB and would much rather store it physically and quit paying for a service I can&amp;#39;t really use anyways. We&amp;#39;re on a 1.25TB cap so downloading it all in one go really isn&amp;#39;t an option. I could theoretically do it over time but I&amp;#39;d really prefer to just avoid the headache of that. Is there a service that&amp;#39;ll do this for me or am I better of just sticking with the bill for now?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is a stupid question, as much as I&amp;#39;m more of a hoarder than the typical PC owner, I really haven&amp;#39;t delved in to this stuff much before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azlleh", "is_robot_indexable": true, "report_reasons": null, "author": "NovaResonance", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azlleh/service_that_will_download_my_google_drive_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azlleh/service_that_will_download_my_google_drive_to_a/", "subreddit_subscribers": 734526, "created_utc": 1708859777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the right place for this. Basically I\u2019m looking for visual tools to list the releases on music labels and the artists involved for archival purposes. I also want to be able to link connections between the artists involved and other labels that they might be on. \n\nI\u2019ve looked into mind map tools, but I guess I\u2019m looking for something that\u2019s a sort of hybrid between a flow chart/folder/mind map system. Any ideas? ", "author_fullname": "t2_c5z8xir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archival tool for organizing music labels and the artists/releases for them. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b05tmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708912794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the right place for this. Basically I\u2019m looking for visual tools to list the releases on music labels and the artists involved for archival purposes. I also want to be able to link connections between the artists involved and other labels that they might be on. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked into mind map tools, but I guess I\u2019m looking for something that\u2019s a sort of hybrid between a flow chart/folder/mind map system. Any ideas? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b05tmn", "is_robot_indexable": true, "report_reasons": null, "author": "666noise", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b05tmn/archival_tool_for_organizing_music_labels_and_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b05tmn/archival_tool_for_organizing_music_labels_and_the/", "subreddit_subscribers": 734526, "created_utc": 1708912794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background:  \nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the \"2\" different types of medium).   \n\n\nMain part  \nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don't experience any performance issues so i wouldn't go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  \n\n\nMain question  \nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  \n\n\nThanks for reading and appreciate all the advice!  \n\n\n&amp;#x200B;\n\nTLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can't seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.", "author_fullname": "t2_31744y31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "12TB drives and rebuild time with RAID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpunm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708873159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background:&lt;br/&gt;\nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the &amp;quot;2&amp;quot; different types of medium).   &lt;/p&gt;\n\n&lt;p&gt;Main part&lt;br/&gt;\nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don&amp;#39;t experience any performance issues so i wouldn&amp;#39;t go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  &lt;/p&gt;\n\n&lt;p&gt;Main question&lt;br/&gt;\nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  &lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and appreciate all the advice!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can&amp;#39;t seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpunm", "is_robot_indexable": true, "report_reasons": null, "author": "WietseTDX", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "subreddit_subscribers": 734526, "created_utc": 1708873159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been collecting thousands of images over the last couple years. I've gotten into a really bad habit of not throwing them into a folder Instead i just throw them into a random nsfw folder. \n\nI wanted to organize my collection, but was curious. Are there any applications that uses a site like saucenao and can throw them into a folder to that series, other than having to saucenao image every single pic one by one?", "author_fullname": "t2_tx5qzvt0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organize hentai collection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b059cw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708911128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been collecting thousands of images over the last couple years. I&amp;#39;ve gotten into a really bad habit of not throwing them into a folder Instead i just throw them into a random nsfw folder. &lt;/p&gt;\n\n&lt;p&gt;I wanted to organize my collection, but was curious. Are there any applications that uses a site like saucenao and can throw them into a folder to that series, other than having to saucenao image every single pic one by one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b059cw", "is_robot_indexable": true, "report_reasons": null, "author": "Xvailer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b059cw/how_to_organize_hentai_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b059cw/how_to_organize_hentai_collection/", "subreddit_subscribers": 734526, "created_utc": 1708911128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to download the full image, and not the parts o. I checked the network tab, some of the images I was able to get some from there. But for the rest, I see them as small images, which together will provide me with a full image  \nmage  \nage  \n\n\nhttps://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;format=png&amp;auto=webp&amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b\n\nIs there a way to download the full image, and not the small parts of a full image?", "author_fullname": "t2_b2929", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download images from the website stored as multiple small images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pbfkt847xrkc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 69, "x": 108, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0279e52efbd3fac977043864d2e44fac962825"}, {"y": 139, "x": 216, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1df4e9dd4d74a5de388e4a241744bb294737baa5"}, {"y": 206, "x": 320, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4bfd7832accb11256c6106bdd69bf75a4df6b6c6"}, {"y": 412, "x": 640, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13218cb5fd4393ddf01355e490f83f45468c33cd"}, {"y": 619, "x": 960, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2b57873ef01381c4d58a81400af0aa8693f71bab"}, {"y": 696, "x": 1080, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7b3359944c716fd58278de0e79a8dfa4ecd39c"}], "s": {"y": 720, "x": 1116, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;format=png&amp;auto=webp&amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b"}, "id": "pbfkt847xrkc1"}}, "name": "t3_1azuudk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gn8qHHoZZ4RmfzFibwYEAM6KB4am957Qqlw25ZP-sFc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708885400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to download the full image, and not the parts o. I checked the network tab, some of the images I was able to get some from there. But for the rest, I see them as small images, which together will provide me with a full image&lt;br/&gt;\nmage&lt;br/&gt;\nage  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b\"&gt;https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there a way to download the full image, and not the small parts of a full image?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azuudk", "is_robot_indexable": true, "report_reasons": null, "author": "rip777", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azuudk/how_to_download_images_from_the_website_stored_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azuudk/how_to_download_images_from_the_website_stored_as/", "subreddit_subscribers": 734526, "created_utc": 1708885400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. \n\nI went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn't apply to my build), or the onboard SATA controller is terrible.\n\nThe motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can't find the controllers used on this motherboard.\n\nMy question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I'm unaware of, as opposed to using the onboard SATA ports?", "author_fullname": "t2_74e8mz3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New unRAID build, need advice for onboard SATA ports vs  LSI/HBA PCIe card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpmz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708872583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ca.pcpartpicker.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. &lt;/p&gt;\n\n&lt;p&gt;I went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn&amp;#39;t apply to my build), or the onboard SATA controller is terrible.&lt;/p&gt;\n\n&lt;p&gt;The motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can&amp;#39;t find the controllers used on this motherboard.&lt;/p&gt;\n\n&lt;p&gt;My question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I&amp;#39;m unaware of, as opposed to using the onboard SATA ports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpmz7", "is_robot_indexable": true, "report_reasons": null, "author": "JakeHa0991", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpmz7/new_unraid_build_need_advice_for_onboard_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "subreddit_subscribers": 734526, "created_utc": 1708872583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  \n\n\nSo that:\n\nIf I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.  \n&lt;BUT&gt;  \nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  \n\n\nIf it is \"doable\" how does one do it?  \n\n\nThe purpose would be to have a \"hidden\" cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  \n\n\n&amp;#x200B;", "author_fullname": "t2_3kfdots", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to have CD-Audio on a BDXL disc and data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpb90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708871677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  &lt;/p&gt;\n\n&lt;p&gt;So that:&lt;/p&gt;\n\n&lt;p&gt;If I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.&lt;br/&gt;\n&amp;lt;BUT&amp;gt;&lt;br/&gt;\nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  &lt;/p&gt;\n\n&lt;p&gt;If it is &amp;quot;doable&amp;quot; how does one do it?  &lt;/p&gt;\n\n&lt;p&gt;The purpose would be to have a &amp;quot;hidden&amp;quot; cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpb90", "is_robot_indexable": true, "report_reasons": null, "author": "trseeker", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "subreddit_subscribers": 734526, "created_utc": 1708871677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkyd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9c2bi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "japan", "selftext": "# Introduction\n\nFor anybody that has a 1999 copy of Kodansha's CD-ROM version of '[Encyclopedia of Japan](https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan)', here's instructions on how to install and run it on Windows 11 (x64).\n\nSurprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.\n\nWhilst I have made a backup ISO file, ~~I unfortunately can't lawfully distribute it. As such, please don't ask.~~\n\n**Edit: The ISO file has been uploaded [here](https://archive.org/details/EOJ10) based on [this](https://archive.org/about/dmca.php) statement. Do not use this software if you have no legal right to do so.**\n\nFor those without a copy, if you would like to access its contents, you can do so via [JapanKnowledge](https://japanknowledge.com/en/contents/eoj/).\n\n# Installation\n\nFollow the steps below:\n\n1. Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of `Internet Explorer 5`.\n2. Attempt to launch the application. If you receive an error message concerning `MCI32.OCX`, proceed with steps 3-7.\n3. Type `Command Prompt` into the taskbar `Search` box; this is located next to the `Start` menu.\n4. In the search results, right-click `Command Prompt` and select `Run as administrator`.\n5. Copy and paste the following command: `regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX`.\n6. If the command doesn't instantly execute, press the `Enter/Return` key.\n7. Confirm that `MCI32.OCX` has been registered, and then close `Command Prompt`.\n8. Launch the encyclopedia application.\n\n# Missing Media\n\nWhen a media file is unable to load, you will see an `X` instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.\n\n# Fix\n\nFollow the steps below:\n\n1. `Open` the CD-ROM.\n2. Go to: `[CD DRIVE LETTER]:\\eoj`.\n3. Copy the: `[CD DRIVE LETTER]:\\eoj\\media` folder.\n4. Paste the entire folder to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan`.\n5. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n6. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n7. Copy: `Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0`.\n8. Paste this into the top bar and press `Enter/Return`.\n9. Right-click `MEDIA_PATH` and select `Modify`. Change the `Value data` to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media`.\n10. Click `OK` and close.\n\n# Unable to Print\n\nDuring testing, the `Print` button did not function.\n\n# Fix\n\nIn Windows 11, use `Ctrl + P` to print pages.\n\nTo remove the header and footer, follow the steps below:\n\n1. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n2. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n3. Copy: `Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup`.\n4. Paste this into the top bar and press `Enter/Return`.\n5. In the empty space below `(Default)`, right-click and select `New` \\-&gt; `String Value`. Type `header` and press `Enter/Return`.\n6. Repeat the step above, but type `footer` instead.\n7. Close `Registry Editor`.\n\nNote: This is a system-wide change. You will need to right-click the values and select `Delete` to reverse the change.\n\n# Uninstallation\n\nWhen uninstalling, the uninstaller may ask if you would like to delete `Mci32.ocx` and `Comdlg32.OCX`. Click `No` or `No to All`. Copies of these files should remain in: `C:\\Windows\\SysWOW64`.", "author_fullname": "t2_noxre9enh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/japan", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayrh2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 130, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 130, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708809657.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708770653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;For anybody that has a 1999 copy of Kodansha&amp;#39;s CD-ROM version of &amp;#39;&lt;a href=\"https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan\"&gt;Encyclopedia of Japan&lt;/a&gt;&amp;#39;, here&amp;#39;s instructions on how to install and run it on Windows 11 (x64).&lt;/p&gt;\n\n&lt;p&gt;Surprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.&lt;/p&gt;\n\n&lt;p&gt;Whilst I have made a backup ISO file, &lt;del&gt;I unfortunately can&amp;#39;t lawfully distribute it. As such, please don&amp;#39;t ask.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit: The ISO file has been uploaded &lt;a href=\"https://archive.org/details/EOJ10\"&gt;here&lt;/a&gt; based on &lt;a href=\"https://archive.org/about/dmca.php\"&gt;this&lt;/a&gt; statement. Do not use this software if you have no legal right to do so.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For those without a copy, if you would like to access its contents, you can do so via &lt;a href=\"https://japanknowledge.com/en/contents/eoj/\"&gt;JapanKnowledge&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Installation&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of &lt;code&gt;Internet Explorer 5&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Attempt to launch the application. If you receive an error message concerning &lt;code&gt;MCI32.OCX&lt;/code&gt;, proceed with steps 3-7.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Command Prompt&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Command Prompt&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy and paste the following command: &lt;code&gt;regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;If the command doesn&amp;#39;t instantly execute, press the &lt;code&gt;Enter/Return&lt;/code&gt; key.&lt;/li&gt;\n&lt;li&gt;Confirm that &lt;code&gt;MCI32.OCX&lt;/code&gt; has been registered, and then close &lt;code&gt;Command Prompt&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Launch the encyclopedia application.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Missing Media&lt;/h1&gt;\n\n&lt;p&gt;When a media file is unable to load, you will see an &lt;code&gt;X&lt;/code&gt; instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;Open&lt;/code&gt; the CD-ROM.&lt;/li&gt;\n&lt;li&gt;Go to: &lt;code&gt;[CD DRIVE LETTER]:\\eoj&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy the: &lt;code&gt;[CD DRIVE LETTER]:\\eoj\\media&lt;/code&gt; folder.&lt;/li&gt;\n&lt;li&gt;Paste the entire folder to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Right-click &lt;code&gt;MEDIA_PATH&lt;/code&gt; and select &lt;code&gt;Modify&lt;/code&gt;. Change the &lt;code&gt;Value data&lt;/code&gt; to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Click &lt;code&gt;OK&lt;/code&gt; and close.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Unable to Print&lt;/h1&gt;\n\n&lt;p&gt;During testing, the &lt;code&gt;Print&lt;/code&gt; button did not function.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;In Windows 11, use &lt;code&gt;Ctrl + P&lt;/code&gt; to print pages.&lt;/p&gt;\n\n&lt;p&gt;To remove the header and footer, follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;In the empty space below &lt;code&gt;(Default)&lt;/code&gt;, right-click and select &lt;code&gt;New&lt;/code&gt; -&amp;gt; &lt;code&gt;String Value&lt;/code&gt;. Type &lt;code&gt;header&lt;/code&gt; and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Repeat the step above, but type &lt;code&gt;footer&lt;/code&gt; instead.&lt;/li&gt;\n&lt;li&gt;Close &lt;code&gt;Registry Editor&lt;/code&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Note: This is a system-wide change. You will need to right-click the values and select &lt;code&gt;Delete&lt;/code&gt; to reverse the change.&lt;/p&gt;\n\n&lt;h1&gt;Uninstallation&lt;/h1&gt;\n\n&lt;p&gt;When uninstalling, the uninstaller may ask if you would like to delete &lt;code&gt;Mci32.ocx&lt;/code&gt; and &lt;code&gt;Comdlg32.OCX&lt;/code&gt;. Click &lt;code&gt;No&lt;/code&gt; or &lt;code&gt;No to All&lt;/code&gt;. Copies of these files should remain in: &lt;code&gt;C:\\Windows\\SysWOW64&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh2u", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1ayrh2k", "is_robot_indexable": true, "report_reasons": null, "author": "YokaiZukan", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 825602, "created_utc": 1708770653.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708857334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkyd5", "is_robot_indexable": true, "report_reasons": null, "author": "zombieshavebrains", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1ayrh2k", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkyd5/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 734526, "created_utc": 1708857334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Help! I am trying to understand how SnapRAID works.\n\nI have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?", "author_fullname": "t2_e4owahyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnapRAID Cloned Disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgdp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708840213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708839802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help! I am trying to understand how SnapRAID works.&lt;/p&gt;\n\n&lt;p&gt;I have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgdp2", "is_robot_indexable": true, "report_reasons": null, "author": "x5KSAM", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "subreddit_subscribers": 734526, "created_utc": 1708839802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. \n\nHowever, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.", "author_fullname": "t2_4kz0kk4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy and best method to clean VHS tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azg3ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708838838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. &lt;/p&gt;\n\n&lt;p&gt;However, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azg3ig", "is_robot_indexable": true, "report_reasons": null, "author": "harshcloud", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "subreddit_subscribers": 734526, "created_utc": 1708838838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been getting a lot of ads for the Ugreen NAS that is on Kickstarter and I'm tempted to buy a 2-bay one for homelab testing purposes. It seems like it's geared towards the home user and not businesses but I am curious if anyone else on here is backing it on Kickstarter or has any inside scoop on it", "author_fullname": "t2_ahe7v45b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ugreen NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b031my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708905133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been getting a lot of ads for the Ugreen NAS that is on Kickstarter and I&amp;#39;m tempted to buy a 2-bay one for homelab testing purposes. It seems like it&amp;#39;s geared towards the home user and not businesses but I am curious if anyone else on here is backing it on Kickstarter or has any inside scoop on it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b031my", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Value-732", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b031my/ugreen_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b031my/ugreen_nas/", "subreddit_subscribers": 734526, "created_utc": 1708905133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there some kind of tool or bulk powershell command to check them all and fix any errors? ", "author_fullname": "t2_chkcqqba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick way to check disk integrity across 24 SAS drives on Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b0271n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708902948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there some kind of tool or bulk powershell command to check them all and fix any errors? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b0271n", "is_robot_indexable": true, "report_reasons": null, "author": "regtf", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b0271n/quick_way_to_check_disk_integrity_across_24_sas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b0271n/quick_way_to_check_disk_integrity_across_24_sas/", "subreddit_subscribers": 734526, "created_utc": 1708902948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need font management advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwo9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_uo1tmrw", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "fonts", "selftext": "Hi folks\n\nOver the last two decades I've accumulated about 20k fonts from various sources on multiple machines running different OSes. I just dumped all of them into a single location. Currently everything is classified alphabetically. \n\nI'd like to split this collection into: serif, sans-serif, display,  symbols, etc. And also Commercial (Adobe, Linotype, etc) and Open Source. Get rid of duplicates or bad fonts, etc. \n\nIn other words just clean it up in a more meaningful way than just alphabetically. \n\nIf I try importing the A folder into Apple Font Book for example, either Font Book beach balls forever or asks me to manually click Activate a thousand times over. \n\nAnyone know of a program (macOS, Windows or Linux) or script that can automate this?\n\nThanks!\n\n", "author_fullname": "t2_uo1tmrw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need font management advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/fonts", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwjtk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708889449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.fonts", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks&lt;/p&gt;\n\n&lt;p&gt;Over the last two decades I&amp;#39;ve accumulated about 20k fonts from various sources on multiple machines running different OSes. I just dumped all of them into a single location. Currently everything is classified alphabetically. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to split this collection into: serif, sans-serif, display,  symbols, etc. And also Commercial (Adobe, Linotype, etc) and Open Source. Get rid of duplicates or bad fonts, etc. &lt;/p&gt;\n\n&lt;p&gt;In other words just clean it up in a more meaningful way than just alphabetically. &lt;/p&gt;\n\n&lt;p&gt;If I try importing the A folder into Apple Font Book for example, either Font Book beach balls forever or asks me to manually click Activate a thousand times over. &lt;/p&gt;\n\n&lt;p&gt;Anyone know of a program (macOS, Windows or Linux) or script that can automate this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qurp", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1azwjtk", "is_robot_indexable": true, "report_reasons": null, "author": "heliomedia", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/fonts/comments/1azwjtk/need_font_management_advice/", "subreddit_subscribers": 36357, "created_utc": 1708889449.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708889748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.fonts", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azwo9w", "is_robot_indexable": true, "report_reasons": null, "author": "heliomedia", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1azwjtk", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azwo9w/need_font_management_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "subreddit_subscribers": 734526, "created_utc": 1708889748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17's pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn't support Live photos according to some comments. \n\nI can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don't know what apps to use as of now. Another problem seems to be that I have \\~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they're backed up, but I know that's probably difficult.\n\nDoes anyone do something similar on their devices? Please let me know. thank you!", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up photos (including Live Photos) from iPhone to Android?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azqv6a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17&amp;#39;s pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn&amp;#39;t support Live photos according to some comments. &lt;/p&gt;\n\n&lt;p&gt;I can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don&amp;#39;t know what apps to use as of now. Another problem seems to be that I have ~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they&amp;#39;re backed up, but I know that&amp;#39;s probably difficult.&lt;/p&gt;\n\n&lt;p&gt;Does anyone do something similar on their devices? Please let me know. thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqv6a", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "subreddit_subscribers": 734526, "created_utc": 1708875765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi people,  \nI'm a beginner recently subscribed to Hetzner Storage Box.  \nI'm finding it hard to mount it to my Hetzner VPS.   \nI'm stuck on this for the past week.  \nThe info regarding Hetzner Storage box and the mounting options are so rare in the internet.  \n\n\nAppreciate the help. Thanks ", "author_fullname": "t2_1nxbeabb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with mounting Hetzner Storage box to VPS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpv9n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708873202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people,&lt;br/&gt;\nI&amp;#39;m a beginner recently subscribed to Hetzner Storage Box.&lt;br/&gt;\nI&amp;#39;m finding it hard to mount it to my Hetzner VPS.&lt;br/&gt;\nI&amp;#39;m stuck on this for the past week.&lt;br/&gt;\nThe info regarding Hetzner Storage box and the mounting options are so rare in the internet.  &lt;/p&gt;\n\n&lt;p&gt;Appreciate the help. Thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpv9n", "is_robot_indexable": true, "report_reasons": null, "author": "BossZkie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpv9n/need_help_with_mounting_hetzner_storage_box_to_vps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpv9n/need_help_with_mounting_hetzner_storage_box_to_vps/", "subreddit_subscribers": 734526, "created_utc": 1708873202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "NetworkInfrastructureException('Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\\\\n&lt;html&gt;\\\\n  &lt;head&gt;\\\\n    &lt;title&gt;503 Backend fetch failed&lt;/title&gt;\\\\n  &lt;/head&gt;\\\\n  &lt;body&gt;\\\\n    &lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\\\\n    &lt;p&gt;Backend fetch failed&lt;/p&gt;\\\\n    &lt;h3&gt;Guru Meditation:&lt;/h3&gt;\\\\n    &lt;p&gt;XID: 802841683&lt;/p&gt;\\\\n    &lt;hr&gt;\\\\n    &lt;p&gt;Varnish cache server&lt;/p&gt;\\\\n  &lt;/body&gt;\\\\n&lt;/html&gt;\\\\n')\u2026 (Copy note to see full error)\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1659, in Start\n\nhydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nDuring handling of the above exception, another exception occurred:\n\n&amp;#x200B;\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\importing\\\\[ClientImportFileSeeds.py](https://ClientImportFileSeeds.py)\", line 1378, in WorkOnURL\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1977, in WaitUntilDone\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1698, in Start\n\nhydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\nI've only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don't know how to fix, internet searches aren't yielding relevant results. Help is appreciated.", "author_fullname": "t2_4p22esrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Hydrus Downloader, need help getting passed 503 to download from kemono", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azos8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708870187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NetworkInfrastructureException(&amp;#39;Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;\\n&amp;lt;html&amp;gt;\\n  &amp;lt;head&amp;gt;\\n    &amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;\\n  &amp;lt;/head&amp;gt;\\n  &amp;lt;body&amp;gt;\\n    &amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;\\n    &amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;\\n    &amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;\\n    &amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;\\n    &amp;lt;hr&amp;gt;\\n    &amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;\\n  &amp;lt;/body&amp;gt;\\n&amp;lt;/html&amp;gt;\\n&amp;#39;)\u2026 (Copy note to see full error)&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1659, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;During handling of the above exception, another exception occurred:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\importing\\&lt;a href=\"https://ClientImportFileSeeds.py\"&gt;ClientImportFileSeeds.py&lt;/a&gt;&amp;quot;, line 1378, in WorkOnURL&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1977, in WaitUntilDone&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1698, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don&amp;#39;t know how to fix, internet searches aren&amp;#39;t yielding relevant results. Help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azos8e", "is_robot_indexable": true, "report_reasons": null, "author": "Vynsyx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "subreddit_subscribers": 734526, "created_utc": 1708870187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to replace a failing HGST Deskstar NAS 3.5-Inch 4TB 7200RPM SATA III 64MB Cache from 10 years ago that I use in my Synology DS1513+ I have other 4 similar drives, but those are OK.\n\nI'm in Europe and looking for a decent 7200 RPM drive that is not absurdly expensive. I stumbled upon an Ultrastar DC HC310 7200 RPM for 150 euros, which seems a nice deal.\n\nI could not find any big cons for that drive, only that it's \"noisy\", but the HGSTs that I currently have are not silent at all, so if the noise level is the same I can handle it.\n\nAs I know this sub has knowledgeable people, I wanted you guys' opinion on that HDD. Are there any other good alternatives?\n\nAlso found a Seagate IronWolf Pro NAS 4TB for 130 euros. And a Western Digital WD Red Pro NAS 4TB for 150 euros.\n\nThanks.", "author_fullname": "t2_kj39d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a 4TB 7200RPM NAS drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b04dwo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708909562.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708908708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to replace a failing HGST Deskstar NAS 3.5-Inch 4TB 7200RPM SATA III 64MB Cache from 10 years ago that I use in my Synology DS1513+ I have other 4 similar drives, but those are OK.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in Europe and looking for a decent 7200 RPM drive that is not absurdly expensive. I stumbled upon an Ultrastar DC HC310 7200 RPM for 150 euros, which seems a nice deal.&lt;/p&gt;\n\n&lt;p&gt;I could not find any big cons for that drive, only that it&amp;#39;s &amp;quot;noisy&amp;quot;, but the HGSTs that I currently have are not silent at all, so if the noise level is the same I can handle it.&lt;/p&gt;\n\n&lt;p&gt;As I know this sub has knowledgeable people, I wanted you guys&amp;#39; opinion on that HDD. Are there any other good alternatives?&lt;/p&gt;\n\n&lt;p&gt;Also found a Seagate IronWolf Pro NAS 4TB for 130 euros. And a Western Digital WD Red Pro NAS 4TB for 150 euros.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b04dwo", "is_robot_indexable": true, "report_reasons": null, "author": "N3RO-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b04dwo/looking_for_a_4tb_7200rpm_nas_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b04dwo/looking_for_a_4tb_7200rpm_nas_drive/", "subreddit_subscribers": 734526, "created_utc": 1708908708.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}