{"kind": "Listing", "data": {"after": "t3_1axzaza", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Share your favorite tips for writing better SQL, your pet peeves and best practices.", "author_fullname": "t2_svxm6jt4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your Top SQL Query Optimization tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axd7cy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 115, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 115, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708625364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Share your favorite tips for writing better SQL, your pet peeves and best practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axd7cy", "is_robot_indexable": true, "report_reasons": null, "author": "data_engineer_", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axd7cy/what_are_your_top_sql_query_optimization_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axd7cy/what_are_your_top_sql_query_optimization_tips/", "subreddit_subscribers": 163070, "created_utc": 1708625364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d \n", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior Engineers: Advice to your Noobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axo1k5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708651804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1axo1k5", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "subreddit_subscribers": 163070, "created_utc": 1708651804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?\n\n[https://www.talend.com/products/talend-open-studio/](https://www.talend.com/products/talend-open-studio/)\n\nThanks!\n\nEdit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file", "author_fullname": "t2_icbe4i2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talend is no longer free", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyooe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708705682.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708689321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.talend.com/products/talend-open-studio/\"&gt;https://www.talend.com/products/talend-open-studio/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?auto=webp&amp;s=192d811c869f7f0f606e5214026e4e26d2eb98e4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ead6cad649baf31abeb39ae4955178d556090bc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b438b5d24374741a22c280a9bd1e926e4be4cbc", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf28fc6b8c9932dc30d18080c1d4b55a85218dd", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d102f5369b8ef2f8f386bb390271874be381c4b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b9dbef65759861df9c8adac10ecc69badde2028", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecedd45042969b44355fcc440d0b3e1c96f94429", "width": 1080, "height": 565}], "variants": {}, "id": "Vf_rH0Wd4M6dY9BsVOYZiTrai7iv0IM8pn7AIO9Y-q4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyooe", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Bug9572", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "subreddit_subscribers": 163070, "created_utc": 1708689321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been in the data engineering world for several years. Lately I've been increasingly interested in the management side of things, especially data governance. Is it worthwhile to delve deeper into this field through a more formal path, such as a diploma or year-long course?\n\nabout \"as a data engineer\" I mean that I see a lot of people from other fields like law in this kind of teams and not many from engineering.\n\nThanks", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it worth studying data governance as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axcz59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708624849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been in the data engineering world for several years. Lately I&amp;#39;ve been increasingly interested in the management side of things, especially data governance. Is it worthwhile to delve deeper into this field through a more formal path, such as a diploma or year-long course?&lt;/p&gt;\n\n&lt;p&gt;about &amp;quot;as a data engineer&amp;quot; I mean that I see a lot of people from other fields like law in this kind of teams and not many from engineering.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1axcz59", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axcz59/is_it_worth_studying_data_governance_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axcz59/is_it_worth_studying_data_governance_as_a_data/", "subreddit_subscribers": 163070, "created_utc": 1708624849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed\n\nHow is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? ", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is spark really used and deployed in production in companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay0572", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708694000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed&lt;/p&gt;\n\n&lt;p&gt;How is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay0572", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "subreddit_subscribers": 163070, "created_utc": 1708694000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**There's a large scale finance data migration project going on in my company (FAANG)** moving processes and data between two enterprise tools. It's been running for &gt;5 years, with \\~100 people working on it, and it's still not finished. \n\n**Can you guys help me build intuition on why this is such a long running project.** I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. \n\n&amp;#x200B;", "author_fullname": "t2_tszztjadn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Migration Projects - the good, the bad, and the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtzgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708670588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;There&amp;#39;s a large scale finance data migration project going on in my company (FAANG)&lt;/strong&gt; moving processes and data between two enterprise tools. It&amp;#39;s been running for &amp;gt;5 years, with ~100 people working on it, and it&amp;#39;s still not finished. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Can you guys help me build intuition on why this is such a long running project.&lt;/strong&gt; I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axtzgp", "is_robot_indexable": true, "report_reasons": null, "author": "jmack_startups", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "subreddit_subscribers": 163070, "created_utc": 1708670588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light ", "author_fullname": "t2_pt8bni2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be best online course as an entry point to understand Data engineering ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axqynj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708660414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axqynj", "is_robot_indexable": true, "report_reasons": null, "author": "joy_warzone", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "subreddit_subscribers": 163070, "created_utc": 1708660414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data analyst fresh out of college, but I don't really generate business insights, I'm more on the ETL-database side of things. I'm working with legacy systems, most of it is MS Access/Excel (~20 TBs worth of .accdb/.xlsx files on a remote drive, yes I know it's bad) with some hints of SQL server (no admin/write privileges, only read). I still try to follow good ETL practices, having most of the transformations and filters at the SQL layer, then moving around data with pandas (pd.read_xxx and df.to_xxx all day). My pipelines are very amateurish, I've written several pipelines so far, all of them rely on pandas and pyodbc/sqlalchemy. \n\nI'm working on creating a mini ELT datalake for one of our reports that pulls from several Access files and tables. I have pipelines running from Access to SQLite on a daily basis, I have the tables split into several SQLite files. When it's time to run the report (usually once a month, occasionally adhoc with some extra transformations) I have some python code that reads and transforms the multiple SQLite files in parallel with concurrent.futures.ProcessPoolExecutor, then outputs the report into a lovely spreedsheet. It speeds up the process for this report by a few min, it's definitely my coolest project so far, and it's something I hope to implement for more\nreports as well.\n\nHow do seasoned, experienced engineers working with modern stacks perceive this kind of work? I would like to work with a major cloud service one day like AWS/Azure/GCP, I have some self-experience spinning up MySQL databases and hosting small websites in Azure, but no big data work.", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with legacy systems, how do seasoned engineers perceive this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axbzso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708623749.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708622522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analyst fresh out of college, but I don&amp;#39;t really generate business insights, I&amp;#39;m more on the ETL-database side of things. I&amp;#39;m working with legacy systems, most of it is MS Access/Excel (~20 TBs worth of .accdb/.xlsx files on a remote drive, yes I know it&amp;#39;s bad) with some hints of SQL server (no admin/write privileges, only read). I still try to follow good ETL practices, having most of the transformations and filters at the SQL layer, then moving around data with pandas (pd.read_xxx and df.to_xxx all day). My pipelines are very amateurish, I&amp;#39;ve written several pipelines so far, all of them rely on pandas and pyodbc/sqlalchemy. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on creating a mini ELT datalake for one of our reports that pulls from several Access files and tables. I have pipelines running from Access to SQLite on a daily basis, I have the tables split into several SQLite files. When it&amp;#39;s time to run the report (usually once a month, occasionally adhoc with some extra transformations) I have some python code that reads and transforms the multiple SQLite files in parallel with concurrent.futures.ProcessPoolExecutor, then outputs the report into a lovely spreedsheet. It speeds up the process for this report by a few min, it&amp;#39;s definitely my coolest project so far, and it&amp;#39;s something I hope to implement for more\nreports as well.&lt;/p&gt;\n\n&lt;p&gt;How do seasoned, experienced engineers working with modern stacks perceive this kind of work? I would like to work with a major cloud service one day like AWS/Azure/GCP, I have some self-experience spinning up MySQL databases and hosting small websites in Azure, but no big data work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axbzso", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axbzso/working_with_legacy_systems_how_do_seasoned/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axbzso/working_with_legacy_systems_how_do_seasoned/", "subreddit_subscribers": 163070, "created_utc": 1708622522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at the [Gitlab handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/#data-sources) and they have a list of their data sources along with how frequently data is update in their warehouse, e.g. every 24 hours.\n\nImagine I'm building a data warehouse, and my source is a database which is constantly changing 24 hours day - say I run an online store and people are constantly making orders.\n\nI want to lift several tables to load into a raw zone in my data warehouse - no transformations at this point, just lift and shift. I need to be careful to not overload the source database. The data are constantly changing at source, but I need the data in my warehouse to be taken at the same point in time - i.e. I don't want to extract and load one table as at 12:00am but then extract and load a related table as at 12:05am or the data in both tables could be out of sync.\n\nHow would you handle data which is constantly changing? Is there something I can read about this somewhere? I'm worried that I end up loading tables at different points in time and they end up out of sync. ", "author_fullname": "t2_40ho7lud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ELT with constantly changing data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axf0tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708629632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at the &lt;a href=\"https://handbook.gitlab.com/handbook/business-technology/data-team/platform/#data-sources\"&gt;Gitlab handbook&lt;/a&gt; and they have a list of their data sources along with how frequently data is update in their warehouse, e.g. every 24 hours.&lt;/p&gt;\n\n&lt;p&gt;Imagine I&amp;#39;m building a data warehouse, and my source is a database which is constantly changing 24 hours day - say I run an online store and people are constantly making orders.&lt;/p&gt;\n\n&lt;p&gt;I want to lift several tables to load into a raw zone in my data warehouse - no transformations at this point, just lift and shift. I need to be careful to not overload the source database. The data are constantly changing at source, but I need the data in my warehouse to be taken at the same point in time - i.e. I don&amp;#39;t want to extract and load one table as at 12:00am but then extract and load a related table as at 12:05am or the data in both tables could be out of sync.&lt;/p&gt;\n\n&lt;p&gt;How would you handle data which is constantly changing? Is there something I can read about this somewhere? I&amp;#39;m worried that I end up loading tables at different points in time and they end up out of sync. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?auto=webp&amp;s=af544e78828882798d7fe4d1c42454d6bd21dc22", "width": 875, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=828e0ee040e1722af1bf0dbb719d2b846ec766b4", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f99184941ef4a831059c95c0f3bfb24a7de84249", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b5e1d3118a33c25bab44e78f2a95e971cbca6e5", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a04b3ad42aa3e307db53deca2024a610d384848", "width": 640, "height": 447}], "variants": {}, "id": "v9bukcUTEDutaePTQidDeR95NYA8AyYs-tp2j6EyUkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axf0tl", "is_robot_indexable": true, "report_reasons": null, "author": "powerbihelpme", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axf0tl/how_to_elt_with_constantly_changing_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axf0tl/how_to_elt_with_constantly_changing_data/", "subreddit_subscribers": 163070, "created_utc": 1708629632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your thoughts on Data Vault modeling?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to Data Vault when not to Data Vault?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axd8vc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708625453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your thoughts on Data Vault modeling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axd8vc", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axd8vc/when_to_data_vault_when_not_to_data_vault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axd8vc/when_to_data_vault_when_not_to_data_vault/", "subreddit_subscribers": 163070, "created_utc": 1708625453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp; Rust", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How fast can we process a CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axugw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708672377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datapythonista.me", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp;amp; Rust&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axugw5", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axugw5/how_fast_can_we_process_a_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "subreddit_subscribers": 163070, "created_utc": 1708672377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gublzilb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Data Security Posture Management (DSPM)? Complete Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1axy2cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M5F4dQXDSkqez7A-6fvl-0EItp-A93gMVURpFQc5MjE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708687025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "sentra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?auto=webp&amp;s=4fc2e74e785a98fcf491a54f6d845f23fdf61448", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ae88047c44f63dbbbfe00d62b672062240c4e45", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ed8e0d52078cd9a5fd49e0c76987a32beb02628", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec26db953705789122b7328821f4ecd60c4c989b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0adcf53813a8efc9e5f3362f85a0fdb291f00370", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=117a736fcce06d12bb67a78e0dff934de25d6e55", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9cb7a992e9dd0d444e1556279c0f2a8edc8c82a", "width": 1080, "height": 564}], "variants": {}, "id": "RMpa0UQT5yvwro9ukXYf0Fb7og01FOuSx2zmc_RCqBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axy2cn", "is_robot_indexable": true, "report_reasons": null, "author": "newmudbat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axy2cn/what_is_data_security_posture_management_dspm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "subreddit_subscribers": 163070, "created_utc": 1708687025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a best practice for creating staging models in terms of fields used from the base table?\n\n&amp;#x200B;\n\nFor example, let's say I have a base table named 'customers' with 30 fields and I only use let's say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?\n\n&amp;#x200B;\n\nMy gut tells me to go with the former but I've not yet heard of or read of a best practice around this.\n\n&amp;#x200B;\n\nAny input would greatly appreciated!", "author_fullname": "t2_kmhrbrzny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for folks who utilize a staging layer on top of base tables.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axsaur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708664752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a best practice for creating staging models in terms of fields used from the base table?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For example, let&amp;#39;s say I have a base table named &amp;#39;customers&amp;#39; with 30 fields and I only use let&amp;#39;s say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My gut tells me to go with the former but I&amp;#39;ve not yet heard of or read of a best practice around this.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any input would greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axsaur", "is_robot_indexable": true, "report_reasons": null, "author": "datageek200", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "subreddit_subscribers": 163070, "created_utc": 1708664752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAm currently facing one big challenge at work. \n\nI joined a company 8 months ago. We are building a Saas product that mainly exposes some AI analysis results with a Frontend app. The system is mainly based on an in house ETL solution.   \nWe load the data in Postgres, transfer it to Elasticsearch for analysis systems, then we load the results back to Postgres for a back office app, and then  we serialize the results to Elasticsearch again in an easily queryable format. The results are exploited mainly for analytical workloads.  \n\n\nWe found the process really slow and painfull. Especially when we have to reprocess the data for some diverse reasons. For example, when we adjusts our AI models or when we change some of the labels associated with data, we have to reprocess the data in order to reflect the changes. Also we found Elasticsearch not to be resilient to some high writes load, causing the process to stop due to some 429 Too many requests response. Maybe we can handle this differently. Any advice would be welcomed.  \n\n\nThe solution we are trying to push these days, is to use only postgres for the whole analysis and reduce the data moving during analysis (since its heavily error prone and fragile). This way our pipeline would only analyse the data present and update the results in one place (postgres).\n\n  \nDo you think its possible to achieve this without denormalisation (as we doing today) ?  \n\n\nI would like to hear your taugths about this and how you would solve it ?   \n", "author_fullname": "t2_bimho7u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axcyhb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708632770.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708624812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Am currently facing one big challenge at work. &lt;/p&gt;\n\n&lt;p&gt;I joined a company 8 months ago. We are building a Saas product that mainly exposes some AI analysis results with a Frontend app. The system is mainly based on an in house ETL solution.&lt;br/&gt;\nWe load the data in Postgres, transfer it to Elasticsearch for analysis systems, then we load the results back to Postgres for a back office app, and then  we serialize the results to Elasticsearch again in an easily queryable format. The results are exploited mainly for analytical workloads.  &lt;/p&gt;\n\n&lt;p&gt;We found the process really slow and painfull. Especially when we have to reprocess the data for some diverse reasons. For example, when we adjusts our AI models or when we change some of the labels associated with data, we have to reprocess the data in order to reflect the changes. Also we found Elasticsearch not to be resilient to some high writes load, causing the process to stop due to some 429 Too many requests response. Maybe we can handle this differently. Any advice would be welcomed.  &lt;/p&gt;\n\n&lt;p&gt;The solution we are trying to push these days, is to use only postgres for the whole analysis and reduce the data moving during analysis (since its heavily error prone and fragile). This way our pipeline would only analyse the data present and update the results in one place (postgres).&lt;/p&gt;\n\n&lt;p&gt;Do you think its possible to achieve this without denormalisation (as we doing today) ?  &lt;/p&gt;\n\n&lt;p&gt;I would like to hear your taugths about this and how you would solve it ?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axcyhb", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Fudge9617", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axcyhb/alternatives_to_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axcyhb/alternatives_to_etl/", "subreddit_subscribers": 163070, "created_utc": 1708624812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?", "author_fullname": "t2_79w2zsix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be the best way to learn DE as a beginner? Courses vs practice questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyw74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708690002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyw74", "is_robot_indexable": true, "report_reasons": null, "author": "p200g", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "subreddit_subscribers": 163070, "created_utc": 1708690002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?\n\nWhere do you like to blog and where do you like to read blogs on data engineering?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your favorite place to blog about data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzpek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?&lt;/p&gt;\n\n&lt;p&gt;Where do you like to blog and where do you like to read blogs on data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzpek", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "subreddit_subscribers": 163070, "created_utc": 1708692655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!  \n\n\nI am in the process of evaluating different possible setups for a data lakehouse on GCP.   \n\n\nI have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   \n\n\nI have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn't have to be immense.   \n\n\nI retrieve on two columns: datetime and a \"tags\" column, which is a string.   \n\n\n1. I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? \n2. Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? \n3. During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?\n4. After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. \n\nThanks for the help!  \n", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a Data Lakehouse / Delta Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtb5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708668162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!  &lt;/p&gt;\n\n&lt;p&gt;I am in the process of evaluating different possible setups for a data lakehouse on GCP.   &lt;/p&gt;\n\n&lt;p&gt;I have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   &lt;/p&gt;\n\n&lt;p&gt;I have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn&amp;#39;t have to be immense.   &lt;/p&gt;\n\n&lt;p&gt;I retrieve on two columns: datetime and a &amp;quot;tags&amp;quot; column, which is a string.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? &lt;/li&gt;\n&lt;li&gt;Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? &lt;/li&gt;\n&lt;li&gt;During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?&lt;/li&gt;\n&lt;li&gt;After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for the help!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axtb5k", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "subreddit_subscribers": 163070, "created_utc": 1708668162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Checking many websites or newsletters every day? Feedly?  \nIs there something similar to TechCrunch for data industry?  \nHow do you keep up with new tools or database releases?\n\nI'm building a news aggregator/feed for me and my team, and hopefully others ([here the is link to it](https://recactus.net/eye/), it\u2019s still under development tho), and I\u2019m looking for news sources to improve the feed.", "author_fullname": "t2_125gsz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you stay up to date with industry tools and changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axdu88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708626811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Checking many websites or newsletters every day? Feedly?&lt;br/&gt;\nIs there something similar to TechCrunch for data industry?&lt;br/&gt;\nHow do you keep up with new tools or database releases?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a news aggregator/feed for me and my team, and hopefully others (&lt;a href=\"https://recactus.net/eye/\"&gt;here the is link to it&lt;/a&gt;, it\u2019s still under development tho), and I\u2019m looking for news sources to improve the feed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axdu88", "is_robot_indexable": true, "report_reasons": null, "author": "pebolax", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axdu88/how_do_you_stay_up_to_date_with_industry_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axdu88/how_do_you_stay_up_to_date_with_industry_tools/", "subreddit_subscribers": 163070, "created_utc": 1708626811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are evaluating whether to go with multiple smaller databases or one large one. \n\nSmall databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. \n\nI perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. \n\nWhat is your view on this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple small databases x One large", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzofv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are evaluating whether to go with multiple smaller databases or one large one. &lt;/p&gt;\n\n&lt;p&gt;Small databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. &lt;/p&gt;\n\n&lt;p&gt;I perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzofv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "subreddit_subscribers": 163070, "created_utc": 1708692572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as\n\n* TimescaleDB\n* AWS Timestream\n* influxdb\n* Big Query (last resource, AWS resources used at the company)\n\n&amp;#x200B;\n\nHere com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series DB start to end analytics platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axt54n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708667589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TimescaleDB&lt;/li&gt;\n&lt;li&gt;AWS Timestream&lt;/li&gt;\n&lt;li&gt;influxdb&lt;/li&gt;\n&lt;li&gt;Big Query (last resource, AWS resources used at the company)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axt54n", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "subreddit_subscribers": 163070, "created_utc": 1708667589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were ingested originally they were brought in as varchar(max). I've been tasked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?", "author_fullname": "t2_bz1aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate data type selection for SQL tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axit9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708642689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708638541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were ingested originally they were brought in as varchar(max). I&amp;#39;ve been tasked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axit9z", "is_robot_indexable": true, "report_reasons": null, "author": "Im_probably_naked", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "subreddit_subscribers": 163070, "created_utc": 1708638541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Glue ETL jobs let's you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it's first time I hear about Ray what is it?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ray vs Spark for AWS Glue jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axfi1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708630793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Glue ETL jobs let&amp;#39;s you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it&amp;#39;s first time I hear about Ray what is it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axfi1x", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "subreddit_subscribers": 163070, "created_utc": 1708630793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The history of orchestration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay1xm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JL64oWP3P7j7r3efdomZYATWIuJCsRtjWSYUNKszufU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708698923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dedp.online", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?auto=webp&amp;s=11be9257ae07b7390dcb7e79a355ee4b340530ef", "width": 1384, "height": 978}, "resolutions": [{"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=286c500866d2361011a93f5cab4e424d9c39cfd1", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65be31fd19eef9a9096dda521a1a695134b28ac4", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa542e3036ee8925758313bc111b2dffe97d843c", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1dd86b2814ea7bbed6c4ce63b664f61769435d7c", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb0036fadf648c776f4babf19b6104d67182db4e", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e99de5072042d7f10c96fe752d22a9f2fa052751", "width": 1080, "height": 763}], "variants": {}, "id": "tGvlSbDcYV912Yv0Pzp_4vBc8R8zSTLhvB837XgyVrQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ay1xm1", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ay1xm1/the_history_of_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "subreddit_subscribers": 163070, "created_utc": 1708698923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute `fav_color` would not store `red` and `blue` but rather `0` and `4`. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:\n\n1. Create a seed for each attribute's code translation and reference and join it  to the base table in my models\n2. Add the decoded column directly in my model's sql\n\nWhat do you see as a preferred way? Is there maybe another way?", "author_fullname": "t2_mwoc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Wanted: How do you deal with coded categorical values in dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzttw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708693059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute &lt;code&gt;fav_color&lt;/code&gt; would not store &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;blue&lt;/code&gt; but rather &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt;. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a seed for each attribute&amp;#39;s code translation and reference and join it  to the base table in my models&lt;/li&gt;\n&lt;li&gt;Add the decoded column directly in my model&amp;#39;s sql&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you see as a preferred way? Is there maybe another way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzttw", "is_robot_indexable": true, "report_reasons": null, "author": "rick854", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "subreddit_subscribers": 163070, "created_utc": 1708693059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.\n\nI was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It'd make matching between timeseries and more static information stored in classic db tables easier.\n\n- anyone experience moving from either prometheus/influxdbv2 to timescale?\n- prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?", "author_fullname": "t2_av9qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Prometheus, InfluxDB &amp; MariaDB to PostgreSQL&amp;TimeScaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzaza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708691370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.&lt;/p&gt;\n\n&lt;p&gt;I was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It&amp;#39;d make matching between timeseries and more static information stored in classic db tables easier.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;anyone experience moving from either prometheus/influxdbv2 to timescale?&lt;/li&gt;\n&lt;li&gt;prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzaza", "is_robot_indexable": true, "report_reasons": null, "author": "thingthatgoesbump", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "subreddit_subscribers": 163070, "created_utc": 1708691370.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}