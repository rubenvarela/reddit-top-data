{"kind": "Listing", "data": {"after": "t3_1axsatb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d \n", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior Engineers: Advice to your Noobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axo1k5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708651804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1axo1k5", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "subreddit_subscribers": 163091, "created_utc": 1708651804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?\n\n[https://www.talend.com/products/talend-open-studio/](https://www.talend.com/products/talend-open-studio/)\n\nThanks!\n\nEdit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file", "author_fullname": "t2_icbe4i2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talend is no longer free", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyooe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708705682.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708689321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.talend.com/products/talend-open-studio/\"&gt;https://www.talend.com/products/talend-open-studio/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?auto=webp&amp;s=192d811c869f7f0f606e5214026e4e26d2eb98e4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ead6cad649baf31abeb39ae4955178d556090bc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b438b5d24374741a22c280a9bd1e926e4be4cbc", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf28fc6b8c9932dc30d18080c1d4b55a85218dd", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d102f5369b8ef2f8f386bb390271874be381c4b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b9dbef65759861df9c8adac10ecc69badde2028", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecedd45042969b44355fcc440d0b3e1c96f94429", "width": 1080, "height": 565}], "variants": {}, "id": "Vf_rH0Wd4M6dY9BsVOYZiTrai7iv0IM8pn7AIO9Y-q4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyooe", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Bug9572", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "subreddit_subscribers": 163091, "created_utc": 1708689321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed\n\nHow is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? ", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is spark really used and deployed in production in companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay0572", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708694000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed&lt;/p&gt;\n\n&lt;p&gt;How is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay0572", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "subreddit_subscribers": 163091, "created_utc": 1708694000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**There's a large scale finance data migration project going on in my company (FAANG)** moving processes and data between two enterprise tools. It's been running for &gt;5 years, with \\~100 people working on it, and it's still not finished. \n\n**Can you guys help me build intuition on why this is such a long running project.** I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. \n\n&amp;#x200B;", "author_fullname": "t2_tszztjadn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Migration Projects - the good, the bad, and the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtzgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708670588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;There&amp;#39;s a large scale finance data migration project going on in my company (FAANG)&lt;/strong&gt; moving processes and data between two enterprise tools. It&amp;#39;s been running for &amp;gt;5 years, with ~100 people working on it, and it&amp;#39;s still not finished. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Can you guys help me build intuition on why this is such a long running project.&lt;/strong&gt; I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axtzgp", "is_robot_indexable": true, "report_reasons": null, "author": "jmack_startups", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "subreddit_subscribers": 163091, "created_utc": 1708670588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light ", "author_fullname": "t2_pt8bni2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be best online course as an entry point to understand Data engineering ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axqynj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708660414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axqynj", "is_robot_indexable": true, "report_reasons": null, "author": "joy_warzone", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "subreddit_subscribers": 163091, "created_utc": 1708660414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at the [Gitlab handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/#data-sources) and they have a list of their data sources along with how frequently data is update in their warehouse, e.g. every 24 hours.\n\nImagine I'm building a data warehouse, and my source is a database which is constantly changing 24 hours day - say I run an online store and people are constantly making orders.\n\nI want to lift several tables to load into a raw zone in my data warehouse - no transformations at this point, just lift and shift. I need to be careful to not overload the source database. The data are constantly changing at source, but I need the data in my warehouse to be taken at the same point in time - i.e. I don't want to extract and load one table as at 12:00am but then extract and load a related table as at 12:05am or the data in both tables could be out of sync.\n\nHow would you handle data which is constantly changing? Is there something I can read about this somewhere? I'm worried that I end up loading tables at different points in time and they end up out of sync. ", "author_fullname": "t2_40ho7lud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ELT with constantly changing data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axf0tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708629632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at the &lt;a href=\"https://handbook.gitlab.com/handbook/business-technology/data-team/platform/#data-sources\"&gt;Gitlab handbook&lt;/a&gt; and they have a list of their data sources along with how frequently data is update in their warehouse, e.g. every 24 hours.&lt;/p&gt;\n\n&lt;p&gt;Imagine I&amp;#39;m building a data warehouse, and my source is a database which is constantly changing 24 hours day - say I run an online store and people are constantly making orders.&lt;/p&gt;\n\n&lt;p&gt;I want to lift several tables to load into a raw zone in my data warehouse - no transformations at this point, just lift and shift. I need to be careful to not overload the source database. The data are constantly changing at source, but I need the data in my warehouse to be taken at the same point in time - i.e. I don&amp;#39;t want to extract and load one table as at 12:00am but then extract and load a related table as at 12:05am or the data in both tables could be out of sync.&lt;/p&gt;\n\n&lt;p&gt;How would you handle data which is constantly changing? Is there something I can read about this somewhere? I&amp;#39;m worried that I end up loading tables at different points in time and they end up out of sync. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?auto=webp&amp;s=af544e78828882798d7fe4d1c42454d6bd21dc22", "width": 875, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=828e0ee040e1722af1bf0dbb719d2b846ec766b4", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f99184941ef4a831059c95c0f3bfb24a7de84249", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b5e1d3118a33c25bab44e78f2a95e971cbca6e5", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/UglaOVILJ_CxNzNc0sHwmfkzd_dL9QDi-Gxn3VyjC1o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a04b3ad42aa3e307db53deca2024a610d384848", "width": 640, "height": 447}], "variants": {}, "id": "v9bukcUTEDutaePTQidDeR95NYA8AyYs-tp2j6EyUkc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axf0tl", "is_robot_indexable": true, "report_reasons": null, "author": "powerbihelpme", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axf0tl/how_to_elt_with_constantly_changing_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axf0tl/how_to_elt_with_constantly_changing_data/", "subreddit_subscribers": 163091, "created_utc": 1708629632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp; Rust", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How fast can we process a CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axugw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708672377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datapythonista.me", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp;amp; Rust&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axugw5", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axugw5/how_fast_can_we_process_a_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "subreddit_subscribers": 163091, "created_utc": 1708672377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?", "author_fullname": "t2_79w2zsix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be the best way to learn DE as a beginner? Courses vs practice questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyw74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708690002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyw74", "is_robot_indexable": true, "report_reasons": null, "author": "p200g", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "subreddit_subscribers": 163091, "created_utc": 1708690002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a best practice for creating staging models in terms of fields used from the base table?\n\n&amp;#x200B;\n\nFor example, let's say I have a base table named 'customers' with 30 fields and I only use let's say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?\n\n&amp;#x200B;\n\nMy gut tells me to go with the former but I've not yet heard of or read of a best practice around this.\n\n&amp;#x200B;\n\nAny input would greatly appreciated!", "author_fullname": "t2_kmhrbrzny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for folks who utilize a staging layer on top of base tables.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axsaur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708664752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a best practice for creating staging models in terms of fields used from the base table?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For example, let&amp;#39;s say I have a base table named &amp;#39;customers&amp;#39; with 30 fields and I only use let&amp;#39;s say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My gut tells me to go with the former but I&amp;#39;ve not yet heard of or read of a best practice around this.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any input would greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axsaur", "is_robot_indexable": true, "report_reasons": null, "author": "datageek200", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "subreddit_subscribers": 163091, "created_utc": 1708664752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?\n\nWhere do you like to blog and where do you like to read blogs on data engineering?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your favorite place to blog about data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzpek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?&lt;/p&gt;\n\n&lt;p&gt;Where do you like to blog and where do you like to read blogs on data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzpek", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "subreddit_subscribers": 163091, "created_utc": 1708692655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gublzilb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Data Security Posture Management (DSPM)? Complete Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1axy2cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M5F4dQXDSkqez7A-6fvl-0EItp-A93gMVURpFQc5MjE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708687025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "sentra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?auto=webp&amp;s=4fc2e74e785a98fcf491a54f6d845f23fdf61448", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ae88047c44f63dbbbfe00d62b672062240c4e45", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ed8e0d52078cd9a5fd49e0c76987a32beb02628", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec26db953705789122b7328821f4ecd60c4c989b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0adcf53813a8efc9e5f3362f85a0fdb291f00370", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=117a736fcce06d12bb67a78e0dff934de25d6e55", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9cb7a992e9dd0d444e1556279c0f2a8edc8c82a", "width": 1080, "height": 564}], "variants": {}, "id": "RMpa0UQT5yvwro9ukXYf0Fb7og01FOuSx2zmc_RCqBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axy2cn", "is_robot_indexable": true, "report_reasons": null, "author": "newmudbat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axy2cn/what_is_data_security_posture_management_dspm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "subreddit_subscribers": 163091, "created_utc": 1708687025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title is the question. I never jumped on the dbt train it just didn't work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren't willing to use it for their stuff.\n\nI tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. \n\nDuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it's a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.\n\nThanks!", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB at your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ay7847", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708711456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title is the question. I never jumped on the dbt train it just didn&amp;#39;t work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren&amp;#39;t willing to use it for their stuff.&lt;/p&gt;\n\n&lt;p&gt;I tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. &lt;/p&gt;\n\n&lt;p&gt;DuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it&amp;#39;s a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay7847", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "subreddit_subscribers": 163091, "created_utc": 1708711456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute `fav_color` would not store `red` and `blue` but rather `0` and `4`. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:\n\n1. Create a seed for each attribute's code translation and reference and join it  to the base table in my models\n2. Add the decoded column directly in my model's sql\n\nWhat do you see as a preferred way? Is there maybe another way?", "author_fullname": "t2_mwoc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Wanted: How do you deal with coded categorical values in dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzttw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708693059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute &lt;code&gt;fav_color&lt;/code&gt; would not store &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;blue&lt;/code&gt; but rather &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt;. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a seed for each attribute&amp;#39;s code translation and reference and join it  to the base table in my models&lt;/li&gt;\n&lt;li&gt;Add the decoded column directly in my model&amp;#39;s sql&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you see as a preferred way? Is there maybe another way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzttw", "is_robot_indexable": true, "report_reasons": null, "author": "rick854", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "subreddit_subscribers": 163091, "created_utc": 1708693059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!  \n\n\nI am in the process of evaluating different possible setups for a data lakehouse on GCP.   \n\n\nI have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   \n\n\nI have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn't have to be immense.   \n\n\nI retrieve on two columns: datetime and a \"tags\" column, which is a string.   \n\n\n1. I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? \n2. Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? \n3. During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?\n4. After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. \n\nThanks for the help!  \n", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a Data Lakehouse / Delta Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtb5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708668162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!  &lt;/p&gt;\n\n&lt;p&gt;I am in the process of evaluating different possible setups for a data lakehouse on GCP.   &lt;/p&gt;\n\n&lt;p&gt;I have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   &lt;/p&gt;\n\n&lt;p&gt;I have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn&amp;#39;t have to be immense.   &lt;/p&gt;\n\n&lt;p&gt;I retrieve on two columns: datetime and a &amp;quot;tags&amp;quot; column, which is a string.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? &lt;/li&gt;\n&lt;li&gt;Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? &lt;/li&gt;\n&lt;li&gt;During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?&lt;/li&gt;\n&lt;li&gt;After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for the help!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axtb5k", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "subreddit_subscribers": 163091, "created_utc": 1708668162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\nI need you help. I am working for a startup (let's call it *Darth Vader*) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:\n\n1. Download data from Darth Vader's social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.\n2. Copy all these data in a Google Sheets (*Dashbord*).\n3. Compute some analytics: Engagement Rate, Followers Growth, etc.\n\nNow, let's talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:\n\n* It does not work for all the social networks.\n* It does not provide all the data that I need.\n* Bad connection with Google Sheets.\n\nExtra: we have a free account on [Later](https://later.com/). But, I have never used it because I have no access.\n\nDisclaimer: there is no employee that has competence in data engineering.\n\nMay you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.\n\nThank You so much", "author_fullname": "t2_a1lozp3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP ME!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ay7kx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708713042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708712300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;I need you help. I am working for a startup (let&amp;#39;s call it &lt;em&gt;Darth Vader&lt;/em&gt;) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download data from Darth Vader&amp;#39;s social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.&lt;/li&gt;\n&lt;li&gt;Copy all these data in a Google Sheets (&lt;em&gt;Dashbord&lt;/em&gt;).&lt;/li&gt;\n&lt;li&gt;Compute some analytics: Engagement Rate, Followers Growth, etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, let&amp;#39;s talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It does not work for all the social networks.&lt;/li&gt;\n&lt;li&gt;It does not provide all the data that I need.&lt;/li&gt;\n&lt;li&gt;Bad connection with Google Sheets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extra: we have a free account on &lt;a href=\"https://later.com/\"&gt;Later&lt;/a&gt;. But, I have never used it because I have no access.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: there is no employee that has competence in data engineering.&lt;/p&gt;\n\n&lt;p&gt;May you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.&lt;/p&gt;\n\n&lt;p&gt;Thank You so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay7kx7", "is_robot_indexable": true, "report_reasons": null, "author": "Frugoljno", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7kx7/help_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7kx7/help_me/", "subreddit_subscribers": 163091, "created_utc": 1708712300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i'm need to integrate a bunch of data from some api's around 50 million rows and need some advice to integrate that.\n\nI'm looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.\n\nObviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?\n\nThanks for your support ", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your ingestation tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay60ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708708688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i&amp;#39;m need to integrate a bunch of data from some api&amp;#39;s around 50 million rows and need some advice to integrate that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Obviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your support &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay60ha", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "subreddit_subscribers": 163091, "created_utc": 1708708688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are evaluating whether to go with multiple smaller databases or one large one. \n\nSmall databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. \n\nI perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. \n\nWhat is your view on this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple small databases x One large", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzofv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are evaluating whether to go with multiple smaller databases or one large one. &lt;/p&gt;\n\n&lt;p&gt;Small databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. &lt;/p&gt;\n\n&lt;p&gt;I perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzofv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "subreddit_subscribers": 163091, "created_utc": 1708692572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.\n\nI was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It'd make matching between timeseries and more static information stored in classic db tables easier.\n\n- anyone experience moving from either prometheus/influxdbv2 to timescale?\n- prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?", "author_fullname": "t2_av9qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Prometheus, InfluxDB &amp; MariaDB to PostgreSQL&amp;TimeScaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzaza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708691370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.&lt;/p&gt;\n\n&lt;p&gt;I was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It&amp;#39;d make matching between timeseries and more static information stored in classic db tables easier.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;anyone experience moving from either prometheus/influxdbv2 to timescale?&lt;/li&gt;\n&lt;li&gt;prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzaza", "is_robot_indexable": true, "report_reasons": null, "author": "thingthatgoesbump", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "subreddit_subscribers": 163091, "created_utc": 1708691370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was thinking of learning IAC and trying to use it in our system, but I don't see the picture of how it is useful for a stable system (that just works and doesn't change often). We have development and production environments (that are similar except for the cost and powers), and we kind of just let them work. Are there any benefits that I can suggest from having this infrastructure encoded? I do understand the benefits of encoding (version control, easier governance), but also it is a kind of additional documentation that needs to be maintained.", "author_fullname": "t2_ep4hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the use cases of Terraform / IAC for BI platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axv0cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708674543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of learning IAC and trying to use it in our system, but I don&amp;#39;t see the picture of how it is useful for a stable system (that just works and doesn&amp;#39;t change often). We have development and production environments (that are similar except for the cost and powers), and we kind of just let them work. Are there any benefits that I can suggest from having this infrastructure encoded? I do understand the benefits of encoding (version control, easier governance), but also it is a kind of additional documentation that needs to be maintained.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axv0cw", "is_robot_indexable": true, "report_reasons": null, "author": "Volume999", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axv0cw/what_are_the_use_cases_of_terraform_iac_for_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axv0cw/what_are_the_use_cases_of_terraform_iac_for_bi/", "subreddit_subscribers": 163091, "created_utc": 1708674543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as\n\n* TimescaleDB\n* AWS Timestream\n* influxdb\n* Big Query (last resource, AWS resources used at the company)\n\n&amp;#x200B;\n\nHere com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series DB start to end analytics platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axt54n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708667589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TimescaleDB&lt;/li&gt;\n&lt;li&gt;AWS Timestream&lt;/li&gt;\n&lt;li&gt;influxdb&lt;/li&gt;\n&lt;li&gt;Big Query (last resource, AWS resources used at the company)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axt54n", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "subreddit_subscribers": 163091, "created_utc": 1708667589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were ingested originally they were brought in as varchar(max). I've been tasked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?", "author_fullname": "t2_bz1aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate data type selection for SQL tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axit9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708642689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708638541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were ingested originally they were brought in as varchar(max). I&amp;#39;ve been tasked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axit9z", "is_robot_indexable": true, "report_reasons": null, "author": "Im_probably_naked", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "subreddit_subscribers": 163091, "created_utc": 1708638541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Glue ETL jobs let's you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it's first time I hear about Ray what is it?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ray vs Spark for AWS Glue jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axfi1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708630793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Glue ETL jobs let&amp;#39;s you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it&amp;#39;s first time I hear about Ray what is it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axfi1x", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "subreddit_subscribers": 163091, "created_utc": 1708630793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The history of orchestration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay1xm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JL64oWP3P7j7r3efdomZYATWIuJCsRtjWSYUNKszufU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708698923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dedp.online", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?auto=webp&amp;s=11be9257ae07b7390dcb7e79a355ee4b340530ef", "width": 1384, "height": 978}, "resolutions": [{"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=286c500866d2361011a93f5cab4e424d9c39cfd1", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65be31fd19eef9a9096dda521a1a695134b28ac4", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa542e3036ee8925758313bc111b2dffe97d843c", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1dd86b2814ea7bbed6c4ce63b664f61769435d7c", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb0036fadf648c776f4babf19b6104d67182db4e", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e99de5072042d7f10c96fe752d22a9f2fa052751", "width": 1080, "height": 763}], "variants": {}, "id": "tGvlSbDcYV912Yv0Pzp_4vBc8R8zSTLhvB837XgyVrQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ay1xm1", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ay1xm1/the_history_of_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "subreddit_subscribers": 163091, "created_utc": 1708698923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pretty much title, I'm intentionally failing a dag with a retry of 10 minutes, and I'm noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I'm misunderstanding something.", "author_fullname": "t2_orzx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it correct behaviour for airflow to hold onto a task which is 'up for retry' and prevent workers from picking up queue items while it waits for the retry period? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzk4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much title, I&amp;#39;m intentionally failing a dag with a retry of 10 minutes, and I&amp;#39;m noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I&amp;#39;m misunderstanding something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzk4y", "is_robot_indexable": true, "report_reasons": null, "author": "Mrfunnynuts", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "subreddit_subscribers": 163091, "created_utc": 1708692183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have what probably sounds like a newbie question, but something I am being told doesn\u2019t sound right and I want to fact check it.\n\nSituation: We work with a well established trusted SaaS vendor. We need to send a daily file to their file server. They have a pretty standard https post API end point we can use to send the file to this server. It has pgp encryption so it should pass security protocols. In small scale testing, my team can easily do this with postman and soapui. \n\nProblem: We have asked IT to productionalize this call so we don\u2019t have to manually run the call everyday. They have come back with a 6 month lead time because they say we have to build our own api to communicate with this other API. I am struggling to see why this would be the case. Am I crazy or Is there a valid reason this may be needed? It feels easy enough to just make the call. I am trying to figure out how much to push on this since I am definitely not an expert.", "author_fullname": "t2_qxm2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calling APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axsatb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708664749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what probably sounds like a newbie question, but something I am being told doesn\u2019t sound right and I want to fact check it.&lt;/p&gt;\n\n&lt;p&gt;Situation: We work with a well established trusted SaaS vendor. We need to send a daily file to their file server. They have a pretty standard https post API end point we can use to send the file to this server. It has pgp encryption so it should pass security protocols. In small scale testing, my team can easily do this with postman and soapui. &lt;/p&gt;\n\n&lt;p&gt;Problem: We have asked IT to productionalize this call so we don\u2019t have to manually run the call everyday. They have come back with a 6 month lead time because they say we have to build our own api to communicate with this other API. I am struggling to see why this would be the case. Am I crazy or Is there a valid reason this may be needed? It feels easy enough to just make the call. I am trying to figure out how much to push on this since I am definitely not an expert.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axsatb", "is_robot_indexable": true, "report_reasons": null, "author": "Jhp8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axsatb/calling_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axsatb/calling_apis/", "subreddit_subscribers": 163091, "created_utc": 1708664749.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}