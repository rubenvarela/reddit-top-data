{"kind": "Listing", "data": {"after": "t3_1ayacv2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed\n\nHow is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? ", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is spark really used and deployed in production in companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay0572", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708694000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed&lt;/p&gt;\n\n&lt;p&gt;How is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay0572", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "subreddit_subscribers": 163234, "created_utc": 1708694000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.\n\nSome have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.\n\nWhat is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semantic Layers: the good, the bad &amp; the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayg1r3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708732992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.&lt;/p&gt;\n\n&lt;p&gt;Some have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.&lt;/p&gt;\n\n&lt;p&gt;What is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayg1r3", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "subreddit_subscribers": 163234, "created_utc": 1708732992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title is the question. I never jumped on the dbt train it just didn't work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren't willing to use it for their stuff.\n\nI tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. \n\nDuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it's a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.\n\nThanks!", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB at your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7847", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708711456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title is the question. I never jumped on the dbt train it just didn&amp;#39;t work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren&amp;#39;t willing to use it for their stuff.&lt;/p&gt;\n\n&lt;p&gt;I tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. &lt;/p&gt;\n\n&lt;p&gt;DuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it&amp;#39;s a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay7847", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "subreddit_subscribers": 163234, "created_utc": 1708711456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn't the best but it's slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.\n\nWhat are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What defines a good Python pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay8kfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708714658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn&amp;#39;t the best but it&amp;#39;s slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.&lt;/p&gt;\n\n&lt;p&gt;What are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay8kfq", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "subreddit_subscribers": 163234, "created_utc": 1708714658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?", "author_fullname": "t2_79w2zsix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be the best way to learn DE as a beginner? Courses vs practice questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyw74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708690002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyw74", "is_robot_indexable": true, "report_reasons": null, "author": "p200g", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "subreddit_subscribers": 163234, "created_utc": 1708690002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A senior director of engineering told me that a data engineer's role job is merely just maintenance work once the data pipeline is ready. How true or false is this? Assuming it's true, doesn't it make sense for companies to just hire data engineers on a contract basis? \nAlso, is the job market for data engineers saturated in general for the same reason? ", "author_fullname": "t2_n60i6t07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data engineering better off as a contract position ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayp1bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708760968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A senior director of engineering told me that a data engineer&amp;#39;s role job is merely just maintenance work once the data pipeline is ready. How true or false is this? Assuming it&amp;#39;s true, doesn&amp;#39;t it make sense for companies to just hire data engineers on a contract basis? \nAlso, is the job market for data engineers saturated in general for the same reason? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayp1bg", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedLettuce97", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayp1bg/is_data_engineering_better_off_as_a_contract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayp1bg/is_data_engineering_better_off_as_a_contract/", "subreddit_subscribers": 163234, "created_utc": 1708760968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good evening everyone, I am currently pursuing a master's degree in Computer Science, with a focus Artificial Intelligence, I've done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I've been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?\n\n\nTLDR\n\nI am doing an internship as data engineer, would it be wasted later if i \"change career\" to data scientist? is there a career that brings these two worlds together?\nThank you very much!", "author_fullname": "t2_d4ged0nj5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From data engineer to data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay6i2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708709788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening everyone, I am currently pursuing a master&amp;#39;s degree in Computer Science, with a focus Artificial Intelligence, I&amp;#39;ve done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I&amp;#39;ve been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?&lt;/p&gt;\n\n&lt;p&gt;TLDR&lt;/p&gt;\n\n&lt;p&gt;I am doing an internship as data engineer, would it be wasted later if i &amp;quot;change career&amp;quot; to data scientist? is there a career that brings these two worlds together?\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ay6i2r", "is_robot_indexable": true, "report_reasons": null, "author": "No-Garden9594", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "subreddit_subscribers": 163234, "created_utc": 1708709788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a Data Engineer 2 years ago when I was very junior to the role for a medium sized marketing company. I don't have a CS degree. I picked up a lot of stuff on my own. I really enjoyed working with with my boss at the time because he was super helpful and made work fun. However the only caveat was that he took on a lot of the work and gave me and the rest of my team the easy stuff. I was working on mostly ETL type work and building. I did this for a year and I really loved it.\n\nOnce that project was complete, I moved over to a more production support role and just found it more difficult as we have a ton of vendors and it took me a while to figure out the system holistically. I am not strong with debugging since I am new to this type of work (going through other people's code and fixing things, testing, etc). I am getting a better grasp at it now, but I still have a long way to go. I can debug, but being in production support, I have to find the RC and apply fixes so it doesn't happen again. This is harder for me since I am not as fast as my more experienced peers.\n\n3 months ago there have been a huge focus on efficiency and lots of DEs were getting let go/forced out. They were handing out PIPs to a lot of people. Management only wants very experienced senior people now and no longer want junior/intermediate DEs. I'm 100% sure that I'm next since they've been announcing it all over that we now need to meet their new performance metrics. I'm a lot clearer of what is required of me now to move up, but I am obviously not making the cut and I am sure I will be given a PIP soon and out the door within the next 1-2 months.\n\nThis experience itself made me feel more exposed and questioning my ability to be a DE. I joined this company when a new DE team was being formed and we didn't have anything already in place so I picked up a lot of bad practices and never learned proper things like TDD, best coding practices, etc.\n\nI looked through job postings and I really don't think I can make the cut. Job postings appear to only want Senior DEs. If I could continue with being a DE, I don't mind. But a lot of external factors like what I did at this company and what will be expected of these roles point to me not being able to. The production support aspect is bothering me and making me question if I can do this.\n\nLooking for advice to see what I can/should do. I need a more supportive team to help me grow like with my first boss, but allow me to be challenged/grow. Does this exist? Or should I focus on a career change by pivoting?", "author_fullname": "t2_4kmb0igx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting let go and needing some advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayov6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708760524.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708760306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a Data Engineer 2 years ago when I was very junior to the role for a medium sized marketing company. I don&amp;#39;t have a CS degree. I picked up a lot of stuff on my own. I really enjoyed working with with my boss at the time because he was super helpful and made work fun. However the only caveat was that he took on a lot of the work and gave me and the rest of my team the easy stuff. I was working on mostly ETL type work and building. I did this for a year and I really loved it.&lt;/p&gt;\n\n&lt;p&gt;Once that project was complete, I moved over to a more production support role and just found it more difficult as we have a ton of vendors and it took me a while to figure out the system holistically. I am not strong with debugging since I am new to this type of work (going through other people&amp;#39;s code and fixing things, testing, etc). I am getting a better grasp at it now, but I still have a long way to go. I can debug, but being in production support, I have to find the RC and apply fixes so it doesn&amp;#39;t happen again. This is harder for me since I am not as fast as my more experienced peers.&lt;/p&gt;\n\n&lt;p&gt;3 months ago there have been a huge focus on efficiency and lots of DEs were getting let go/forced out. They were handing out PIPs to a lot of people. Management only wants very experienced senior people now and no longer want junior/intermediate DEs. I&amp;#39;m 100% sure that I&amp;#39;m next since they&amp;#39;ve been announcing it all over that we now need to meet their new performance metrics. I&amp;#39;m a lot clearer of what is required of me now to move up, but I am obviously not making the cut and I am sure I will be given a PIP soon and out the door within the next 1-2 months.&lt;/p&gt;\n\n&lt;p&gt;This experience itself made me feel more exposed and questioning my ability to be a DE. I joined this company when a new DE team was being formed and we didn&amp;#39;t have anything already in place so I picked up a lot of bad practices and never learned proper things like TDD, best coding practices, etc.&lt;/p&gt;\n\n&lt;p&gt;I looked through job postings and I really don&amp;#39;t think I can make the cut. Job postings appear to only want Senior DEs. If I could continue with being a DE, I don&amp;#39;t mind. But a lot of external factors like what I did at this company and what will be expected of these roles point to me not being able to. The production support aspect is bothering me and making me question if I can do this.&lt;/p&gt;\n\n&lt;p&gt;Looking for advice to see what I can/should do. I need a more supportive team to help me grow like with my first boss, but allow me to be challenged/grow. Does this exist? Or should I focus on a career change by pivoting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ayov6r", "is_robot_indexable": true, "report_reasons": null, "author": "codingisfun4me", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayov6r/getting_let_go_and_needing_some_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayov6r/getting_let_go_and_needing_some_advice/", "subreddit_subscribers": 163234, "created_utc": 1708760306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to prove a point.\n\nExecutives in every organization are jumping into AI which is an ongoing trend. \n\nHowever, it should not be at the expense of dismissing data engineering. \n\nIf data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. ", "author_fullname": "t2_aryc45smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we say Data Engineering is a precursor to AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqx22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708768510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to prove a point.&lt;/p&gt;\n\n&lt;p&gt;Executives in every organization are jumping into AI which is an ongoing trend. &lt;/p&gt;\n\n&lt;p&gt;However, it should not be at the expense of dismissing data engineering. &lt;/p&gt;\n\n&lt;p&gt;If data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqx22", "is_robot_indexable": true, "report_reasons": null, "author": "Paperplaneflyr", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "subreddit_subscribers": 163234, "created_utc": 1708768510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i'm need to integrate a bunch of data from some api's around 50 million rows and need some advice to integrate that.\n\nI'm looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.\n\nObviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?\n\nThanks for your support ", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your ingestation tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay60ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708708688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i&amp;#39;m need to integrate a bunch of data from some api&amp;#39;s around 50 million rows and need some advice to integrate that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Obviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your support &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay60ha", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "subreddit_subscribers": 163234, "created_utc": 1708708688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute `fav_color` would not store `red` and `blue` but rather `0` and `4`. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:\n\n1. Create a seed for each attribute's code translation and reference and join it  to the base table in my models\n2. Add the decoded column directly in my model's sql\n\nWhat do you see as a preferred way? Is there maybe another way?", "author_fullname": "t2_mwoc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Wanted: How do you deal with coded categorical values in dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzttw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708693059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute &lt;code&gt;fav_color&lt;/code&gt; would not store &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;blue&lt;/code&gt; but rather &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt;. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a seed for each attribute&amp;#39;s code translation and reference and join it  to the base table in my models&lt;/li&gt;\n&lt;li&gt;Add the decoded column directly in my model&amp;#39;s sql&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you see as a preferred way? Is there maybe another way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzttw", "is_robot_indexable": true, "report_reasons": null, "author": "rick854", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "subreddit_subscribers": 163234, "created_utc": 1708693059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?\n\nWhere do you like to blog and where do you like to read blogs on data engineering?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your favorite place to blog about data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzpek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?&lt;/p&gt;\n\n&lt;p&gt;Where do you like to blog and where do you like to read blogs on data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzpek", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "subreddit_subscribers": 163234, "created_utc": 1708692655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \nI\u2019m overwhelmed with all the info I\u2019ve right now, I am graduating this semester, I have strong foundations of Python and sql and I know a bit of mongoDB. I am planning to apply for data engineer roles and I\u2019ve made a plan (need inputs/corrections). \n\nMy plan as of now \nPython \u27a1\ufe0f SQL \u27a1\ufe0f Spark \u27a1\ufe0f Cloud \u27a1\ufe0f Airflow \u27a1\ufe0f GIT \n\n1. Should I learn Apache spark or pyspark( Ik this is built on spark but has some limitations) \n2. What does spark + databricks and language is Pyspark mean? \n\nCan someone please mentor me and guide through this and provide resources.\n\nI am gonna graduate soon and I\u2019m very clueless right now \ud83d\ude10", "author_fullname": "t2_ftbeovnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance Required !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayo5h7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708757525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m overwhelmed with all the info I\u2019ve right now, I am graduating this semester, I have strong foundations of Python and sql and I know a bit of mongoDB. I am planning to apply for data engineer roles and I\u2019ve made a plan (need inputs/corrections). &lt;/p&gt;\n\n&lt;p&gt;My plan as of now \nPython \u27a1\ufe0f SQL \u27a1\ufe0f Spark \u27a1\ufe0f Cloud \u27a1\ufe0f Airflow \u27a1\ufe0f GIT &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I learn Apache spark or pyspark( Ik this is built on spark but has some limitations) &lt;/li&gt;\n&lt;li&gt;What does spark + databricks and language is Pyspark mean? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can someone please mentor me and guide through this and provide resources.&lt;/p&gt;\n\n&lt;p&gt;I am gonna graduate soon and I\u2019m very clueless right now \ud83d\ude10&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ayo5h7", "is_robot_indexable": true, "report_reasons": null, "author": "happyplantt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayo5h7/guidance_required/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayo5h7/guidance_required/", "subreddit_subscribers": 163234, "created_utc": 1708757525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our company is undergoing a major \"v1 to v2\" shift to microservices, which is resulting in our back-end completely redesigning all our data sources. This includes also removing entire concepts for some entities and adding new ones we haven't had before.\n\nWhile in the ideal world the data team would have had more of a say in these design choices, that's not the world I'm living in. We're now being tasked with essentially \"figuring it out\" on our end. While ingesting the raw data into our data lake isn't necessarily a concern (our ingestion pattern and pipelines will remain the same and just point to new containers), our data warehouse layer is a HUGE concern.\n\nI'm having a hard time finding guidance out there for best practices around merging \"versions\" of data structures so that historical reporting will still be reasonably feasible.\n\nInterested to know what others have done. Do you rebuild your data warehouse based on v2 and then shunt in v1 data as best as you can? Do you have separate scripts for v1 and v2 data for the same tables and then combine at a later stage, or do you end up with monstrously huge scripts that create tables with both v1 and v2 data \"unioned\" together (essentially doubling the code for each table)? How do you handle your schemas? Do you keep v1 and v2 in separate schemas, or do you keep related v1 and v2 data together in the same spot? Etc. etc.\n\nJust looking for literally any insight or suggestions folks may have! This is a massive undertaking and while we're working really hard on it and have likely answers to some of the questions above, we're still working it all out and I'm worried about missing major things.", "author_fullname": "t2_b67xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for handling a major underlying shift in back-end data structures to still enable historical reporting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aycstw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708724967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our company is undergoing a major &amp;quot;v1 to v2&amp;quot; shift to microservices, which is resulting in our back-end completely redesigning all our data sources. This includes also removing entire concepts for some entities and adding new ones we haven&amp;#39;t had before.&lt;/p&gt;\n\n&lt;p&gt;While in the ideal world the data team would have had more of a say in these design choices, that&amp;#39;s not the world I&amp;#39;m living in. We&amp;#39;re now being tasked with essentially &amp;quot;figuring it out&amp;quot; on our end. While ingesting the raw data into our data lake isn&amp;#39;t necessarily a concern (our ingestion pattern and pipelines will remain the same and just point to new containers), our data warehouse layer is a HUGE concern.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time finding guidance out there for best practices around merging &amp;quot;versions&amp;quot; of data structures so that historical reporting will still be reasonably feasible.&lt;/p&gt;\n\n&lt;p&gt;Interested to know what others have done. Do you rebuild your data warehouse based on v2 and then shunt in v1 data as best as you can? Do you have separate scripts for v1 and v2 data for the same tables and then combine at a later stage, or do you end up with monstrously huge scripts that create tables with both v1 and v2 data &amp;quot;unioned&amp;quot; together (essentially doubling the code for each table)? How do you handle your schemas? Do you keep v1 and v2 in separate schemas, or do you keep related v1 and v2 data together in the same spot? Etc. etc.&lt;/p&gt;\n\n&lt;p&gt;Just looking for literally any insight or suggestions folks may have! This is a massive undertaking and while we&amp;#39;re working really hard on it and have likely answers to some of the questions above, we&amp;#39;re still working it all out and I&amp;#39;m worried about missing major things.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aycstw", "is_robot_indexable": true, "report_reasons": null, "author": "eelwheel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aycstw/best_practices_for_handling_a_major_underlying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aycstw/best_practices_for_handling_a_major_underlying/", "subreddit_subscribers": 163234, "created_utc": 1708724967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\nI need you help. I am working for a startup (let's call it *Darth Vader*) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:\n\n1. Download data from Darth Vader's social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.\n2. Copy all these data in a Google Sheets (*Dashbord*).\n3. Compute some analytics: Engagement Rate, Followers Growth, etc.\n\nNow, let's talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:\n\n* It does not work for all the social networks.\n* It does not provide all the data that I need.\n* Bad connection with Google Sheets.\n\nExtra: we have a free account on [Later](https://later.com/). But, I have never used it because I have no access.\n\nDisclaimer: there is no employee that has competence in data engineering.\n\nMay you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.\n\nThank You so much", "author_fullname": "t2_a1lozp3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP ME!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7kx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708713042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708712300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;I need you help. I am working for a startup (let&amp;#39;s call it &lt;em&gt;Darth Vader&lt;/em&gt;) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download data from Darth Vader&amp;#39;s social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.&lt;/li&gt;\n&lt;li&gt;Copy all these data in a Google Sheets (&lt;em&gt;Dashbord&lt;/em&gt;).&lt;/li&gt;\n&lt;li&gt;Compute some analytics: Engagement Rate, Followers Growth, etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, let&amp;#39;s talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It does not work for all the social networks.&lt;/li&gt;\n&lt;li&gt;It does not provide all the data that I need.&lt;/li&gt;\n&lt;li&gt;Bad connection with Google Sheets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extra: we have a free account on &lt;a href=\"https://later.com/\"&gt;Later&lt;/a&gt;. But, I have never used it because I have no access.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: there is no employee that has competence in data engineering.&lt;/p&gt;\n\n&lt;p&gt;May you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.&lt;/p&gt;\n\n&lt;p&gt;Thank You so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay7kx7", "is_robot_indexable": true, "report_reasons": null, "author": "Frugoljno", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7kx7/help_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7kx7/help_me/", "subreddit_subscribers": 163234, "created_utc": 1708712300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are evaluating whether to go with multiple smaller databases or one large one. \n\nSmall databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. \n\nI perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. \n\nWhat is your view on this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple small databases x One large", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzofv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are evaluating whether to go with multiple smaller databases or one large one. &lt;/p&gt;\n\n&lt;p&gt;Small databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. &lt;/p&gt;\n\n&lt;p&gt;I perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzofv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "subreddit_subscribers": 163234, "created_utc": 1708692572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.\n\nI was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It'd make matching between timeseries and more static information stored in classic db tables easier.\n\n- anyone experience moving from either prometheus/influxdbv2 to timescale?\n- prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?", "author_fullname": "t2_av9qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Prometheus, InfluxDB &amp; MariaDB to PostgreSQL&amp;TimeScaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzaza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708691370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.&lt;/p&gt;\n\n&lt;p&gt;I was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It&amp;#39;d make matching between timeseries and more static information stored in classic db tables easier.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;anyone experience moving from either prometheus/influxdbv2 to timescale?&lt;/li&gt;\n&lt;li&gt;prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzaza", "is_robot_indexable": true, "report_reasons": null, "author": "thingthatgoesbump", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "subreddit_subscribers": 163234, "created_utc": 1708691370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company's data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  \n\n\nWith that said...  \n\n\n1. What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  \n\n\n2. There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.", "author_fullname": "t2_m0llzgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proper handling, storage and cleaning of manual extracts from a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aysd2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708774029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company&amp;#39;s data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  &lt;/p&gt;\n\n&lt;p&gt;With that said...  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aysd2j", "is_robot_indexable": true, "report_reasons": null, "author": "Bhagafat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "subreddit_subscribers": 163234, "created_utc": 1708774029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All I've ever known is Databricks, back when I was working at a company that handled PBs of data. I am now at a place handling exponentially less data (tiny adtech company, data is ad campaign metrics; so no streaming, just batch ingestion, reading). I have been quite accustomed to doing everything in Pyspark and would hate to go to a SQL-only workflow.\n\nThus, I'm having trouble conceptualizing what a Clickhouse stack would look like for us. We would be storing data in s3 (via some \"EL\" job), and ideally I'd be doing analysis with Polars in a Jupyter notebook (the \"T\"), and  creating dataviz dashboards on some other platform. Could Clickhouse occupy the middle db layer here, and if so, would I be wasting its powerfulness and efficiencies if I am doing stuff downstream in Polars instead of in its analytics suite?", "author_fullname": "t2_co2klnt5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coming from Databricks, a bit confused about Clickhouse + Python support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayeqyb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708729722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All I&amp;#39;ve ever known is Databricks, back when I was working at a company that handled PBs of data. I am now at a place handling exponentially less data (tiny adtech company, data is ad campaign metrics; so no streaming, just batch ingestion, reading). I have been quite accustomed to doing everything in Pyspark and would hate to go to a SQL-only workflow.&lt;/p&gt;\n\n&lt;p&gt;Thus, I&amp;#39;m having trouble conceptualizing what a Clickhouse stack would look like for us. We would be storing data in s3 (via some &amp;quot;EL&amp;quot; job), and ideally I&amp;#39;d be doing analysis with Polars in a Jupyter notebook (the &amp;quot;T&amp;quot;), and  creating dataviz dashboards on some other platform. Could Clickhouse occupy the middle db layer here, and if so, would I be wasting its powerfulness and efficiencies if I am doing stuff downstream in Polars instead of in its analytics suite?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayeqyb", "is_robot_indexable": true, "report_reasons": null, "author": "This-Profession-952", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayeqyb/coming_from_databricks_a_bit_confused_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayeqyb/coming_from_databricks_a_bit_confused_about/", "subreddit_subscribers": 163234, "created_utc": 1708729722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The history of orchestration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay1xm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JL64oWP3P7j7r3efdomZYATWIuJCsRtjWSYUNKszufU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708698923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dedp.online", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?auto=webp&amp;s=11be9257ae07b7390dcb7e79a355ee4b340530ef", "width": 1384, "height": 978}, "resolutions": [{"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=286c500866d2361011a93f5cab4e424d9c39cfd1", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65be31fd19eef9a9096dda521a1a695134b28ac4", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa542e3036ee8925758313bc111b2dffe97d843c", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1dd86b2814ea7bbed6c4ce63b664f61769435d7c", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb0036fadf648c776f4babf19b6104d67182db4e", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e99de5072042d7f10c96fe752d22a9f2fa052751", "width": 1080, "height": 763}], "variants": {}, "id": "tGvlSbDcYV912Yv0Pzp_4vBc8R8zSTLhvB837XgyVrQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ay1xm1", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ay1xm1/the_history_of_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "subreddit_subscribers": 163234, "created_utc": 1708698923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pretty much title, I'm intentionally failing a dag with a retry of 10 minutes, and I'm noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I'm misunderstanding something.", "author_fullname": "t2_orzx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it correct behaviour for airflow to hold onto a task which is 'up for retry' and prevent workers from picking up queue items while it waits for the retry period? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzk4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much title, I&amp;#39;m intentionally failing a dag with a retry of 10 minutes, and I&amp;#39;m noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I&amp;#39;m misunderstanding something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzk4y", "is_robot_indexable": true, "report_reasons": null, "author": "Mrfunnynuts", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "subreddit_subscribers": 163234, "created_utc": 1708692183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All , \n\nWe are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 \n\nI am currently looking at \n- Airbyte \n- Fivetran \n- Meltano \n\nI am looking at combination of Airbyte and Meltano \n\nAirbyte for OOB low Volume and simple Low Code Scenerios \n\nMeltano for heavy lifting \n\nFivetran is out of picture due to the price and ROI we will be making out of it \n\nI am stuck at a point where I need evaluate below aspects \n\n- Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot \n- how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times \n- how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .\n\nThanks\nSiddu Hussain V\n\n\n", "author_fullname": "t2_7yg4aphe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you perform Load testing and security measures for a ELT tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqbu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708766087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All , &lt;/p&gt;\n\n&lt;p&gt;We are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 &lt;/p&gt;\n\n&lt;p&gt;I am currently looking at \n- Airbyte \n- Fivetran \n- Meltano &lt;/p&gt;\n\n&lt;p&gt;I am looking at combination of Airbyte and Meltano &lt;/p&gt;\n\n&lt;p&gt;Airbyte for OOB low Volume and simple Low Code Scenerios &lt;/p&gt;\n\n&lt;p&gt;Meltano for heavy lifting &lt;/p&gt;\n\n&lt;p&gt;Fivetran is out of picture due to the price and ROI we will be making out of it &lt;/p&gt;\n\n&lt;p&gt;I am stuck at a point where I need evaluate below aspects &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot &lt;/li&gt;\n&lt;li&gt;how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times &lt;/li&gt;\n&lt;li&gt;how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks\nSiddu Hussain V&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqbu3", "is_robot_indexable": true, "report_reasons": null, "author": "Immediate-Force6602", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "subreddit_subscribers": 163234, "created_utc": 1708766087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I'm trying to guess what would be the best way to move around data from postgresql to typesense (well, can be anything at this point).  \nMy biggest challenge is more about correctly detecting changes and only stream those to the destination after the first offload.  \n\n\nI've tried Airbyte and it kinda works (what a damn behemoth it is btw), but despite having logical replication enabled and CDC selected, it seems to extract everything regardless of the fact that no changes were made in the origin table, I don't know if I'm doing something wrong but my stupid ass understands that if no changes were made to the table, then no related transactions made it in the wal either, thus no extraction should take place at all... But this is not my case.  \n\n\nFirst of all I would like to understand if my interpretation is correct and that Airbyte is doing something that it shouldn't do in the first place (I'm ignoring the sync method, because the problem is the fact that is reading everything everytime), secondly what can be done about this.\n\nDebezium? Dagster?\n\nOn a related note: currently I'm using a table as result of multiple statements and joins (then I would've added some triggers to check data consistency between the tables and act accordingly), ideally a materialized view with a cron-based refresh seems easier (10 sec x 3M rows), but then how I'm supposed to track the changes and only stream those, since MV doesn't support CDC afaik and seems to refresh entirely?  \n\n\nSorry if my message is confused, I did my best.  \n\n\nCan someone please help me?", "author_fullname": "t2_gfk84466l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data from postgresql to typesense?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayfv19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708732523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m trying to guess what would be the best way to move around data from postgresql to typesense (well, can be anything at this point).&lt;br/&gt;\nMy biggest challenge is more about correctly detecting changes and only stream those to the destination after the first offload.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried Airbyte and it kinda works (what a damn behemoth it is btw), but despite having logical replication enabled and CDC selected, it seems to extract everything regardless of the fact that no changes were made in the origin table, I don&amp;#39;t know if I&amp;#39;m doing something wrong but my stupid ass understands that if no changes were made to the table, then no related transactions made it in the wal either, thus no extraction should take place at all... But this is not my case.  &lt;/p&gt;\n\n&lt;p&gt;First of all I would like to understand if my interpretation is correct and that Airbyte is doing something that it shouldn&amp;#39;t do in the first place (I&amp;#39;m ignoring the sync method, because the problem is the fact that is reading everything everytime), secondly what can be done about this.&lt;/p&gt;\n\n&lt;p&gt;Debezium? Dagster?&lt;/p&gt;\n\n&lt;p&gt;On a related note: currently I&amp;#39;m using a table as result of multiple statements and joins (then I would&amp;#39;ve added some triggers to check data consistency between the tables and act accordingly), ideally a materialized view with a cron-based refresh seems easier (10 sec x 3M rows), but then how I&amp;#39;m supposed to track the changes and only stream those, since MV doesn&amp;#39;t support CDC afaik and seems to refresh entirely?  &lt;/p&gt;\n\n&lt;p&gt;Sorry if my message is confused, I did my best.  &lt;/p&gt;\n\n&lt;p&gt;Can someone please help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayfv19", "is_robot_indexable": true, "report_reasons": null, "author": "SpatolaNellaRoccia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayfv19/moving_data_from_postgresql_to_typesense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayfv19/moving_data_from_postgresql_to_typesense/", "subreddit_subscribers": 163234, "created_utc": 1708732523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nSorry in advance if this is a dumb question.\n\nWhen I try and export my Spark Dataframe as a parquet file ala **spark.write.parquet(...)** (using PySpark), the result is a directory instead of a file. It still reads in fine with Spark, but other apps don't know what to do with it because it's a directory.\n\nHow can I convert it to a file? I can't seem to be able to remove the \"Directory\" attribute from the file in  Windows. \n\nBy the way, it does this on both Google Cloud and Windows (running Spark locally) - so it doesn't seem OS specific. Is it due to the filesystem I am saving the parquet file in? I always just save in a bucket or (on Windows) my C drive.\n\nThanks!", "author_fullname": "t2_4kns99rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Creating Directories Instead of Files When Saving Parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayehhc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708729090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;Sorry in advance if this is a dumb question.&lt;/p&gt;\n\n&lt;p&gt;When I try and export my Spark Dataframe as a parquet file ala &lt;strong&gt;spark.write.parquet(...)&lt;/strong&gt; (using PySpark), the result is a directory instead of a file. It still reads in fine with Spark, but other apps don&amp;#39;t know what to do with it because it&amp;#39;s a directory.&lt;/p&gt;\n\n&lt;p&gt;How can I convert it to a file? I can&amp;#39;t seem to be able to remove the &amp;quot;Directory&amp;quot; attribute from the file in  Windows. &lt;/p&gt;\n\n&lt;p&gt;By the way, it does this on both Google Cloud and Windows (running Spark locally) - so it doesn&amp;#39;t seem OS specific. Is it due to the filesystem I am saving the parquet file in? I always just save in a bucket or (on Windows) my C drive.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayehhc", "is_robot_indexable": true, "report_reasons": null, "author": "JustinPooDough", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayehhc/spark_creating_directories_instead_of_files_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayehhc/spark_creating_directories_instead_of_files_when/", "subreddit_subscribers": 163234, "created_utc": 1708729090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vaz49s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Experience in Data Engineering vs Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayacv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I1Wonq9x1HXhyPM01WQh9QeEWi-VcXCwxWc6babfEuE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708719029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/kyelabs/p/data-engineering-vs-software-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?auto=webp&amp;s=040e07865e22530f6ec8123b4212df05043917b6", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f3ca4cc448b74064654a35746fcc8519693f909", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d45aa59ce4625027dbc925b954f9fee35374d04", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33c446699610f7287fe4959fe950cc38b1d78965", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5f3735775e3f42d0569d5ce8998997346238cd8", "width": 640, "height": 333}], "variants": {}, "id": "yg_YgoVZeMu1-hTTZyBi7QDp6jbS2Jp2eVu09M6cZFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ayacv2", "is_robot_indexable": true, "report_reasons": null, "author": "stringofsense", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayacv2/my_experience_in_data_engineering_vs_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/kyelabs/p/data-engineering-vs-software-engineering", "subreddit_subscribers": 163234, "created_utc": 1708719029.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}