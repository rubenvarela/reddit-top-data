{"kind": "Listing", "data": {"after": "t3_1axsatb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?\n\n[https://www.talend.com/products/talend-open-studio/](https://www.talend.com/products/talend-open-studio/)\n\nThanks!\n\nEdit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file", "author_fullname": "t2_icbe4i2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talend is no longer free", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyooe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708705682.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708689321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.talend.com/products/talend-open-studio/\"&gt;https://www.talend.com/products/talend-open-studio/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?auto=webp&amp;s=192d811c869f7f0f606e5214026e4e26d2eb98e4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ead6cad649baf31abeb39ae4955178d556090bc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b438b5d24374741a22c280a9bd1e926e4be4cbc", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf28fc6b8c9932dc30d18080c1d4b55a85218dd", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d102f5369b8ef2f8f386bb390271874be381c4b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b9dbef65759861df9c8adac10ecc69badde2028", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_-pTYg2vRdIkNLaC4SomsVS_KxmQZK8_axut1ak5NZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecedd45042969b44355fcc440d0b3e1c96f94429", "width": 1080, "height": 565}], "variants": {}, "id": "Vf_rH0Wd4M6dY9BsVOYZiTrai7iv0IM8pn7AIO9Y-q4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyooe", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Bug9572", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/", "subreddit_subscribers": 163142, "created_utc": 1708689321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d \n", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior Engineers: Advice to your Noobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axo1k5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708651804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Senior engineers here, how can I as a junior engineer read, and digest the information from reading books like \u201cSpark the definitive guide\u201d and \u201cDesigning data intensive applications\u201d &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1axo1k5", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axo1k5/senior_engineers_advice_to_your_noobs/", "subreddit_subscribers": 163142, "created_utc": 1708651804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed\n\nHow is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? ", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is spark really used and deployed in production in companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay0572", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708694000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only academic knowledge of spark and haven\u2019t used it in company. I learnt and passed the spark certification, but I don\u2019t have idea how spark is typically used in companies and how it is deployed&lt;/p&gt;\n\n&lt;p&gt;How is it deployed and used? Is it via aws emr or Databricks? I read about the ways in which we decide the number of executors and memory for spark. If we use it in Databricks, do we have to do that step manually? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay0572", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay0572/how_is_spark_really_used_and_deployed_in/", "subreddit_subscribers": 163142, "created_utc": 1708694000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There's a large scale finance data migration project going on in my company moving processes and data between two enterprise tools. It's been running for &gt;5 years, with \\~100 people working on it, and it's still not finished. \n\n**Can you guys help me build intuition on why this is such a long running project.** I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. \n\n&amp;#x200B;", "author_fullname": "t2_tszztjadn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Migration Projects - the good, the bad, and the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtzgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708736679.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708670588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a large scale finance data migration project going on in my company moving processes and data between two enterprise tools. It&amp;#39;s been running for &amp;gt;5 years, with ~100 people working on it, and it&amp;#39;s still not finished. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Can you guys help me build intuition on why this is such a long running project.&lt;/strong&gt; I understand the scale but would love to hear anecdotes or gotchas from your experience working on data migration projects. What worked? What did not? Where was the highest complexity? Were any tools particularly helpful for the migration process. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axtzgp", "is_robot_indexable": true, "report_reasons": null, "author": "jmack_startups", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtzgp/data_migration_projects_the_good_the_bad_and_the/", "subreddit_subscribers": 163142, "created_utc": 1708670588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light ", "author_fullname": "t2_pt8bni2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be best online course as an entry point to understand Data engineering ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axqynj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708660414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone \nI am an automation QA engineer who is trying to switch to Data engineering \nI\u2019ve been looking up for courses in Coursera \nWhat would be the a good course (boot camp) kind of course where I can learn data engineering.\nI know the course alone won\u2019t help me, I will work on projects and contribute to open source .\nBut as the first step which course would be good\nI saw IBM data engineering course offered in coursera is it any good?\nExperts pls shed some light &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axqynj", "is_robot_indexable": true, "report_reasons": null, "author": "joy_warzone", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axqynj/what_would_be_best_online_course_as_an_entry/", "subreddit_subscribers": 163142, "created_utc": 1708660414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title is the question. I never jumped on the dbt train it just didn't work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren't willing to use it for their stuff.\n\nI tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. \n\nDuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it's a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.\n\nThanks!", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB at your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7847", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708711456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title is the question. I never jumped on the dbt train it just didn&amp;#39;t work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren&amp;#39;t willing to use it for their stuff.&lt;/p&gt;\n\n&lt;p&gt;I tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. &lt;/p&gt;\n\n&lt;p&gt;DuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it&amp;#39;s a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay7847", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "subreddit_subscribers": 163142, "created_utc": 1708711456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn't the best but it's slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.\n\nWhat are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What defines a good Python pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay8kfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708714658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn&amp;#39;t the best but it&amp;#39;s slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.&lt;/p&gt;\n\n&lt;p&gt;What are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay8kfq", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "subreddit_subscribers": 163142, "created_utc": 1708714658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?", "author_fullname": "t2_79w2zsix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be the best way to learn DE as a beginner? Courses vs practice questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axyw74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708690002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, As a beginner in DE, What will be the best way to learn DE as a beginner? Will it going through a course in Udacity or Coursera or learning by practice on some projects? What will be the best way to practice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axyw74", "is_robot_indexable": true, "report_reasons": null, "author": "p200g", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axyw74/what_will_be_the_best_way_to_learn_de_as_a/", "subreddit_subscribers": 163142, "created_utc": 1708690002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp; Rust", "author_fullname": "t2_98aju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How fast can we process a CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axugw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708672377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datapythonista.me", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Great article by Marc Garcia outlining how you can speed up your CSV processing with Polars &amp;amp; Rust&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axugw5", "is_robot_indexable": true, "report_reasons": null, "author": "matt78whoop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axugw5/how_fast_can_we_process_a_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file", "subreddit_subscribers": 163142, "created_utc": 1708672377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a best practice for creating staging models in terms of fields used from the base table?\n\n&amp;#x200B;\n\nFor example, let's say I have a base table named 'customers' with 30 fields and I only use let's say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?\n\n&amp;#x200B;\n\nMy gut tells me to go with the former but I've not yet heard of or read of a best practice around this.\n\n&amp;#x200B;\n\nAny input would greatly appreciated!", "author_fullname": "t2_kmhrbrzny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for folks who utilize a staging layer on top of base tables.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axsaur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708664752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a best practice for creating staging models in terms of fields used from the base table?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For example, let&amp;#39;s say I have a base table named &amp;#39;customers&amp;#39; with 30 fields and I only use let&amp;#39;s say 15-20 for analytics. Should I only pull those 15-20 into the staging layer or is it best practice to pull all 30?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My gut tells me to go with the former but I&amp;#39;ve not yet heard of or read of a best practice around this.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any input would greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axsaur", "is_robot_indexable": true, "report_reasons": null, "author": "datageek200", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axsaur/question_for_folks_who_utilize_a_staging_layer_on/", "subreddit_subscribers": 163142, "created_utc": 1708664752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?\n\nWhere do you like to blog and where do you like to read blogs on data engineering?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your favorite place to blog about data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzpek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medium, Substack, Wordpress, Blogger, Hashnode, Dev, Tumblr?&lt;/p&gt;\n\n&lt;p&gt;Where do you like to blog and where do you like to read blogs on data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzpek", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzpek/whats_your_favorite_place_to_blog_about_data/", "subreddit_subscribers": 163142, "created_utc": 1708692655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gublzilb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Data Security Posture Management (DSPM)? Complete Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1axy2cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M5F4dQXDSkqez7A-6fvl-0EItp-A93gMVURpFQc5MjE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708687025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "sentra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?auto=webp&amp;s=4fc2e74e785a98fcf491a54f6d845f23fdf61448", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ae88047c44f63dbbbfe00d62b672062240c4e45", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ed8e0d52078cd9a5fd49e0c76987a32beb02628", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec26db953705789122b7328821f4ecd60c4c989b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0adcf53813a8efc9e5f3362f85a0fdb291f00370", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=117a736fcce06d12bb67a78e0dff934de25d6e55", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/PgB2navL51bhOqdX4C1IMMgyOX0KfRepaUvzoBip8tI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9cb7a992e9dd0d444e1556279c0f2a8edc8c82a", "width": 1080, "height": 564}], "variants": {}, "id": "RMpa0UQT5yvwro9ukXYf0Fb7og01FOuSx2zmc_RCqBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1axy2cn", "is_robot_indexable": true, "report_reasons": null, "author": "newmudbat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axy2cn/what_is_data_security_posture_management_dspm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.sentra.io/data-security-posture-management/what-is-dspm", "subreddit_subscribers": 163142, "created_utc": 1708687025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.\n\nSome have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.\n\nWhat is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semantic Layers: the good, the bad &amp; the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ayg1r3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708732992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.&lt;/p&gt;\n\n&lt;p&gt;Some have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.&lt;/p&gt;\n\n&lt;p&gt;What is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayg1r3", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "subreddit_subscribers": 163142, "created_utc": 1708732992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute `fav_color` would not store `red` and `blue` but rather `0` and `4`. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:\n\n1. Create a seed for each attribute's code translation and reference and join it  to the base table in my models\n2. Add the decoded column directly in my model's sql\n\nWhat do you see as a preferred way? Is there maybe another way?", "author_fullname": "t2_mwoc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Wanted: How do you deal with coded categorical values in dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzttw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708693059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have the situation that I receive a ton of data from public institutions that store categorical data in an integer encoded format. So for example the attribute &lt;code&gt;fav_color&lt;/code&gt; would not store &lt;code&gt;red&lt;/code&gt; and &lt;code&gt;blue&lt;/code&gt; but rather &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt;. The encoding logic is then often delivered in a PDF (\ud83d\ude2b). The dataset is then enriched by my company and afterwards send back to the institutions. Now I am new to dbt and would like to know how you folks handle the materialization of these codes. I see two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a seed for each attribute&amp;#39;s code translation and reference and join it  to the base table in my models&lt;/li&gt;\n&lt;li&gt;Add the decoded column directly in my model&amp;#39;s sql&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you see as a preferred way? Is there maybe another way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzttw", "is_robot_indexable": true, "report_reasons": null, "author": "rick854", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzttw/advice_wanted_how_do_you_deal_with_coded/", "subreddit_subscribers": 163142, "created_utc": 1708693059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!  \n\n\nI am in the process of evaluating different possible setups for a data lakehouse on GCP.   \n\n\nI have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   \n\n\nI have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn't have to be immense.   \n\n\nI retrieve on two columns: datetime and a \"tags\" column, which is a string.   \n\n\n1. I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? \n2. Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? \n3. During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?\n4. After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. \n\nThanks for the help!  \n", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a Data Lakehouse / Delta Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axtb5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708668162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!  &lt;/p&gt;\n\n&lt;p&gt;I am in the process of evaluating different possible setups for a data lakehouse on GCP.   &lt;/p&gt;\n\n&lt;p&gt;I have roughly 100MB of writes daily, which I have to read out over the next few weeks, after that the data is only there for long-term storage.   &lt;/p&gt;\n\n&lt;p&gt;I have to retrieve based on fixed times during the day. The amount of reads are therefore limited and the throughput doesn&amp;#39;t have to be immense.   &lt;/p&gt;\n\n&lt;p&gt;I retrieve on two columns: datetime and a &amp;quot;tags&amp;quot; column, which is a string.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I would love to know what you guys would go with in terms of file format (I tend to parquet, but delta-tables, seem also be a valid option, but would restrict the table format I can go with)? &lt;/li&gt;\n&lt;li&gt;Based on my requirements and the simple retrieval what table format would you gow ith (delta lake, hudi, iceberg)? Or would you simple partition the data in the data lake (e.g. through the folder structure with day/keyword/file) and retrieve the relevant parquet files in the correct folders? Or as a last option (use BigQuery to query the data)? &lt;/li&gt;\n&lt;li&gt;During insertion of the data, do I have to perform any additional steps to optimize the data for the different table formats?&lt;/li&gt;\n&lt;li&gt;After the data is retrieved, I have a bunch of filtering steps, but they are always performed on a small subset of the data. What query execution would be viable (I tend to single node e.g. DataFusion, Polars, maybe even in DuckDB)? Operations are mainly some calculations on text data or embeddings. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for the help!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axtb5k", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axtb5k/setting_up_a_data_lakehouse_delta_lake/", "subreddit_subscribers": 163142, "created_utc": 1708668162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\nI need you help. I am working for a startup (let's call it *Darth Vader*) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:\n\n1. Download data from Darth Vader's social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.\n2. Copy all these data in a Google Sheets (*Dashbord*).\n3. Compute some analytics: Engagement Rate, Followers Growth, etc.\n\nNow, let's talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:\n\n* It does not work for all the social networks.\n* It does not provide all the data that I need.\n* Bad connection with Google Sheets.\n\nExtra: we have a free account on [Later](https://later.com/). But, I have never used it because I have no access.\n\nDisclaimer: there is no employee that has competence in data engineering.\n\nMay you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.\n\nThank You so much", "author_fullname": "t2_a1lozp3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP ME!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7kx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708713042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708712300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;I need you help. I am working for a startup (let&amp;#39;s call it &lt;em&gt;Darth Vader&lt;/em&gt;) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download data from Darth Vader&amp;#39;s social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.&lt;/li&gt;\n&lt;li&gt;Copy all these data in a Google Sheets (&lt;em&gt;Dashbord&lt;/em&gt;).&lt;/li&gt;\n&lt;li&gt;Compute some analytics: Engagement Rate, Followers Growth, etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, let&amp;#39;s talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It does not work for all the social networks.&lt;/li&gt;\n&lt;li&gt;It does not provide all the data that I need.&lt;/li&gt;\n&lt;li&gt;Bad connection with Google Sheets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extra: we have a free account on &lt;a href=\"https://later.com/\"&gt;Later&lt;/a&gt;. But, I have never used it because I have no access.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: there is no employee that has competence in data engineering.&lt;/p&gt;\n\n&lt;p&gt;May you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.&lt;/p&gt;\n\n&lt;p&gt;Thank You so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay7kx7", "is_robot_indexable": true, "report_reasons": null, "author": "Frugoljno", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7kx7/help_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7kx7/help_me/", "subreddit_subscribers": 163142, "created_utc": 1708712300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i'm need to integrate a bunch of data from some api's around 50 million rows and need some advice to integrate that.\n\nI'm looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.\n\nObviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?\n\nThanks for your support ", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your ingestation tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay60ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708708688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i&amp;#39;m need to integrate a bunch of data from some api&amp;#39;s around 50 million rows and need some advice to integrate that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Obviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your support &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay60ha", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "subreddit_subscribers": 163142, "created_utc": 1708708688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are evaluating whether to go with multiple smaller databases or one large one. \n\nSmall databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. \n\nI perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. \n\nWhat is your view on this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple small databases x One large", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzofv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are evaluating whether to go with multiple smaller databases or one large one. &lt;/p&gt;\n\n&lt;p&gt;Small databases should contain e.g. referential data, data users manually inputs (django admin) etc. The problem with this is ofcourse eventually we would need to join multiple databases together (how? virtualization like Cube.js, Dremio? or some API gateway? or engines like Trino, Athena, Spark?) (what about UX and speed then?) on the other hand it gives us the benefits of microservices - we can maintain, update etc each database independently on others. Also we can treat some of them as more critical than the others. &lt;/p&gt;\n\n&lt;p&gt;I perceive a one large database (warehouse) as a more standard solution? On the other hand its really a one big database that will be mission critical, hard to update, maybe slower etc. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axzofv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzofv/multiple_small_databases_x_one_large/", "subreddit_subscribers": 163142, "created_utc": 1708692572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.\n\nI was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It'd make matching between timeseries and more static information stored in classic db tables easier.\n\n- anyone experience moving from either prometheus/influxdbv2 to timescale?\n- prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?", "author_fullname": "t2_av9qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Prometheus, InfluxDB &amp; MariaDB to PostgreSQL&amp;TimeScaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzaza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708691370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data spread out over 3 systems: 2 TSDBs (prometheus and InfluxDBv2) and one MariaDB.&lt;/p&gt;\n\n&lt;p&gt;I was considering moving all of this into PostgreSQL+TimeScaleDB to have one back-end to manage. It&amp;#39;d make matching between timeseries and more static information stored in classic db tables easier.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;anyone experience moving from either prometheus/influxdbv2 to timescale?&lt;/li&gt;\n&lt;li&gt;prometheus has this concept of exporters, agents which collect data from sources and make it available for scraping by prometheus. Is there something I could replace these with which would write data to TimeScaleDB?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzaza", "is_robot_indexable": true, "report_reasons": null, "author": "thingthatgoesbump", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzaza/moving_prometheus_influxdb_mariadb_to/", "subreddit_subscribers": 163142, "created_utc": 1708691370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was thinking of learning IAC and trying to use it in our system, but I don't see the picture of how it is useful for a stable system (that just works and doesn't change often). We have development and production environments (that are similar except for the cost and powers), and we kind of just let them work. Are there any benefits that I can suggest from having this infrastructure encoded? I do understand the benefits of encoding (version control, easier governance), but also it is a kind of additional documentation that needs to be maintained.", "author_fullname": "t2_ep4hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the use cases of Terraform / IAC for BI platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axv0cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708674543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of learning IAC and trying to use it in our system, but I don&amp;#39;t see the picture of how it is useful for a stable system (that just works and doesn&amp;#39;t change often). We have development and production environments (that are similar except for the cost and powers), and we kind of just let them work. Are there any benefits that I can suggest from having this infrastructure encoded? I do understand the benefits of encoding (version control, easier governance), but also it is a kind of additional documentation that needs to be maintained.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axv0cw", "is_robot_indexable": true, "report_reasons": null, "author": "Volume999", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axv0cw/what_are_the_use_cases_of_terraform_iac_for_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axv0cw/what_are_the_use_cases_of_terraform_iac_for_bi/", "subreddit_subscribers": 163142, "created_utc": 1708674543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as\n\n* TimescaleDB\n* AWS Timestream\n* influxdb\n* Big Query (last resource, AWS resources used at the company)\n\n&amp;#x200B;\n\nHere com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series DB start to end analytics platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axt54n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708667589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow engineerings, I am looking to build an internal analytics platform from start to end for user behavior and I need some advice, on the backend we will start collecting attributes like likes, time in items, and some other parameters, I estimate around 20 at a rate of 100k Interactions every hour, Since the idea is to track during the time this behavior for individuals, aggregate the data by location too, and gain some insights at the level of demographics, I was thinking more of a Time series DB. Some options Poped up here before such as&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TimescaleDB&lt;/li&gt;\n&lt;li&gt;AWS Timestream&lt;/li&gt;\n&lt;li&gt;influxdb&lt;/li&gt;\n&lt;li&gt;Big Query (last resource, AWS resources used at the company)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here com your wisdom, which one do you recommend? also was thinking of using DBT before the BI tools to facilitate the queries for reporting, is this an overkill? lol. And as BI tools were thinking power BI or Grafana. Lastly, do you think the guys in the backend could face any problems with the rate of writing into the DB at scale? any good practices to recommend them? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axt54n", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axt54n/timeseries_db_start_to_end_analytics_platform/", "subreddit_subscribers": 163142, "created_utc": 1708667589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good evening everyone, I am currently pursuing a master's degree in Computer Science, with a focus Artificial Intelligence, I've done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I've been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?\n\n\nTLDR\n\nI am doing an internship as data engineer, would it be wasted later if i \"change career\" to data scientist? is there a career that brings these two worlds together?\nThank you very much!", "author_fullname": "t2_d4ged0nj5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From data engineer to data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay6i2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708709788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening everyone, I am currently pursuing a master&amp;#39;s degree in Computer Science, with a focus Artificial Intelligence, I&amp;#39;ve done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I&amp;#39;ve been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?&lt;/p&gt;\n\n&lt;p&gt;TLDR&lt;/p&gt;\n\n&lt;p&gt;I am doing an internship as data engineer, would it be wasted later if i &amp;quot;change career&amp;quot; to data scientist? is there a career that brings these two worlds together?\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ay6i2r", "is_robot_indexable": true, "report_reasons": null, "author": "No-Garden9594", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "subreddit_subscribers": 163142, "created_utc": 1708709788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The history of orchestration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay1xm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JL64oWP3P7j7r3efdomZYATWIuJCsRtjWSYUNKszufU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708698923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dedp.online", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?auto=webp&amp;s=11be9257ae07b7390dcb7e79a355ee4b340530ef", "width": 1384, "height": 978}, "resolutions": [{"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=286c500866d2361011a93f5cab4e424d9c39cfd1", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65be31fd19eef9a9096dda521a1a695134b28ac4", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa542e3036ee8925758313bc111b2dffe97d843c", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1dd86b2814ea7bbed6c4ce63b664f61769435d7c", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb0036fadf648c776f4babf19b6104d67182db4e", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/L-pcmPlunNcMEbND947nVxDSH5nFyrmw725BqIjRmB8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e99de5072042d7f10c96fe752d22a9f2fa052751", "width": 1080, "height": 763}], "variants": {}, "id": "tGvlSbDcYV912Yv0Pzp_4vBc8R8zSTLhvB837XgyVrQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ay1xm1", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ay1xm1/the_history_of_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dedp.online/part-2/4-ce/bash-stored-procedure-etl-python-script.html", "subreddit_subscribers": 163142, "created_utc": 1708698923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pretty much title, I'm intentionally failing a dag with a retry of 10 minutes, and I'm noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I'm misunderstanding something.", "author_fullname": "t2_orzx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it correct behaviour for airflow to hold onto a task which is 'up for retry' and prevent workers from picking up queue items while it waits for the retry period? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axzk4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708692183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much title, I&amp;#39;m intentionally failing a dag with a retry of 10 minutes, and I&amp;#39;m noticing that airflow is letting all workers get to the same point of failure, retry is going to happen in 10 minutes, but the workers just sit on the same task instead of what I assumed would happen which is that those tasks are rescheduled and airflow moves onto the next 16 tasks. Just wondering if this is intended functionality of if I&amp;#39;m misunderstanding something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axzk4y", "is_robot_indexable": true, "report_reasons": null, "author": "Mrfunnynuts", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axzk4y/is_it_correct_behaviour_for_airflow_to_hold_onto/", "subreddit_subscribers": 163142, "created_utc": 1708692183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have what probably sounds like a newbie question, but something I am being told doesn\u2019t sound right and I want to fact check it.\n\nSituation: We work with a well established trusted SaaS vendor. We need to send a daily file to their file server. They have a pretty standard https post API end point we can use to send the file to this server. It has pgp encryption so it should pass security protocols. In small scale testing, my team can easily do this with postman and soapui. \n\nProblem: We have asked IT to productionalize this call so we don\u2019t have to manually run the call everyday. They have come back with a 6 month lead time because they say we have to build our own api to communicate with this other API. I am struggling to see why this would be the case. Am I crazy or Is there a valid reason this may be needed? It feels easy enough to just make the call. I am trying to figure out how much to push on this since I am definitely not an expert.", "author_fullname": "t2_qxm2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calling APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axsatb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708664749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what probably sounds like a newbie question, but something I am being told doesn\u2019t sound right and I want to fact check it.&lt;/p&gt;\n\n&lt;p&gt;Situation: We work with a well established trusted SaaS vendor. We need to send a daily file to their file server. They have a pretty standard https post API end point we can use to send the file to this server. It has pgp encryption so it should pass security protocols. In small scale testing, my team can easily do this with postman and soapui. &lt;/p&gt;\n\n&lt;p&gt;Problem: We have asked IT to productionalize this call so we don\u2019t have to manually run the call everyday. They have come back with a 6 month lead time because they say we have to build our own api to communicate with this other API. I am struggling to see why this would be the case. Am I crazy or Is there a valid reason this may be needed? It feels easy enough to just make the call. I am trying to figure out how much to push on this since I am definitely not an expert.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axsatb", "is_robot_indexable": true, "report_reasons": null, "author": "Jhp8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axsatb/calling_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axsatb/calling_apis/", "subreddit_subscribers": 163142, "created_utc": 1708664749.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}