{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.\n\nSome have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.\n\nWhat is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semantic Layers: the good, the bad &amp; the ugly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayg1r3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708732992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe that Semantic Layers are here to stay. There are more and more companies offering them, and they provide capabilities that are difficult to implement in pure SQL (like time series analysis: YTD, YoY etc.). I see them adding to the context available to LLMs when using natural language to converse with your data.&lt;/p&gt;\n\n&lt;p&gt;Some have been around years such as MS SSAS, which has moved from MDX to DAX and now sits (for many users) in the PowerBI Service. But in true MS fashion, it\u2019s limited to only being easily accessible to MS products.&lt;/p&gt;\n\n&lt;p&gt;What is the community experience with other Semantic Layers (ideally code based that you can review a PR for) that work well with Power BI but are also accessible to other tools via Rest, GraphQL or others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayg1r3", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayg1r3/semantic_layers_the_good_the_bad_the_ugly/", "subreddit_subscribers": 163282, "created_utc": 1708732992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to prove a point.\n\nExecutives in every organization are jumping into AI which is an ongoing trend. \n\nHowever, it should not be at the expense of dismissing data engineering. \n\nIf data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. ", "author_fullname": "t2_aryc45smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we say Data Engineering is a precursor to AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqx22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708768510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to prove a point.&lt;/p&gt;\n\n&lt;p&gt;Executives in every organization are jumping into AI which is an ongoing trend. &lt;/p&gt;\n\n&lt;p&gt;However, it should not be at the expense of dismissing data engineering. &lt;/p&gt;\n\n&lt;p&gt;If data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqx22", "is_robot_indexable": true, "report_reasons": null, "author": "Paperplaneflyr", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "subreddit_subscribers": 163282, "created_utc": 1708768510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title is the question. I never jumped on the dbt train it just didn't work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren't willing to use it for their stuff.\n\nI tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. \n\nDuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it's a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.\n\nThanks!", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB at your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7847", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708711456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title is the question. I never jumped on the dbt train it just didn&amp;#39;t work for my company our DE team is knee deep in Pyspark, Python, our on SQL framework and K8s and our analysts weren&amp;#39;t willing to use it for their stuff.&lt;/p&gt;\n\n&lt;p&gt;I tried it out locally to see if it would be a good replacement for anything we do and was not convinced it was better than our current workflows using pyspark, python, and SQL scripts with airflow and K8s. Also using YAML is just jarring for calling functions and doing logic. &lt;/p&gt;\n\n&lt;p&gt;DuckDB does catch my eye though I love SQLite and it seems like the OLAP SQLite so I wanted to get some ideas as to how people are using it in production day to day so I can decide whether or not it&amp;#39;s a good fit for some of our upcoming projects or refactors of some of our older unreliable stuff.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay7847", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7847/how_are_you_using_duckdb_at_your_company/", "subreddit_subscribers": 163282, "created_utc": 1708711456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A senior director of engineering told me that a data engineer's role job is merely just maintenance work once the data pipeline is ready. How true or false is this? Assuming it's true, doesn't it make sense for companies to just hire data engineers on a contract basis? \nAlso, is the job market for data engineers saturated in general for the same reason? ", "author_fullname": "t2_n60i6t07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data engineering better off as a contract position ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayp1bg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708760968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A senior director of engineering told me that a data engineer&amp;#39;s role job is merely just maintenance work once the data pipeline is ready. How true or false is this? Assuming it&amp;#39;s true, doesn&amp;#39;t it make sense for companies to just hire data engineers on a contract basis? \nAlso, is the job market for data engineers saturated in general for the same reason? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayp1bg", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedLettuce97", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayp1bg/is_data_engineering_better_off_as_a_contract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayp1bg/is_data_engineering_better_off_as_a_contract/", "subreddit_subscribers": 163282, "created_utc": 1708760968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn't the best but it's slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.\n\nWhat are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What defines a good Python pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay8kfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708714658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of the data after filtering at the SQL layer is usually no more than 200k rows, then I move the data around with some combo of pandas + pyodbc/sqlalchemy. Performance isn&amp;#39;t the best but it&amp;#39;s slightly faster than our existing legacy processes, and lots of intermediary stages were automated with a few lines of code.&lt;/p&gt;\n\n&lt;p&gt;What are some guidelines that you try to follow when making pipelines from scratch, especially working with legacy systems? I have some cool packages like dask and pyarrow at my disposal, but my data is never big enough to justify using those over simple SQL+pandas. Also open to hearing guidelines for big data and/or cloud-based pipelines too!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay8kfq", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay8kfq/what_defines_a_good_python_pipeline/", "subreddit_subscribers": 163282, "created_utc": 1708714658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a Data Engineer 2 years ago when I was very junior to the role for a medium sized marketing company. I don't have a CS degree. I picked up a lot of stuff on my own. I really enjoyed working with with my boss at the time because he was super helpful and made work fun. However the only caveat was that he took on a lot of the work and gave me and the rest of my team the easy stuff. I was working on mostly ETL type work and building. I did this for a year and I really loved it.\n\nOnce that project was complete, I moved over to a more production support role and just found it more difficult as we have a ton of vendors and it took me a while to figure out the system holistically. I am not strong with debugging since I am new to this type of work (going through other people's code and fixing things, testing, etc). I am getting a better grasp at it now, but I still have a long way to go. I can debug, but being in production support, I have to find the RC and apply fixes so it doesn't happen again. This is harder for me since I am not as fast as my more experienced peers.\n\n3 months ago there have been a huge focus on efficiency and lots of DEs were getting let go/forced out. They were handing out PIPs to a lot of people. Management only wants very experienced senior people now and no longer want junior/intermediate DEs. I'm 100% sure that I'm next since they've been announcing it all over that we now need to meet their new performance metrics. I'm a lot clearer of what is required of me now to move up, but I am obviously not making the cut and I am sure I will be given a PIP soon and out the door within the next 1-2 months.\n\nThis experience itself made me feel more exposed and questioning my ability to be a DE. I joined this company when a new DE team was being formed and we didn't have anything already in place so I picked up a lot of bad practices and never learned proper things like TDD, best coding practices, etc.\n\nI looked through job postings and I really don't think I can make the cut. Job postings appear to only want Senior DEs. If I could continue with being a DE, I don't mind. But a lot of external factors like what I did at this company and what will be expected of these roles point to me not being able to. The production support aspect is bothering me and making me question if I can do this.\n\nLooking for advice to see what I can/should do. I need a more supportive team to help me grow like with my first boss, but allow me to be challenged/grow. Does this exist? Or should I focus on a career change by pivoting?", "author_fullname": "t2_4kmb0igx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting let go and needing some advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayov6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708760524.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708760306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a Data Engineer 2 years ago when I was very junior to the role for a medium sized marketing company. I don&amp;#39;t have a CS degree. I picked up a lot of stuff on my own. I really enjoyed working with with my boss at the time because he was super helpful and made work fun. However the only caveat was that he took on a lot of the work and gave me and the rest of my team the easy stuff. I was working on mostly ETL type work and building. I did this for a year and I really loved it.&lt;/p&gt;\n\n&lt;p&gt;Once that project was complete, I moved over to a more production support role and just found it more difficult as we have a ton of vendors and it took me a while to figure out the system holistically. I am not strong with debugging since I am new to this type of work (going through other people&amp;#39;s code and fixing things, testing, etc). I am getting a better grasp at it now, but I still have a long way to go. I can debug, but being in production support, I have to find the RC and apply fixes so it doesn&amp;#39;t happen again. This is harder for me since I am not as fast as my more experienced peers.&lt;/p&gt;\n\n&lt;p&gt;3 months ago there have been a huge focus on efficiency and lots of DEs were getting let go/forced out. They were handing out PIPs to a lot of people. Management only wants very experienced senior people now and no longer want junior/intermediate DEs. I&amp;#39;m 100% sure that I&amp;#39;m next since they&amp;#39;ve been announcing it all over that we now need to meet their new performance metrics. I&amp;#39;m a lot clearer of what is required of me now to move up, but I am obviously not making the cut and I am sure I will be given a PIP soon and out the door within the next 1-2 months.&lt;/p&gt;\n\n&lt;p&gt;This experience itself made me feel more exposed and questioning my ability to be a DE. I joined this company when a new DE team was being formed and we didn&amp;#39;t have anything already in place so I picked up a lot of bad practices and never learned proper things like TDD, best coding practices, etc.&lt;/p&gt;\n\n&lt;p&gt;I looked through job postings and I really don&amp;#39;t think I can make the cut. Job postings appear to only want Senior DEs. If I could continue with being a DE, I don&amp;#39;t mind. But a lot of external factors like what I did at this company and what will be expected of these roles point to me not being able to. The production support aspect is bothering me and making me question if I can do this.&lt;/p&gt;\n\n&lt;p&gt;Looking for advice to see what I can/should do. I need a more supportive team to help me grow like with my first boss, but allow me to be challenged/grow. Does this exist? Or should I focus on a career change by pivoting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ayov6r", "is_robot_indexable": true, "report_reasons": null, "author": "codingisfun4me", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayov6r/getting_let_go_and_needing_some_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayov6r/getting_let_go_and_needing_some_advice/", "subreddit_subscribers": 163282, "created_utc": 1708760306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good evening everyone, I am currently pursuing a master's degree in Computer Science, with a focus Artificial Intelligence, I've done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I've been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?\n\n\nTLDR\n\nI am doing an internship as data engineer, would it be wasted later if i \"change career\" to data scientist? is there a career that brings these two worlds together?\nThank you very much!", "author_fullname": "t2_d4ged0nj5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From data engineer to data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay6i2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708709788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening everyone, I am currently pursuing a master&amp;#39;s degree in Computer Science, with a focus Artificial Intelligence, I&amp;#39;ve done subjects such as machine and deep learning which I really enjoyed.\nI am at my last year, and I&amp;#39;ve been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.\nThe internship lasts for 6 months and by that time I should have completed my degree.\nI like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.\nWould these 6 moths as a data engineer be wasted doing this? \nIs there a career that brings the world of data scientist and data engineering together?&lt;/p&gt;\n\n&lt;p&gt;TLDR&lt;/p&gt;\n\n&lt;p&gt;I am doing an internship as data engineer, would it be wasted later if i &amp;quot;change career&amp;quot; to data scientist? is there a career that brings these two worlds together?\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ay6i2r", "is_robot_indexable": true, "report_reasons": null, "author": "No-Garden9594", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay6i2r/from_data_engineer_to_data_scientist/", "subreddit_subscribers": 163282, "created_utc": 1708709788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i'm need to integrate a bunch of data from some api's around 50 million rows and need some advice to integrate that.\n\nI'm looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.\n\nObviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?\n\nThanks for your support ", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your ingestation tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay60ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708708688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i&amp;#39;m need to integrate a bunch of data from some api&amp;#39;s around 50 million rows and need some advice to integrate that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking if i can write a script and put into a lamda function that make the request and write to a s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Obviusly all the data not in a single request so what can be the way to parallelize the ingestion or what can i do this task?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your support &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ay60ha", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay60ha/what_are_your_ingestation_tool/", "subreddit_subscribers": 163282, "created_utc": 1708708688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I\u2019ve been working as a Web Developer for 2 years and I would like to change my career in the direction of Data Science/Data Engineering. For now I try to understand in depth what is the difference between those fields and if Data Engineering would be the right choice for me. \n\nCould some data engineers tell me how your working day looks like? Is it more about writing a code or about designing databases? \n\nI like python and have some experience with web development with Django as well as with pandas, numpy and PySpark. Besides I like theoretical mathematics. After reading some short descriptions of Data Science/Machine Learning/AI I feel like I would be the most interested in ML, but maybe Data Engineering would be a good entry point in this direction. ", "author_fullname": "t2_tz7awh8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a position as a Junior Data Engineer a good entry point towards working with data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aytrbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708778920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019ve been working as a Web Developer for 2 years and I would like to change my career in the direction of Data Science/Data Engineering. For now I try to understand in depth what is the difference between those fields and if Data Engineering would be the right choice for me. &lt;/p&gt;\n\n&lt;p&gt;Could some data engineers tell me how your working day looks like? Is it more about writing a code or about designing databases? &lt;/p&gt;\n\n&lt;p&gt;I like python and have some experience with web development with Django as well as with pandas, numpy and PySpark. Besides I like theoretical mathematics. After reading some short descriptions of Data Science/Machine Learning/AI I feel like I would be the most interested in ML, but maybe Data Engineering would be a good entry point in this direction. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aytrbb", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Affect4004", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aytrbb/is_a_position_as_a_junior_data_engineer_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aytrbb/is_a_position_as_a_junior_data_engineer_a_good/", "subreddit_subscribers": 163282, "created_utc": 1708778920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our company is undergoing a major \"v1 to v2\" shift to microservices, which is resulting in our back-end completely redesigning all our data sources. This includes also removing entire concepts for some entities and adding new ones we haven't had before.\n\nWhile in the ideal world the data team would have had more of a say in these design choices, that's not the world I'm living in. We're now being tasked with essentially \"figuring it out\" on our end. While ingesting the raw data into our data lake isn't necessarily a concern (our ingestion pattern and pipelines will remain the same and just point to new containers), our data warehouse layer is a HUGE concern.\n\nI'm having a hard time finding guidance out there for best practices around merging \"versions\" of data structures so that historical reporting will still be reasonably feasible.\n\nInterested to know what others have done. Do you rebuild your data warehouse based on v2 and then shunt in v1 data as best as you can? Do you have separate scripts for v1 and v2 data for the same tables and then combine at a later stage, or do you end up with monstrously huge scripts that create tables with both v1 and v2 data \"unioned\" together (essentially doubling the code for each table)? How do you handle your schemas? Do you keep v1 and v2 in separate schemas, or do you keep related v1 and v2 data together in the same spot? Etc. etc.\n\nJust looking for literally any insight or suggestions folks may have! This is a massive undertaking and while we're working really hard on it and have likely answers to some of the questions above, we're still working it all out and I'm worried about missing major things.", "author_fullname": "t2_b67xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for handling a major underlying shift in back-end data structures to still enable historical reporting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aycstw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708724967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our company is undergoing a major &amp;quot;v1 to v2&amp;quot; shift to microservices, which is resulting in our back-end completely redesigning all our data sources. This includes also removing entire concepts for some entities and adding new ones we haven&amp;#39;t had before.&lt;/p&gt;\n\n&lt;p&gt;While in the ideal world the data team would have had more of a say in these design choices, that&amp;#39;s not the world I&amp;#39;m living in. We&amp;#39;re now being tasked with essentially &amp;quot;figuring it out&amp;quot; on our end. While ingesting the raw data into our data lake isn&amp;#39;t necessarily a concern (our ingestion pattern and pipelines will remain the same and just point to new containers), our data warehouse layer is a HUGE concern.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time finding guidance out there for best practices around merging &amp;quot;versions&amp;quot; of data structures so that historical reporting will still be reasonably feasible.&lt;/p&gt;\n\n&lt;p&gt;Interested to know what others have done. Do you rebuild your data warehouse based on v2 and then shunt in v1 data as best as you can? Do you have separate scripts for v1 and v2 data for the same tables and then combine at a later stage, or do you end up with monstrously huge scripts that create tables with both v1 and v2 data &amp;quot;unioned&amp;quot; together (essentially doubling the code for each table)? How do you handle your schemas? Do you keep v1 and v2 in separate schemas, or do you keep related v1 and v2 data together in the same spot? Etc. etc.&lt;/p&gt;\n\n&lt;p&gt;Just looking for literally any insight or suggestions folks may have! This is a massive undertaking and while we&amp;#39;re working really hard on it and have likely answers to some of the questions above, we&amp;#39;re still working it all out and I&amp;#39;m worried about missing major things.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aycstw", "is_robot_indexable": true, "report_reasons": null, "author": "eelwheel", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aycstw/best_practices_for_handling_a_major_underlying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aycstw/best_practices_for_handling_a_major_underlying/", "subreddit_subscribers": 163282, "created_utc": 1708724967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\nI need you help. I am working for a startup (let's call it *Darth Vader*) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:\n\n1. Download data from Darth Vader's social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.\n2. Copy all these data in a Google Sheets (*Dashbord*).\n3. Compute some analytics: Engagement Rate, Followers Growth, etc.\n\nNow, let's talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:\n\n* It does not work for all the social networks.\n* It does not provide all the data that I need.\n* Bad connection with Google Sheets.\n\nExtra: we have a free account on [Later](https://later.com/). But, I have never used it because I have no access.\n\nDisclaimer: there is no employee that has competence in data engineering.\n\nMay you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.\n\nThank You so much", "author_fullname": "t2_a1lozp3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP ME!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7kx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708713042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708712300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;I need you help. I am working for a startup (let&amp;#39;s call it &lt;em&gt;Darth Vader&lt;/em&gt;) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download data from Darth Vader&amp;#39;s social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.&lt;/li&gt;\n&lt;li&gt;Copy all these data in a Google Sheets (&lt;em&gt;Dashbord&lt;/em&gt;).&lt;/li&gt;\n&lt;li&gt;Compute some analytics: Engagement Rate, Followers Growth, etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, let&amp;#39;s talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It does not work for all the social networks.&lt;/li&gt;\n&lt;li&gt;It does not provide all the data that I need.&lt;/li&gt;\n&lt;li&gt;Bad connection with Google Sheets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Extra: we have a free account on &lt;a href=\"https://later.com/\"&gt;Later&lt;/a&gt;. But, I have never used it because I have no access.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: there is no employee that has competence in data engineering.&lt;/p&gt;\n\n&lt;p&gt;May you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.&lt;/p&gt;\n\n&lt;p&gt;Thank You so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay7kx7", "is_robot_indexable": true, "report_reasons": null, "author": "Frugoljno", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7kx7/help_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7kx7/help_me/", "subreddit_subscribers": 163282, "created_utc": 1708712300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company's data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  \n\n\nWith that said...  \n\n\n1. What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  \n\n\n2. There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.", "author_fullname": "t2_m0llzgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proper handling, storage and cleaning of manual extracts from a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aysd2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708774029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company&amp;#39;s data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  &lt;/p&gt;\n\n&lt;p&gt;With that said...  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aysd2j", "is_robot_indexable": true, "report_reasons": null, "author": "Bhagafat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "subreddit_subscribers": 163282, "created_utc": 1708774029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All , \n\nWe are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 \n\nI am currently looking at \n- Airbyte \n- Fivetran \n- Meltano \n\nI am looking at combination of Airbyte and Meltano \n\nAirbyte for OOB low Volume and simple Low Code Scenerios \n\nMeltano for heavy lifting \n\nFivetran is out of picture due to the price and ROI we will be making out of it \n\nI am stuck at a point where I need evaluate below aspects \n\n- Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot \n- how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times \n- how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .\n\nThanks\nSiddu Hussain V\n\n\n", "author_fullname": "t2_7yg4aphe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you perform Load testing and security measures for a ELT tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqbu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708766087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All , &lt;/p&gt;\n\n&lt;p&gt;We are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 &lt;/p&gt;\n\n&lt;p&gt;I am currently looking at \n- Airbyte \n- Fivetran \n- Meltano &lt;/p&gt;\n\n&lt;p&gt;I am looking at combination of Airbyte and Meltano &lt;/p&gt;\n\n&lt;p&gt;Airbyte for OOB low Volume and simple Low Code Scenerios &lt;/p&gt;\n\n&lt;p&gt;Meltano for heavy lifting &lt;/p&gt;\n\n&lt;p&gt;Fivetran is out of picture due to the price and ROI we will be making out of it &lt;/p&gt;\n\n&lt;p&gt;I am stuck at a point where I need evaluate below aspects &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot &lt;/li&gt;\n&lt;li&gt;how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times &lt;/li&gt;\n&lt;li&gt;how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks\nSiddu Hussain V&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqbu3", "is_robot_indexable": true, "report_reasons": null, "author": "Immediate-Force6602", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "subreddit_subscribers": 163282, "created_utc": 1708766087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I'm trying to guess what would be the best way to move around data from postgresql to typesense (well, can be anything at this point).  \nMy biggest challenge is more about correctly detecting changes and only stream those to the destination after the first offload.  \n\n\nI've tried Airbyte and it kinda works (what a damn behemoth it is btw), but despite having logical replication enabled and CDC selected, it seems to extract everything regardless of the fact that no changes were made in the origin table, I don't know if I'm doing something wrong but my stupid ass understands that if no changes were made to the table, then no related transactions made it in the wal either, thus no extraction should take place at all... But this is not my case.  \n\n\nFirst of all I would like to understand if my interpretation is correct and that Airbyte is doing something that it shouldn't do in the first place (I'm ignoring the sync method, because the problem is the fact that is reading everything everytime), secondly what can be done about this.\n\nDebezium? Dagster?\n\nOn a related note: currently I'm using a table as result of multiple statements and joins (then I would've added some triggers to check data consistency between the tables and act accordingly), ideally a materialized view with a cron-based refresh seems easier (10 sec x 3M rows), but then how I'm supposed to track the changes and only stream those, since MV doesn't support CDC afaik and seems to refresh entirely?  \n\n\nSorry if my message is confused, I did my best.  \n\n\nCan someone please help me?", "author_fullname": "t2_gfk84466l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data from postgresql to typesense?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayfv19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708732523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m trying to guess what would be the best way to move around data from postgresql to typesense (well, can be anything at this point).&lt;br/&gt;\nMy biggest challenge is more about correctly detecting changes and only stream those to the destination after the first offload.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried Airbyte and it kinda works (what a damn behemoth it is btw), but despite having logical replication enabled and CDC selected, it seems to extract everything regardless of the fact that no changes were made in the origin table, I don&amp;#39;t know if I&amp;#39;m doing something wrong but my stupid ass understands that if no changes were made to the table, then no related transactions made it in the wal either, thus no extraction should take place at all... But this is not my case.  &lt;/p&gt;\n\n&lt;p&gt;First of all I would like to understand if my interpretation is correct and that Airbyte is doing something that it shouldn&amp;#39;t do in the first place (I&amp;#39;m ignoring the sync method, because the problem is the fact that is reading everything everytime), secondly what can be done about this.&lt;/p&gt;\n\n&lt;p&gt;Debezium? Dagster?&lt;/p&gt;\n\n&lt;p&gt;On a related note: currently I&amp;#39;m using a table as result of multiple statements and joins (then I would&amp;#39;ve added some triggers to check data consistency between the tables and act accordingly), ideally a materialized view with a cron-based refresh seems easier (10 sec x 3M rows), but then how I&amp;#39;m supposed to track the changes and only stream those, since MV doesn&amp;#39;t support CDC afaik and seems to refresh entirely?  &lt;/p&gt;\n\n&lt;p&gt;Sorry if my message is confused, I did my best.  &lt;/p&gt;\n\n&lt;p&gt;Can someone please help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayfv19", "is_robot_indexable": true, "report_reasons": null, "author": "SpatolaNellaRoccia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayfv19/moving_data_from_postgresql_to_typesense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayfv19/moving_data_from_postgresql_to_typesense/", "subreddit_subscribers": 163282, "created_utc": 1708732523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All I've ever known is Databricks, back when I was working at a company that handled PBs of data. I am now at a place handling exponentially less data (tiny adtech company, data is ad campaign metrics; so no streaming, just batch ingestion, reading). I have been quite accustomed to doing everything in Pyspark and would hate to go to a SQL-only workflow.\n\nThus, I'm having trouble conceptualizing what a Clickhouse stack would look like for us. We would be storing data in s3 (via some \"EL\" job), and ideally I'd be doing analysis with Polars in a Jupyter notebook (the \"T\"), and  creating dataviz dashboards on some other platform. Could Clickhouse occupy the middle db layer here, and if so, would I be wasting its powerfulness and efficiencies if I am doing stuff downstream in Polars instead of in its analytics suite?", "author_fullname": "t2_co2klnt5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coming from Databricks, a bit confused about Clickhouse + Python support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayeqyb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708729722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All I&amp;#39;ve ever known is Databricks, back when I was working at a company that handled PBs of data. I am now at a place handling exponentially less data (tiny adtech company, data is ad campaign metrics; so no streaming, just batch ingestion, reading). I have been quite accustomed to doing everything in Pyspark and would hate to go to a SQL-only workflow.&lt;/p&gt;\n\n&lt;p&gt;Thus, I&amp;#39;m having trouble conceptualizing what a Clickhouse stack would look like for us. We would be storing data in s3 (via some &amp;quot;EL&amp;quot; job), and ideally I&amp;#39;d be doing analysis with Polars in a Jupyter notebook (the &amp;quot;T&amp;quot;), and  creating dataviz dashboards on some other platform. Could Clickhouse occupy the middle db layer here, and if so, would I be wasting its powerfulness and efficiencies if I am doing stuff downstream in Polars instead of in its analytics suite?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayeqyb", "is_robot_indexable": true, "report_reasons": null, "author": "This-Profession-952", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayeqyb/coming_from_databricks_a_bit_confused_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayeqyb/coming_from_databricks_a_bit_confused_about/", "subreddit_subscribers": 163282, "created_utc": 1708729722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nSorry in advance if this is a dumb question.\n\nWhen I try and export my Spark Dataframe as a parquet file ala **spark.write.parquet(...)** (using PySpark), the result is a directory instead of a file. It still reads in fine with Spark, but other apps don't know what to do with it because it's a directory.\n\nHow can I convert it to a file? I can't seem to be able to remove the \"Directory\" attribute from the file in  Windows. \n\nBy the way, it does this on both Google Cloud and Windows (running Spark locally) - so it doesn't seem OS specific. Is it due to the filesystem I am saving the parquet file in? I always just save in a bucket or (on Windows) my C drive.\n\nThanks!", "author_fullname": "t2_4kns99rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Creating Directories Instead of Files When Saving Parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayehhc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708729090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;Sorry in advance if this is a dumb question.&lt;/p&gt;\n\n&lt;p&gt;When I try and export my Spark Dataframe as a parquet file ala &lt;strong&gt;spark.write.parquet(...)&lt;/strong&gt; (using PySpark), the result is a directory instead of a file. It still reads in fine with Spark, but other apps don&amp;#39;t know what to do with it because it&amp;#39;s a directory.&lt;/p&gt;\n\n&lt;p&gt;How can I convert it to a file? I can&amp;#39;t seem to be able to remove the &amp;quot;Directory&amp;quot; attribute from the file in  Windows. &lt;/p&gt;\n\n&lt;p&gt;By the way, it does this on both Google Cloud and Windows (running Spark locally) - so it doesn&amp;#39;t seem OS specific. Is it due to the filesystem I am saving the parquet file in? I always just save in a bucket or (on Windows) my C drive.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ayehhc", "is_robot_indexable": true, "report_reasons": null, "author": "JustinPooDough", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayehhc/spark_creating_directories_instead_of_files_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayehhc/spark_creating_directories_instead_of_files_when/", "subreddit_subscribers": 163282, "created_utc": 1708729090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vaz49s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Experience in Data Engineering vs Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayacv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I1Wonq9x1HXhyPM01WQh9QeEWi-VcXCwxWc6babfEuE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708719029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/kyelabs/p/data-engineering-vs-software-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?auto=webp&amp;s=040e07865e22530f6ec8123b4212df05043917b6", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f3ca4cc448b74064654a35746fcc8519693f909", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d45aa59ce4625027dbc925b954f9fee35374d04", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33c446699610f7287fe4959fe950cc38b1d78965", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/5gfTZJqSFf7p2LGekLc9Tce6tm8M-8DDtoILcYDw9M8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5f3735775e3f42d0569d5ce8998997346238cd8", "width": 640, "height": 333}], "variants": {}, "id": "yg_YgoVZeMu1-hTTZyBi7QDp6jbS2Jp2eVu09M6cZFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ayacv2", "is_robot_indexable": true, "report_reasons": null, "author": "stringofsense", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayacv2/my_experience_in_data_engineering_vs_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/kyelabs/p/data-engineering-vs-software-engineering", "subreddit_subscribers": 163282, "created_utc": 1708719029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I need to cron several scripts for ETL. \n\nI'll be pulling from a mongoDB and APIs, transform the data and send it to BigQuery most likely (haven't decided where to send it).\n\nWhat's the best option to implement here? I need to run them on cloud. I'll be dealing with up to 10GB of data at most.", "author_fullname": "t2_4rpxclpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help deciding where to run ETL scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay98md", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708716302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I need to cron several scripts for ETL. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be pulling from a mongoDB and APIs, transform the data and send it to BigQuery most likely (haven&amp;#39;t decided where to send it).&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best option to implement here? I need to run them on cloud. I&amp;#39;ll be dealing with up to 10GB of data at most.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay98md", "is_robot_indexable": true, "report_reasons": null, "author": "RedBlueWhiteBlack", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay98md/need_help_deciding_where_to_run_etl_scripts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay98md/need_help_deciding_where_to_run_etl_scripts/", "subreddit_subscribers": 163282, "created_utc": 1708716302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am no data architect, but am on the business side with some data background. My company is working on putting together a data mart in databricks for my team.\n\nI am more accustomed to SQL tables with normalized structure and various fact and dimension tables.\n\nThe data engineers developing the solution for our team are claiming that in databricks, large flat tables are preferred in design. It doesn't really make sense to me to purposefully use bloated tables in a data model with repeating data rows of data.... I could see that maybe for a view or report, but not as a part of the data model.\n\nI was under the impression that databricks was a good place for unstructured data to do the transformations to it... not to purposefully  create a data model of bloated tables. Are the engineers telling me correctly that databricks is not as well suited for a traditional data model like you would see in SQL? Can someone explain to me so I better understand?\n\nThis is coming from them asking me to upload a bloated table as a reference data set with data repeated multiple times over each row instead of splitting it out into multiple unique tables that could be queried into a view that shows the large flat singular table.\n\nI just don't know enough about databricks and it didn't sound right and would love someone to explain in a way that makes sense to me.", "author_fullname": "t2_14xs29os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks- Model structure question from a non-data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay7h6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708712056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am no data architect, but am on the business side with some data background. My company is working on putting together a data mart in databricks for my team.&lt;/p&gt;\n\n&lt;p&gt;I am more accustomed to SQL tables with normalized structure and various fact and dimension tables.&lt;/p&gt;\n\n&lt;p&gt;The data engineers developing the solution for our team are claiming that in databricks, large flat tables are preferred in design. It doesn&amp;#39;t really make sense to me to purposefully use bloated tables in a data model with repeating data rows of data.... I could see that maybe for a view or report, but not as a part of the data model.&lt;/p&gt;\n\n&lt;p&gt;I was under the impression that databricks was a good place for unstructured data to do the transformations to it... not to purposefully  create a data model of bloated tables. Are the engineers telling me correctly that databricks is not as well suited for a traditional data model like you would see in SQL? Can someone explain to me so I better understand?&lt;/p&gt;\n\n&lt;p&gt;This is coming from them asking me to upload a bloated table as a reference data set with data repeated multiple times over each row instead of splitting it out into multiple unique tables that could be queried into a view that shows the large flat singular table.&lt;/p&gt;\n\n&lt;p&gt;I just don&amp;#39;t know enough about databricks and it didn&amp;#39;t sound right and would love someone to explain in a way that makes sense to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ay7h6g", "is_robot_indexable": true, "report_reasons": null, "author": "PepeSilviaConspiracy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay7h6g/databricks_model_structure_question_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay7h6g/databricks_model_structure_question_from_a/", "subreddit_subscribers": 163282, "created_utc": 1708712056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone here who works in a B2B SaaS environment, particularly in PLG cases?  \n\n\nI am curious about how your interaction with sales and marketing teams looks like. How do you make yourself as a data leader helpful to them? What does the request normally look like coming from them?  \n\n\nI feel that in our case, two departments are siloed, and we struggle to speak the same language. ", "author_fullname": "t2_evy8q46", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cooperation between data leadership and sales and marketing teams in B2B SaaS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ay4eng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708704905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone here who works in a B2B SaaS environment, particularly in PLG cases?  &lt;/p&gt;\n\n&lt;p&gt;I am curious about how your interaction with sales and marketing teams looks like. How do you make yourself as a data leader helpful to them? What does the request normally look like coming from them?  &lt;/p&gt;\n\n&lt;p&gt;I feel that in our case, two departments are siloed, and we struggle to speak the same language. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ay4eng", "is_robot_indexable": true, "report_reasons": null, "author": "zkid18", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ay4eng/cooperation_between_data_leadership_and_sales_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ay4eng/cooperation_between_data_leadership_and_sales_and/", "subreddit_subscribers": 163282, "created_utc": 1708704905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I highly recommend checking out the **'Mr.K Talks Tech'** channel for exceptional **Azure Data Engineering tutorials**. The content is incredibly helpful, and what sets it apart is the inclusion of **real-time project videos**, making it easier for everyone to understand. Dive into practical examples and enhance your Azure Data Engineering skills. Don't miss out on this valuable resource!\n\nFor easy access to video, click on the link below:\n\n[https://youtu.be/iQ41WqhHglk?si=RH\\_uxYxlLe6YT4Vu](https://youtu.be/iQ41WqhHglk?si=RH_uxYxlLe6YT4Vu)", "author_fullname": "t2_sv9vf7cxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn data engineering for FREE on YouTube with highly insightful content developed by Mr. K Talks Tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ayxdof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708789050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I highly recommend checking out the &lt;strong&gt;&amp;#39;Mr.K Talks Tech&amp;#39;&lt;/strong&gt; channel for exceptional &lt;strong&gt;Azure Data Engineering tutorials&lt;/strong&gt;. The content is incredibly helpful, and what sets it apart is the inclusion of &lt;strong&gt;real-time project videos&lt;/strong&gt;, making it easier for everyone to understand. Dive into practical examples and enhance your Azure Data Engineering skills. Don&amp;#39;t miss out on this valuable resource!&lt;/p&gt;\n\n&lt;p&gt;For easy access to video, click on the link below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/iQ41WqhHglk?si=RH_uxYxlLe6YT4Vu\"&gt;https://youtu.be/iQ41WqhHglk?si=RH_uxYxlLe6YT4Vu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DUqYGX77dC6WijEoNYPhZgiKkf3gCCqLlpcSRfcVRkY.jpg?auto=webp&amp;s=c59a5ec271f21f8e39a84bba96d63956986e73ff", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DUqYGX77dC6WijEoNYPhZgiKkf3gCCqLlpcSRfcVRkY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b8c8b22d9527eb78f4389b7dc758ae953da8a5d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DUqYGX77dC6WijEoNYPhZgiKkf3gCCqLlpcSRfcVRkY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a59dcc290665b9bbbf20550731b4349bd00c9167", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DUqYGX77dC6WijEoNYPhZgiKkf3gCCqLlpcSRfcVRkY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=01f2ae97c2c0762846ed662fcad5bc566be83149", "width": 320, "height": 240}], "variants": {}, "id": "aro7l7Lo-IENjfaK36YDSOzEO2r1p8PxqT51MKgwKtI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ayxdof", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible_Bus_6463", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayxdof/learn_data_engineering_for_free_on_youtube_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayxdof/learn_data_engineering_for_free_on_youtube_with/", "subreddit_subscribers": 163282, "created_utc": 1708789050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! I am currently a QA Engineer with 1 YoE doing Data QA, after that year I decided to start learning DE and make a transition to this exciting field.\n\nCouple weeks ago I came across Mage for developing data pipelines and it has been fun and amazing being able to do with such ease. \n\nI have worked a little bit with Airflow and always considered it intimidating. My question is if I want to really make a change to DE from QA, should I learn airflow first since it seems like industry standard or how easy will it be to change from mage to airflow?", "author_fullname": "t2_6gi7m77k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mage Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aye89g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708728449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am currently a QA Engineer with 1 YoE doing Data QA, after that year I decided to start learning DE and make a transition to this exciting field.&lt;/p&gt;\n\n&lt;p&gt;Couple weeks ago I came across Mage for developing data pipelines and it has been fun and amazing being able to do with such ease. &lt;/p&gt;\n\n&lt;p&gt;I have worked a little bit with Airflow and always considered it intimidating. My question is if I want to really make a change to DE from QA, should I learn airflow first since it seems like industry standard or how easy will it be to change from mage to airflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aye89g", "is_robot_indexable": true, "report_reasons": null, "author": "PandaPbj", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aye89g/mage_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aye89g/mage_pipelines/", "subreddit_subscribers": 163282, "created_utc": 1708728449.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}