{"kind": "Listing", "data": {"after": "t3_1al0usn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I really liked the simplicity of the [One Billion Row Challenge (1BRC)](https://github.com/gunnarmorling/1brc) that took off last month.  It was fun to see lots of people apply different tools to the same simple-yet-clear problem \u201cHow do you parse, process, and aggregate a large CSV file as quickly as possible?\u201dFor fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset \ud83d\ude42.  \n\nData lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.\n\nWe (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see [this blogpost](https://medium.com/coiled-hq/one-trillion-row-challenge-5bfd4c3b8aef) and [this repository](https://github.com/coiled/1trc/)", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One Trillion Row Challenge (1TRC)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al2r0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707312672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707311999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really liked the simplicity of the &lt;a href=\"https://github.com/gunnarmorling/1brc\"&gt;One Billion Row Challenge (1BRC)&lt;/a&gt; that took off last month.  It was fun to see lots of people apply different tools to the same simple-yet-clear problem \u201cHow do you parse, process, and aggregate a large CSV file as quickly as possible?\u201dFor fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset \ud83d\ude42.  &lt;/p&gt;\n\n&lt;p&gt;Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.&lt;/p&gt;\n\n&lt;p&gt;We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see &lt;a href=\"https://medium.com/coiled-hq/one-trillion-row-challenge-5bfd4c3b8aef\"&gt;this blogpost&lt;/a&gt; and &lt;a href=\"https://github.com/coiled/1trc/\"&gt;this repository&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?auto=webp&amp;s=558d446820ecd10674797b0e6f49fdc49856cf1e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da65835d0e6cbeef974df8d596e45ac5dc483843", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b51501ef584c9420dcd557eeb630dae5484b90f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dec7f7846841361ae42f1ff5a57c5c64305bbfa", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac5329409c2cd1ff7938f6f90ffc364fe33e65d9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=91ddd2cd259cedfb6aed6c8a05275dab5dff7375", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/pdudUKqg1ppT11_6ZsY7IYKKLZso-5yFA5dsZzxBhCE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ac4b5bfa44aaa021bee7aa7a99a716955fb1674", "width": 1080, "height": 540}], "variants": {}, "id": "G7IWiakRJ9OfkjMXOnB1vYS7kjkkSa22LpbV5hcjxvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1al2r0o", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al2r0o/one_trillion_row_challenge_1trc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al2r0o/one_trillion_row_challenge_1trc/", "subreddit_subscribers": 159022, "created_utc": 1707311999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, to preface, I'm venting a bit here but it's also somewhat of a genuine question.   \nStory - I recently applied to a senior DE position for a well known consulting company. For the record, I've worked in Senior DE/BI roles over the past few years and I have a number of former colleagues and friends who work at this specific company so I know their tech stack and business fairly well. Also, for the record I am not a software engineer. I can hack my way through python or an OOP/functional language but SQL is my native dialect. Anyways, I applied for this role and the only glaring omission on my resume was Python experience. Given that I qualified in every other way the recruiter had me move forward to the technical assessment. The assessment was conducted in codility and there were three parts, a python coding portion, a sql coding portion and AWS questions. Coming out of the assessment I felt pretty good but I knew full well that my python solution was pretty rudimentary (admittedly), however it was functional and passed the test cases correctly. Anyways, I find out a few days later from the internal recruiter that my test results didn't fare so well. Although my sql solution was excellent and most of the AWS questions I answered correctly, my python solution wasn't efficient enough and failed on too many edge cases. As such the technical team couldn't recommend I move forward with the interview process (much to my dismay). Now, again... I never said I was a competent Python programmer, in fact I fully admitted that I had very little hands on experience in a business setting coding with python but I'm very familiar with OOP concepts and can pick up any language if/when needed. Either way it seemed like in this case my solution needed to impress the team more than it did.   \nSo, this brings me back to something the recruiter told me initially... her exact words were \"our data engineers are really software engineers at heart\". I'm wondering if this is becoming more and more the case as time goes on. When I got into BI and DE years ago SQL was the language of most importance (at least in my past roles)... now it seems that that isn't quite the case anymore. Thoughts?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data engineers really just \"software engineers\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al3d2f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707313855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, to preface, I&amp;#39;m venting a bit here but it&amp;#39;s also somewhat of a genuine question.&lt;br/&gt;\nStory - I recently applied to a senior DE position for a well known consulting company. For the record, I&amp;#39;ve worked in Senior DE/BI roles over the past few years and I have a number of former colleagues and friends who work at this specific company so I know their tech stack and business fairly well. Also, for the record I am not a software engineer. I can hack my way through python or an OOP/functional language but SQL is my native dialect. Anyways, I applied for this role and the only glaring omission on my resume was Python experience. Given that I qualified in every other way the recruiter had me move forward to the technical assessment. The assessment was conducted in codility and there were three parts, a python coding portion, a sql coding portion and AWS questions. Coming out of the assessment I felt pretty good but I knew full well that my python solution was pretty rudimentary (admittedly), however it was functional and passed the test cases correctly. Anyways, I find out a few days later from the internal recruiter that my test results didn&amp;#39;t fare so well. Although my sql solution was excellent and most of the AWS questions I answered correctly, my python solution wasn&amp;#39;t efficient enough and failed on too many edge cases. As such the technical team couldn&amp;#39;t recommend I move forward with the interview process (much to my dismay). Now, again... I never said I was a competent Python programmer, in fact I fully admitted that I had very little hands on experience in a business setting coding with python but I&amp;#39;m very familiar with OOP concepts and can pick up any language if/when needed. Either way it seemed like in this case my solution needed to impress the team more than it did.&lt;br/&gt;\nSo, this brings me back to something the recruiter told me initially... her exact words were &amp;quot;our data engineers are really software engineers at heart&amp;quot;. I&amp;#39;m wondering if this is becoming more and more the case as time goes on. When I got into BI and DE years ago SQL was the language of most importance (at least in my past roles)... now it seems that that isn&amp;#39;t quite the case anymore. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1al3d2f", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al3d2f/are_data_engineers_really_just_software_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al3d2f/are_data_engineers_really_just_software_engineers/", "subreddit_subscribers": 159022, "created_utc": 1707313855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the titles says, I\u2019m curious about the different methods people are using for API versioning/Schema Management. \n\nDo you hard code schemas or infer them? How do you feel about the trade off between those two methods?\n\nHave you implemented data versioning, or a backfill process for new API versions if not?", "author_fullname": "t2_15uzwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys handle upgrading API versions/schema management for data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aklk2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707255730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the titles says, I\u2019m curious about the different methods people are using for API versioning/Schema Management. &lt;/p&gt;\n\n&lt;p&gt;Do you hard code schemas or infer them? How do you feel about the trade off between those two methods?&lt;/p&gt;\n\n&lt;p&gt;Have you implemented data versioning, or a backfill process for new API versions if not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aklk2q", "is_robot_indexable": true, "report_reasons": null, "author": "EngiNerd9000", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aklk2q/how_do_you_guys_handle_upgrading_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aklk2q/how_do_you_guys_handle_upgrading_api/", "subreddit_subscribers": 159022, "created_utc": 1707255730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was in 2 year break from career for higher studies(not relevant to DE). I'm trying to get back and I feel like industry is totally different in last 2 year. I'm preparing for job interviews. What tools should I focus? What would the company expect to answer from a mid level DE.", "author_fullname": "t2_3gwtyk10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the expectation from mid level DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akqqw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707269103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was in 2 year break from career for higher studies(not relevant to DE). I&amp;#39;m trying to get back and I feel like industry is totally different in last 2 year. I&amp;#39;m preparing for job interviews. What tools should I focus? What would the company expect to answer from a mid level DE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akqqw5", "is_robot_indexable": true, "report_reasons": null, "author": "BoneCollecfor", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akqqw5/whats_the_expectation_from_mid_level_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akqqw5/whats_the_expectation_from_mid_level_de/", "subreddit_subscribers": 159022, "created_utc": 1707269103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently on 38k in London straight out of uni. I\u2019m coming onto have one year of experience and have found other jobs in data science for about 45-50k. The roles are Python heavy which is ok for me because I did data science as a masters. My current role is purely sql and azure.\n\nCan anyone with experience let me know if the job prospects for data science are greater than data engineering?\n\nBoth would be in the same industry (insurnace)", "author_fullname": "t2_9wz5l9vg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I leave my data engineering role for data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akz3j6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707297960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently on 38k in London straight out of uni. I\u2019m coming onto have one year of experience and have found other jobs in data science for about 45-50k. The roles are Python heavy which is ok for me because I did data science as a masters. My current role is purely sql and azure.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with experience let me know if the job prospects for data science are greater than data engineering?&lt;/p&gt;\n\n&lt;p&gt;Both would be in the same industry (insurnace)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1akz3j6", "is_robot_indexable": true, "report_reasons": null, "author": "Due_Statistician2604", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akz3j6/should_i_leave_my_data_engineering_role_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akz3j6/should_i_leave_my_data_engineering_role_for_data/", "subreddit_subscribers": 159022, "created_utc": 1707297960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there! I'm a dev advocate at Airbyte, and we put together a list of industry trends all data engineers should know about - as well as some practical tips to make sure you're ahead of the curve. We also spoke with experts across the industry to get their take!  \n\n\nWhat trends are you most worried/excited/nonplussed by?\n\n[https://airbyte.com/blog/data-engineering-landscape-2024](https://airbyte.com/blog/data-engineering-landscape-2024)", "author_fullname": "t2_5h5nqi7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Navigating the Data Engineering Landscape 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al8dni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707326881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! I&amp;#39;m a dev advocate at Airbyte, and we put together a list of industry trends all data engineers should know about - as well as some practical tips to make sure you&amp;#39;re ahead of the curve. We also spoke with experts across the industry to get their take!  &lt;/p&gt;\n\n&lt;p&gt;What trends are you most worried/excited/nonplussed by?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://airbyte.com/blog/data-engineering-landscape-2024\"&gt;https://airbyte.com/blog/data-engineering-landscape-2024&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?auto=webp&amp;s=4f84d1ff281c2afb45a43770d330187581e49ee7", "width": 2540, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca99d3398b6ff856a22035090962d34f333209c", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b962bc4ee1b866402194e0c06f399b2f35306755", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db7540334ec37e3fa5f8ed2d4bf0c35f39cebed0", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=24f667e9fc90cd33ef4964813a4138a54b2f6f9a", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d06cac90f5421b6d157974af24c383386576be78", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/SwJhwtl1x6Cah75-2N41Azx681vNJlGK71zGA5z-dfc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41edd56cf0d23b7d3f94badbe058b5501de55c39", "width": 1080, "height": 612}], "variants": {}, "id": "fQIhx03Js0q7C-jgMTsBiCcEfNmX0A-1cp8NthF0avY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1al8dni", "is_robot_indexable": true, "report_reasons": null, "author": "Chemical-Treat6596", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al8dni/navigating_the_data_engineering_landscape_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al8dni/navigating_the_data_engineering_landscape_2024/", "subreddit_subscribers": 159022, "created_utc": 1707326881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we use S3 for ingesting raw source data. We save it in unchanged format from source. Then in the next stage we transform them using python apps and save them as parquet files to S3 again. After that we take parquet files and load them into Postgres as final stage for consumers. \n\nHow do you handle realtime events data in such architecture to make them available to consumers within lets say 1-5s? I think dumping data from Kafka/RabbitMQ to go through the entire pipeline (raw, parquet, postgres) would take longer and saving them directly into postgres is probably not a good idea. Maybe dumping into S3 AND into Postgres? Or having another database for \u201crealtime\u201d events? Or is there any best practice for such case?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u201cLakehouse\u201d - Realtime data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al96or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707328839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we use S3 for ingesting raw source data. We save it in unchanged format from source. Then in the next stage we transform them using python apps and save them as parquet files to S3 again. After that we take parquet files and load them into Postgres as final stage for consumers. &lt;/p&gt;\n\n&lt;p&gt;How do you handle realtime events data in such architecture to make them available to consumers within lets say 1-5s? I think dumping data from Kafka/RabbitMQ to go through the entire pipeline (raw, parquet, postgres) would take longer and saving them directly into postgres is probably not a good idea. Maybe dumping into S3 AND into Postgres? Or having another database for \u201crealtime\u201d events? Or is there any best practice for such case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1al96or", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al96or/lakehouse_realtime_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al96or/lakehouse_realtime_data/", "subreddit_subscribers": 159022, "created_utc": 1707328839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like Hadoop has mapreduce \nDatabricks and Aws glue use Spark\nWhat does snowflake use for computing?", "author_fullname": "t2_pp1jtay3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is compute engine in SnowFlake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akzzx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707301930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like Hadoop has mapreduce \nDatabricks and Aws glue use Spark\nWhat does snowflake use for computing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akzzx2", "is_robot_indexable": true, "report_reasons": null, "author": "mysticsoul1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akzzx2/what_is_compute_engine_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akzzx2/what_is_compute_engine_in_snowflake/", "subreddit_subscribers": 159022, "created_utc": 1707301930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cross posting this question, maybe some data engineers can help me with my dilemma. I'm limited to a vanilla anaconda python environment, keep that in mind.\n\nI have a huge table in MS Access (4.5 million rows, 27 columns). I have a connection variable with pyodbc, and a simple select query. I have generator = pd.read_sql(query, conn, chunksize=10000), which stores an iterable object where each iteration is a dataframe (read pandas docs). \n\nI want to loop over this single object, do some pandas transformations, append each df to a list in parallel, then pd.concat the list after. I've tried to write a parallelization script using either ProcessPoolExecutor or MultiThreadExecutor, but I always run into little issues and chatgpt just keeps going in circles and doesn't help.\n\nAny suggestions how to loop over a single object in parallel, where each iteration contains a dataframe?", "author_fullname": "t2_lfyfd0fn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to iterate over a single object in parallel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akltyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707256373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cross posting this question, maybe some data engineers can help me with my dilemma. I&amp;#39;m limited to a vanilla anaconda python environment, keep that in mind.&lt;/p&gt;\n\n&lt;p&gt;I have a huge table in MS Access (4.5 million rows, 27 columns). I have a connection variable with pyodbc, and a simple select query. I have generator = pd.read_sql(query, conn, chunksize=10000), which stores an iterable object where each iteration is a dataframe (read pandas docs). &lt;/p&gt;\n\n&lt;p&gt;I want to loop over this single object, do some pandas transformations, append each df to a list in parallel, then pd.concat the list after. I&amp;#39;ve tried to write a parallelization script using either ProcessPoolExecutor or MultiThreadExecutor, but I always run into little issues and chatgpt just keeps going in circles and doesn&amp;#39;t help.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions how to loop over a single object in parallel, where each iteration contains a dataframe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akltyk", "is_robot_indexable": true, "report_reasons": null, "author": "velimino", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akltyk/how_to_iterate_over_a_single_object_in_parallel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akltyk/how_to_iterate_over_a_single_object_in_parallel/", "subreddit_subscribers": 159022, "created_utc": 1707256373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone successfully tried the 250 MB Lambda Layer limit workaround by packaging their dependencies in EFS? Does it affect Lambda init time? What about concurrency - how many concurrent lambda executions can read from the shared EFS? Can serverless be used to deploy such lambda functions that use EFS for the importing the modules? Please don't suggest using containerized lambda functions .", "author_fullname": "t2_v8qhw4wd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using EFS for bigger python packages in AWS Lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al0znu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707306667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707305959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully tried the 250 MB Lambda Layer limit workaround by packaging their dependencies in EFS? Does it affect Lambda init time? What about concurrency - how many concurrent lambda executions can read from the shared EFS? Can serverless be used to deploy such lambda functions that use EFS for the importing the modules? Please don&amp;#39;t suggest using containerized lambda functions .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al0znu", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Love_648", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al0znu/using_efs_for_bigger_python_packages_in_aws_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al0znu/using_efs_for_bigger_python_packages_in_aws_lambda/", "subreddit_subscribers": 159022, "created_utc": 1707305959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm asking here because I assume you all know so much better.\n\nI'm not a data engineer and I'm working now just using Colab for a project I need to automate. It fetches data from BigQuery, then process the data, does NLP inference with a few APIs (hosting the models elsewhere) and then processes it back to another BigQuery table. Not difficult stuff but because it will process maybe around 10,000 - 20,000 rows and uses several NLPs to analyze it, it can take up to 30-45 mins, longer if there are errors and so on. \n\nI have looked at Prefect? I'm not picky for the tool and can really work with low-code solutions as long as it is intuitive. Would be great to have it serverless and then obviously run on a schedule (in batches - one time per day) so it's easy to set up and run. Is there a tool like this that isn't super expensive? I have one like this but it's with NodeJS so no dice. \n\nIf not, what are the best choices here? Would love some suggestions!", "author_fullname": "t2_n8b2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best option for automated \"long-running\" ETL pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1alcsfo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707337764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m asking here because I assume you all know so much better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not a data engineer and I&amp;#39;m working now just using Colab for a project I need to automate. It fetches data from BigQuery, then process the data, does NLP inference with a few APIs (hosting the models elsewhere) and then processes it back to another BigQuery table. Not difficult stuff but because it will process maybe around 10,000 - 20,000 rows and uses several NLPs to analyze it, it can take up to 30-45 mins, longer if there are errors and so on. &lt;/p&gt;\n\n&lt;p&gt;I have looked at Prefect? I&amp;#39;m not picky for the tool and can really work with low-code solutions as long as it is intuitive. Would be great to have it serverless and then obviously run on a schedule (in batches - one time per day) so it&amp;#39;s easy to set up and run. Is there a tool like this that isn&amp;#39;t super expensive? I have one like this but it&amp;#39;s with NodeJS so no dice. &lt;/p&gt;\n\n&lt;p&gt;If not, what are the best choices here? Would love some suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1alcsfo", "is_robot_indexable": true, "report_reasons": null, "author": "ilsilfverskiold", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1alcsfo/whats_the_best_option_for_automated_longrunning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1alcsfo/whats_the_best_option_for_automated_longrunning/", "subreddit_subscribers": 159022, "created_utc": 1707337764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I couldn't find a subreddit for dbt or an answer to this on Google so I'll put this to you guys\n\nI want to generate a database documentation for my dbt models which contains a list of columns contained in each table. I know I can use `dbt docs generate`  to create the documentation but as far as I understand I need to manually document the existing columns in the schema.yml so that they show in the documentation? So every time someone delete/add a column in the SQL they would need to reflect that change in the schema.yml file?  \n\n\nIt seems like there should be a way to automatically sync that column list from the corresponding SQL file but I don't seem to find how to do it. Does anyone have a solution?  \n", "author_fullname": "t2_grnvlbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically syncing DBT column schema with actual columns in corresponding SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al7hv9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707324726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I couldn&amp;#39;t find a subreddit for dbt or an answer to this on Google so I&amp;#39;ll put this to you guys&lt;/p&gt;\n\n&lt;p&gt;I want to generate a database documentation for my dbt models which contains a list of columns contained in each table. I know I can use &lt;code&gt;dbt docs generate&lt;/code&gt;  to create the documentation but as far as I understand I need to manually document the existing columns in the schema.yml so that they show in the documentation? So every time someone delete/add a column in the SQL they would need to reflect that change in the schema.yml file?  &lt;/p&gt;\n\n&lt;p&gt;It seems like there should be a way to automatically sync that column list from the corresponding SQL file but I don&amp;#39;t seem to find how to do it. Does anyone have a solution?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al7hv9", "is_robot_indexable": true, "report_reasons": null, "author": "Kenoai", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al7hv9/automatically_syncing_dbt_column_schema_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al7hv9/automatically_syncing_dbt_column_schema_with/", "subreddit_subscribers": 159022, "created_utc": 1707324726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a data analyst and have an opportunity to make a switch to a DE role. It\u2019s a mid level role, and would be an internal transfer. I am very good with SQL, have a bit more than general data modeling experience, have set up all the data infrastructure for my team (DAGs / tasks / data models in our BI tools), but my Python is very basic. \n\nLooking for some guidance on the Python bit, as I\u2019ve been trying to study up in my freetime a bit more. I know the interview will go over general syntax, data manipulation, working with SQL DBs, and a few other things. I\u2019m planning to focus catching up on pandas mainly, but would love some guidance from yall on if there are specifics I should focus on? Thanks in advance!", "author_fullname": "t2_bbzr0sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have an interview and need some guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akxu61", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707292361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a data analyst and have an opportunity to make a switch to a DE role. It\u2019s a mid level role, and would be an internal transfer. I am very good with SQL, have a bit more than general data modeling experience, have set up all the data infrastructure for my team (DAGs / tasks / data models in our BI tools), but my Python is very basic. &lt;/p&gt;\n\n&lt;p&gt;Looking for some guidance on the Python bit, as I\u2019ve been trying to study up in my freetime a bit more. I know the interview will go over general syntax, data manipulation, working with SQL DBs, and a few other things. I\u2019m planning to focus catching up on pandas mainly, but would love some guidance from yall on if there are specifics I should focus on? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akxu61", "is_robot_indexable": true, "report_reasons": null, "author": "NoDistractionz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akxu61/have_an_interview_and_need_some_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akxu61/have_an_interview_and_need_some_guidance/", "subreddit_subscribers": 159022, "created_utc": 1707292361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve made it to the final round of a job interview for a data engineer job that I\u2019m very interested in. I\u2019ve already done the hard steps and technical assessments. This final round is a \u201cproduct interview\u201d with a project manager. \n\nAny idea what I can expect? Agile questions and how I\u2019ve worked in the past? Actual project management questions like T-shirt sizing? Questions about the industry and company\u2019s actual product? \n\nWhat have you all seen in these types of interviews?", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akmwb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707258976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve made it to the final round of a job interview for a data engineer job that I\u2019m very interested in. I\u2019ve already done the hard steps and technical assessments. This final round is a \u201cproduct interview\u201d with a project manager. &lt;/p&gt;\n\n&lt;p&gt;Any idea what I can expect? Agile questions and how I\u2019ve worked in the past? Actual project management questions like T-shirt sizing? Questions about the industry and company\u2019s actual product? &lt;/p&gt;\n\n&lt;p&gt;What have you all seen in these types of interviews?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akmwb7", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akmwb7/product_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akmwb7/product_interview/", "subreddit_subscribers": 159022, "created_utc": 1707258976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a student of data engineering, and I just built my first pipelines using terraform, airflow, dbt, cosmos, snowflake, bigquery etc..\n\nBut all the tools I used were free... How the heck does Hashicorp (for terraform), Apache (for airflow), DBT labs (for dbt), and Astronomer (for cosmos) make any money? \n\nSorry just one of those embarrasingly basic questions but I still don't get it", "author_fullname": "t2_fq68u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do these companies make money?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aldl7r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707339789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a student of data engineering, and I just built my first pipelines using terraform, airflow, dbt, cosmos, snowflake, bigquery etc..&lt;/p&gt;\n\n&lt;p&gt;But all the tools I used were free... How the heck does Hashicorp (for terraform), Apache (for airflow), DBT labs (for dbt), and Astronomer (for cosmos) make any money? &lt;/p&gt;\n\n&lt;p&gt;Sorry just one of those embarrasingly basic questions but I still don&amp;#39;t get it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aldl7r", "is_robot_indexable": true, "report_reasons": null, "author": "KimchiFitness", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aldl7r/how_do_these_companies_make_money/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aldl7r/how_do_these_companies_make_money/", "subreddit_subscribers": 159022, "created_utc": 1707339789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, \n\n\nI have a model and I\u2019m ready to deploy in AWS.  I\u2019ve been trying to research which implementation is best between Lambda and SageMaker and I\u2019m hoping to get some informed opinions. \n\n# More Background \n\n1) The model is a PyTorch prebuilt model and works great off the shelf for our purposes, so we won\u2019t need to do any training. (Otherwise I'm thinking we'd definitely need to with SageMaker)\n\n2) Using GPUs would be great but not necessary. Does either option use GPU?  I would assume SageMaker could/does, but it's not clear from anything I've read.\n\n3) Concerns around speed are huge.  We are expecting for the most part it will be quiet and then we'll receive a bolus of requests in the hundreds/min.  We're concerned that if the model takes to long to load or warm-up then it will lead to bad user experiences.  So we definitely want fast warm-up times\n\n4) It seems that when implementing in Lambda we have the choice to embed the model in a docker image on ECR or placing the model artifacts in an S3 bucket.  Is one of those options better for speed than the other?\n\n\nThank you for your responses!\n\n- Data Scallion", "author_fullname": "t2_dffy2296", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Lambda or SageMaker Implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aldbi2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707339111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt;\n\n&lt;p&gt;I have a model and I\u2019m ready to deploy in AWS.  I\u2019ve been trying to research which implementation is best between Lambda and SageMaker and I\u2019m hoping to get some informed opinions. &lt;/p&gt;\n\n&lt;h1&gt;More Background&lt;/h1&gt;\n\n&lt;p&gt;1) The model is a PyTorch prebuilt model and works great off the shelf for our purposes, so we won\u2019t need to do any training. (Otherwise I&amp;#39;m thinking we&amp;#39;d definitely need to with SageMaker)&lt;/p&gt;\n\n&lt;p&gt;2) Using GPUs would be great but not necessary. Does either option use GPU?  I would assume SageMaker could/does, but it&amp;#39;s not clear from anything I&amp;#39;ve read.&lt;/p&gt;\n\n&lt;p&gt;3) Concerns around speed are huge.  We are expecting for the most part it will be quiet and then we&amp;#39;ll receive a bolus of requests in the hundreds/min.  We&amp;#39;re concerned that if the model takes to long to load or warm-up then it will lead to bad user experiences.  So we definitely want fast warm-up times&lt;/p&gt;\n\n&lt;p&gt;4) It seems that when implementing in Lambda we have the choice to embed the model in a docker image on ECR or placing the model artifacts in an S3 bucket.  Is one of those options better for speed than the other?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your responses!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Scallion&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aldbi2", "is_robot_indexable": true, "report_reasons": null, "author": "data_scallion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aldbi2/aws_lambda_or_sagemaker_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aldbi2/aws_lambda_or_sagemaker_implementation/", "subreddit_subscribers": 159022, "created_utc": 1707339111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm looking for a crash course that can help me quickly pick up Pyspark and Apache Airflow I work in Python, AWS, and JavaScript. I also have good experience with RDS. Nowadays, companies are looking for a wide range of skills, so I want to make sure I have a basic understanding of how Pyspark/Apache Airflow works. I prefer a more hands-on course rather than just theory, but I'm looking for something quick as I don't have much time or money to join a boot camp. ", "author_fullname": "t2_4l1p62pr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crash course - PySpark/Aache Airlow (Interview purpose)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1alcila", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707337093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a crash course that can help me quickly pick up Pyspark and Apache Airflow I work in Python, AWS, and JavaScript. I also have good experience with RDS. Nowadays, companies are looking for a wide range of skills, so I want to make sure I have a basic understanding of how Pyspark/Apache Airflow works. I prefer a more hands-on course rather than just theory, but I&amp;#39;m looking for something quick as I don&amp;#39;t have much time or money to join a boot camp. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1alcila", "is_robot_indexable": true, "report_reasons": null, "author": "kchatter20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1alcila/crash_course_pysparkaache_airlow_interview_purpose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1alcila/crash_course_pysparkaache_airlow_interview_purpose/", "subreddit_subscribers": 159022, "created_utc": 1707337093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Make sense managed storage for instance single node in redshift?", "author_fullname": "t2_9s0c496k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ala1ut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707330963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Make sense managed storage for instance single node in redshift?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ala1ut", "is_robot_indexable": true, "report_reasons": null, "author": "Dwoler", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ala1ut/redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ala1ut/redshift/", "subreddit_subscribers": 159022, "created_utc": 1707330963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Organise Your dbt Projects Better And Boost Domain Expertise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al7ryy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1707325419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/datagibberish/p/dbt-screaming-architecture?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1al7ryy", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1al7ryy/how_to_organise_your_dbt_projects_better_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/datagibberish/p/dbt-screaming-architecture?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "subreddit_subscribers": 159022, "created_utc": 1707325419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks vs Snowflake: A Complete 2024 Comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1al7lro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/93ThqwNjwaiHUqHSNh3gyFCjrGajEXr9VgvIc6xYxx4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707325003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/sync-computing/databricks-vs-snowflake-a-complete-2024-comparison-462eac35b639", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?auto=webp&amp;s=ffb6e5b4bf82b1aead61cfa59b4fd11d46e274c9", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=839b1dd4bf7d04a5274050467a602d9bc0218b0d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59e8fdb5e92b41bf70fa1c54714f3de952b2cfc3", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c06dd241ef3c7f1545336f4b862f4ae3a9db719", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fb9bb8543f755027c2819ac786d83ac7749439c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b9980d77f90eea4d6a664a3ea7a928ff46c8b6e", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/0tRB5YMuLsbeWdfOwwuZwvi0SJ0c5--3tZPEvhRUV6I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57f71bb4c7355a746a6cba05d8eb5e7be633786d", "width": 1080, "height": 564}], "variants": {}, "id": "kiqrEhBTTjoBsiL1NYClVHoer1of1O-IZmLZtyUlf3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1al7lro", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al7lro/databricks_vs_snowflake_a_complete_2024_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/sync-computing/databricks-vs-snowflake-a-complete-2024-comparison-462eac35b639", "subreddit_subscribers": 159022, "created_utc": 1707325003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm using Airflow since 4 years to schedule dags/tasks, in a different companies. Before that, I was mainly using cron on Linux servers, so I really appreciate Airflow's features.  \nHowever, in my case, **DAGs and tasks are often quite basics**, and **do not require a cluster infrastructure**. The tasks are mainly python scripts, that use network resources (few memory and cpu).And only few people access to Airflow web server.\n\n&amp;#x200B;\n\nThe main problem here, it's that **Airflow can be quite expensive** in some cases. \n\n\\- If you are using cloud services to host it (like Astronomer, Google Cloud Composer, etc..), prices can be crazy\n\n\\- If you handle the infrastructure, the maintenance cost can be also important (especially if you don't have dedicated team for that).\n\n&amp;#x200B;\n\nBut, in any case, I don't want to switch back to cron jobs for several reasons:\n\n\\- The main dashboard of Airflow is super useful, especially to spot errors\n\n\\- You can easily restart tasks in the past\n\n\\- You can easily access logs for a dedicated task\n\n\\- And other cool features\n\n&amp;#x200B;\n\nSo I'm trying to **host Airflow on a single server**. But, I have the feeling that Airflow is not optimal to run on a single server. First, because it doesn't support standalone database like Sqlite in production environment, so you need resources to run database server like MySQL or PostgreSQL on your server. Also, you have the web server that consume some resources.\n\nI'm quite interested to know how you optimize costs? And what do you think about hosting Airflow on a single server? Do you know any other similar tools that are better for a single server infrastructure?\n\nThanks in advance!", "author_fullname": "t2_thx6yy3ho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow - How to reduce cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al5i0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707319977.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707319672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Airflow since 4 years to schedule dags/tasks, in a different companies. Before that, I was mainly using cron on Linux servers, so I really appreciate Airflow&amp;#39;s features.&lt;br/&gt;\nHowever, in my case, &lt;strong&gt;DAGs and tasks are often quite basics&lt;/strong&gt;, and &lt;strong&gt;do not require a cluster infrastructure&lt;/strong&gt;. The tasks are mainly python scripts, that use network resources (few memory and cpu).And only few people access to Airflow web server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The main problem here, it&amp;#39;s that &lt;strong&gt;Airflow can be quite expensive&lt;/strong&gt; in some cases. &lt;/p&gt;\n\n&lt;p&gt;- If you are using cloud services to host it (like Astronomer, Google Cloud Composer, etc..), prices can be crazy&lt;/p&gt;\n\n&lt;p&gt;- If you handle the infrastructure, the maintenance cost can be also important (especially if you don&amp;#39;t have dedicated team for that).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But, in any case, I don&amp;#39;t want to switch back to cron jobs for several reasons:&lt;/p&gt;\n\n&lt;p&gt;- The main dashboard of Airflow is super useful, especially to spot errors&lt;/p&gt;\n\n&lt;p&gt;- You can easily restart tasks in the past&lt;/p&gt;\n\n&lt;p&gt;- You can easily access logs for a dedicated task&lt;/p&gt;\n\n&lt;p&gt;- And other cool features&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m trying to &lt;strong&gt;host Airflow on a single server&lt;/strong&gt;. But, I have the feeling that Airflow is not optimal to run on a single server. First, because it doesn&amp;#39;t support standalone database like Sqlite in production environment, so you need resources to run database server like MySQL or PostgreSQL on your server. Also, you have the web server that consume some resources.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite interested to know how you optimize costs? And what do you think about hosting Airflow on a single server? Do you know any other similar tools that are better for a single server infrastructure?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1al5i0k", "is_robot_indexable": true, "report_reasons": null, "author": "4-sushi", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al5i0k/airflow_how_to_reduce_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al5i0k/airflow_how_to_reduce_cost/", "subreddit_subscribers": 159022, "created_utc": 1707319672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys!  \nI need to create a system architecture for a university course. \n\nRoughly said there isa fictional company that has an existing BI-system with data warehouse &amp; ML-toolsto analyse historical data. Now I need to implement a real-time functionality to display temperature sensor data so that the workers can react quickly once the temperature is too high in the production machines.  \nI could use the existing BI-system as batch layer and put a streaming layer on top for the sensor data. Now in my opinion, it would make sense to process all data together via Kafka &amp; Flink and THEN send the processed data to the already existing DWH in the batch layer.  \nAnd now we're getting to my question: There are so many different definitions as to where exactly Lambda differentiates itself from Kappa that I'm confused as to what is correct. Here's the different definitions that I read so far:\n\n1. If the data source sends data to a batch layer AND a streaming layer (so sending it to 2 different places), it's lambda. So according to this definition, my solution would be Kappa because the data doesn't split right at the source, but instead after processing. So I'd put a Kappa Architecture on top of an existing Architecture.\n2. It's Lambda if the speed layer sends data to the batch layer AND to the serving layer, so that it can be batch processed and at the same time be shown in a realtime dashboard. According to this definition, my solution would be Lambda.\n3. It's Lambda if there is a Batch, Streaming &amp; Serving Layer in the Architecture.   \n\n\nIf someone could help me to figure this out, that would be highly appreciated!", "author_fullname": "t2_btjfbxoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between Lambda &amp; Kappa in detail", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al5dz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707319373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;br/&gt;\nI need to create a system architecture for a university course. &lt;/p&gt;\n\n&lt;p&gt;Roughly said there isa fictional company that has an existing BI-system with data warehouse &amp;amp; ML-toolsto analyse historical data. Now I need to implement a real-time functionality to display temperature sensor data so that the workers can react quickly once the temperature is too high in the production machines.&lt;br/&gt;\nI could use the existing BI-system as batch layer and put a streaming layer on top for the sensor data. Now in my opinion, it would make sense to process all data together via Kafka &amp;amp; Flink and THEN send the processed data to the already existing DWH in the batch layer.&lt;br/&gt;\nAnd now we&amp;#39;re getting to my question: There are so many different definitions as to where exactly Lambda differentiates itself from Kappa that I&amp;#39;m confused as to what is correct. Here&amp;#39;s the different definitions that I read so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If the data source sends data to a batch layer AND a streaming layer (so sending it to 2 different places), it&amp;#39;s lambda. So according to this definition, my solution would be Kappa because the data doesn&amp;#39;t split right at the source, but instead after processing. So I&amp;#39;d put a Kappa Architecture on top of an existing Architecture.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s Lambda if the speed layer sends data to the batch layer AND to the serving layer, so that it can be batch processed and at the same time be shown in a realtime dashboard. According to this definition, my solution would be Lambda.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s Lambda if there is a Batch, Streaming &amp;amp; Serving Layer in the Architecture.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If someone could help me to figure this out, that would be highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al5dz2", "is_robot_indexable": true, "report_reasons": null, "author": "No_Dentist7884", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al5dz2/difference_between_lambda_kappa_in_detail/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al5dz2/difference_between_lambda_kappa_in_detail/", "subreddit_subscribers": 159022, "created_utc": 1707319373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, our organization has lots of hive jobs. Logic wise they are simple, we read from base tables -&gt; apply transformations-&gt; dump into final table.\n\nIssue is there are 100+ columns which makes these scripts super long. When we need to add new column it gets very cumbersome to do.\n\nAre there any visual tools available where we can drag and drop tables to create a flow ? Or anything similar to improve readability and maintainability of code would be really helpful", "author_fullname": "t2_tpgjfxis1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A visual tool to generate hive queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al4xd2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707318144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, our organization has lots of hive jobs. Logic wise they are simple, we read from base tables -&amp;gt; apply transformations-&amp;gt; dump into final table.&lt;/p&gt;\n\n&lt;p&gt;Issue is there are 100+ columns which makes these scripts super long. When we need to add new column it gets very cumbersome to do.&lt;/p&gt;\n\n&lt;p&gt;Are there any visual tools available where we can drag and drop tables to create a flow ? Or anything similar to improve readability and maintainability of code would be really helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al4xd2", "is_robot_indexable": true, "report_reasons": null, "author": "data-geeko", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al4xd2/a_visual_tool_to_generate_hive_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al4xd2/a_visual_tool_to_generate_hive_queries/", "subreddit_subscribers": 159022, "created_utc": 1707318144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys. \n\nRight now I have dag in Airflow that uses operators to get data everyday from an endpoint, transform it and putting it into several tables. The thing is the endpoint doesn\u2019t let me get updates data from a specific date, so I have to filter once I get the whole raw data\n\nWhat do you think I should do? Is there\u2019s any Data engineering tool to let me create pipelines and deal with databases or is Airflow good to do this kind of things?\n\nThanks in advance", "author_fullname": "t2_617m3t2e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do think I should do to update a database automatically using Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al1yne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707309436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. &lt;/p&gt;\n\n&lt;p&gt;Right now I have dag in Airflow that uses operators to get data everyday from an endpoint, transform it and putting it into several tables. The thing is the endpoint doesn\u2019t let me get updates data from a specific date, so I have to filter once I get the whole raw data&lt;/p&gt;\n\n&lt;p&gt;What do you think I should do? Is there\u2019s any Data engineering tool to let me create pipelines and deal with databases or is Airflow good to do this kind of things?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1al1yne", "is_robot_indexable": true, "report_reasons": null, "author": "Moradisten", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al1yne/what_do_think_i_should_do_to_update_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al1yne/what_do_think_i_should_do_to_update_a_database/", "subreddit_subscribers": 159022, "created_utc": 1707309436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im looking for a way to orchestrate data and requests dynamically based on some sample ids (that can be in the hundreds or thousands) without any invasive API workarounds.\n\n&amp;#x200B;\n\nI'm looking for something such as a load balancer, which can take the \\[sample size\\] as input and spread it through multiple subflows with different schedules dynamically. The idea would be to spread the calls across the day and to make the result of each subflow independent, such as if one fails, I do not have to re run the entire samples, just the subsample which failed. How can I do this?\n\n&amp;#x200B;\n\nI'm using prefect.", "author_fullname": "t2_a9360hkx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data orchestration and API limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1al0usn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707305431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking for a way to orchestrate data and requests dynamically based on some sample ids (that can be in the hundreds or thousands) without any invasive API workarounds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something such as a load balancer, which can take the [sample size] as input and spread it through multiple subflows with different schedules dynamically. The idea would be to spread the calls across the day and to make the result of each subflow independent, such as if one fails, I do not have to re run the entire samples, just the subsample which failed. How can I do this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using prefect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al0usn", "is_robot_indexable": true, "report_reasons": null, "author": "Different_Fee6785", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al0usn/data_orchestration_and_api_limits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al0usn/data_orchestration_and_api_limits/", "subreddit_subscribers": 159022, "created_utc": 1707305431.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}