{"kind": "Listing", "data": {"after": "t3_1aku4vj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the titles says, I\u2019m curious about the different methods people are using for API versioning/Schema Management. \n\nDo you hard code schemas or infer them? How do you feel about the trade off between those two methods?\n\nHave you implemented data versioning, or a backfill process for new API versions if not?", "author_fullname": "t2_15uzwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys handle upgrading API versions/schema management for data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aklk2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707255730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the titles says, I\u2019m curious about the different methods people are using for API versioning/Schema Management. &lt;/p&gt;\n\n&lt;p&gt;Do you hard code schemas or infer them? How do you feel about the trade off between those two methods?&lt;/p&gt;\n\n&lt;p&gt;Have you implemented data versioning, or a backfill process for new API versions if not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aklk2q", "is_robot_indexable": true, "report_reasons": null, "author": "EngiNerd9000", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aklk2q/how_do_you_guys_handle_upgrading_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aklk2q/how_do_you_guys_handle_upgrading_api/", "subreddit_subscribers": 158923, "created_utc": 1707255730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm a junior Data Engineer and I'm trying to gauge if my workload is normal for my level. I'm the sole support for a Data Analysis team of about 8 people, maintaining a data lake (which urgently needs improvement but there's no time), and managing some intense responsibilities:\n\n* Building and maintaining pipelines for updating customer data directly in production databases.\n* Developing crucial pipelines for compliance, where errors could have serious legal consequences.\n* Handling tasks in the Production environment, involving data transfers between various production databases (both SQL and NoSQL).\n* No analytics/ML/AI task whatsoever is the primary priority. \n\nRecently, it seems like any task that could be done via APIs or requires orchestration (like using Airflow) lands on my desk. This is adding to the pressure, especially since I'm the only DE in my team after my senior colleague left.\n\nIs handling such a wide and high-stakes range of responsibilities typical for a junior DE? How do you all cope with similar stress and workload? Any advice or insights would be incredibly helpful!\n\nThanks a lot!", "author_fullname": "t2_lsi4fru5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Overwhelmed with Tasks - Is This Normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akh1po", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707244928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a junior Data Engineer and I&amp;#39;m trying to gauge if my workload is normal for my level. I&amp;#39;m the sole support for a Data Analysis team of about 8 people, maintaining a data lake (which urgently needs improvement but there&amp;#39;s no time), and managing some intense responsibilities:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Building and maintaining pipelines for updating customer data directly in production databases.&lt;/li&gt;\n&lt;li&gt;Developing crucial pipelines for compliance, where errors could have serious legal consequences.&lt;/li&gt;\n&lt;li&gt;Handling tasks in the Production environment, involving data transfers between various production databases (both SQL and NoSQL).&lt;/li&gt;\n&lt;li&gt;No analytics/ML/AI task whatsoever is the primary priority. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Recently, it seems like any task that could be done via APIs or requires orchestration (like using Airflow) lands on my desk. This is adding to the pressure, especially since I&amp;#39;m the only DE in my team after my senior colleague left.&lt;/p&gt;\n\n&lt;p&gt;Is handling such a wide and high-stakes range of responsibilities typical for a junior DE? How do you all cope with similar stress and workload? Any advice or insights would be incredibly helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akh1po", "is_robot_indexable": true, "report_reasons": null, "author": "kasliaskj", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akh1po/data_engineer_overwhelmed_with_tasks_is_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akh1po/data_engineer_overwhelmed_with_tasks_is_this/", "subreddit_subscribers": 158923, "created_utc": 1707244928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was in 2 year break from career for higher studies(not relevant to DE). I'm trying to get back and I feel like industry is totally different in last 2 year. I'm preparing for job interviews. What tools should I focus? What would the company expect to answer from a mid level DE.", "author_fullname": "t2_3gwtyk10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the expectation from mid level DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akqqw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707269103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was in 2 year break from career for higher studies(not relevant to DE). I&amp;#39;m trying to get back and I feel like industry is totally different in last 2 year. I&amp;#39;m preparing for job interviews. What tools should I focus? What would the company expect to answer from a mid level DE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akqqw5", "is_robot_indexable": true, "report_reasons": null, "author": "BoneCollecfor", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akqqw5/whats_the_expectation_from_mid_level_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akqqw5/whats_the_expectation_from_mid_level_de/", "subreddit_subscribers": 158923, "created_utc": 1707269103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI don\u2019t have any experience, knowledge or whatever when it comes to data, it was even recently that I discovered the whole field. From my limited things that I\u2019ve read I think I\u2019m more inclined for DE rather than DA as I would like to build stuff. \n\n( no software engineer or development experience either ) \n\nSo my question is: how much harder is it to skip DA and go straight to DE?\n\nQuestion 2: I\u2019ve stopped at datacamp.com - is this a good platform to learn?\n\nThanks in advance for anyone who comments and reads. \n\nJust as a side note I\u2019m going to be let go from work so I\u2019ll have quite a bit of free time to devote (outside my resume senders and parental duties) so I\u2019ll be able to spare 5-7/8 hours a day at a minimum. I know it\u2019s a rather stupid question but is there like an average timeframe of time needed to be somewhat job ready? (I live in Eastern Europe so the competition I think is not as fierce as like in the US for instance)", "author_fullname": "t2_p2ujlh1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering as a beginner?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akfodk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707241609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have any experience, knowledge or whatever when it comes to data, it was even recently that I discovered the whole field. From my limited things that I\u2019ve read I think I\u2019m more inclined for DE rather than DA as I would like to build stuff. &lt;/p&gt;\n\n&lt;p&gt;( no software engineer or development experience either ) &lt;/p&gt;\n\n&lt;p&gt;So my question is: how much harder is it to skip DA and go straight to DE?&lt;/p&gt;\n\n&lt;p&gt;Question 2: I\u2019ve stopped at datacamp.com - is this a good platform to learn?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for anyone who comments and reads. &lt;/p&gt;\n\n&lt;p&gt;Just as a side note I\u2019m going to be let go from work so I\u2019ll have quite a bit of free time to devote (outside my resume senders and parental duties) so I\u2019ll be able to spare 5-7/8 hours a day at a minimum. I know it\u2019s a rather stupid question but is there like an average timeframe of time needed to be somewhat job ready? (I live in Eastern Europe so the competition I think is not as fierce as like in the US for instance)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1akfodk", "is_robot_indexable": true, "report_reasons": null, "author": "Cautious-Tart-7117", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akfodk/data_engineering_as_a_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akfodk/data_engineering_as_a_beginner/", "subreddit_subscribers": 158923, "created_utc": 1707241609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I have an interview next week for an AWS data engineer. I need resources for preparations.\n\nI have been working as a data engineer for the past 3 years. But I have experience in the Hadoop ecosystem. I am trying to enter in AWS. So I practice on a few services like Athena, Glue, EMR, Lambda, S3. \nWhat should I do to Ace my interview as I don't have any production level AWS experience. They have asked for 2 YOE for this profile. Can you share some articles, blogs, and videos where I can prepare?\nThis is my first interview for AWS data engineer specific profile..", "author_fullname": "t2_rj4f3193", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Data Engineer interview preparation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akdkrv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707236362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have an interview next week for an AWS data engineer. I need resources for preparations.&lt;/p&gt;\n\n&lt;p&gt;I have been working as a data engineer for the past 3 years. But I have experience in the Hadoop ecosystem. I am trying to enter in AWS. So I practice on a few services like Athena, Glue, EMR, Lambda, S3. \nWhat should I do to Ace my interview as I don&amp;#39;t have any production level AWS experience. They have asked for 2 YOE for this profile. Can you share some articles, blogs, and videos where I can prepare?\nThis is my first interview for AWS data engineer specific profile..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akdkrv", "is_robot_indexable": true, "report_reasons": null, "author": "believer_369", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akdkrv/aws_data_engineer_interview_preparation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akdkrv/aws_data_engineer_interview_preparation/", "subreddit_subscribers": 158923, "created_utc": 1707236362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm heavily immersed in refining my data processing workflows with Pandas, Spark, and Polars, aiming to achieve more clean and maintainable code. I have 6 years of experience working with these tools and I always faced challenges with maintainability of dataframe transformation logic.\n\nOne core challenge I frequently encounter revolves around the inherent statefulness of dataframes. For example, the creation of intermediary columns that facilitate the generation of additional columns introduces a complex web of dependencies. This complexity is further compounded when undertaking tasks like renaming columns, which essentially alters the dataframe's state and demands meticulous mental mapping to track these transformations throughout the code.\n\nThis state management issue often manifests as a significant mental burden, requiring constant vigilance to ensure that the evolution of the dataframe's structure doesn't introduce errors or reduce code readability. It's akin to juggling multiple balls where each ball represents a column whose state (or existence) depends on the successful catch and throw of another.\n\nTo navigate this complexity, I've adopted a common engineering approach centered around the single responsibility principle of functions. Within this paradigm, each function is designed as a self-contained unit where all necessary dataframe manipulations occur in isolation. The function's sole responsibility is to output the final dataframe in its desired state, ensuring that any intermediary artifacts are transient and efficiently managed or disposed of by the memory management system. This strategy not only clarifies the function's purpose and output but also significantly reduces the cognitive load by encapsulating the state transitions within a defined scope.\n\nDespite these strategies, I still grapple with the brittle nature of hardcoding column names and  mentally keeping track of the dataframe transformation logic, because dataframe transformation usually is procedural by nature.\n\nAre there patterns, tools, or methodologies you've found effective in addressing these challenges? How do you manage the intricate dance of dataframe state management while keeping your code clean and maintainable?", "author_fullname": "t2_tux1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on cleaner Dataframe Management in Pandas/Spark/Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akkdos", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707252959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m heavily immersed in refining my data processing workflows with Pandas, Spark, and Polars, aiming to achieve more clean and maintainable code. I have 6 years of experience working with these tools and I always faced challenges with maintainability of dataframe transformation logic.&lt;/p&gt;\n\n&lt;p&gt;One core challenge I frequently encounter revolves around the inherent statefulness of dataframes. For example, the creation of intermediary columns that facilitate the generation of additional columns introduces a complex web of dependencies. This complexity is further compounded when undertaking tasks like renaming columns, which essentially alters the dataframe&amp;#39;s state and demands meticulous mental mapping to track these transformations throughout the code.&lt;/p&gt;\n\n&lt;p&gt;This state management issue often manifests as a significant mental burden, requiring constant vigilance to ensure that the evolution of the dataframe&amp;#39;s structure doesn&amp;#39;t introduce errors or reduce code readability. It&amp;#39;s akin to juggling multiple balls where each ball represents a column whose state (or existence) depends on the successful catch and throw of another.&lt;/p&gt;\n\n&lt;p&gt;To navigate this complexity, I&amp;#39;ve adopted a common engineering approach centered around the single responsibility principle of functions. Within this paradigm, each function is designed as a self-contained unit where all necessary dataframe manipulations occur in isolation. The function&amp;#39;s sole responsibility is to output the final dataframe in its desired state, ensuring that any intermediary artifacts are transient and efficiently managed or disposed of by the memory management system. This strategy not only clarifies the function&amp;#39;s purpose and output but also significantly reduces the cognitive load by encapsulating the state transitions within a defined scope.&lt;/p&gt;\n\n&lt;p&gt;Despite these strategies, I still grapple with the brittle nature of hardcoding column names and  mentally keeping track of the dataframe transformation logic, because dataframe transformation usually is procedural by nature.&lt;/p&gt;\n\n&lt;p&gt;Are there patterns, tools, or methodologies you&amp;#39;ve found effective in addressing these challenges? How do you manage the intricate dance of dataframe state management while keeping your code clean and maintainable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akkdos", "is_robot_indexable": true, "report_reasons": null, "author": "caksters", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akkdos/seeking_advice_on_cleaner_dataframe_management_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akkdos/seeking_advice_on_cleaner_dataframe_management_in/", "subreddit_subscribers": 158923, "created_utc": 1707252959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_owff7qyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How AI can automate web scraping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1akisp6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ArNC3AO-EKgfsn5CfycMZpX7MzffEAck_hM-FHmlTto.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707249166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kadoa.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kadoa.com/blog/automate-web-scraping-with-ai", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?auto=webp&amp;s=48667df12031fa856c2e1c1ea60a3a629f5f7e16", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d2a328f0993335f10b80f1a01566d27b2d53035", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aacd001ce5a904cf802f08b26e681e18d4b3e79b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a479fb2edb9eb148ae2ce53e2316900544a9971b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4062749fc58cfe89d64673da0f3497d76e0d531f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cad45aebbf8cfeaf8adc7c0fd6ac78b283b02635", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/kcW6UlrDT1oeMXtVUmRBArEDld1mx-4Cw78bCQ_HbKU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4c62a17c4554d10c6ffda5dc9b790422249ffca", "width": 1080, "height": 567}], "variants": {}, "id": "xZUc-0iCxdtoR87mJ21dFr-3xRMDO2OqD8nWRoFGIXc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1akisp6", "is_robot_indexable": true, "report_reasons": null, "author": "madredditscientist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akisp6/how_ai_can_automate_web_scraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kadoa.com/blog/automate-web-scraping-with-ai", "subreddit_subscribers": 158923, "created_utc": 1707249166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cross posting this question, maybe some data engineers can help me with my dilemma. I'm limited to a vanilla anaconda python environment, keep that in mind.\n\nI have a huge table in MS Access (4.5 million rows, 27 columns). I have a connection variable with pyodbc, and a simple select query. I have generator = pd.read_sql(query, conn, chunksize=10000), which stores an iterable object where each iteration is a dataframe (read pandas docs). \n\nI want to loop over this single object, do some pandas transformations, append each df to a list in parallel, then pd.concat the list after. I've tried to write a parallelization script using either ProcessPoolExecutor or MultiThreadExecutor, but I always run into little issues and chatgpt just keeps going in circles and doesn't help.\n\nAny suggestions how to loop over a single object in parallel, where each iteration contains a dataframe?", "author_fullname": "t2_lfyfd0fn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to iterate over a single object in parallel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akltyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707256373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cross posting this question, maybe some data engineers can help me with my dilemma. I&amp;#39;m limited to a vanilla anaconda python environment, keep that in mind.&lt;/p&gt;\n\n&lt;p&gt;I have a huge table in MS Access (4.5 million rows, 27 columns). I have a connection variable with pyodbc, and a simple select query. I have generator = pd.read_sql(query, conn, chunksize=10000), which stores an iterable object where each iteration is a dataframe (read pandas docs). &lt;/p&gt;\n\n&lt;p&gt;I want to loop over this single object, do some pandas transformations, append each df to a list in parallel, then pd.concat the list after. I&amp;#39;ve tried to write a parallelization script using either ProcessPoolExecutor or MultiThreadExecutor, but I always run into little issues and chatgpt just keeps going in circles and doesn&amp;#39;t help.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions how to loop over a single object in parallel, where each iteration contains a dataframe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akltyk", "is_robot_indexable": true, "report_reasons": null, "author": "velimino", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akltyk/how_to_iterate_over_a_single_object_in_parallel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akltyk/how_to_iterate_over_a_single_object_in_parallel/", "subreddit_subscribers": 158923, "created_utc": 1707256373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a Lead Tableau Dev / Product Analyst. I am interested in data engineering. I recently completed a bootcamp for data engineering. I'm happy with my compensation and job but always interested in learning. In addition, I survived large layoffs recently, and I know that gaining new skills could be helpful in the future.\n\nOur company is in the process of moving from on-prem to GCP. They are offering free trainings and covering the cost of GCP certs (only if successfully passed). Since I have a general interest in data engineering, I thought it might make sense to go through the trainings and take the certs in this order:\n\nAssociate Cloud Engineer --&gt; Professional Cloud Architect --&gt; Professional Data Engineer\n\nWork is a bit slow right now, so I have been prepping for the ACE. I'm 6 hours through a 20-hour course, and I honestly find it all pretty boring. So my question: Is this a complete waste of time for me since I'm not necessarily looking to become a Cloud Architect or something? Or is there value in continuing the plan stated above and this path could help me transition into DE in my current company in the case that I do make that decision?", "author_fullname": "t2_1j5yar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I wasting my time with GCP Certification Prep?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akgc57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707243208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a Lead Tableau Dev / Product Analyst. I am interested in data engineering. I recently completed a bootcamp for data engineering. I&amp;#39;m happy with my compensation and job but always interested in learning. In addition, I survived large layoffs recently, and I know that gaining new skills could be helpful in the future.&lt;/p&gt;\n\n&lt;p&gt;Our company is in the process of moving from on-prem to GCP. They are offering free trainings and covering the cost of GCP certs (only if successfully passed). Since I have a general interest in data engineering, I thought it might make sense to go through the trainings and take the certs in this order:&lt;/p&gt;\n\n&lt;p&gt;Associate Cloud Engineer --&amp;gt; Professional Cloud Architect --&amp;gt; Professional Data Engineer&lt;/p&gt;\n\n&lt;p&gt;Work is a bit slow right now, so I have been prepping for the ACE. I&amp;#39;m 6 hours through a 20-hour course, and I honestly find it all pretty boring. So my question: Is this a complete waste of time for me since I&amp;#39;m not necessarily looking to become a Cloud Architect or something? Or is there value in continuing the plan stated above and this path could help me transition into DE in my current company in the case that I do make that decision?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1akgc57", "is_robot_indexable": true, "report_reasons": null, "author": "cryptobro21", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akgc57/am_i_wasting_my_time_with_gcp_certification_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akgc57/am_i_wasting_my_time_with_gcp_certification_prep/", "subreddit_subscribers": 158923, "created_utc": 1707243208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently on 38k in London straight out of uni. I\u2019m coming onto have one year of experience and have found other jobs in data science for about 45-50k. The roles are Python heavy which is ok for me because I did data science as a masters. My current role is purely sql and azure.\n\nCan anyone with experience let me know if the job prospects for data science are greater than data engineering?\n\nBoth would be in the same industry (insurnace)", "author_fullname": "t2_9wz5l9vg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I leave my data engineering role for data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akz3j6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707297960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently on 38k in London straight out of uni. I\u2019m coming onto have one year of experience and have found other jobs in data science for about 45-50k. The roles are Python heavy which is ok for me because I did data science as a masters. My current role is purely sql and azure.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with experience let me know if the job prospects for data science are greater than data engineering?&lt;/p&gt;\n\n&lt;p&gt;Both would be in the same industry (insurnace)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1akz3j6", "is_robot_indexable": true, "report_reasons": null, "author": "Due_Statistician2604", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akz3j6/should_i_leave_my_data_engineering_role_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akz3j6/should_i_leave_my_data_engineering_role_for_data/", "subreddit_subscribers": 158923, "created_utc": 1707297960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a data analyst and have an opportunity to make a switch to a DE role. It\u2019s a mid level role, and would be an internal transfer. I am very good with SQL, have a bit more than general data modeling experience, have set up all the data infrastructure for my team (DAGs / tasks / data models in our BI tools), but my Python is very basic. \n\nLooking for some guidance on the Python bit, as I\u2019ve been trying to study up in my freetime a bit more. I know the interview will go over general syntax, data manipulation, working with SQL DBs, and a few other things. I\u2019m planning to focus catching up on pandas mainly, but would love some guidance from yall on if there are specifics I should focus on? Thanks in advance!", "author_fullname": "t2_bbzr0sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have an interview and need some guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akxu61", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707292361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a data analyst and have an opportunity to make a switch to a DE role. It\u2019s a mid level role, and would be an internal transfer. I am very good with SQL, have a bit more than general data modeling experience, have set up all the data infrastructure for my team (DAGs / tasks / data models in our BI tools), but my Python is very basic. &lt;/p&gt;\n\n&lt;p&gt;Looking for some guidance on the Python bit, as I\u2019ve been trying to study up in my freetime a bit more. I know the interview will go over general syntax, data manipulation, working with SQL DBs, and a few other things. I\u2019m planning to focus catching up on pandas mainly, but would love some guidance from yall on if there are specifics I should focus on? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akxu61", "is_robot_indexable": true, "report_reasons": null, "author": "NoDistractionz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akxu61/have_an_interview_and_need_some_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akxu61/have_an_interview_and_need_some_guidance/", "subreddit_subscribers": 158923, "created_utc": 1707292361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone successfully tried the 250 MB Lambda Layer limit workaround by packaging their dependencies in EFS? Does it affect Lambda init time? What about concurrency - how many concurrent lambda executions can read from the shared EFS? Can serverless be used to deploy such lambda functions that use EFS for the importing the modules? Please don't suggest using containerized lambda functions .", "author_fullname": "t2_v8qhw4wd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using EFS for bigger python packages in AWS Lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1al0znu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707306667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707305959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully tried the 250 MB Lambda Layer limit workaround by packaging their dependencies in EFS? Does it affect Lambda init time? What about concurrency - how many concurrent lambda executions can read from the shared EFS? Can serverless be used to deploy such lambda functions that use EFS for the importing the modules? Please don&amp;#39;t suggest using containerized lambda functions .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al0znu", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Love_648", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al0znu/using_efs_for_bigger_python_packages_in_aws_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al0znu/using_efs_for_bigger_python_packages_in_aws_lambda/", "subreddit_subscribers": 158923, "created_utc": 1707305959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not so long ago there was a post here comparing Fundamentals of Data Engineering to the Data Warehouse Toolkit where people seemed to have had a much better experience with the latter, and found the other book inflated and without much content.\n\nAnother book I've seen thrown around is Designing data intensive applications, what has been your experience with it. Is there another suggestion that might be better?", "author_fullname": "t2_ra02aic6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akxqwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707291979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not so long ago there was a post here comparing Fundamentals of Data Engineering to the Data Warehouse Toolkit where people seemed to have had a much better experience with the latter, and found the other book inflated and without much content.&lt;/p&gt;\n\n&lt;p&gt;Another book I&amp;#39;ve seen thrown around is Designing data intensive applications, what has been your experience with it. Is there another suggestion that might be better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akxqwr", "is_robot_indexable": true, "report_reasons": null, "author": "soposih_jaevel", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1akxqwr/book_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akxqwr/book_advice/", "subreddit_subscribers": 158923, "created_utc": 1707291979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synthetic Data In A Nutshell \u2014 Why Founding Went 10x In This Market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1akxlj3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EMhCWMik8xRV-bN33kJkT8PqOvtPYs2BNiFcGPZ-ni8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707291337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "svenbalnojan.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://svenbalnojan.medium.com/synthetic-data-in-a-nutshell-why-founding-went-10x-in-this-market-50ffe9ec6223", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?auto=webp&amp;s=22f2fa9c9fc04d6c372ca85d1330ee3942e8f4db", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d897749c5e339280295c2ad4b36b2736ec72a0a7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b5b2b4e2c73eb70aed885c2bc4de4477e53531c", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad634926bde0c62d727f923f4eaa77bdf3470f34", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=710cfb581083f2865d5a8bdc76f19fbe79dc8dd4", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/swzQgmOhOPpNQghEwm_Vg94aQDH6ee_PvKO7oxKm7GI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d4660f94adc8b2f13fab8f074c9f2de35f6ccac1", "width": 960, "height": 960}], "variants": {}, "id": "UiUR_PyZNxm20jf9DhtXb-e0BG93LPHCA8tSn6oGs1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1akxlj3", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akxlj3/synthetic_data_in_a_nutshell_why_founding_went/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://svenbalnojan.medium.com/synthetic-data-in-a-nutshell-why-founding-went-10x-in-this-market-50ffe9ec6223", "subreddit_subscribers": 158923, "created_utc": 1707291337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After spending years learning full stack web dev, the hiring market for web dev crashes! Anyway, I figure some of my skills picked up should transition to Data Engineering okay. I found I like working with data, or at least inputting it and searching it for common mistakes, and other stuff, like using spreadsheets to calculate manufacturing in EvE online... So I'm thinking I might be a good fit for Data Engineering.  \n\n\nBefore I dive in like I did with Web Dev what is the hiring market like right now? How is the hiring market looking for the future? How much additional skills might one who learned MERN TS stack need to be able to work as a Data Engineer?  \n\n\nWould it be realistic to get a job working remotely or even going into freelancing?  \n\n\nAny further advice you'd like to share? Thanks. ", "author_fullname": "t2_ka5w0ftpm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking new opportunity as Web Dev becomes difficult to get into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akvavk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707282798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After spending years learning full stack web dev, the hiring market for web dev crashes! Anyway, I figure some of my skills picked up should transition to Data Engineering okay. I found I like working with data, or at least inputting it and searching it for common mistakes, and other stuff, like using spreadsheets to calculate manufacturing in EvE online... So I&amp;#39;m thinking I might be a good fit for Data Engineering.  &lt;/p&gt;\n\n&lt;p&gt;Before I dive in like I did with Web Dev what is the hiring market like right now? How is the hiring market looking for the future? How much additional skills might one who learned MERN TS stack need to be able to work as a Data Engineer?  &lt;/p&gt;\n\n&lt;p&gt;Would it be realistic to get a job working remotely or even going into freelancing?  &lt;/p&gt;\n\n&lt;p&gt;Any further advice you&amp;#39;d like to share? Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1akvavk", "is_robot_indexable": true, "report_reasons": null, "author": "IllBeGoodIPromiseV3", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akvavk/seeking_new_opportunity_as_web_dev_becomes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akvavk/seeking_new_opportunity_as_web_dev_becomes/", "subreddit_subscribers": 158923, "created_utc": 1707282798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve made it to the final round of a job interview for a data engineer job that I\u2019m very interested in. I\u2019ve already done the hard steps and technical assessments. This final round is a \u201cproduct interview\u201d with a project manager. \n\nAny idea what I can expect? Agile questions and how I\u2019ve worked in the past? Actual project management questions like T-shirt sizing? Questions about the industry and company\u2019s actual product? \n\nWhat have you all seen in these types of interviews?", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akmwb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707258976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve made it to the final round of a job interview for a data engineer job that I\u2019m very interested in. I\u2019ve already done the hard steps and technical assessments. This final round is a \u201cproduct interview\u201d with a project manager. &lt;/p&gt;\n\n&lt;p&gt;Any idea what I can expect? Agile questions and how I\u2019ve worked in the past? Actual project management questions like T-shirt sizing? Questions about the industry and company\u2019s actual product? &lt;/p&gt;\n\n&lt;p&gt;What have you all seen in these types of interviews?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akmwb7", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akmwb7/product_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akmwb7/product_interview/", "subreddit_subscribers": 158923, "created_utc": 1707258976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im looking for a way to orchestrate data and requests dynamically based on some sample ids (that can be in the hundreds or thousands) without any invasive API workarounds.\n\n&amp;#x200B;\n\nI'm looking for something such as a load balancer, which can take the \\[sample size\\] as input and spread it through multiple subflows with different schedules dynamically. The idea would be to spread the calls across the day and to make the result of each subflow independent, such as if one fails, I do not have to re run the entire samples, just the subsample which failed. How can I do this?\n\n&amp;#x200B;\n\nI'm using prefect.", "author_fullname": "t2_a9360hkx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data orchestration and API limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1al0usn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707305431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking for a way to orchestrate data and requests dynamically based on some sample ids (that can be in the hundreds or thousands) without any invasive API workarounds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something such as a load balancer, which can take the [sample size] as input and spread it through multiple subflows with different schedules dynamically. The idea would be to spread the calls across the day and to make the result of each subflow independent, such as if one fails, I do not have to re run the entire samples, just the subsample which failed. How can I do this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using prefect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1al0usn", "is_robot_indexable": true, "report_reasons": null, "author": "Different_Fee6785", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1al0usn/data_orchestration_and_api_limits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1al0usn/data_orchestration_and_api_limits/", "subreddit_subscribers": 158923, "created_utc": 1707305431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like Hadoop has mapreduce \nDatabricks and Aws glue use Spark\nWhat does snowflake use for computing?", "author_fullname": "t2_pp1jtay3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is compute engine in SnowFlake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akzzx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707301930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like Hadoop has mapreduce \nDatabricks and Aws glue use Spark\nWhat does snowflake use for computing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1akzzx2", "is_robot_indexable": true, "report_reasons": null, "author": "mysticsoul1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akzzx2/what_is_compute_engine_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akzzx2/what_is_compute_engine_in_snowflake/", "subreddit_subscribers": 158923, "created_utc": 1707301930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!\n\nHas anyone ever set up a database for a recommendation system? \n\nI would love to know what database you used and what data model to set it up. \n\nI is basically collecting feedback data for ML outputs to set up a pipeline to refine the models over time and run A/B tests. \n\nThe user information is stored pretty standard in a table with id, name, some metadata fields (e.g. tags), a description, and some more information. The recommendations are texts, so they are stored with id, text, title, and some metadata fields (e.g. topic, ...).   \nWhat can happen, that too many recommendations pass through the different filters, so we rerank and filter them.   \n\n\nI am flexible to set it up relational or NoSQL. The feedback is in form of clicks or likes and is related to the user and the item. So I probably need an interaction type, also the value for the interaction (e.g. in the form of up or down vote or a 1 value in the case of a click).  \nIf anyone has seen this in the wild:   \n\\- What database did you use?   \n\\- How did you set up the data models of recommendations and the feedback?   \n\\- What is the potential tradeoff between NoSQL and relational?   \n\\- Did you also store non-interactions (e.g., did not click), or should I leave this implicit?   \n\\- Should I even store this in one, or should I store it separately? \n\nThanks!", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database / Data Model for Recommendation Systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akzasi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707298902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;Has anyone ever set up a database for a recommendation system? &lt;/p&gt;\n\n&lt;p&gt;I would love to know what database you used and what data model to set it up. &lt;/p&gt;\n\n&lt;p&gt;I is basically collecting feedback data for ML outputs to set up a pipeline to refine the models over time and run A/B tests. &lt;/p&gt;\n\n&lt;p&gt;The user information is stored pretty standard in a table with id, name, some metadata fields (e.g. tags), a description, and some more information. The recommendations are texts, so they are stored with id, text, title, and some metadata fields (e.g. topic, ...).&lt;br/&gt;\nWhat can happen, that too many recommendations pass through the different filters, so we rerank and filter them.   &lt;/p&gt;\n\n&lt;p&gt;I am flexible to set it up relational or NoSQL. The feedback is in form of clicks or likes and is related to the user and the item. So I probably need an interaction type, also the value for the interaction (e.g. in the form of up or down vote or a 1 value in the case of a click).&lt;br/&gt;\nIf anyone has seen this in the wild:&lt;br/&gt;\n- What database did you use?&lt;br/&gt;\n- How did you set up the data models of recommendations and the feedback?&lt;br/&gt;\n- What is the potential tradeoff between NoSQL and relational?&lt;br/&gt;\n- Did you also store non-interactions (e.g., did not click), or should I leave this implicit?&lt;br/&gt;\n- Should I even store this in one, or should I store it separately? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1akzasi", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akzasi/database_data_model_for_recommendation_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akzasi/database_data_model_for_recommendation_systems/", "subreddit_subscribers": 158923, "created_utc": 1707298902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! I am currently looking for a platform for time-series geospatial data for my new project. Ive been initialy thinking about just using Spark for it (its iot so the volume can be decent) but found the geomesa and it looks promising on first sight. Does anyone have any recomendations regarding the tooling for such use case?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Timeseries geospatial db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aketcy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707239460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am currently looking for a platform for time-series geospatial data for my new project. Ive been initialy thinking about just using Spark for it (its iot so the volume can be decent) but found the geomesa and it looks promising on first sight. Does anyone have any recomendations regarding the tooling for such use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aketcy", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aketcy/timeseries_geospatial_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aketcy/timeseries_geospatial_db/", "subreddit_subscribers": 158923, "created_utc": 1707239460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My friend has cleared interviews for data engineering roles. But he has been in data qa roles during these years but has acquired the knowledge and has cleared the interviews. \n\nThe question is, will hiring team reject his candidature because his designation is QA analyst?\n\nNEED INPUTS!!", "author_fullname": "t2_jz6c71oa2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition Inputs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akai6z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707228291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My friend has cleared interviews for data engineering roles. But he has been in data qa roles during these years but has acquired the knowledge and has cleared the interviews. &lt;/p&gt;\n\n&lt;p&gt;The question is, will hiring team reject his candidature because his designation is QA analyst?&lt;/p&gt;\n\n&lt;p&gt;NEED INPUTS!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1akai6z", "is_robot_indexable": true, "report_reasons": null, "author": "unsung_zero1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1akai6z/transition_inputs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1akai6z/transition_inputs/", "subreddit_subscribers": 158923, "created_utc": 1707228291.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy,\n\nDoes anyone have experience getting data out of a vendor hosted instance of Archer GRC? At creation, our firm chose not to prioritize reporting as part of the set up. So we're not sending any files from Archer to our DW/LH and we didn't pay for any API support (to my knowledge), so the API doesn't stay synced to the underlying data structure.\n\nThe only way to get data out that most people are using is to use the global reports (canned system) or to create personal views. Manually extract, transform etc.\n\nWhile attempting to figure my way through this miasma, I started probing the network via developer tools in Chrome, and noticed all of this code pointing to various APIs. So I grabbed one of the URLs and threw it into a browser and lo and behold it returned the table structure of the application. I also found some APIs for \"master reports\", which I believe are these global reports.\n\nI'm not super savvy in the API/JSON space. But, if the API is returning this information, would it be possible to construct a JSON query to pull the data back from a global report? Within the application I have 'user admin' rights, which means I can create new objects through the front end, archive things, generate canned or personal reports, etc. Nothing in the back end.\n\nI've also tried an OData connection via PowerBI and sometimes it works and sometimes it doesn't. It certainly doesn't produce any viable data because the reporting API isn't synced to the data base. Or, at least that's the story I've been told.\n\n[https://community.fabric.microsoft.com/t5/Desktop/Odata-Feed-with-Archer-App/m-p/512178](https://community.fabric.microsoft.com/t5/Desktop/Odata-Feed-with-Archer-App/m-p/512178)", "author_fullname": "t2_nm35te9ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data out of vendor hosted Archer GRC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aka1i9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707227921.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707226982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy,&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience getting data out of a vendor hosted instance of Archer GRC? At creation, our firm chose not to prioritize reporting as part of the set up. So we&amp;#39;re not sending any files from Archer to our DW/LH and we didn&amp;#39;t pay for any API support (to my knowledge), so the API doesn&amp;#39;t stay synced to the underlying data structure.&lt;/p&gt;\n\n&lt;p&gt;The only way to get data out that most people are using is to use the global reports (canned system) or to create personal views. Manually extract, transform etc.&lt;/p&gt;\n\n&lt;p&gt;While attempting to figure my way through this miasma, I started probing the network via developer tools in Chrome, and noticed all of this code pointing to various APIs. So I grabbed one of the URLs and threw it into a browser and lo and behold it returned the table structure of the application. I also found some APIs for &amp;quot;master reports&amp;quot;, which I believe are these global reports.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not super savvy in the API/JSON space. But, if the API is returning this information, would it be possible to construct a JSON query to pull the data back from a global report? Within the application I have &amp;#39;user admin&amp;#39; rights, which means I can create new objects through the front end, archive things, generate canned or personal reports, etc. Nothing in the back end.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried an OData connection via PowerBI and sometimes it works and sometimes it doesn&amp;#39;t. It certainly doesn&amp;#39;t produce any viable data because the reporting API isn&amp;#39;t synced to the data base. Or, at least that&amp;#39;s the story I&amp;#39;ve been told.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://community.fabric.microsoft.com/t5/Desktop/Odata-Feed-with-Archer-App/m-p/512178\"&gt;https://community.fabric.microsoft.com/t5/Desktop/Odata-Feed-with-Archer-App/m-p/512178&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YRYMhDtBw5PdRrdehm0ewOTWySHUGZ3X7Pd0uafNV5M.jpg?auto=webp&amp;s=5f5585a8b42b0699bdaa0acecd36bc74dfae6dad", "width": 374, "height": 27}, "resolutions": [{"url": "https://external-preview.redd.it/YRYMhDtBw5PdRrdehm0ewOTWySHUGZ3X7Pd0uafNV5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76eb553029bc56d6f6f61822651787642dcf7db4", "width": 108, "height": 7}, {"url": "https://external-preview.redd.it/YRYMhDtBw5PdRrdehm0ewOTWySHUGZ3X7Pd0uafNV5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9cc458a67b6a40aee53e3017491c4bdc03134de", "width": 216, "height": 15}, {"url": "https://external-preview.redd.it/YRYMhDtBw5PdRrdehm0ewOTWySHUGZ3X7Pd0uafNV5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee86a31ced269619d2170c83e694064470d08bb3", "width": 320, "height": 23}], "variants": {}, "id": "RIHXq0j_cVBoaiQCmmsHPc-d7ysfvfiqs2NUsfC0l9Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aka1i9", "is_robot_indexable": true, "report_reasons": null, "author": "joyfulcartographer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aka1i9/getting_data_out_of_vendor_hosted_archer_grc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aka1i9/getting_data_out_of_vendor_hosted_archer_grc/", "subreddit_subscribers": 158923, "created_utc": 1707226982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Essential \"Personality Traits\" You Need in Your Data Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1ak90hf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AazrAh59ADVGpWW6U_R5ZsIv4s91OyRGRF0vcGlUq0Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707223844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/data-platform-characteristics", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?auto=webp&amp;s=835cbb3577c864e560cde16075e05573c855bf9c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=114eb65ce16791d26a2a5244d6e68def5dfb5731", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a178a3b43fa0c9885b7d61702f8eb1aa3feb4048", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3cc807eddf6487be52e8e6d68420b50b2ebda6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6eb2d7240ed0c724dc8b54dc084d02cbefcb1da7", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f8068024ccb2aebd08012f9262918e19442cc923", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9fxruPvM4Aq1jFm9VQGdGXbrKpNjholZU3JaIHe8250.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b70174eb9ef516c067053ae025f40c8ff188a074", "width": 1080, "height": 540}], "variants": {}, "id": "qDsmBIcWYaV7ZcDEWn0fARfcQ2CFJLo5BOo__P0T18g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ak90hf", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ak90hf/the_essential_personality_traits_you_need_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/data-platform-characteristics", "subreddit_subscribers": 158923, "created_utc": 1707223844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data scientist. I switched to a new company recently. In my previous company, we had a data proc cluster where we used to house all our data. The cluster also had web interfaces where I could start a Pyspark Notebook(a jupyter notebook with Pyspark) and access the data/build models etc. \nMy new team also has a data proc cluster where they keep their data but they use Pyspark Shell to do any kind of data analysis and submit Pyspark jobs to train models.\nI have good enough coding skills but have no experience of distributed computing environment. I have never had to worry about python packages - a simple pip install in the Pyspark notebook would do the trick in my previous company. I don't really understand the meaning of executors, instances, maxresultsize etc. When I was asked about Spark in the interview I was able to answer all the questions related to concepts/apis of Pyspark. But, I am finding this shell/Spark submit method too tedious. I know it is my brain resisting to learn something new. I want to ask, is this an industry standard? I find Notebooks much better to do ML. Should I push my new team to  provision for notebooks? Can I do that myself? If yes, how? What are some good resources to learn more about distributed computing environments? How are they setup? How is computation performed, etc.\nIt would be great if I could get answers to any of these questions. Any general career advice is also appreciated.\nThanks", "author_fullname": "t2_nbam7tl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark Notebooks v/s Pyspark shells", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ak896c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707221261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data scientist. I switched to a new company recently. In my previous company, we had a data proc cluster where we used to house all our data. The cluster also had web interfaces where I could start a Pyspark Notebook(a jupyter notebook with Pyspark) and access the data/build models etc. \nMy new team also has a data proc cluster where they keep their data but they use Pyspark Shell to do any kind of data analysis and submit Pyspark jobs to train models.\nI have good enough coding skills but have no experience of distributed computing environment. I have never had to worry about python packages - a simple pip install in the Pyspark notebook would do the trick in my previous company. I don&amp;#39;t really understand the meaning of executors, instances, maxresultsize etc. When I was asked about Spark in the interview I was able to answer all the questions related to concepts/apis of Pyspark. But, I am finding this shell/Spark submit method too tedious. I know it is my brain resisting to learn something new. I want to ask, is this an industry standard? I find Notebooks much better to do ML. Should I push my new team to  provision for notebooks? Can I do that myself? If yes, how? What are some good resources to learn more about distributed computing environments? How are they setup? How is computation performed, etc.\nIt would be great if I could get answers to any of these questions. Any general career advice is also appreciated.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ak896c", "is_robot_indexable": true, "report_reasons": null, "author": "Due-Establishment882", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ak896c/pyspark_notebooks_vs_pyspark_shells/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ak896c/pyspark_notebooks_vs_pyspark_shells/", "subreddit_subscribers": 158923, "created_utc": 1707221261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nI am a data engineer with 2 YOE. I am looking to make a job switch. I am looking for a good resume template which will be specific to data engineering. Also advise me on making a job switch after 2 YOE in data engineering.", "author_fullname": "t2_5e0plhyyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume template for data engineer with 2 YOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aku4vj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707279096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nI am a data engineer with 2 YOE. I am looking to make a job switch. I am looking for a good resume template which will be specific to data engineering. Also advise me on making a job switch after 2 YOE in data engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aku4vj", "is_robot_indexable": true, "report_reasons": null, "author": "General-Ride247", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aku4vj/resume_template_for_data_engineer_with_2_yoe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aku4vj/resume_template_for_data_engineer_with_2_yoe/", "subreddit_subscribers": 158923, "created_utc": 1707279096.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}