{"kind": "Listing", "data": {"after": "t3_1akc1nd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9dr83fgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "My Rack. It's a mixture of servers running mostly TrueNAS. One running EXSi 7 and another running Windows 10 Pro. The main server is a 36 drive Supermicro chassis. It has a X11DPH-T with two Xeon Gold 6138 CPUs, 512GB RAM, 8 intel 800GB SSD, 2 Optane 900P drives, SAS3 HBAs and HGST 8TB Ent drives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vvmq6ysmu0hc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de6b3620993cd7f0a5bce97017cda982e7bd28b3"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=858d9ae71d07e96c02c8b533d3ddf3265e43af1f"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bee38f6d06580515d96614414c2b446df2f4385"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=591436e63628e0565dcf7cd974ea7bc503ba22eb"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ddf2649bd1fd2ac7505a9ba4c20b4c70cbe9984"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d59181d08e02d1c7f72f8741c8340b3a5c150da1"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/vvmq6ysmu0hc1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=c5ea409659904817da502e57d40d35874053a609"}, "id": "vvmq6ysmu0hc1"}, "up2z70tmu0hc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e126a485c58ad34eb421419e7b68bbf70c10d47c"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82b8bd139c217ffd079fce509a5126de48bcb529"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=462db9335f04a59d01d0a1164ab2495fc6463598"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=572bc392dceee4b8f14dccfa3d3cb617da274ece"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9da5fcb271bf90e7c345aee455de2011c80e0181"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24663a1882e3e51159a57fa2ffe6e2e1c3ca731d"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/up2z70tmu0hc1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=7536b91da0a31432afcb9970787c0ae0132fc527"}, "id": "up2z70tmu0hc1"}}, "name": "t3_1akk5ky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 365, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "My Rack", "media_id": "vvmq6ysmu0hc1", "id": 401215511}, {"media_id": "up2z70tmu0hc1", "id": 401215512}]}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 365, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aiv7F3ykJy1_a-6Drrp0VL43X0fYAfsHfvFYz0BIgAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707252410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1akk5ky", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akk5ky", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Koothrappalli", "discussion_type": null, "num_comments": 125, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akk5ky/my_rack_its_a_mixture_of_servers_running_mostly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1akk5ky", "subreddit_subscribers": 730850, "created_utc": 1707252410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Short version with no context for the content of the videos: I have 4.5 TB of .mkv files on my hard drive, and a bunch of people who want to download some of them. I have a TrueNAS Scale server that runs 24/7 but only has 22 Mbp/s upload. I don't really know what the best way to share them to people are. I'm thinking of putting up a torrent, but I don't know where. Another site known for hosting an archive of this kind of content exists, but I've reached out to the owners and they're pretty much certain that they're going to get a DMCA and have to remove them. Maybe the Internet Archive, but I suspect they might get a DMCA too. Any guidance is appreciated.\n\n\n\nThis is the yt-dlp command  I used. Cunningham's law me and tell me how awful it is so that I know what I should use next time:\n\n    yt-dlp \\\n            -a yt-dlp-list.txt \\\n            -o \"%(uploader)s (%(uploader_id)s)/%(upload_date)s - %(title)s - (%(duration)ss) [%(resolution)s] [%(id)s].%(ext)s\" \\\n            --download-archive yt-dlp-archive.txt \\\n            --cookies-from-browser firefox \\\n            --ignore-errors \\\n            --merge-output-format mkv \\\n            --sub-langs all \\\n            --write-subs \\\n            --embed-subs \\\n            --add-metadata \\\n            --write-description \\\n            --write-thumbnail \\\n            --write-comments \\\n            --embed-thumbnail \\\n            --embed-info-json \\\n            --write-info-json \\\n            --windows-filenames \\\n\nSelen Tatsuki was a Vtuber who was employed by vtuber company Nijisanji's English branch. When she was terminated, she had the highest subscriber count of  any of their female members in the English branch (and 5th highest overall). She was extremely popular and beloved by her community. She was best known for her FPS gaming skills, being top 500 in Apex Legends at one point, her contagious laughter.  If you want to get a feel for what she was like, this is a good video: https://www.youtube.com/watch?v=elnFh8VpeKQ\n\n\nI don't have time to go into all the details, unfortunately, Nijisanji has shown itself to be either cartoonishly evil or cartoonishly incompetent, and have terminated Selen's contract. Nijisanji had Selen terminated (fired) for reasons I (and many others) consider to be completely unjust, especially considering the way they went about doing it. As Nijisanji owns the rights to the character of Nijisanji, and that changing a Vtuber's performer is considered an unforgivable sin in this industry, the character is gone forever now, especially since all the videos on her channel were deleted too. I could go over a laundry list of of awful things that Nijisanji has done in the past year, but all YOU guys need to know is that they deleted all of Selen's videos from her channel with ZERO warning. In this subreddit, I think that qualifies as an unforgivable sin. Thankfully, I had the foresight to back everything up beforehand (I had a feeling that this was going to happen).\n\nFor comparison on how this kind of thing should be handled, look up how Yozora Mel's termination was handled.\n\n\nThankfully, Selen's story seems to have a happy ending. She's moved back to her old account named Dokibird, and is planning to return to streaming tomorrow. Normally, talking about this kind of thing is a HUGE sin in the vtubing community, but when she said \"Please let everyone know that this is where I am now, I hope you all find me again and we can laugh together again.\" and people realized how Nijisanji did her dirty, the community said \"You know what? Fuck this rule\" and spread her name far and wide.\n\nThat said, DO NOT harass any of the other vtubers working for Nijisanji. Some people have already done so, and it's awful. Basically all of them announced that they were taking a break the day the news was released. To put it mildly, they aren't having a good time right now. I have a bad feeling that I'm going to end up in this situation again soon (even though I hope I don't have to).", "author_fullname": "t2_jefiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Yesterday, all the videos on Selen Tatsuki's youtube channel were deleted when her contract with her employers was terminated. A few days earlier, I downloaded them all with yt-dlp. Now I have 4.5 TB of videos on my hard drive and I want to share them with her fans. WTF do I do now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akrawr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 103, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 103, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707270663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Short version with no context for the content of the videos: I have 4.5 TB of .mkv files on my hard drive, and a bunch of people who want to download some of them. I have a TrueNAS Scale server that runs 24/7 but only has 22 Mbp/s upload. I don&amp;#39;t really know what the best way to share them to people are. I&amp;#39;m thinking of putting up a torrent, but I don&amp;#39;t know where. Another site known for hosting an archive of this kind of content exists, but I&amp;#39;ve reached out to the owners and they&amp;#39;re pretty much certain that they&amp;#39;re going to get a DMCA and have to remove them. Maybe the Internet Archive, but I suspect they might get a DMCA too. Any guidance is appreciated.&lt;/p&gt;\n\n&lt;p&gt;This is the yt-dlp command  I used. Cunningham&amp;#39;s law me and tell me how awful it is so that I know what I should use next time:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;yt-dlp \\\n        -a yt-dlp-list.txt \\\n        -o &amp;quot;%(uploader)s (%(uploader_id)s)/%(upload_date)s - %(title)s - (%(duration)ss) [%(resolution)s] [%(id)s].%(ext)s&amp;quot; \\\n        --download-archive yt-dlp-archive.txt \\\n        --cookies-from-browser firefox \\\n        --ignore-errors \\\n        --merge-output-format mkv \\\n        --sub-langs all \\\n        --write-subs \\\n        --embed-subs \\\n        --add-metadata \\\n        --write-description \\\n        --write-thumbnail \\\n        --write-comments \\\n        --embed-thumbnail \\\n        --embed-info-json \\\n        --write-info-json \\\n        --windows-filenames \\\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Selen Tatsuki was a Vtuber who was employed by vtuber company Nijisanji&amp;#39;s English branch. When she was terminated, she had the highest subscriber count of  any of their female members in the English branch (and 5th highest overall). She was extremely popular and beloved by her community. She was best known for her FPS gaming skills, being top 500 in Apex Legends at one point, her contagious laughter.  If you want to get a feel for what she was like, this is a good video: &lt;a href=\"https://www.youtube.com/watch?v=elnFh8VpeKQ\"&gt;https://www.youtube.com/watch?v=elnFh8VpeKQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have time to go into all the details, unfortunately, Nijisanji has shown itself to be either cartoonishly evil or cartoonishly incompetent, and have terminated Selen&amp;#39;s contract. Nijisanji had Selen terminated (fired) for reasons I (and many others) consider to be completely unjust, especially considering the way they went about doing it. As Nijisanji owns the rights to the character of Nijisanji, and that changing a Vtuber&amp;#39;s performer is considered an unforgivable sin in this industry, the character is gone forever now, especially since all the videos on her channel were deleted too. I could go over a laundry list of of awful things that Nijisanji has done in the past year, but all YOU guys need to know is that they deleted all of Selen&amp;#39;s videos from her channel with ZERO warning. In this subreddit, I think that qualifies as an unforgivable sin. Thankfully, I had the foresight to back everything up beforehand (I had a feeling that this was going to happen).&lt;/p&gt;\n\n&lt;p&gt;For comparison on how this kind of thing should be handled, look up how Yozora Mel&amp;#39;s termination was handled.&lt;/p&gt;\n\n&lt;p&gt;Thankfully, Selen&amp;#39;s story seems to have a happy ending. She&amp;#39;s moved back to her old account named Dokibird, and is planning to return to streaming tomorrow. Normally, talking about this kind of thing is a HUGE sin in the vtubing community, but when she said &amp;quot;Please let everyone know that this is where I am now, I hope you all find me again and we can laugh together again.&amp;quot; and people realized how Nijisanji did her dirty, the community said &amp;quot;You know what? Fuck this rule&amp;quot; and spread her name far and wide.&lt;/p&gt;\n\n&lt;p&gt;That said, DO NOT harass any of the other vtubers working for Nijisanji. Some people have already done so, and it&amp;#39;s awful. Basically all of them announced that they were taking a break the day the news was released. To put it mildly, they aren&amp;#39;t having a good time right now. I have a bad feeling that I&amp;#39;m going to end up in this situation again soon (even though I hope I don&amp;#39;t have to).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vDy14nrb4QFcyTcUgcXVOXgU6u3i1nwM_N2DA3gqkqM.jpg?auto=webp&amp;s=b61786931862de181cd20e2baba0fd40d4b09a78", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/vDy14nrb4QFcyTcUgcXVOXgU6u3i1nwM_N2DA3gqkqM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa9e0d22c18a02859550ea44ac2ab87e9d878382", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/vDy14nrb4QFcyTcUgcXVOXgU6u3i1nwM_N2DA3gqkqM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4290bd52953f1c2eae4295eb6f41c461688cb350", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/vDy14nrb4QFcyTcUgcXVOXgU6u3i1nwM_N2DA3gqkqM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2eba07a7efe1660c42cbff275b7a2a90d13cfb11", "width": 320, "height": 240}], "variants": {}, "id": "z7UHxQmgUy5SZyK8FpsCrDiX9gAR0bCSt0ts4s8nxIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akrawr", "is_robot_indexable": true, "report_reasons": null, "author": "ajshell1", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1akrawr/yesterday_all_the_videos_on_selen_tatsukis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akrawr/yesterday_all_the_videos_on_selen_tatsukis/", "subreddit_subscribers": 730850, "created_utc": 1707270663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So this is a tale of a rookie mistake..\n\nI decided to resize my movie's partition so i could allocate a bit of space to a scratch partition... no problem.. Spin up IM-Magic and just steal back some of that space..\n\nLittle did I know that by doing this my movies drive would come back completely empty.. 8Tb of movies gone.. whoops.\n\nLuckily i have a local copy on another system and it's only 2 days old so i'll just jump on goodsync, set up a new job and copy everything back. It'll take about 20 hours but that's fine.. and it was.. until i rebooted for an unrelated reason and then went to start the Goodsync job again..\n\nExcept what i did was run the normal job that syncs my live drive to the backup drive.. thus deleting abotu 120GB of stuff before i realised why it was running quite so quickly..\n\nthankfully my Backblaze B2 backup (which isn't actually complete yet) was far enough along that i could recover what i needed to a B2 zip file and then pull it down using Rclone.\n\nEveryone always says.. a backup isn't a backup until it's been tested and.. well i think i've pretty much tested my strategy (unintentionally) quite well here :)\n\nEdit : Kudos to backblaze for having a service that actually adhere's to the \"unlimited\" statement.. OK so the restore process using the official app is less than perfect for subscribers here, but they offer a workaround using their B2 storage.. i'm happy to pay the short term cost of using B2 to get some of my data back using rclone.\n\nEdit2 : maybe we need a /r/talesofdataloss", "author_fullname": "t2_4cfky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whoops.... 8TB gone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akpke9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707266408.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707265787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So this is a tale of a rookie mistake..&lt;/p&gt;\n\n&lt;p&gt;I decided to resize my movie&amp;#39;s partition so i could allocate a bit of space to a scratch partition... no problem.. Spin up IM-Magic and just steal back some of that space..&lt;/p&gt;\n\n&lt;p&gt;Little did I know that by doing this my movies drive would come back completely empty.. 8Tb of movies gone.. whoops.&lt;/p&gt;\n\n&lt;p&gt;Luckily i have a local copy on another system and it&amp;#39;s only 2 days old so i&amp;#39;ll just jump on goodsync, set up a new job and copy everything back. It&amp;#39;ll take about 20 hours but that&amp;#39;s fine.. and it was.. until i rebooted for an unrelated reason and then went to start the Goodsync job again..&lt;/p&gt;\n\n&lt;p&gt;Except what i did was run the normal job that syncs my live drive to the backup drive.. thus deleting abotu 120GB of stuff before i realised why it was running quite so quickly..&lt;/p&gt;\n\n&lt;p&gt;thankfully my Backblaze B2 backup (which isn&amp;#39;t actually complete yet) was far enough along that i could recover what i needed to a B2 zip file and then pull it down using Rclone.&lt;/p&gt;\n\n&lt;p&gt;Everyone always says.. a backup isn&amp;#39;t a backup until it&amp;#39;s been tested and.. well i think i&amp;#39;ve pretty much tested my strategy (unintentionally) quite well here :)&lt;/p&gt;\n\n&lt;p&gt;Edit : Kudos to backblaze for having a service that actually adhere&amp;#39;s to the &amp;quot;unlimited&amp;quot; statement.. OK so the restore process using the official app is less than perfect for subscribers here, but they offer a workaround using their B2 storage.. i&amp;#39;m happy to pay the short term cost of using B2 to get some of my data back using rclone.&lt;/p&gt;\n\n&lt;p&gt;Edit2 : maybe we need a &lt;a href=\"/r/talesofdataloss\"&gt;/r/talesofdataloss&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "64TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akpke9", "is_robot_indexable": true, "report_reasons": null, "author": "d4nm3d", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1akpke9/whoops_8tb_gone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akpke9/whoops_8tb_gone/", "subreddit_subscribers": 730850, "created_utc": 1707265787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know you guys generally aren't the biggest fans of cloud storage, and I agree you shouldn't fully rely on cloud storage only.\n\nBut if you have use case for it, you might be interested in this massive cloud storage comparison table I made as I was researching this topic:\n\n[https://docs.google.com/spreadsheets/d/1cEd65XDW3gBHnRsJ0rbq3V\\_B28mKySHiMPAZvArHiiA/edit#gid=0](https://docs.google.com/spreadsheets/d/1cEd65XDW3gBHnRsJ0rbq3V_B28mKySHiMPAZvArHiiA/edit#gid=0)\n\nIt's got 27 cloud storage providers compared on 50+ features, as well as pricing options (monthly/yearly/lifetime) per storage amount, and customer review scores. Took me over 100 hours to make, so I hope it helps someone out.\n\nI've also included links to more information for every feature, so you can see how it's implemented or can be used per storage provider.\n\nAlso, if you have any feedback, let me know! Can still make some changes.", "author_fullname": "t2_vrzlt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a huge comparison table to help you find the best cloud storage provider for your use case (free/paid/lifetime)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aketpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707239485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know you guys generally aren&amp;#39;t the biggest fans of cloud storage, and I agree you shouldn&amp;#39;t fully rely on cloud storage only.&lt;/p&gt;\n\n&lt;p&gt;But if you have use case for it, you might be interested in this massive cloud storage comparison table I made as I was researching this topic:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.google.com/spreadsheets/d/1cEd65XDW3gBHnRsJ0rbq3V_B28mKySHiMPAZvArHiiA/edit#gid=0\"&gt;https://docs.google.com/spreadsheets/d/1cEd65XDW3gBHnRsJ0rbq3V_B28mKySHiMPAZvArHiiA/edit#gid=0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s got 27 cloud storage providers compared on 50+ features, as well as pricing options (monthly/yearly/lifetime) per storage amount, and customer review scores. Took me over 100 hours to make, so I hope it helps someone out.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also included links to more information for every feature, so you can see how it&amp;#39;s implemented or can be used per storage provider.&lt;/p&gt;\n\n&lt;p&gt;Also, if you have any feedback, let me know! Can still make some changes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?auto=webp&amp;s=d8c5ef17ff2f0f917899bb6b774cc1737c7a30ad", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e84545b3f048ca629824d018c7f2fb3a82093897", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7bb3ebc1b6f899fd6f5ddc4e6dc8006ab8263db", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e27039c634500829467ab1b9d14cd798ec2cc1b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0d4f12760968d176944ea352115900c5e778868", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52bb0fb7d7ccd20e320628bdf865a2e6de7d76dc", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/X_JTnKj8YnyMItnzMXYlcyz3jOAr5PQubRrl04xhDhM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=114bba49eeb7fc34501690e6578b5fb4e1e5d3d2", "width": 1080, "height": 567}], "variants": {}, "id": "YPi1CYr1KieuBwArDgZF_VnjTxTV6LCx1RlN0yc0Bv8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1aketpm", "is_robot_indexable": true, "report_reasons": null, "author": "CookieDelivery", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1aketpm/i_made_a_huge_comparison_table_to_help_you_find/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1aketpm/i_made_a_huge_comparison_table_to_help_you_find/", "subreddit_subscribers": 730850, "created_utc": 1707239485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello datahoarders!\n\nI know I've been posting quite a bit of stuff about optical media lately. I'm at the end of rejigging my approach a little. I kind of go through a similar pattern every few years with backup and archive stuff. Make a few changes. Document them for those interested. And then go back to \"setting and forgetting it\".\n\nI know that those using optical media constitute a minority of this subreddit. But I feel that those who are skeptical often have similar questions. So this is my little attempt to set out the use-case for those who are interested in this ... unconventional approach.  For readability, I'll format this as an FAQ (for additional readability I might recreate this as a blog. But this is my first attempt).\n\n*All of course only my flawed opinions. Feel free of course to disagree/critique etc.*\n\n***Why use optical media for ANYTHING in the year 2024?***\n\nOptical media isn't dead yet. Blu Rays remain popular with home cinema buffs etc. But given that this is the datahoarders sub let's assume that we're looking at this question from the standpoint of data preservation.\n\nOptical media has one major redeeming quality and that's its relative stability over age. I would contend that optical media is the most stable form of physical medium for holding digital data that has yet come to market. Microsoft and others are doing some amazing prototyping research with storing data on glass. But it's still (AFAIK) quite a while away from commercialisation.\n\nSo optical media remains a viable choice for *some* people who wish to create archive data for cold (ie offline) storage. Optical media has a relatively small maximum capacity (Sony's 128GB discs are the largest that have yet come to the mass consumer market). However for people like videographers, photographers, and people needing to *archive* personal data stores, it can weirdly kinda make sense (I would add to this common 'use case' list podcasters and authors: you can fit a pretty vast amount of text in 100GB!)\n\n***Why specifically archive data on optical rather than keep backups?***\n\nYou can of course store backups on optical media rather than archives if they will fit. However, read/write speeds are also a constraint. I think of optical media as LTO's simpler twin in consumer tech. It's good for keeping data that you might need in the future. Of course, archive copies of data can also store as backups. The distinction can be somewhat wooly. But if we think of backups as \"restore your OS quickly to a previous point in time\" ... optical is the wrong tool for the job.\n\n***Why not use 'hot' (internet connected) storage?***\n\nYou can build your own nice little backup setup using NASes and servers, of course. I love my NAS!\n\nOne reason why people might wish to choose optical for archival storage is that it's offline and it's WORM.\n\nStoring archival data on optical media is a crude but effective way of air-gapping it from whatever you're worried about. Because storing it requires no power, you can also do things like store it in safe vault boxes, home safes, etc. If you need to add physical protection to your data store, optical keeps some doors open.\n\n***What about LTO?***\n\nWhen I think about optical media for data archival I think mostly about two groups of potential users: individuals who are concerned about their data longevity and SMBs. Getting \"into\" optical media is vastly cheaper than getting \"into\" LTO ($100 burner vs. $5K burner).\n\nThere ARE such things as optical jukeboxes that aggregate sets of high capacity BDXL discs into cartridges which some cool robotics for retrieval. However in the enterprise, I don't think optical will be a serious contender unless and until high capacity discs at a far lower price point come to market.\n\nLTO may be the kind of archival in the enterprise. But when it comes to offline/cold storage specifically, optical media trumps it from a data stability standpoint (and HDD and SSD and other flash memory storage media).\n\n***What about the cloud?***\n\nI love optical media in large part because I don't want to be dependent upon cloud storage for holding even a single copy of my data over the long term.\n\nThere's also something immensely satisfying about being able to create your own data pool physically. Optical media has essentially no OpEx. In an ideal situation, once you write onto good discs, the data remains good for decades - and hopefully quite a bit longer.\n\nI'd agree that this benefit can be replicated by deploying your own \"cloud\" by owning the server/NAS/etc. Either approach appeals to me. It's nice to have copies of your data on hardware that you physically own and have can access.\n\n***What optical media do you recommend buying?***\n\nThe M-Disc comes up quite frequently on this subreddit and has spawned enormous skepticism as well as some theories (Verbatim is selling regular HTL BD-R media as M-Discs!). Personally I have yet to see compelling proof to support this accusation.\n\nHOWEVER I do increasingly believe that the M-Disc Blu Ray is  ... not necessary. Regular Blu Ray discs (HTL kind) use an inorganic recording layer. Verbatim's technology is called MABL (metal ablative recording layer). But other manufacturers have come up with their own spins on this.\n\nI have attempted to get answers from Verbatim as to what the real difference is if they're both inorganic anyway. I have yet to receive an answer beyond \"the M-Disc is what we recommend for archival\". I also couldn't help but notice that the longevity for M-Disc BD-R has gone down to a \"few hundred years\" and that the M-Disc patent only refers to the DVD variant. All these things arouse my suspicion unfortunately.\n\nMore importantly, perhaps, I've found multiple sources stating that MABL can be good for 100 years. To me, this is more than enough time. Media of this nature is cheaper and easier to source than the MDisc.\n\nMy recommendation is to buy good discs that are explicitly marketed either as a) archival-grade or b) marketed with a lifetime projection, like 100 years. Amazon Japan I've discovered is a surprisingly fertile source.\n\n***Can a regular Blu Ray burner write M-Discs?***\n\nYes and if you read the old Millenniata press releases you'll notice that this was always the case.\n\n***If so why do some Blu Ray writers say \"M-Disc compatible\"?***\n\nMarketing as far as I can tell.\n\n***What about \"archival grade\" CDs and DVDs?***\n\nThe skinny of this tech is \"we added a layer of gold to try avoid corrosion to the recording layer.\" But the recording layer is still an organic dye. These discs look awesome but I have more confidence in inorganic media (lower capacities aside).\n\n***What about rewritable media?***\n\nIf cold storage archival is what you're going for, absolutely avoid these. A recording layer that's easy to wipe and rewrite is a conflicting objective to a recording layer that's ideally extremely stable.\n\n***I haven't thought about optical media since the noughties. What are the options these days?***\n\nIn Blu Ray: 25GB, 50GB (BR-DL), 100GB (BDXL), 128GB (BDXL - only Sony make these to date).\n\n***Any burner recommendations?***\n\nI'm skeptical of thin line external burners. I'd trust an internal SATA drive or a SATA drive connected via an enclosure more. I feel like these things need a direct power supply ideally. I've heard a lot of good things about Pioneer's hardware.\n\n***If you do this don't you end up with thousands of discs?***\n\nI haven't found that the stuff I've archived takes up an inordinate amount of space.\n\n***How should I store my burned discs?***\n\nJewel cases are best. Keep them out of the sun (this is vital). There's an ISO standard with specific parameters around temperature, RH, temperature gradients, and RH variants. I don't think you need to buy a humidity controlled cabinet. Just keep them somewhere sensible.\n\n***Any other things that are good to know?***\n\nYou can use parity data and error correction code to proactively prevent against corruption. But the primary objective should be selecting media that has a very low chance of that.\n\n***Can you encrypt discs?***\n\nYes. Very easily.\n\n***What about labelling?***\n\nDon't use labels on discs. If you're going to write on them, write (ideally) using an optical media safe market and on the transparent inset of the disc where there's no data being stored.\n\n***Other ideas?***\n\nQR codes or some other barcodes on jewel cases to make it easy to identify contents. A digital cataloging software like VVV or WinCatalog. Keep the discs in sequential order. And stuff gets pretty easy to locate.\n\n***What about offsite copies?***\n\nI burn every disc twice and keep one copy offsite. If you own two properties you're perfectly set up for this.\n\n***What about deprecation?***\n\nWhen that's a real pressing concern move your stuff over to the next medium for preservation. But remember that the floppy disc barely holds more than 1 Mb and finding USB drives is still pretty straightforward. If you're really worried, consider buying an extra drive. I reckon people will have time to figure this out and attempting to predict the future is futile. \n\n***What about checksums?***\n\nFolks more experienced at this than me have pointed out that these have limited utility and that parity data is a lot more helpful (error detection and repair). Or ECC. That being said you can easily calculate checksums and store them in your digital catalog.\n\n\\---\n\nProbably more stuff but this should be plenty of information and I'm done with the computer for the day!\n\n&amp;#x200B;", "author_fullname": "t2_poc45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use optical media for digital archiving in 2024? Here's my full FAQ!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akhe3i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707245955.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707245753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello datahoarders!&lt;/p&gt;\n\n&lt;p&gt;I know I&amp;#39;ve been posting quite a bit of stuff about optical media lately. I&amp;#39;m at the end of rejigging my approach a little. I kind of go through a similar pattern every few years with backup and archive stuff. Make a few changes. Document them for those interested. And then go back to &amp;quot;setting and forgetting it&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I know that those using optical media constitute a minority of this subreddit. But I feel that those who are skeptical often have similar questions. So this is my little attempt to set out the use-case for those who are interested in this ... unconventional approach.  For readability, I&amp;#39;ll format this as an FAQ (for additional readability I might recreate this as a blog. But this is my first attempt).&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;All of course only my flawed opinions. Feel free of course to disagree/critique etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why use optical media for ANYTHING in the year 2024?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Optical media isn&amp;#39;t dead yet. Blu Rays remain popular with home cinema buffs etc. But given that this is the datahoarders sub let&amp;#39;s assume that we&amp;#39;re looking at this question from the standpoint of data preservation.&lt;/p&gt;\n\n&lt;p&gt;Optical media has one major redeeming quality and that&amp;#39;s its relative stability over age. I would contend that optical media is the most stable form of physical medium for holding digital data that has yet come to market. Microsoft and others are doing some amazing prototyping research with storing data on glass. But it&amp;#39;s still (AFAIK) quite a while away from commercialisation.&lt;/p&gt;\n\n&lt;p&gt;So optical media remains a viable choice for &lt;em&gt;some&lt;/em&gt; people who wish to create archive data for cold (ie offline) storage. Optical media has a relatively small maximum capacity (Sony&amp;#39;s 128GB discs are the largest that have yet come to the mass consumer market). However for people like videographers, photographers, and people needing to &lt;em&gt;archive&lt;/em&gt; personal data stores, it can weirdly kinda make sense (I would add to this common &amp;#39;use case&amp;#39; list podcasters and authors: you can fit a pretty vast amount of text in 100GB!)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why specifically archive data on optical rather than keep backups?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You can of course store backups on optical media rather than archives if they will fit. However, read/write speeds are also a constraint. I think of optical media as LTO&amp;#39;s simpler twin in consumer tech. It&amp;#39;s good for keeping data that you might need in the future. Of course, archive copies of data can also store as backups. The distinction can be somewhat wooly. But if we think of backups as &amp;quot;restore your OS quickly to a previous point in time&amp;quot; ... optical is the wrong tool for the job.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why not use &amp;#39;hot&amp;#39; (internet connected) storage?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You can build your own nice little backup setup using NASes and servers, of course. I love my NAS!&lt;/p&gt;\n\n&lt;p&gt;One reason why people might wish to choose optical for archival storage is that it&amp;#39;s offline and it&amp;#39;s WORM.&lt;/p&gt;\n\n&lt;p&gt;Storing archival data on optical media is a crude but effective way of air-gapping it from whatever you&amp;#39;re worried about. Because storing it requires no power, you can also do things like store it in safe vault boxes, home safes, etc. If you need to add physical protection to your data store, optical keeps some doors open.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about LTO?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When I think about optical media for data archival I think mostly about two groups of potential users: individuals who are concerned about their data longevity and SMBs. Getting &amp;quot;into&amp;quot; optical media is vastly cheaper than getting &amp;quot;into&amp;quot; LTO ($100 burner vs. $5K burner).&lt;/p&gt;\n\n&lt;p&gt;There ARE such things as optical jukeboxes that aggregate sets of high capacity BDXL discs into cartridges which some cool robotics for retrieval. However in the enterprise, I don&amp;#39;t think optical will be a serious contender unless and until high capacity discs at a far lower price point come to market.&lt;/p&gt;\n\n&lt;p&gt;LTO may be the kind of archival in the enterprise. But when it comes to offline/cold storage specifically, optical media trumps it from a data stability standpoint (and HDD and SSD and other flash memory storage media).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about the cloud?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I love optical media in large part because I don&amp;#39;t want to be dependent upon cloud storage for holding even a single copy of my data over the long term.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also something immensely satisfying about being able to create your own data pool physically. Optical media has essentially no OpEx. In an ideal situation, once you write onto good discs, the data remains good for decades - and hopefully quite a bit longer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d agree that this benefit can be replicated by deploying your own &amp;quot;cloud&amp;quot; by owning the server/NAS/etc. Either approach appeals to me. It&amp;#39;s nice to have copies of your data on hardware that you physically own and have can access.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What optical media do you recommend buying?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The M-Disc comes up quite frequently on this subreddit and has spawned enormous skepticism as well as some theories (Verbatim is selling regular HTL BD-R media as M-Discs!). Personally I have yet to see compelling proof to support this accusation.&lt;/p&gt;\n\n&lt;p&gt;HOWEVER I do increasingly believe that the M-Disc Blu Ray is  ... not necessary. Regular Blu Ray discs (HTL kind) use an inorganic recording layer. Verbatim&amp;#39;s technology is called MABL (metal ablative recording layer). But other manufacturers have come up with their own spins on this.&lt;/p&gt;\n\n&lt;p&gt;I have attempted to get answers from Verbatim as to what the real difference is if they&amp;#39;re both inorganic anyway. I have yet to receive an answer beyond &amp;quot;the M-Disc is what we recommend for archival&amp;quot;. I also couldn&amp;#39;t help but notice that the longevity for M-Disc BD-R has gone down to a &amp;quot;few hundred years&amp;quot; and that the M-Disc patent only refers to the DVD variant. All these things arouse my suspicion unfortunately.&lt;/p&gt;\n\n&lt;p&gt;More importantly, perhaps, I&amp;#39;ve found multiple sources stating that MABL can be good for 100 years. To me, this is more than enough time. Media of this nature is cheaper and easier to source than the MDisc.&lt;/p&gt;\n\n&lt;p&gt;My recommendation is to buy good discs that are explicitly marketed either as a) archival-grade or b) marketed with a lifetime projection, like 100 years. Amazon Japan I&amp;#39;ve discovered is a surprisingly fertile source.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Can a regular Blu Ray burner write M-Discs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Yes and if you read the old Millenniata press releases you&amp;#39;ll notice that this was always the case.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;If so why do some Blu Ray writers say &amp;quot;M-Disc compatible&amp;quot;?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Marketing as far as I can tell.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about &amp;quot;archival grade&amp;quot; CDs and DVDs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The skinny of this tech is &amp;quot;we added a layer of gold to try avoid corrosion to the recording layer.&amp;quot; But the recording layer is still an organic dye. These discs look awesome but I have more confidence in inorganic media (lower capacities aside).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about rewritable media?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;If cold storage archival is what you&amp;#39;re going for, absolutely avoid these. A recording layer that&amp;#39;s easy to wipe and rewrite is a conflicting objective to a recording layer that&amp;#39;s ideally extremely stable.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I haven&amp;#39;t thought about optical media since the noughties. What are the options these days?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In Blu Ray: 25GB, 50GB (BR-DL), 100GB (BDXL), 128GB (BDXL - only Sony make these to date).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Any burner recommendations?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m skeptical of thin line external burners. I&amp;#39;d trust an internal SATA drive or a SATA drive connected via an enclosure more. I feel like these things need a direct power supply ideally. I&amp;#39;ve heard a lot of good things about Pioneer&amp;#39;s hardware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;If you do this don&amp;#39;t you end up with thousands of discs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t found that the stuff I&amp;#39;ve archived takes up an inordinate amount of space.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;How should I store my burned discs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Jewel cases are best. Keep them out of the sun (this is vital). There&amp;#39;s an ISO standard with specific parameters around temperature, RH, temperature gradients, and RH variants. I don&amp;#39;t think you need to buy a humidity controlled cabinet. Just keep them somewhere sensible.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Any other things that are good to know?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You can use parity data and error correction code to proactively prevent against corruption. But the primary objective should be selecting media that has a very low chance of that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Can you encrypt discs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Yes. Very easily.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about labelling?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t use labels on discs. If you&amp;#39;re going to write on them, write (ideally) using an optical media safe market and on the transparent inset of the disc where there&amp;#39;s no data being stored.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Other ideas?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;QR codes or some other barcodes on jewel cases to make it easy to identify contents. A digital cataloging software like VVV or WinCatalog. Keep the discs in sequential order. And stuff gets pretty easy to locate.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about offsite copies?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I burn every disc twice and keep one copy offsite. If you own two properties you&amp;#39;re perfectly set up for this.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about deprecation?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When that&amp;#39;s a real pressing concern move your stuff over to the next medium for preservation. But remember that the floppy disc barely holds more than 1 Mb and finding USB drives is still pretty straightforward. If you&amp;#39;re really worried, consider buying an extra drive. I reckon people will have time to figure this out and attempting to predict the future is futile. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What about checksums?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Folks more experienced at this than me have pointed out that these have limited utility and that parity data is a lot more helpful (error detection and repair). Or ECC. That being said you can easily calculate checksums and store them in your digital catalog.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Probably more stuff but this should be plenty of information and I&amp;#39;m done with the computer for the day!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akhe3i", "is_robot_indexable": true, "report_reasons": null, "author": "danielrosehill", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akhe3i/why_use_optical_media_for_digital_archiving_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akhe3i/why_use_optical_media_for_digital_archiving_in/", "subreddit_subscribers": 730850, "created_utc": 1707245753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_poc45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Template for labelling system for physical media inventory (Blu Rays). Anyone have their own system and/or suggestions for improvement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1akakwl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YL70ZIf_aMXNGMin1CuEYFq8hq7hWYQwCsEVOCjH1Mk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707228501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/vcdz4rjq2zgc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?auto=webp&amp;s=9b690aa85c5d468fd5c47beac0af0172b313cac1", "width": 2024, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b86482c348eef38b2f65814cc07d5d39b91bd90", "width": 108, "height": 54}, {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f01008cf9a593fb7541980c458ef2fcf5108758a", "width": 216, "height": 109}, {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44451651c23ef5317565d46c4e99b8c239ae2724", "width": 320, "height": 161}, {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ab7eac613ff2f1aa9de4b360a042ae238eb48e5", "width": 640, "height": 323}, {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bfb5271be8add10f50cf0e4cc010e324817a8bdb", "width": 960, "height": 485}, {"url": "https://preview.redd.it/vcdz4rjq2zgc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c061e7f898b97faedcbc1515cea8aafa6fcf8090", "width": 1080, "height": 546}], "variants": {}, "id": "7eWfTy0jHHvdQmhrbM2_8hHON2LORpRu_UNzCACIkw8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akakwl", "is_robot_indexable": true, "report_reasons": null, "author": "danielrosehill", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akakwl/template_for_labelling_system_for_physical_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/vcdz4rjq2zgc1.png", "subreddit_subscribers": 730850, "created_utc": 1707228501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Came across ten 600GB drives from a decommissioned server. What is the best way to determine if these old drives are any good? I know that one of the drives in the server (which was setup with Raid5) had failed and needed to be replaced but the server was retired rather than fixed. \n\nIs there a good, hopefully free app that can scan the drives and show me the bad one and also tell me if I should even bother with the others. \n\nDrives are HP brand and had been spinning for 5 years before shutting down the server.", "author_fullname": "t2_hq1bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Came across ten 600GB drives from old server. What is the best way to determine if old drives are any good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akdib0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707236198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Came across ten 600GB drives from a decommissioned server. What is the best way to determine if these old drives are any good? I know that one of the drives in the server (which was setup with Raid5) had failed and needed to be replaced but the server was retired rather than fixed. &lt;/p&gt;\n\n&lt;p&gt;Is there a good, hopefully free app that can scan the drives and show me the bad one and also tell me if I should even bother with the others. &lt;/p&gt;\n\n&lt;p&gt;Drives are HP brand and had been spinning for 5 years before shutting down the server.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akdib0", "is_robot_indexable": true, "report_reasons": null, "author": "savekevin", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akdib0/came_across_ten_600gb_drives_from_old_server_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akdib0/came_across_ten_600gb_drives_from_old_server_what/", "subreddit_subscribers": 730850, "created_utc": 1707236198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several folders of Godot game project, where some of them are exact duplicates, whereas others are ones which I have continued working on.\n\nIs there a tool I can use to find exact duplicates so I can delete them?", "author_fullname": "t2_l20rgpzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool to check if folders have the same content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akj96c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707250261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several folders of Godot game project, where some of them are exact duplicates, whereas others are ones which I have continued working on.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool I can use to find exact duplicates so I can delete them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akj96c", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate-Record951", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akj96c/is_there_a_tool_to_check_if_folders_have_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akj96c/is_there_a_tool_to_check_if_folders_have_the_same/", "subreddit_subscribers": 730850, "created_utc": 1707250261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm setting up an OpenMediaVault NAS this week and since this is my first time working with ZFS, I thought I'd ask the pros for some advice.  So the hard drives I have available are:\n\n* 4x 10TB (empty)\n* 2x 12TB (1 full, 1 empty)\n* 1x 14TB (full)\n* 1x 16TB (about 1/2 full)\n\nMy plan is to put the empty 12TB drive into a vdev with the 10TB drives for 40TB capacity after single redundancy. Then I'll put it in a pool, migrate all of my data, and create a second vdev with the remaining 12, 14, and 16TB drives, for an additional 24TB of single-redundancy capacity.\n\nIf none of that sounds off, I'm also curious as to whether I should consider ZLOG/L2ARC drives. The system is primarily a Plex server (maybe 5 concurrent users tops, runs a handful of other services). I have several SSDs laying around: 256, 500, 2x120, 240, 128. And I suppose if there's any of those left over I should set up some fast storage set aside for metadata / transcoding / config / etc. Operating system will be on a 500 GB NVME drive.\n\nAnd of course if there's anything else I might be missing, please let me know that too. I thought I was good at computers, but some of this ZFS stuff gets complicated!", "author_fullname": "t2_due19f7ez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up NAS, need some ZFS guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akjrg2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707251458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m setting up an OpenMediaVault NAS this week and since this is my first time working with ZFS, I thought I&amp;#39;d ask the pros for some advice.  So the hard drives I have available are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;4x 10TB (empty)&lt;/li&gt;\n&lt;li&gt;2x 12TB (1 full, 1 empty)&lt;/li&gt;\n&lt;li&gt;1x 14TB (full)&lt;/li&gt;\n&lt;li&gt;1x 16TB (about 1/2 full)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My plan is to put the empty 12TB drive into a vdev with the 10TB drives for 40TB capacity after single redundancy. Then I&amp;#39;ll put it in a pool, migrate all of my data, and create a second vdev with the remaining 12, 14, and 16TB drives, for an additional 24TB of single-redundancy capacity.&lt;/p&gt;\n\n&lt;p&gt;If none of that sounds off, I&amp;#39;m also curious as to whether I should consider ZLOG/L2ARC drives. The system is primarily a Plex server (maybe 5 concurrent users tops, runs a handful of other services). I have several SSDs laying around: 256, 500, 2x120, 240, 128. And I suppose if there&amp;#39;s any of those left over I should set up some fast storage set aside for metadata / transcoding / config / etc. Operating system will be on a 500 GB NVME drive.&lt;/p&gt;\n\n&lt;p&gt;And of course if there&amp;#39;s anything else I might be missing, please let me know that too. I thought I was good at computers, but some of this ZFS stuff gets complicated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akjrg2", "is_robot_indexable": true, "report_reasons": null, "author": "DrivewayAvalanch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akjrg2/setting_up_nas_need_some_zfs_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akjrg2/setting_up_nas_need_some_zfs_guidance/", "subreddit_subscribers": 730850, "created_utc": 1707251458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just ordered a HF2-SU3S3 per all the suggestions here. Will be running just 2 drives in RAID-1 for a backup of all my files. (I already have other cloud and disk backups, not just relying on the RAID)\n\nWhat are the best value for relatively small (8tb or larger) drives right now? The mediasonic page said something about it not being compatible with power saving drives like WDD enterprise?", "author_fullname": "t2_4rsekswq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best moderate price disks for Mediasonic ProBox?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akfb3z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707240712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just ordered a HF2-SU3S3 per all the suggestions here. Will be running just 2 drives in RAID-1 for a backup of all my files. (I already have other cloud and disk backups, not just relying on the RAID)&lt;/p&gt;\n\n&lt;p&gt;What are the best value for relatively small (8tb or larger) drives right now? The mediasonic page said something about it not being compatible with power saving drives like WDD enterprise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akfb3z", "is_robot_indexable": true, "report_reasons": null, "author": "TslaNCorn", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akfb3z/best_moderate_price_disks_for_mediasonic_probox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akfb3z/best_moderate_price_disks_for_mediasonic_probox/", "subreddit_subscribers": 730850, "created_utc": 1707240712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Over the years ive managed to filter down what i want to keep to fit into one bin \u2014 we\u2019re talking preschool through grad school. Now, it feels like even this one bin is too much. Im on the fence about digitizing everything because i have a feeling i still wont want to get rid of the bin. Also, any tips on the fastest, least invasive way of scanning everything? Lots of staple action going on too.", "author_fullname": "t2_tf5641elm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notes from school", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akcyi0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707234762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the years ive managed to filter down what i want to keep to fit into one bin \u2014 we\u2019re talking preschool through grad school. Now, it feels like even this one bin is too much. Im on the fence about digitizing everything because i have a feeling i still wont want to get rid of the bin. Also, any tips on the fastest, least invasive way of scanning everything? Lots of staple action going on too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1akcyi0", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant-Possession62", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akcyi0/notes_from_school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akcyi0/notes_from_school/", "subreddit_subscribers": 730850, "created_utc": 1707234762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,i have a Canon Scan Lide 400 wich is an optical 4800dpi scanner and would like to digitalize and archive grandpa old photos. I'm new here to hoarding pics but i would like to discuss something before i start the process!\n\nGiven that in cameras world (not the same device, i know ..), a full frame sensors is way better than APS-C sensors, also due to pixel size (FF is **bigger**);\n\nI was just wondering if setting lower scan res, the scanner uses more \"reading\" pixels to scan the same point, resulting in a **bigger** \"combined\\_sensor\\_pixel\" block instead of one single pixel, giving better results in terms of quality and grain.\n\nI mean: if i scan at max 4800dpi each pixel reads a point, but what if i set 1200dpi (1/4 of max optical)? Do 4 pixels read the same point? I would have less resolution but maybe a better image overall?\n\n(sorry if i didn't used right scan technical terms, but hope i give the idea)\n\nI've done some scan test in TIFF with 4000dpi vs 2000dpi and the 2kdpi looks more sharp and clean!I have to run more scan tests to be sure (and i'll do) but i've seen this at the moment.\n\nAnyone has already tested photo scanning in this way?\n\nPS: at the end i'll keep TIFF archive as master and JPG for everyday use!  \n\n\nEDIT: we're talking about analog developed photos", "author_fullname": "t2_2fa047cs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitalizing old photos, best (and not necessarily MAX) dpi?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ak9l7f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707229622.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707225633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,i have a Canon Scan Lide 400 wich is an optical 4800dpi scanner and would like to digitalize and archive grandpa old photos. I&amp;#39;m new here to hoarding pics but i would like to discuss something before i start the process!&lt;/p&gt;\n\n&lt;p&gt;Given that in cameras world (not the same device, i know ..), a full frame sensors is way better than APS-C sensors, also due to pixel size (FF is &lt;strong&gt;bigger&lt;/strong&gt;);&lt;/p&gt;\n\n&lt;p&gt;I was just wondering if setting lower scan res, the scanner uses more &amp;quot;reading&amp;quot; pixels to scan the same point, resulting in a &lt;strong&gt;bigger&lt;/strong&gt; &amp;quot;combined_sensor_pixel&amp;quot; block instead of one single pixel, giving better results in terms of quality and grain.&lt;/p&gt;\n\n&lt;p&gt;I mean: if i scan at max 4800dpi each pixel reads a point, but what if i set 1200dpi (1/4 of max optical)? Do 4 pixels read the same point? I would have less resolution but maybe a better image overall?&lt;/p&gt;\n\n&lt;p&gt;(sorry if i didn&amp;#39;t used right scan technical terms, but hope i give the idea)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done some scan test in TIFF with 4000dpi vs 2000dpi and the 2kdpi looks more sharp and clean!I have to run more scan tests to be sure (and i&amp;#39;ll do) but i&amp;#39;ve seen this at the moment.&lt;/p&gt;\n\n&lt;p&gt;Anyone has already tested photo scanning in this way?&lt;/p&gt;\n\n&lt;p&gt;PS: at the end i&amp;#39;ll keep TIFF archive as master and JPG for everyday use!  &lt;/p&gt;\n\n&lt;p&gt;EDIT: we&amp;#39;re talking about analog developed photos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "14TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1ak9l7f", "is_robot_indexable": true, "report_reasons": null, "author": "recursivepointer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1ak9l7f/digitalizing_old_photos_best_and_not_necessarily/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1ak9l7f/digitalizing_old_photos_best_and_not_necessarily/", "subreddit_subscribers": 730850, "created_utc": 1707225633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to transfer a folder that is 30GB+ but every file sharing site I can find automatically zips the file when downloading, are there any sites that don't do this?\n\nThanks", "author_fullname": "t2_foj4ohr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best file sharing site that doesn't automatically ZIP files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1akwivv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707287102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to transfer a folder that is 30GB+ but every file sharing site I can find automatically zips the file when downloading, are there any sites that don&amp;#39;t do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akwivv", "is_robot_indexable": true, "report_reasons": null, "author": "GazF1888", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akwivv/best_file_sharing_site_that_doesnt_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akwivv/best_file_sharing_site_that_doesnt_automatically/", "subreddit_subscribers": 730850, "created_utc": 1707287102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So this is my first go with JBODs I have a disk array with Truenas installed dont have any issues with it but what im trying to do is connect the EMC to my DL360 G9 to pass through the disks to vmware. \n\nNow Ive installed an LSI 9200-8e and flashed it to P20 IT firmware, when the server boots up and initializes the LSI card I can see all the hard drives in it, but in smart storage app i dont see the LSI card. In VMWare 7 I can see the LSI card but it says there are no disks that i can add to a data store. \n\nAnyone been through this and have any ideas? Oh and in the HP Smart storage app when i run a diagnosis it sees the drives and the LSI card, but i cant configure anything it only sees it when running the report. ", "author_fullname": "t2_cnda7z3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dell EMC KTN-STL3 JBOD with an HP DL360 G9 and VMware?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1akwfzp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707286803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So this is my first go with JBODs I have a disk array with Truenas installed dont have any issues with it but what im trying to do is connect the EMC to my DL360 G9 to pass through the disks to vmware. &lt;/p&gt;\n\n&lt;p&gt;Now Ive installed an LSI 9200-8e and flashed it to P20 IT firmware, when the server boots up and initializes the LSI card I can see all the hard drives in it, but in smart storage app i dont see the LSI card. In VMWare 7 I can see the LSI card but it says there are no disks that i can add to a data store. &lt;/p&gt;\n\n&lt;p&gt;Anyone been through this and have any ideas? Oh and in the HP Smart storage app when i run a diagnosis it sees the drives and the LSI card, but i cant configure anything it only sees it when running the report. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akwfzp", "is_robot_indexable": true, "report_reasons": null, "author": "Koldenshitzen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akwfzp/dell_emc_ktnstl3_jbod_with_an_hp_dl360_g9_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akwfzp/dell_emc_ktnstl3_jbod_with_an_hp_dl360_g9_and/", "subreddit_subscribers": 730850, "created_utc": 1707286803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a long one, apologies.\u00a0\n\nHoping to get some suggestions from you smart folks. I'm looking to build a solution for several tasks.\n\n  \n**Primarily:**\u00a0\n\n* Local data backup &amp; recovery - regular, automatic onsite for PC &amp; mobile phone, potentially more devices in the future\n* Cloud storage/file server - access files locally or remotely, 24/7\n* Media server - manage, organize, and stream my media locally or remotely 24/7 - speed and latency are key\n* Local torrent server for downloading &amp; seeding Linux ISOs 24/7 - very low power consumption is key\n\n**Planned upgrades for later:**\u00a0\n\n* Secure shared access to files/folders &amp; guest accounts for friends/family\n* NVR for up to 4 security cameras (not purchased yet)\n\n**Notes**\n\nRelatively low noise and power consumption are considerations. Electricity is expensive in my area. Currently on a PC running Win 10 Pro (soon dual boot w/Linux) with a few HDDs for media and personal files, transitioning to SSDs for all files I interact with directly. Debating putting the SSDs on the network instead of inside the PC. I realize this may add complexity and require upgrading to 10GbE NIC\u00a0+ router to get access speeds &amp; latency approaching the internal SATA connection.\n\nBackups don't need to be fast, so considering getting a few 20tb HDDs for the lower cost.\n\nCurrently torrenting on a tiny, dedicated headless server based on Raspberry Pi 3 with a 3TB portable USB drive that's been nearly full for several years. It's rock solid, runs cool, completely silent, and sips less than 1W of power, which is great. But it's slow as hell, temporarily freezes if I transfer a file or load too many torrrents at once, and really limits my space. It's also annoying having to SSH into a CLI, and manually copying large completed torrents to my local machine over a 100MB connection.\u00a0\n\nNot interested in a seedbox service or other ongoing subscription.  \nNot interested in a prebuilt NAS like Qnap or Synology. They look beautiful, super simple, reliable, and can do a lot, but I'd prefer to build it myself for increased privacy (self hosted), security, flexibility, upgradeability, and affordability relative to the specs.\n\n**Questions:**\u00a0\n\n1. Would it make sense for one box to do all the above? My guess is no since some tasks have conflicting aims. If not, what might be a smart setup?\n2. If I put my 4 unused 2.5\" SATA SSDs (total 32TB) in RAID 0 inside the PC now (managed by the BIOS I guess), how easy would it be to move the array to a NAS or enclosure later? Is it as simple as installing them in the same order and choosing the same RAID type?\n3. I don't necessarily need RAID 0 for added speed, I just want a single volume for all SSDs - not 4, without losing any expensive storage space to parity, as with RAID 5.\n\nAppreciate any feedback. ", "author_fullname": "t2_nh2ls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for first NAS/backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akt8ho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707276298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a long one, apologies.\u00a0&lt;/p&gt;\n\n&lt;p&gt;Hoping to get some suggestions from you smart folks. I&amp;#39;m looking to build a solution for several tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Primarily:&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Local data backup &amp;amp; recovery - regular, automatic onsite for PC &amp;amp; mobile phone, potentially more devices in the future&lt;/li&gt;\n&lt;li&gt;Cloud storage/file server - access files locally or remotely, 24/7&lt;/li&gt;\n&lt;li&gt;Media server - manage, organize, and stream my media locally or remotely 24/7 - speed and latency are key&lt;/li&gt;\n&lt;li&gt;Local torrent server for downloading &amp;amp; seeding Linux ISOs 24/7 - very low power consumption is key&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Planned upgrades for later:&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Secure shared access to files/folders &amp;amp; guest accounts for friends/family&lt;/li&gt;\n&lt;li&gt;NVR for up to 4 security cameras (not purchased yet)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Relatively low noise and power consumption are considerations. Electricity is expensive in my area. Currently on a PC running Win 10 Pro (soon dual boot w/Linux) with a few HDDs for media and personal files, transitioning to SSDs for all files I interact with directly. Debating putting the SSDs on the network instead of inside the PC. I realize this may add complexity and require upgrading to 10GbE NIC\u00a0+ router to get access speeds &amp;amp; latency approaching the internal SATA connection.&lt;/p&gt;\n\n&lt;p&gt;Backups don&amp;#39;t need to be fast, so considering getting a few 20tb HDDs for the lower cost.&lt;/p&gt;\n\n&lt;p&gt;Currently torrenting on a tiny, dedicated headless server based on Raspberry Pi 3 with a 3TB portable USB drive that&amp;#39;s been nearly full for several years. It&amp;#39;s rock solid, runs cool, completely silent, and sips less than 1W of power, which is great. But it&amp;#39;s slow as hell, temporarily freezes if I transfer a file or load too many torrrents at once, and really limits my space. It&amp;#39;s also annoying having to SSH into a CLI, and manually copying large completed torrents to my local machine over a 100MB connection.\u00a0&lt;/p&gt;\n\n&lt;p&gt;Not interested in a seedbox service or other ongoing subscription.&lt;br/&gt;\nNot interested in a prebuilt NAS like Qnap or Synology. They look beautiful, super simple, reliable, and can do a lot, but I&amp;#39;d prefer to build it myself for increased privacy (self hosted), security, flexibility, upgradeability, and affordability relative to the specs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Would it make sense for one box to do all the above? My guess is no since some tasks have conflicting aims. If not, what might be a smart setup?&lt;/li&gt;\n&lt;li&gt;If I put my 4 unused 2.5&amp;quot; SATA SSDs (total 32TB) in RAID 0 inside the PC now (managed by the BIOS I guess), how easy would it be to move the array to a NAS or enclosure later? Is it as simple as installing them in the same order and choosing the same RAID type?&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t necessarily need RAID 0 for added speed, I just want a single volume for all SSDs - not 4, without losing any expensive storage space to parity, as with RAID 5.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Appreciate any feedback. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akt8ho", "is_robot_indexable": true, "report_reasons": null, "author": "sakuba", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akt8ho/suggestions_for_first_nasbackup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akt8ho/suggestions_for_first_nasbackup_solution/", "subreddit_subscribers": 730850, "created_utc": 1707276298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello everyone,\n\nI recently purchased a 2TB external hard drive where I want to save all my important memories. I would like to get another one of another brand but the same size and I would like to make a 1:1 copy of the first one. Is there any way to automatically copy the contents of the first hard drive to the second one automatically? I mean, every time I add a file on the first hard disk, it will also automatically copy to the second one when it will be connected to the computer. I use a Mac with Apple Silicon. \n\n&amp;#x200B;\n\nThank you for your attention,\n\nhave a nice day", "author_fullname": "t2_kedbpo10r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatic 1:1 copy between two hard drives on Mac", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akiv6r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707249336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently purchased a 2TB external hard drive where I want to save all my important memories. I would like to get another one of another brand but the same size and I would like to make a 1:1 copy of the first one. Is there any way to automatically copy the contents of the first hard drive to the second one automatically? I mean, every time I add a file on the first hard disk, it will also automatically copy to the second one when it will be connected to the computer. I use a Mac with Apple Silicon. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your attention,&lt;/p&gt;\n\n&lt;p&gt;have a nice day&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akiv6r", "is_robot_indexable": true, "report_reasons": null, "author": "ottocent0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akiv6r/automatic_11_copy_between_two_hard_drives_on_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akiv6r/automatic_11_copy_between_two_hard_drives_on_mac/", "subreddit_subscribers": 730850, "created_utc": 1707249336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was looking to download content from a WordPress site that functions like a \"Netflix,\" complete with embedded \"private\" Vimeo videos. My technical skills are somewhat limited, so I'm sharing the method I used, though I'm sure it's not the most efficient.\n\nInitially, I tried tools like httptrack and wget without success. Eventually, I resorted to manually gathering a list of all HTML files using the Linkclump Chrome extension. Then, I imported these files into Internet Download Manager. I also converted the HTML files to TXT files (although this step may have been unnecessary) and merged them into a single file using a simple command in the command prompt.\n\n    copy *.txt merged_file.txt\n    \n\nNext, I used Notepad++ to filter out only the embedded Vimeo code, which I then imported into Excel to create a list of video IDs. With these IDs, I utilized YT-DLP, employing the cookie and referer method, to successfully download the videos.\n\n    yt-dlp --cookies C:\\Users\\valentine\\Desktop\\cookie.txt --referer https://pgnpiano.com/ https://player.vimeo.com/video/123456789\n    \n\nWhile this worked, it's  lengthy and has its drawbacks, particularly regarding file naming if Vimeo titles are not properly named similar to the website format.\n\nI would be interested in finding which is the best method if i was to need to do this again I did attempt to dive into Wordpress feed and API listing to see if i could grab the vimeo IDs and titles but didn't find a method (Not saying its not possible but with my limited knowledge i couldn't find anything)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_byympqd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading Website with external Embedded Vimeo videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akglyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707243876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking to download content from a WordPress site that functions like a &amp;quot;Netflix,&amp;quot; complete with embedded &amp;quot;private&amp;quot; Vimeo videos. My technical skills are somewhat limited, so I&amp;#39;m sharing the method I used, though I&amp;#39;m sure it&amp;#39;s not the most efficient.&lt;/p&gt;\n\n&lt;p&gt;Initially, I tried tools like httptrack and wget without success. Eventually, I resorted to manually gathering a list of all HTML files using the Linkclump Chrome extension. Then, I imported these files into Internet Download Manager. I also converted the HTML files to TXT files (although this step may have been unnecessary) and merged them into a single file using a simple command in the command prompt.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;copy *.txt merged_file.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next, I used Notepad++ to filter out only the embedded Vimeo code, which I then imported into Excel to create a list of video IDs. With these IDs, I utilized YT-DLP, employing the cookie and referer method, to successfully download the videos.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;yt-dlp --cookies C:\\Users\\valentine\\Desktop\\cookie.txt --referer https://pgnpiano.com/ https://player.vimeo.com/video/123456789\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;While this worked, it&amp;#39;s  lengthy and has its drawbacks, particularly regarding file naming if Vimeo titles are not properly named similar to the website format.&lt;/p&gt;\n\n&lt;p&gt;I would be interested in finding which is the best method if i was to need to do this again I did attempt to dive into Wordpress feed and API listing to see if i could grab the vimeo IDs and titles but didn&amp;#39;t find a method (Not saying its not possible but with my limited knowledge i couldn&amp;#39;t find anything)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1akglyw", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Creme3145", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akglyw/downloading_website_with_external_embedded_vimeo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akglyw/downloading_website_with_external_embedded_vimeo/", "subreddit_subscribers": 730850, "created_utc": 1707243876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am recursively hashing bunch of files stored in multiple directories. Which is the standard format to store the hash ?  \n  \nWhich format makes it possible to supply the hash file to an application and check all the files recursively ? Moreover the hash file should be application independent/can be read by different applications.", "author_fullname": "t2_tmqfxaxb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which format to store hash for multiple directories ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akfecu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707240928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am recursively hashing bunch of files stored in multiple directories. Which is the standard format to store the hash ?  &lt;/p&gt;\n\n&lt;p&gt;Which format makes it possible to supply the hash file to an application and check all the files recursively ? Moreover the hash file should be application independent/can be read by different applications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akfecu", "is_robot_indexable": true, "report_reasons": null, "author": "user6553620242", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akfecu/which_format_to_store_hash_for_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akfecu/which_format_to_store_hash_for_multiple/", "subreddit_subscribers": 730850, "created_utc": 1707240928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[The Script](https://github.com/wallacebrf/SMART-to-InfluxDB-Logger) and the associated dashboards also gather the drive model and serial numbers and display that data along with the drive's SMART pass/fail status\n\n[https://raw.githubusercontent.com/wallacebrf/SMART-to-InfluxDB-Logger/main/images/smartscreen\\_HDD.png](https://raw.githubusercontent.com/wallacebrf/SMART-to-InfluxDB-Logger/main/images/smartscreen_HDD.png)", "author_fullname": "t2_1swadf0h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD and NVME SMART data logging to InfluxDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ake724", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707237918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/wallacebrf/SMART-to-InfluxDB-Logger\"&gt;The Script&lt;/a&gt; and the associated dashboards also gather the drive model and serial numbers and display that data along with the drive&amp;#39;s SMART pass/fail status&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://raw.githubusercontent.com/wallacebrf/SMART-to-InfluxDB-Logger/main/images/smartscreen_HDD.png\"&gt;https://raw.githubusercontent.com/wallacebrf/SMART-to-InfluxDB-Logger/main/images/smartscreen_HDD.png&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?auto=webp&amp;s=d22913801f4b7c96868c92ac2cbf0260ca3a99a1", "width": 1920, "height": 6352}, "resolutions": [{"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af7ae746eaf0cf7d9eb1047ae7c772dd4ecc7f53", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=da8a5641fb7a8f04ba2281695a9822626f69de4d", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3e93be0299367bd274579db9c0d7429cfeec180", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2eb5c8170ec24a73e173bb3e467a465e33954dac", "width": 640, "height": 1280}, {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2db03d2680f9ed8fdd3298584a1a3f91cec2209c", "width": 960, "height": 1920}, {"url": "https://external-preview.redd.it/lDmUsBTozBrwrrTHQTlsAJe4tyaTstxI7K6CXYr8NWw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b1b6dde9554740cb37cfc1b14f21511b0b0dbb6", "width": 1080, "height": 2160}], "variants": {}, "id": "Kv6oObXJoeQiyg8R36_l-2KRe6Mh9tnFqbpAzx8zrmw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1ake724", "is_robot_indexable": true, "report_reasons": null, "author": "wallacebrf", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1ake724/hdd_and_nvme_smart_data_logging_to_influxdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1ake724/hdd_and_nvme_smart_data_logging_to_influxdb/", "subreddit_subscribers": 730850, "created_utc": 1707237918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Writing some 500gb worth of data, is it normal for hard drives to make this sound during writing?", "author_fullname": "t2_575d1gv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a normal write sound?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1aktyqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/h72leewo73hc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 720, "scrubber_media_url": "https://v.redd.it/h72leewo73hc1/DASH_96.mp4", "dash_url": "https://v.redd.it/h72leewo73hc1/DASHPlaylist.mpd?a=1709883508%2CZDhhMzIxNTczYWM0ZmU3ODc3MDk5ZjgwZjlmMGM0NWQyYjFjMTY2ODY0NDZhMWVlYzQyZGM1N2VlNzMzOTk3Mw%3D%3D&amp;v=1&amp;f=sd", "duration": 20, "hls_url": "https://v.redd.it/h72leewo73hc1/HLSPlaylist.m3u8?a=1709883508%2CNGViYWViMDk4ZWRmNTlmZWU3ODk2MzZmYWZkYzgyMjVmNWU1MDRlZjgwOTdjZjA3YzM2YTkxODY3OGZmMWMzMQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=aebf857d20efd1966cd9ce62324eece331e71cbf", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707278559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Writing some 500gb worth of data, is it normal for hard drives to make this sound during writing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/h72leewo73hc1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?format=pjpg&amp;auto=webp&amp;s=f505f79165aa088dc42fb7f5a4e4ecfdc63c2132", "width": 936, "height": 1665}, "resolutions": [{"url": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=689805ca1c7fc36535ea81e9ff8c5504fadd942f", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7f8580ea957f2d6fa415f1d2353c336c033c874c", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f7bd2f689576fdff7aa470e2c8355689b9f9d2aa", "width": 320, "height": 569}, {"url": "https://external-preview.redd.it/aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=377a5416e40f40b26adb530a15ce31ec7515a7a5", "width": 640, "height": 1138}], "variants": {}, "id": "aTcxNjkyc283M2hjMQaw0lPZixp-uywrUnA_jAnSUr5Gz1GsdTQDZuD7sPDH"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1aktyqz", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Safe", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1aktyqz/is_this_a_normal_write_sound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/h72leewo73hc1", "subreddit_subscribers": 730850, "created_utc": 1707278559.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/h72leewo73hc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 720, "scrubber_media_url": "https://v.redd.it/h72leewo73hc1/DASH_96.mp4", "dash_url": "https://v.redd.it/h72leewo73hc1/DASHPlaylist.mpd?a=1709883508%2CZDhhMzIxNTczYWM0ZmU3ODc3MDk5ZjgwZjlmMGM0NWQyYjFjMTY2ODY0NDZhMWVlYzQyZGM1N2VlNzMzOTk3Mw%3D%3D&amp;v=1&amp;f=sd", "duration": 20, "hls_url": "https://v.redd.it/h72leewo73hc1/HLSPlaylist.m3u8?a=1709883508%2CNGViYWViMDk4ZWRmNTlmZWU3ODk2MzZmYWZkYzgyMjVmNWU1MDRlZjgwOTdjZjA3YzM2YTkxODY3OGZmMWMzMQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long story short, I have a server (Location A) + 6 work stations (Location B-G) at my business (they're all on the same network)\n\nI want to access the server in Location A from Location X (which is my private home and obviously NOT on the same internet)  \n\n\nI've looked into RDP, VPN, FileZilla and I still can't get it to work. \n\n&amp;#x200B;\n\nHow can I get to work? ", "author_fullname": "t2_jxslh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Access Network Drive From Remote Computer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aktide", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707277151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, I have a server (Location A) + 6 work stations (Location B-G) at my business (they&amp;#39;re all on the same network)&lt;/p&gt;\n\n&lt;p&gt;I want to access the server in Location A from Location X (which is my private home and obviously NOT on the same internet)  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked into RDP, VPN, FileZilla and I still can&amp;#39;t get it to work. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How can I get to work? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1aktide", "is_robot_indexable": true, "report_reasons": null, "author": "hotlineforhelp", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1aktide/access_network_drive_from_remote_computer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1aktide/access_network_drive_from_remote_computer/", "subreddit_subscribers": 730850, "created_utc": 1707277151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\n&amp;#x200B;\n\nSo I haven't really done this before much with TV Shows but I was trying to use MakeMKV to remux these ISO files for this TV Show. It's split across 4 ISOS and when I put one of them into MakeMKV it spits out 1 MKV for like 3-4 episodes. I mounted the ISOS and found that that all episodes are their own VOB files more or less.  \n\n\nI don't really know how to proceed... is there something I'm missing in terms of MakeMKV? Do I just convert the vob files?  ", "author_fullname": "t2_g85w9v3fx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TV Show spread across multiple ISOs...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akqt7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707269286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I haven&amp;#39;t really done this before much with TV Shows but I was trying to use MakeMKV to remux these ISO files for this TV Show. It&amp;#39;s split across 4 ISOS and when I put one of them into MakeMKV it spits out 1 MKV for like 3-4 episodes. I mounted the ISOS and found that that all episodes are their own VOB files more or less.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really know how to proceed... is there something I&amp;#39;m missing in terms of MakeMKV? Do I just convert the vob files?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akqt7a", "is_robot_indexable": true, "report_reasons": null, "author": "gaydevi", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akqt7a/tv_show_spread_across_multiple_isos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akqt7a/tv_show_spread_across_multiple_isos/", "subreddit_subscribers": 730850, "created_utc": 1707269286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have a lot of folders and files I want to organize and tag. So that later i can search/filter results with multiple tags. I thought id go with excel, put all tags in a single cell separated by \";\" but this way i can only search/filter one tag.\n\nEnd result i desire is  search-bar functions on sites like rule34 or various boorus. where you can just type various tags. an exclude function would also be good.\n\nCan you recommend any sofware/ approach/ file that could work for me?", "author_fullname": "t2_bb9q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Folder tagging and multiple tag search/filter ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akg7wl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707242925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a lot of folders and files I want to organize and tag. So that later i can search/filter results with multiple tags. I thought id go with excel, put all tags in a single cell separated by &amp;quot;;&amp;quot; but this way i can only search/filter one tag.&lt;/p&gt;\n\n&lt;p&gt;End result i desire is  search-bar functions on sites like rule34 or various boorus. where you can just type various tags. an exclude function would also be good.&lt;/p&gt;\n\n&lt;p&gt;Can you recommend any sofware/ approach/ file that could work for me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akg7wl", "is_robot_indexable": true, "report_reasons": null, "author": "ViVaVl29", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akg7wl/folder_tagging_and_multiple_tag_searchfilter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akg7wl/folder_tagging_and_multiple_tag_searchfilter/", "subreddit_subscribers": 730850, "created_utc": 1707242925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In [archive.ph](https://archive.ph) there is an option to download zip. However this button has been broken for some time. Do you know if there is an alternative way to do this? I mean, saving [as.zip](https://as.zip) pages archived already in archive.ph", "author_fullname": "t2_non6fur6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download page from archive.ph", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akdcep", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707235780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In &lt;a href=\"https://archive.ph\"&gt;archive.ph&lt;/a&gt; there is an option to download zip. However this button has been broken for some time. Do you know if there is an alternative way to do this? I mean, saving &lt;a href=\"https://as.zip\"&gt;as.zip&lt;/a&gt; pages archived already in archive.ph&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akdcep", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Stoicism", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akdcep/download_page_from_archiveph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akdcep/download_page_from_archiveph/", "subreddit_subscribers": 730850, "created_utc": 1707235780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nNeed to connect Micron 9400 Pro SSD to Asus Pro WS TRX50-SAGE WIFI.  \nThe board has \"1 x SlimSAS slot supports PCIe 4.0 x4\" which I think is SFF-8654 38pin, but I could not find a cable that has U.3 (SFF-8639) PCIe Gen 4 on the other side.\n\nAny help would be much appreciated! ", "author_fullname": "t2_9v26er5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SFF-8654 38pin to PCIe Gen4 NVMe U.3 ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1akc1nd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707232394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Need to connect Micron 9400 Pro SSD to Asus Pro WS TRX50-SAGE WIFI.&lt;br/&gt;\nThe board has &amp;quot;1 x SlimSAS slot supports PCIe 4.0 x4&amp;quot; which I think is SFF-8654 38pin, but I could not find a cable that has U.3 (SFF-8639) PCIe Gen 4 on the other side.&lt;/p&gt;\n\n&lt;p&gt;Any help would be much appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1akc1nd", "is_robot_indexable": true, "report_reasons": null, "author": "QuickWizards", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1akc1nd/sff8654_38pin_to_pcie_gen4_nvme_u3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1akc1nd/sff8654_38pin_to_pcie_gen4_nvme_u3/", "subreddit_subscribers": 730850, "created_utc": 1707232394.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}