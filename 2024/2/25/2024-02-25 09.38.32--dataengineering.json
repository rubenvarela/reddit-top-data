{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to prove a point.\n\nExecutives in every organization are jumping into AI which is an ongoing trend. \n\nHowever, it should not be at the expense of dismissing data engineering. \n\nIf data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. ", "author_fullname": "t2_aryc45smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we say Data Engineering is a precursor to AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqx22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708768510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to prove a point.&lt;/p&gt;\n\n&lt;p&gt;Executives in every organization are jumping into AI which is an ongoing trend. &lt;/p&gt;\n\n&lt;p&gt;However, it should not be at the expense of dismissing data engineering. &lt;/p&gt;\n\n&lt;p&gt;If data is used to train a model or fine-tune an existing model, data might come from the DE process down the line and this needs to be highlighted. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqx22", "is_robot_indexable": true, "report_reasons": null, "author": "Paperplaneflyr", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqx22/can_we_say_data_engineering_is_a_precursor_to_ai/", "subreddit_subscribers": 163453, "created_utc": 1708768510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**\\[Repo\\]** [https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)\n\n# Hello Data enthusiasts! \ud83d\ude4b\ud83c\udffd\u200d\u2642\ufe0f\n\nhttps://preview.redd.it/70u7nk1sknkc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=bbe7a09f4e45c4313b6c21b7716e347a54ed8646\n\nI\u2019m an engineer by heart and a data enthusiast by passion. I have been working with data teams for the past 10 years and have seen the data landscape evolve from **traditional databases** to **modern data lakes** and **data warehouses**.\n\nIn previous roles, I\u2019ve been working closely with customers of AdTech, MarTech and Fintech companies. As an engineer, I\u2019ve built features and products that helped marketers, advertisers and B2C companies engage with their customers better. Dealing with vast amounts of data, that either came from online or offline sources, I always found myself in the middle of newer challenges that came with the data.\n\nOne of the biggest challenges I\u2019ve faced is the ability to move data from one system to another. This is a problem that has been around for a long time and is often referred to as **Extract, Transform, Load (ETL)**. Consolidating data from multiple sources and storing it in a single place is a common problem and while working with teams, I have built custom ETL pipelines to solve this problem.\n\nHowever, there were no mature platforms that could solve this problem at scale. Then as **AWS Glue, Google Dataflow and Apache Nifi** came into the picture, I started to see a shift in the way data was being moved around. Many OSS platforms like *Airbyte, Meltano and Dagster* have come up in recent years to solve this problem.\n\nNow that we are at the cusp of a new era in modern data stacks, 7 out of 10 are using cloud data warehouses and data lakes.\n\nThis has now made life easier for data engineers, especially when I was struggling with ETL pipelines. But later in my career, I started to see a new problem emerge. When marketers, sales teams and growth teams operate with top-of-the-funnel data, while most of the data is stored in the data warehouse, it is not accessible to them, which is a big problem.\n\nThen I saw data teams and growth teams operate in silos. Data teams were busy building ETL pipelines and maintaining the data warehouse. In contrast, growth teams were busy using tools like **Braze, Facebook Ads, Google Ads, Salesforce, Hubspot**, etc. to engage with their customers.\n\n# \ud83d\udcab The Genesis of Multiwoven\n\nAt the initial stages of Multiwoven, our initial idea was to build a product notification platform for product teams, to help them send targeted notifications to their users. But as we started to talk to more customers, we realized that the problem of data silos was much bigger than we thought. We realized that the problem of data silos was not just limited to product teams, but was a problem that was faced by every team in the company.\n\nThat\u2019s when we decided to pivot and build Multiwoven, a **reverse ETL** platform that helps companies move data from their data warehouse to their SaaS platforms. We wanted to build a platform that would help companies make their data actionable across different SaaS platforms.\n\n# \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb Why Open Source?\n\nAs a team, we are strong believers in open source, and the reason behind going open source was twofold. Firstly, cost was always a counterproductive aspect for teams using commercial SAAS platforms. Secondly, we wanted to build a flexible and customizable platform that could give companies the control and governance they needed.\n\n***This has been our humble beginning and we are excited to see where this journey takes us. We are excited to see the impact we can make in the data activation landscape.***\n\n&gt;*Please \u2b50 star our* [*repo on Github*](https://github.com/Multiwoven/multiwoven) *and show us some love. We are always looking for feedback and would love to hear from you.*\n\n**\\[Repo\\]** [https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)", "author_fullname": "t2_rtrd3q97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why I Decided to Build Multiwoven: an Open-source Reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"70u7nk1sknkc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c6d84875ad9fe5b5384172183347cf448fdcfb0"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd6516d2e51489dc8606d32ec9b79b4fd4e50bd3"}, {"y": 150, "x": 320, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aae6942bf1338a4b2ce06405ced4fc71a639fad0"}, {"y": 301, "x": 640, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3e630aa87bd6d294e84b1f5d222023bcd78b0e2"}, {"y": 451, "x": 960, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f4bf4e36b2298f0e7b75acf8252dfdff2549f62"}, {"y": 507, "x": 1080, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74ebf30a798c3c304ae8ff369a9d63761e168396"}], "s": {"y": 1204, "x": 2560, "u": "https://preview.redd.it/70u7nk1sknkc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=bbe7a09f4e45c4313b6c21b7716e347a54ed8646"}, "id": "70u7nk1sknkc1"}}, "name": "t3_1aze7db", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ba02ywnsMpACZ9VFplAkrz0QVt53tVe9DKUaRPfiIfE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1708832648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;[Repo]&lt;/strong&gt; &lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;https://github.com/Multiwoven/multiwoven&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Hello Data enthusiasts! \ud83d\ude4b\ud83c\udffd\u200d\u2642\ufe0f&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/70u7nk1sknkc1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbe7a09f4e45c4313b6c21b7716e347a54ed8646\"&gt;https://preview.redd.it/70u7nk1sknkc1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbe7a09f4e45c4313b6c21b7716e347a54ed8646&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019m an engineer by heart and a data enthusiast by passion. I have been working with data teams for the past 10 years and have seen the data landscape evolve from &lt;strong&gt;traditional databases&lt;/strong&gt; to &lt;strong&gt;modern data lakes&lt;/strong&gt; and &lt;strong&gt;data warehouses&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;In previous roles, I\u2019ve been working closely with customers of AdTech, MarTech and Fintech companies. As an engineer, I\u2019ve built features and products that helped marketers, advertisers and B2C companies engage with their customers better. Dealing with vast amounts of data, that either came from online or offline sources, I always found myself in the middle of newer challenges that came with the data.&lt;/p&gt;\n\n&lt;p&gt;One of the biggest challenges I\u2019ve faced is the ability to move data from one system to another. This is a problem that has been around for a long time and is often referred to as &lt;strong&gt;Extract, Transform, Load (ETL)&lt;/strong&gt;. Consolidating data from multiple sources and storing it in a single place is a common problem and while working with teams, I have built custom ETL pipelines to solve this problem.&lt;/p&gt;\n\n&lt;p&gt;However, there were no mature platforms that could solve this problem at scale. Then as &lt;strong&gt;AWS Glue, Google Dataflow and Apache Nifi&lt;/strong&gt; came into the picture, I started to see a shift in the way data was being moved around. Many OSS platforms like &lt;em&gt;Airbyte, Meltano and Dagster&lt;/em&gt; have come up in recent years to solve this problem.&lt;/p&gt;\n\n&lt;p&gt;Now that we are at the cusp of a new era in modern data stacks, 7 out of 10 are using cloud data warehouses and data lakes.&lt;/p&gt;\n\n&lt;p&gt;This has now made life easier for data engineers, especially when I was struggling with ETL pipelines. But later in my career, I started to see a new problem emerge. When marketers, sales teams and growth teams operate with top-of-the-funnel data, while most of the data is stored in the data warehouse, it is not accessible to them, which is a big problem.&lt;/p&gt;\n\n&lt;p&gt;Then I saw data teams and growth teams operate in silos. Data teams were busy building ETL pipelines and maintaining the data warehouse. In contrast, growth teams were busy using tools like &lt;strong&gt;Braze, Facebook Ads, Google Ads, Salesforce, Hubspot&lt;/strong&gt;, etc. to engage with their customers.&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udcab The Genesis of Multiwoven&lt;/h1&gt;\n\n&lt;p&gt;At the initial stages of Multiwoven, our initial idea was to build a product notification platform for product teams, to help them send targeted notifications to their users. But as we started to talk to more customers, we realized that the problem of data silos was much bigger than we thought. We realized that the problem of data silos was not just limited to product teams, but was a problem that was faced by every team in the company.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s when we decided to pivot and build Multiwoven, a &lt;strong&gt;reverse ETL&lt;/strong&gt; platform that helps companies move data from their data warehouse to their SaaS platforms. We wanted to build a platform that would help companies make their data actionable across different SaaS platforms.&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb Why Open Source?&lt;/h1&gt;\n\n&lt;p&gt;As a team, we are strong believers in open source, and the reason behind going open source was twofold. Firstly, cost was always a counterproductive aspect for teams using commercial SAAS platforms. Secondly, we wanted to build a flexible and customizable platform that could give companies the control and governance they needed.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;This has been our humble beginning and we are excited to see where this journey takes us. We are excited to see the impact we can make in the data activation landscape.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Please \u2b50 star our&lt;/em&gt; &lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;&lt;em&gt;repo on Github&lt;/em&gt;&lt;/a&gt; &lt;em&gt;and show us some love. We are always looking for feedback and would love to hear from you.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;[Repo]&lt;/strong&gt; &lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;https://github.com/Multiwoven/multiwoven&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?auto=webp&amp;s=7bee046b00e43a80d51ec06b5b03fab8fe50e8a6", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c820e531bd69e73f8ab0a6ae0beacd99c2b46eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c80be54ff0c4e74ab2f7726675fcebd3a845b46", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ac265f606906d37f473536da1c0f1402f557f04", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f31b53a48c8d26b2abdbd514797bd69ea9539bcf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d338fcd1ab24aecad9b1dcf39d5649f7cff2a29", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3be250d32cd9b8a61ac25ca44aa1a3ab73cb70a", "width": 1080, "height": 540}], "variants": {}, "id": "KXdCRFA3PFw6uZLyGfHzBPQBTSPbN50XvrsQE2m5iOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1aze7db", "is_robot_indexable": true, "report_reasons": null, "author": "nagstler", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aze7db/why_i_decided_to_build_multiwoven_an_opensource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aze7db/why_i_decided_to_build_multiwoven_an_opensource/", "subreddit_subscribers": 163453, "created_utc": 1708832648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to pick 2 out of the following, please advice on which course(s) packs the most punch.  \n\n\nCurrently a data analyst with no ceritifications to my name. I would like to progress towards a data engineering role but not sure where to start in terms of getting a cert under my name.\n\n**IBM**:\n\n* \"IBM Data Engineering Professional Certificate\" and \"IBM Data Engineering Fundamentals.\"\n* Additionally, IBM provides courses and certifications in specific data-related technologies such as IBM Db2, IBM Cloud Pak for Data, and IBM Watson Studio.\n\n**AWS (Amazon Web Services)**:\n\n* \"AWS Certified Big Data - Specialty\" certification.\n* AWS training covers various data-related services such as Amazon Redshift, Amazon EMR (Elastic MapReduce), AWS Glue, AWS Data Pipeline, and Amazon Kinesis.\n\n**Google Cloud**:\n\n* \"Google Cloud Certified - Professional Data Engineer\" certification.\n* Google Cloud training covers services like Google BigQuery, Google Cloud Dataflow, Google Cloud Dataprep, Google Cloud Pub/Sub, and others.\n\n**Microsoft Azure**:\n\n* \"Microsoft Certified: Azure Data Engineer Associate\" certification.\n* Azure training includes services like Azure SQL Database, Azure Synapse Analytics (formerly Azure SQL Data Warehouse), Azure Data Lake Storage, Azure Databricks, and Azure HDInsight.\n\n**Cloudera**:\n\n* \"Cloudera Certified Associate (CCA) Data Analyst\" and \"Cloudera Certified Professional (CCP) Data Engineer\" certifications.\n* Cloudera training covers Apache Hadoop, Apache Spark, Apache Hive, Apache Impala, and other related technologies.", "author_fullname": "t2_2btsrky1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should be my first ceritificate?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayylbi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708792356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708792077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pick 2 out of the following, please advice on which course(s) packs the most punch.  &lt;/p&gt;\n\n&lt;p&gt;Currently a data analyst with no ceritifications to my name. I would like to progress towards a data engineering role but not sure where to start in terms of getting a cert under my name.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;IBM&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;IBM Data Engineering Professional Certificate&amp;quot; and &amp;quot;IBM Data Engineering Fundamentals.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Additionally, IBM provides courses and certifications in specific data-related technologies such as IBM Db2, IBM Cloud Pak for Data, and IBM Watson Studio.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;AWS (Amazon Web Services)&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;AWS Certified Big Data - Specialty&amp;quot; certification.&lt;/li&gt;\n&lt;li&gt;AWS training covers various data-related services such as Amazon Redshift, Amazon EMR (Elastic MapReduce), AWS Glue, AWS Data Pipeline, and Amazon Kinesis.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Google Cloud&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;Google Cloud Certified - Professional Data Engineer&amp;quot; certification.&lt;/li&gt;\n&lt;li&gt;Google Cloud training covers services like Google BigQuery, Google Cloud Dataflow, Google Cloud Dataprep, Google Cloud Pub/Sub, and others.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Microsoft Azure&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;Microsoft Certified: Azure Data Engineer Associate&amp;quot; certification.&lt;/li&gt;\n&lt;li&gt;Azure training includes services like Azure SQL Database, Azure Synapse Analytics (formerly Azure SQL Data Warehouse), Azure Data Lake Storage, Azure Databricks, and Azure HDInsight.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Cloudera&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;Cloudera Certified Associate (CCA) Data Analyst&amp;quot; and &amp;quot;Cloudera Certified Professional (CCP) Data Engineer&amp;quot; certifications.&lt;/li&gt;\n&lt;li&gt;Cloudera training covers Apache Hadoop, Apache Spark, Apache Hive, Apache Impala, and other related technologies.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ayylbi", "is_robot_indexable": true, "report_reasons": null, "author": "yungfilly", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayylbi/what_should_be_my_first_ceritificate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayylbi/what_should_be_my_first_ceritificate/", "subreddit_subscribers": 163453, "created_utc": 1708792077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I\u2019ve been working as a Web Developer for 2 years and I would like to change my career in the direction of Data Science/Data Engineering. For now I try to understand in depth what is the difference between those fields and if Data Engineering would be the right choice for me. \n\nCould some data engineers tell me how your working day looks like? Is it more about writing a code or about designing databases? \n\nI like python and have some experience with web development with Django as well as with pandas, numpy and PySpark. Besides I like theoretical mathematics. After reading some short descriptions of Data Science/Machine Learning/AI I feel like I would be the most interested in ML, but maybe Data Engineering would be a good entry point in this direction. ", "author_fullname": "t2_tz7awh8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a position as a Junior Data Engineer a good entry point towards working with data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aytrbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708778920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019ve been working as a Web Developer for 2 years and I would like to change my career in the direction of Data Science/Data Engineering. For now I try to understand in depth what is the difference between those fields and if Data Engineering would be the right choice for me. &lt;/p&gt;\n\n&lt;p&gt;Could some data engineers tell me how your working day looks like? Is it more about writing a code or about designing databases? &lt;/p&gt;\n\n&lt;p&gt;I like python and have some experience with web development with Django as well as with pandas, numpy and PySpark. Besides I like theoretical mathematics. After reading some short descriptions of Data Science/Machine Learning/AI I feel like I would be the most interested in ML, but maybe Data Engineering would be a good entry point in this direction. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aytrbb", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Affect4004", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aytrbb/is_a_position_as_a_junior_data_engineer_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aytrbb/is_a_position_as_a_junior_data_engineer_a_good/", "subreddit_subscribers": 163453, "created_utc": 1708778920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm a 25 yo management consultant at a top-tier consulting firm and currently make around $130K per year before bonus. I don't particularly like my job, it can be very daunting working 60-80 hours per week building slides and conducting basic analyses in Excel. However, recently I was staffed on a data strategy study and got exposed on a high level to different data concepts (e.g., data architecture, data governance, etc.) the topics seemed very interesting to me. After that study I requested to get my hands dirty with data so I got staffed on a study that involved deploying a tool that does some geospatial analysis. The tool is a bunch of Jupyter notebooks written as one colleague describes a bunch of \"shitcode\" mainly using pandas and some visualization packages. The study was fun and interesting but it was difficult to learn best practices (e.g., how to write clean code, version control, etc.) as the timelines were tight and there was no mentor to provide guidance.\n\nI think I'm interested in DE as a field and considering investing my time over the next 4-6 months learning DE on the side (after work hours and mainly during weekends) with the ultimate goal of switching to a dedicated data engineering job. I did some research and I think I have a solid understanding of the concepts that I need to learn to break into DE. That being said the only thing holding me back is the pay cut I'm going to take pursuing this transition given the effort I'm going invest over the next months worth it.\n\nWould appreciate your thoughts as DE practitioners on this.\n\nSome context on my background, I graduated with a degree in civil engineering in 2020 and then pursued a master's in transportation engineering graduating in 2021. Been doing consulting work across two different firms.", "author_fullname": "t2_7twkihop", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Management Consulting to Data Engineering? Worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2aq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708801121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m a 25 yo management consultant at a top-tier consulting firm and currently make around $130K per year before bonus. I don&amp;#39;t particularly like my job, it can be very daunting working 60-80 hours per week building slides and conducting basic analyses in Excel. However, recently I was staffed on a data strategy study and got exposed on a high level to different data concepts (e.g., data architecture, data governance, etc.) the topics seemed very interesting to me. After that study I requested to get my hands dirty with data so I got staffed on a study that involved deploying a tool that does some geospatial analysis. The tool is a bunch of Jupyter notebooks written as one colleague describes a bunch of &amp;quot;shitcode&amp;quot; mainly using pandas and some visualization packages. The study was fun and interesting but it was difficult to learn best practices (e.g., how to write clean code, version control, etc.) as the timelines were tight and there was no mentor to provide guidance.&lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;m interested in DE as a field and considering investing my time over the next 4-6 months learning DE on the side (after work hours and mainly during weekends) with the ultimate goal of switching to a dedicated data engineering job. I did some research and I think I have a solid understanding of the concepts that I need to learn to break into DE. That being said the only thing holding me back is the pay cut I&amp;#39;m going to take pursuing this transition given the effort I&amp;#39;m going invest over the next months worth it.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate your thoughts as DE practitioners on this.&lt;/p&gt;\n\n&lt;p&gt;Some context on my background, I graduated with a degree in civil engineering in 2020 and then pursued a master&amp;#39;s in transportation engineering graduating in 2021. Been doing consulting work across two different firms.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1az2aq7", "is_robot_indexable": true, "report_reasons": null, "author": "socidad", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az2aq7/management_consulting_to_data_engineering_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1az2aq7/management_consulting_to_data_engineering_worth_it/", "subreddit_subscribers": 163453, "created_utc": 1708801121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "All, \n\n\r  \nI am currently 35yr with 15 years of experience overall and 5 years of data analytics with some data engineering type work, NLP, and strategy.  I have also done database design and app development this is all wrapped up in the title \"Consultant\". I have certifications in Power BI Associate, Azure Data Fundamentals, and Databricks Fundaments. I have manufacturing management experience and a technician background, as well. I also like to dabble in UX/UI events. I have an undergrad in BS Organizational Leadership.\n\n\r  \nI feel that I would like to become a data architect, Senior Data Engineer, or digital transformation type work. To obtain these roles, I feel that my non-STEM degree will hold me back and would like to get a Masters, among other reasons. I am very fortunate to be in a position where money is not an issue with respect to college tuition.  I feel my calling is to go to the next level within the field.\n\n\r  \nI have been accepted into Syracuse Applied Data Science(online) program and UVA MSBA (hybrid). I am conflicted on which program I should pick. Looking at both the curriculums they are both very similar in the topics they cover, and both are complementary to my background.  \n\n\r  \nDoes anyone have any insight or recommendations for which program to pick?", "author_fullname": "t2_8yface1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MSDS or MSBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azc02d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708825944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All, &lt;/p&gt;\n\n&lt;p&gt;I am currently 35yr with 15 years of experience overall and 5 years of data analytics with some data engineering type work, NLP, and strategy.  I have also done database design and app development this is all wrapped up in the title &amp;quot;Consultant&amp;quot;. I have certifications in Power BI Associate, Azure Data Fundamentals, and Databricks Fundaments. I have manufacturing management experience and a technician background, as well. I also like to dabble in UX/UI events. I have an undergrad in BS Organizational Leadership.&lt;/p&gt;\n\n&lt;p&gt;I feel that I would like to become a data architect, Senior Data Engineer, or digital transformation type work. To obtain these roles, I feel that my non-STEM degree will hold me back and would like to get a Masters, among other reasons. I am very fortunate to be in a position where money is not an issue with respect to college tuition.  I feel my calling is to go to the next level within the field.&lt;/p&gt;\n\n&lt;p&gt;I have been accepted into Syracuse Applied Data Science(online) program and UVA MSBA (hybrid). I am conflicted on which program I should pick. Looking at both the curriculums they are both very similar in the topics they cover, and both are complementary to my background.  &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any insight or recommendations for which program to pick?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1azc02d", "is_robot_indexable": true, "report_reasons": null, "author": "Benmagz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1azc02d/msds_or_msba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1azc02d/msds_or_msba/", "subreddit_subscribers": 163453, "created_utc": 1708825944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have many on-premises relational DBMSs.... MS SQL server, MySQL, Postgres and Oracle. Is there a tool that can read the logs of these servers and do the Change Data Capture?\n\nI would like to use the same tool for all DBMSs.\nI'm not sure, but I heard that Fivetran can do it. Is it true?\n\nAre there alternative solutions? So that Fivetran is expensive.", "author_fullname": "t2_rvp7wzgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best tool to do CDC based on log?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az5wrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708810043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many on-premises relational DBMSs.... MS SQL server, MySQL, Postgres and Oracle. Is there a tool that can read the logs of these servers and do the Change Data Capture?&lt;/p&gt;\n\n&lt;p&gt;I would like to use the same tool for all DBMSs.\nI&amp;#39;m not sure, but I heard that Fivetran can do it. Is it true?&lt;/p&gt;\n\n&lt;p&gt;Are there alternative solutions? So that Fivetran is expensive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1az5wrc", "is_robot_indexable": true, "report_reasons": null, "author": "nivlek_miroma", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az5wrc/what_is_the_best_tool_to_do_cdc_based_on_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1az5wrc/what_is_the_best_tool_to_do_cdc_based_on_log/", "subreddit_subscribers": 163453, "created_utc": 1708810043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI'm working on a personal project to help with learning dimensional modelling and building pipelines with DataBricks and ADF, but things aren't coming together in my head. \n\nMy project is quite basic. I have a single sample 3rd normal form DB, and have designed a Star Schema based around order line items.\n\nI've so far created an ADF pipeline to incrementally extract data on a daily basis into a raw storage area (bronze) as Parquet. Format in Bronze area:\n\n    - Office\n        - 20-03-2024\n        - 21-03-2024\n    - Employee\n        - 20-03-2024\n        - 21-03-2024\n    etc\n\nI'd like to then have a persistent processed/cleaned layer (silver), before then loading into a presentation layer (gold) - which is where I'll have the star schema.\n\nThe schema I've designed involves some tables being joined (e.g., Office &amp; Employee will be joined to form a Sales Rep dimension table). It also involves the use of both SCD1 for some columns, and SCD2 for others.\n\nHowever, not 100% sure on the next steps. \n\nTo take the Office table as an example, I've considered having a single table/area for this in the silver layer, where I just append or update records based on what's coming from Bronze each day. Therefore this table would need to have effective date and end date columns (for SCD2) as well as a surrogate key column. \n\nThen when moving from Silver to Gold, in this example, I would do some kind of join between Office and Employee to form the Sales Rep Dimension, only using records that were updated and appended during current run, and do some kind of upsert to the relevant Gold layer dim table.\n\nI'm just worried I'm either overcomplicating this or missing something, and want to make sure my over-arching idea makes sense. \n\nCan anyone provide some advice, or let me know how you might approach this?", "author_fullname": "t2_osh2xtjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices and rules for moving data through Bonze, Silver, and Gold Layers when a Star Schema &amp; SCD2 are required", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2ofy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708802038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a personal project to help with learning dimensional modelling and building pipelines with DataBricks and ADF, but things aren&amp;#39;t coming together in my head. &lt;/p&gt;\n\n&lt;p&gt;My project is quite basic. I have a single sample 3rd normal form DB, and have designed a Star Schema based around order line items.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve so far created an ADF pipeline to incrementally extract data on a daily basis into a raw storage area (bronze) as Parquet. Format in Bronze area:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;- Office\n    - 20-03-2024\n    - 21-03-2024\n- Employee\n    - 20-03-2024\n    - 21-03-2024\netc\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;d like to then have a persistent processed/cleaned layer (silver), before then loading into a presentation layer (gold) - which is where I&amp;#39;ll have the star schema.&lt;/p&gt;\n\n&lt;p&gt;The schema I&amp;#39;ve designed involves some tables being joined (e.g., Office &amp;amp; Employee will be joined to form a Sales Rep dimension table). It also involves the use of both SCD1 for some columns, and SCD2 for others.&lt;/p&gt;\n\n&lt;p&gt;However, not 100% sure on the next steps. &lt;/p&gt;\n\n&lt;p&gt;To take the Office table as an example, I&amp;#39;ve considered having a single table/area for this in the silver layer, where I just append or update records based on what&amp;#39;s coming from Bronze each day. Therefore this table would need to have effective date and end date columns (for SCD2) as well as a surrogate key column. &lt;/p&gt;\n\n&lt;p&gt;Then when moving from Silver to Gold, in this example, I would do some kind of join between Office and Employee to form the Sales Rep Dimension, only using records that were updated and appended during current run, and do some kind of upsert to the relevant Gold layer dim table.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just worried I&amp;#39;m either overcomplicating this or missing something, and want to make sure my over-arching idea makes sense. &lt;/p&gt;\n\n&lt;p&gt;Can anyone provide some advice, or let me know how you might approach this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1az2ofy", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataPanda", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az2ofy/best_practices_and_rules_for_moving_data_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1az2ofy/best_practices_and_rules_for_moving_data_through/", "subreddit_subscribers": 163453, "created_utc": 1708802038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company's data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  \n\n\nWith that said...  \n\n\n1. What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  \n\n\n2. There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.", "author_fullname": "t2_m0llzgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proper handling, storage and cleaning of manual extracts from a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aysd2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708774029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my job, I rely on some important data contained within a relational database for several data pipelines. The only way anyone can access the data contained within this database is by asking a specific team (call them Team A) at the company to provide a manual extract (atm via a .csv file) on a weekly basis. I am relatively new at the company, but I am not even sure if this Team A actually have read access to the data, nonetheless there are contractors involved who seem to have carved out a position for themselves where only they have write access to this database of our company&amp;#39;s data, whereas nobody at the company itself has write access (and only 1 or 2 of us - if any - have read access). This seems like a silly way of doing things to me but I cannot change anything about it in the near future and have to work within these constraints of receiving manual extracts from the database.  &lt;/p&gt;\n\n&lt;p&gt;With that said...  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What is the best way of storing this data? My thinking is that I just keep the manual extracts within S3, pass those through a python script with polars/duckDB to do a few transformations, then load that into a postgres database on RDS. This seems like I am storing the same data 3 times, but is this just my only option here?  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are quite a few mistakes in the .csv extracts such as misspellings, wrong numbers in certain columns, incorrect naming conventions, etc. that really just need manual cleaning. Since we do not have write access to the original database, at which point in the pipeline should we do these manual cleaning steps so that (a) we do not make any irreversible changes to the extracts we are sent; (b) the steps/logic for how we went to e.g. misspellings in the .csv to cleaner data is properly tracked and logged? This latter point is important - I do not want to just overwrite cells in the .csv files we were sent and be unable to trace exactly what we changed, how much we changed, etc.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aysd2j", "is_robot_indexable": true, "report_reasons": null, "author": "Bhagafat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aysd2j/proper_handling_storage_and_cleaning_of_manual/", "subreddit_subscribers": 163453, "created_utc": 1708774029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All , \n\nWe are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 \n\nI am currently looking at \n- Airbyte \n- Fivetran \n- Meltano \n\nI am looking at combination of Airbyte and Meltano \n\nAirbyte for OOB low Volume and simple Low Code Scenerios \n\nMeltano for heavy lifting \n\nFivetran is out of picture due to the price and ROI we will be making out of it \n\nI am stuck at a point where I need evaluate below aspects \n\n- Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot \n- how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times \n- how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .\n\nThanks\nSiddu Hussain V\n\n\n", "author_fullname": "t2_7yg4aphe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you perform Load testing and security measures for a ELT tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayqbu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708766087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All , &lt;/p&gt;\n\n&lt;p&gt;We are trying to evaluate different ELT tools for data extraction from SAAS tools like Zoom , Asana , Slack , Glean , 8x8 &lt;/p&gt;\n\n&lt;p&gt;I am currently looking at \n- Airbyte \n- Fivetran \n- Meltano &lt;/p&gt;\n\n&lt;p&gt;I am looking at combination of Airbyte and Meltano &lt;/p&gt;\n\n&lt;p&gt;Airbyte for OOB low Volume and simple Low Code Scenerios &lt;/p&gt;\n\n&lt;p&gt;Meltano for heavy lifting &lt;/p&gt;\n\n&lt;p&gt;Fivetran is out of picture due to the price and ROI we will be making out of it &lt;/p&gt;\n\n&lt;p&gt;I am stuck at a point where I need evaluate below aspects &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cost of using Airbyte OSS like compute cost if any of you have used it have usage and cost it will help me a lot &lt;/li&gt;\n&lt;li&gt;how do I perform Stress Testing for both Airbyte and Meltano . Something like load 100k records and see the times &lt;/li&gt;\n&lt;li&gt;how do I make sure as security audit that any tools like this don\u2019t miss use data . Like if there are data leaks from our sources .&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks\nSiddu Hussain V&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ayqbu3", "is_robot_indexable": true, "report_reasons": null, "author": "Immediate-Force6602", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ayqbu3/how_do_you_perform_load_testing_and_security/", "subreddit_subscribers": 163453, "created_utc": 1708766087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with a finance reporting queries. Let's say I have one table called 'finance_entries'. I am using python in my codebase. I set up a connection to a postgressql dB set up on gcp. \nI need to query this table to get different information such as no_of_transactions (as an int) , invalid_tractions(as a data frame), etc.\n\nWould it be better to have one giant query extracting a data frame/ or view with all my conditions, where I have separate functions to get the data I need from that data frame. \n\nOR\n\nWould it be better to have each function have its own query to the table. \n\nI personally think option 2 is cleaner. Would like everyone's opinion and if possible some supporting resource for the rationale. Because I'm gonna need to convince my manager. \n", "author_fullname": "t2_ng90q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Single query or split queries by function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azhvje", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708845228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with a finance reporting queries. Let&amp;#39;s say I have one table called &amp;#39;finance_entries&amp;#39;. I am using python in my codebase. I set up a connection to a postgressql dB set up on gcp. \nI need to query this table to get different information such as no_of_transactions (as an int) , invalid_tractions(as a data frame), etc.&lt;/p&gt;\n\n&lt;p&gt;Would it be better to have one giant query extracting a data frame/ or view with all my conditions, where I have separate functions to get the data I need from that data frame. &lt;/p&gt;\n\n&lt;p&gt;OR&lt;/p&gt;\n\n&lt;p&gt;Would it be better to have each function have its own query to the table. &lt;/p&gt;\n\n&lt;p&gt;I personally think option 2 is cleaner. Would like everyone&amp;#39;s opinion and if possible some supporting resource for the rationale. Because I&amp;#39;m gonna need to convince my manager. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1azhvje", "is_robot_indexable": true, "report_reasons": null, "author": "Pooop69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1azhvje/single_query_or_split_queries_by_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1azhvje/single_query_or_split_queries_by_function/", "subreddit_subscribers": 163453, "created_utc": 1708845228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im looking to build a system that aims to provide better ability to create and manage schedules in resource-constrained projects.\n\nWhat I mean by \u201cresource-constrained\u201d is actually kind of interesting when you think about the effects. Let\u2019s say you\u2019re building a house: you can get the flooring guys in and out within a day (maybe). Scale that project to a hotel though, now your flooring guys can only complete several units in a day. This constraint to select spaces is what I mean.\n\nFlooring might be a dependancy of walls though. And roofing might depend on walls. So you end up with task dependancies, and on a resource-constrained schedule that has an effect of causing all tasks to sort of cascade to completion throughout the project.\n\nThis produces a natural grouping of spaces. Spaces can now be grouped based on where tasks will be completed. For example, if spaces 1, 2, and 3 get flooring done on day 1, spaces 1, 2, and 3 would be a grouping. Similarly, space 4. 5, and 6 might get flooring done on day 2 while the wall guys are in spaces 1, 2, and 3. Spaces 4, 5, and 6 would be another grouping.\n\nOne advantageous effect of these groupings is that a manager can reassign a not-on-schedule space to a different group\u2026 a group performing tasks which are more downstream\u2026 in order to put that space back on-schedule. Though this has the adverse effect of making the new grouping\u2019s workload larger for the remaining duration of the project. Also, this kind of adjustment would be impossible for any spaces in the most downstream grouping.\n\nTypically, there\u2019s a project level set of tasks that all spaces inherit. This is good for easing the schedule creation process but might not precisely represent the reality of the project.\n\nTypically, this is managed via an Excel spreadsheet where tasks are laid out as column headers, space groupings as indices, and expected start dates as matrix values. The space groupings are just CSV formatted text of the spaces contained, making recognition easy. Cell formatting is used to indicate completion of a task within a space grouping. This encoding for status is unfortunate because if status between spaces within a group are desynchronized, say because one space is late, there is no good way to represent that on the schedule.\n\nOne benefit the guys using this thing don\u2019t want to give up is the ease of status changes though. They can open the Excel file, highlight the necessary cells, and accurately update the status for hundreds of spaces in a given second, forgoing any desynchronized spaces within their groups.\n\nWhatever the solution, they want better reporting into the overall state of completion and tardiness for all spaces, groupings, and tasks. They also want the data recorded for analysis of historical trends.\n\nSo Im mining these details for design requirements, like how spaces can elegantly swap between groups in order to be put \u201cback on-schedule.\u201d That would suggest that spaces are assigned tasks but not due dates for those tasks. Rather, spaces inherit their task due dates from the space grouping they\u2019re assigned to.\n\nAnyhow, I believe I can build the necessary backend logic, such as the application database. Perhaps even some basic forms to input data, as well as dashboards. As I\u2019m going through this though, the scheduling aspect feels like I\u2019m reinventing the wheel a bit. Secondly, I worry that a complex frontend will need to be built and that is not my expertise.\n\nI\u2019ve tested various less data-engineering solutions like MS Project, figuring those should be good enough. They\u2019re not good enough for various reasons but mostly because none are designed to manage thousands of tasks, spaces, etc. in elegant ways. The work to manage the project becomes too much when using many of these existing tools, especially when they don\u2019t have an API.\n\nEffectively, I\u2019m wondering if there\u2019s a tool or framework that\u2019s better suited for engineering these types of solutions. Handling schedule data in ways that I mention, helping users perform schedule changes and update statuses?\n\nI\u2019m about to build the whole thing from scratch. Are there tools out there to lift off segments of this workload and lighten the burden?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools For Schedule Management Systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azhkj0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708845763.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708844100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking to build a system that aims to provide better ability to create and manage schedules in resource-constrained projects.&lt;/p&gt;\n\n&lt;p&gt;What I mean by \u201cresource-constrained\u201d is actually kind of interesting when you think about the effects. Let\u2019s say you\u2019re building a house: you can get the flooring guys in and out within a day (maybe). Scale that project to a hotel though, now your flooring guys can only complete several units in a day. This constraint to select spaces is what I mean.&lt;/p&gt;\n\n&lt;p&gt;Flooring might be a dependancy of walls though. And roofing might depend on walls. So you end up with task dependancies, and on a resource-constrained schedule that has an effect of causing all tasks to sort of cascade to completion throughout the project.&lt;/p&gt;\n\n&lt;p&gt;This produces a natural grouping of spaces. Spaces can now be grouped based on where tasks will be completed. For example, if spaces 1, 2, and 3 get flooring done on day 1, spaces 1, 2, and 3 would be a grouping. Similarly, space 4. 5, and 6 might get flooring done on day 2 while the wall guys are in spaces 1, 2, and 3. Spaces 4, 5, and 6 would be another grouping.&lt;/p&gt;\n\n&lt;p&gt;One advantageous effect of these groupings is that a manager can reassign a not-on-schedule space to a different group\u2026 a group performing tasks which are more downstream\u2026 in order to put that space back on-schedule. Though this has the adverse effect of making the new grouping\u2019s workload larger for the remaining duration of the project. Also, this kind of adjustment would be impossible for any spaces in the most downstream grouping.&lt;/p&gt;\n\n&lt;p&gt;Typically, there\u2019s a project level set of tasks that all spaces inherit. This is good for easing the schedule creation process but might not precisely represent the reality of the project.&lt;/p&gt;\n\n&lt;p&gt;Typically, this is managed via an Excel spreadsheet where tasks are laid out as column headers, space groupings as indices, and expected start dates as matrix values. The space groupings are just CSV formatted text of the spaces contained, making recognition easy. Cell formatting is used to indicate completion of a task within a space grouping. This encoding for status is unfortunate because if status between spaces within a group are desynchronized, say because one space is late, there is no good way to represent that on the schedule.&lt;/p&gt;\n\n&lt;p&gt;One benefit the guys using this thing don\u2019t want to give up is the ease of status changes though. They can open the Excel file, highlight the necessary cells, and accurately update the status for hundreds of spaces in a given second, forgoing any desynchronized spaces within their groups.&lt;/p&gt;\n\n&lt;p&gt;Whatever the solution, they want better reporting into the overall state of completion and tardiness for all spaces, groupings, and tasks. They also want the data recorded for analysis of historical trends.&lt;/p&gt;\n\n&lt;p&gt;So Im mining these details for design requirements, like how spaces can elegantly swap between groups in order to be put \u201cback on-schedule.\u201d That would suggest that spaces are assigned tasks but not due dates for those tasks. Rather, spaces inherit their task due dates from the space grouping they\u2019re assigned to.&lt;/p&gt;\n\n&lt;p&gt;Anyhow, I believe I can build the necessary backend logic, such as the application database. Perhaps even some basic forms to input data, as well as dashboards. As I\u2019m going through this though, the scheduling aspect feels like I\u2019m reinventing the wheel a bit. Secondly, I worry that a complex frontend will need to be built and that is not my expertise.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tested various less data-engineering solutions like MS Project, figuring those should be good enough. They\u2019re not good enough for various reasons but mostly because none are designed to manage thousands of tasks, spaces, etc. in elegant ways. The work to manage the project becomes too much when using many of these existing tools, especially when they don\u2019t have an API.&lt;/p&gt;\n\n&lt;p&gt;Effectively, I\u2019m wondering if there\u2019s a tool or framework that\u2019s better suited for engineering these types of solutions. Handling schedule data in ways that I mention, helping users perform schedule changes and update statuses?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m about to build the whole thing from scratch. Are there tools out there to lift off segments of this workload and lighten the burden?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1azhkj0", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1azhkj0/tools_for_schedule_management_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1azhkj0/tools_for_schedule_management_systems/", "subreddit_subscribers": 163453, "created_utc": 1708844100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In nutshell, I have an OLTP Oracle 19c database and I want to do massive queries on my data. However, I don't want to overload my OLTP database.\n\nSo, is it possible to use RAID1 for mirroring my OLTP database in another disk and use another Oracle 19c instance in read-only mode connected to the mirrored disk?\n\nI have never used RAID, so I don't know the limitations.", "author_fullname": "t2_rvp7wzgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I use RAID1 for mirroring an OLTP Oracle 19c to an OLAP Oracle 19c?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az4qli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708808016.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708807208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In nutshell, I have an OLTP Oracle 19c database and I want to do massive queries on my data. However, I don&amp;#39;t want to overload my OLTP database.&lt;/p&gt;\n\n&lt;p&gt;So, is it possible to use RAID1 for mirroring my OLTP database in another disk and use another Oracle 19c instance in read-only mode connected to the mirrored disk?&lt;/p&gt;\n\n&lt;p&gt;I have never used RAID, so I don&amp;#39;t know the limitations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1az4qli", "is_robot_indexable": true, "report_reasons": null, "author": "nivlek_miroma", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az4qli/can_i_use_raid1_for_mirroring_an_oltp_oracle_19c/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1az4qli/can_i_use_raid1_for_mirroring_an_oltp_oracle_19c/", "subreddit_subscribers": 163453, "created_utc": 1708807208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, \n\nAnyone has experience with HPE data fabric which he would like to share?  We are trying to build a data platform onpremise and we are evaluting this product. ", "author_fullname": "t2_76ygd12r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HPE Data Fabric", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az24fn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708800693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, &lt;/p&gt;\n\n&lt;p&gt;Anyone has experience with HPE data fabric which he would like to share?  We are trying to build a data platform onpremise and we are evaluting this product. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1az24fn", "is_robot_indexable": true, "report_reasons": null, "author": "SimilarEstimate6234", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az24fn/hpe_data_fabric/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1az24fn/hpe_data_fabric/", "subreddit_subscribers": 163453, "created_utc": 1708800693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Apache Spark RDD is immutable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2yy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/F9F0Ui-ND5wCoAXg5MXlEqVIwRNlyBDPX4vZoFORv6k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708802758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "luminousmen.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://luminousmen.com/post/why-apache-spark-rdd-is-immutable", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vdI9-uUf8gDHTJ36iKJhsqPTcWh3PxhZw6wL1xe7TL0.jpg?auto=webp&amp;s=778cbaf946cd1ba35546ed21f100f5a0b4ece151", "width": 800, "height": 405}, "resolutions": [{"url": "https://external-preview.redd.it/vdI9-uUf8gDHTJ36iKJhsqPTcWh3PxhZw6wL1xe7TL0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2114b41e78e7996518b8848e91d49d87957616a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vdI9-uUf8gDHTJ36iKJhsqPTcWh3PxhZw6wL1xe7TL0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=768d3518a8b889053f40550c60b72645d9276ee9", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/vdI9-uUf8gDHTJ36iKJhsqPTcWh3PxhZw6wL1xe7TL0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=949cb9a0ce573df9b25be17a62d6b8b1de5d644e", "width": 320, "height": 162}, {"url": "https://external-preview.redd.it/vdI9-uUf8gDHTJ36iKJhsqPTcWh3PxhZw6wL1xe7TL0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2be4fb11d2c73817ba50d49ffdbd0b27e72001ec", "width": 640, "height": 324}], "variants": {}, "id": "2A-mOdpSZG8F-78tz1XH_l5i-4T-tq7AozjaC9tLotQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1az2yy2", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1az2yy2/why_apache_spark_rdd_is_immutable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://luminousmen.com/post/why-apache-spark-rdd-is-immutable", "subreddit_subscribers": 163453, "created_utc": 1708802758.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}