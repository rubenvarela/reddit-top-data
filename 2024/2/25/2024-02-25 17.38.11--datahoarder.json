{"kind": "Listing", "data": {"after": "t3_1azrigy", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those of you worried about not being able to play DivX/XviD AVI files in the future, please enjoy this screenshot of Kodi 20.4 playing a 20+ year old 160x120 RealMedia video. You don't appreciate how amazing FFMPEG is.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1azphye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k7NHtsX5zmn0xyCLmu2SzYLP1fcFr-Hdcb19-xxtQrA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708872198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yusjnrd1uqkc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?auto=webp&amp;s=eb0ccf81b97157c9a0462b7a46c38d253e72e5a3", "width": 3840, "height": 2160}, "resolutions": [{"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3203b1a3197b61f6a87a22f68898e9d09ffb12df", "width": 108, "height": 60}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc4bfe9138044832da7bab68f55a4ba62c4ab9b8", "width": 216, "height": 121}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f83f02327240f5d3c9bc663fa627bcb5aa92b36", "width": 320, "height": 180}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94f8d8600af239eee51bcf180b4845ab9b6c2a37", "width": 640, "height": 360}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=032cf24d07dfbb5b460a6e948a9e5c948977585c", "width": 960, "height": 540}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ec1e1669aa0d7024fd0751c57ccb8d038585841", "width": 1080, "height": 607}], "variants": {}, "id": "5Io6ryMNxvF535M3V7_qZkOZMvFgyw8QrPp5qz5Rf9U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1azphye", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azphye/for_those_of_you_worried_about_not_being_able_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yusjnrd1uqkc1.png", "subreddit_subscribers": 734420, "created_utc": 1708872198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.\n\nAll I did was drag and drop in the windows explorer.\n\nIf I want to routinely back up my drive every few months, should I be using an application?\n\nI feel like dragging and dropping again after the first transfer is suboptimal. \n\nTIA.", "author_fullname": "t2_iaosze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys do your routine offline backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgjh5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708840359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.&lt;/p&gt;\n\n&lt;p&gt;All I did was drag and drop in the windows explorer.&lt;/p&gt;\n\n&lt;p&gt;If I want to routinely back up my drive every few months, should I be using an application?&lt;/p&gt;\n\n&lt;p&gt;I feel like dragging and dropping again after the first transfer is suboptimal. &lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgjh5", "is_robot_indexable": true, "report_reasons": null, "author": "TryTurningItOffAgain", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "subreddit_subscribers": 734420, "created_utc": 1708840359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As thread title asks, what is your method for seeing a drive failure coming ahead of time?", "author_fullname": "t2_cwv8qpxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do people use to check whether HDD has problems/is about to fail?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azm6su", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708862038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As thread title asks, what is your method for seeing a drive failure coming ahead of time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azm6su", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Grocery-48", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azm6su/what_do_people_use_to_check_whether_hdd_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azm6su/what_do_people_use_to_check_whether_hdd_has/", "subreddit_subscribers": 734420, "created_utc": 1708862038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know that the best way is to preserve the original without re-encoding. But I'm actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don't even wanna know about Realmedia compatibility today.", "author_fullname": "t2_1e60y1rd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have videos in divx and xvid codec. Should I encode them to modern standards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkv62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708856980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that the best way is to preserve the original without re-encoding. But I&amp;#39;m actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don&amp;#39;t even wanna know about Realmedia compatibility today.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkv62", "is_robot_indexable": true, "report_reasons": null, "author": "skylinestar1986", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "subreddit_subscribers": 734420, "created_utc": 1708856980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a couple of terabytes of data i really wouldn't like to loose. Right now i have that data on 1 external hard drive that i check maybe every 6 months or so. Another copy on a external SSD that i access frequently. The last copy is on a HDD that the files were originally on but not its not in use. I check that HDD very rarely (Maybe once a year or less). \n\nSo i have 3 copies on 2 types of storage mediums. Is this enough? If the external hard drive fails i have the ssd, if the ssd fails i have the external hard drive, if both fail i have the hdd. ", "author_fullname": "t2_kht4ovrc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my backup solution sufficient?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2r7u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708802214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple of terabytes of data i really wouldn&amp;#39;t like to loose. Right now i have that data on 1 external hard drive that i check maybe every 6 months or so. Another copy on a external SSD that i access frequently. The last copy is on a HDD that the files were originally on but not its not in use. I check that HDD very rarely (Maybe once a year or less). &lt;/p&gt;\n\n&lt;p&gt;So i have 3 copies on 2 types of storage mediums. Is this enough? If the external hard drive fails i have the ssd, if the ssd fails i have the external hard drive, if both fail i have the hdd. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az2r7u", "is_robot_indexable": true, "report_reasons": null, "author": "TengokuDaimakyo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az2r7u/is_my_backup_solution_sufficient/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az2r7u/is_my_backup_solution_sufficient/", "subreddit_subscribers": 734420, "created_utc": 1708802214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "continue\n\n* [5,719,123 subtitles from opensubtitles.org](https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/) \\- subs 1 to 9180517\n* [opensubtitles.org dump - 1 million subtitles - 23 GB](https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/) \\- subs 9180519 to 9521948\n\n## opensubtitles.org.dump.9500000.to.9599999\n\nTODO i will add this part in about 10 days. now its 85% complete\n\n## opensubtitles.org.dump.9600000.to.9699999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;dn=opensubtitles.org.dump.9600000.to.9699999\n\n## opensubtitles.org.dump.9700000.to.9799999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;dn=opensubtitles.org.dump.9700000.to.9799999\n\n## download from github\n\nNOTE i will remove these files from github in some weeks, to keep the repo size below 10GB\n\n`ln` = create hardlinks\n\n    git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n    \n    mkdir opensubtitles.org.dump.9600000.to.9699999\n    ln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n      opensubtitles.org.dump.9600000.to.9699999\n    \n    mkdir opensubtitles.org.dump.9700000.to.9799999\n    ln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n      opensubtitles.org.dump.9700000.to.9799999\n\n## download from archive.org\n\nTODO upload to archive.org for long term storage\n\n## scraper\n\n[https://github.com/milahu/opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n\nmy latest version is still unreleased. it is based on my [aiohttp\\_chromium](https://github.com/milahu/aiohttp_chromium) to bypass cloudflare\n\ni have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com\n\n## problem of trust\n\none problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files\n\n## subtitles server\n\nTODO create a subtitles server to make this usable for thin clients (video players)\n\n* the biggest challenge is the database size of about 150GB\n* use metadata from subtitles\\_all.txt.gz from [https://dl.opensubtitles.org/addons/export/](https://dl.opensubtitles.org/addons/export/) \\- see also `subtitles_all.txt.gz-parse.py` in [opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n* map movie filename to imdb id to subtitles - see also `get-subs.py`\n* map movie filename to movie name to subtitles\n* recode to utf8 - see also `repack.py`\n* remove ads - see also `opensubtitles-ads.txt` and `find_ads.py`\n* maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay", "author_fullname": "t2_naq15kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "subtitles from opensubtitles.org - subs 9500000 to 9799999", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1azqwa4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;continue&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/\"&gt;5,719,123 subtitles from opensubtitles.org&lt;/a&gt; - subs 1 to 9180517&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/\"&gt;opensubtitles.org dump - 1 million subtitles - 23 GB&lt;/a&gt; - subs 9180519 to 9521948&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9500000.to.9599999&lt;/h2&gt;\n\n&lt;p&gt;TODO i will add this part in about 10 days. now its 85% complete&lt;/p&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9600000.to.9699999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;amp;dn=opensubtitles.org.dump.9600000.to.9699999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9700000.to.9799999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;amp;dn=opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from github&lt;/h2&gt;\n\n&lt;p&gt;NOTE i will remove these files from github in some weeks, to keep the repo size below 10GB&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ln&lt;/code&gt; = create hardlinks&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n\nmkdir opensubtitles.org.dump.9600000.to.9699999\nln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n  opensubtitles.org.dump.9600000.to.9699999\n\nmkdir opensubtitles.org.dump.9700000.to.9799999\nln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n  opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from archive.org&lt;/h2&gt;\n\n&lt;p&gt;TODO upload to archive.org for long term storage&lt;/p&gt;\n\n&lt;h2&gt;scraper&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;https://github.com/milahu/opensubtitles-scraper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;my latest version is still unreleased. it is based on my &lt;a href=\"https://github.com/milahu/aiohttp_chromium\"&gt;aiohttp_chromium&lt;/a&gt; to bypass cloudflare&lt;/p&gt;\n\n&lt;p&gt;i have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com&lt;/p&gt;\n\n&lt;h2&gt;problem of trust&lt;/h2&gt;\n\n&lt;p&gt;one problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files&lt;/p&gt;\n\n&lt;h2&gt;subtitles server&lt;/h2&gt;\n\n&lt;p&gt;TODO create a subtitles server to make this usable for thin clients (video players)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the biggest challenge is the database size of about 150GB&lt;/li&gt;\n&lt;li&gt;use metadata from subtitles_all.txt.gz from &lt;a href=\"https://dl.opensubtitles.org/addons/export/\"&gt;https://dl.opensubtitles.org/addons/export/&lt;/a&gt; - see also &lt;code&gt;subtitles_all.txt.gz-parse.py&lt;/code&gt; in &lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;opensubtitles-scraper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to imdb id to subtitles - see also &lt;code&gt;get-subs.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to movie name to subtitles&lt;/li&gt;\n&lt;li&gt;recode to utf8 - see also &lt;code&gt;repack.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;remove ads - see also &lt;code&gt;opensubtitles-ads.txt&lt;/code&gt; and &lt;code&gt;find_ads.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?auto=webp&amp;s=4addee391a8fcdd79d9dc8bb3be80840cfbb2292", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6d87b10c172c2af78696cd0a9017ecffcb12898", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f65d0b1b68d620588e0a5da27334b745e70c8dc", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50392b9b700a2ed49029d9fc031570a4f0e6318b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5df9dc84a8f8e25e12eef950e3fe12459f8c712", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e752c9a858437424fd7d6df3cb09459a0923e91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=261adcbc7bfcb31a1aae5ed22a391bedcf898e0b", "width": 1080, "height": 540}], "variants": {}, "id": "xzPnR4eWjwiQ6_oTfaBERSRsup0-vUB8Zvj-2miVx3M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqwa4", "is_robot_indexable": true, "report_reasons": null, "author": "milahu2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "subreddit_subscribers": 734420, "created_utc": 1708875847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dear all, I have too much data on my macbook pro and like to move it to an external ssd. isn't the Crucial X9 Pro SSD for Mac  simply a regular Crucial X9 Pro SSD formatted with apfs, or are there differences in the hardware? Surely the pro formatted in APFS will be accessible of iPhone &amp; Co? Am I mkissing something? Many thanks :)  \n[https://uk.crucial.com/products/ssd/portable-ssds](https://uk.crucial.com/products/ssd/portable-ssds)", "author_fullname": "t2_h1vgwvsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data to external SSD: Crucial X9 Pro SSD for Mac just a Crucial X9 Pro with APFS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az5gp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708808973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear all, I have too much data on my macbook pro and like to move it to an external ssd. isn&amp;#39;t the Crucial X9 Pro SSD for Mac  simply a regular Crucial X9 Pro SSD formatted with apfs, or are there differences in the hardware? Surely the pro formatted in APFS will be accessible of iPhone &amp;amp; Co? Am I mkissing something? Many thanks :)&lt;br/&gt;\n&lt;a href=\"https://uk.crucial.com/products/ssd/portable-ssds\"&gt;https://uk.crucial.com/products/ssd/portable-ssds&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?auto=webp&amp;s=e2421e6c60b21be472095e594db58bd778600d0f", "width": 1200, "height": 680}, "resolutions": [{"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6a4edc4610d4cb640bedd66be7355faec77ca22", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b80e53c196e0d71cb6768e16b3e461beead737c2", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7e2ed9ebc9833094c4f25cbab410a25e60e8124", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0791fba73a60e469bde8e51ca7f958d506bf20f", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc93901b52df48149de9e881ac6e469a5c38f02", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/JRaoKGPlW2zdYhbun_GgEGTpZpxRScY4bTqFa_e2gu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e97fc43d31f603e47696b6292579785447d2a4f2", "width": 1080, "height": 612}], "variants": {}, "id": "zby-HJAO_eydDuSwVzafSoulGLhD91dKRo6wSmctlmY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az5gp2", "is_robot_indexable": true, "report_reasons": null, "author": "eusmile", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az5gp2/moving_data_to_external_ssd_crucial_x9_pro_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az5gp2/moving_data_to_external_ssd_crucial_x9_pro_ssd/", "subreddit_subscribers": 734420, "created_utc": 1708808973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nI have Seagate external expansion disk 12TB\n\nDisk sentinel shows disk like st12000ne0008\n\nDid anybody already shucking this disk? Can i expect problems like PWDIS?", "author_fullname": "t2_ptdg2w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Before disk shucking with IronWolf inside", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az0uxj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708797614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I have Seagate external expansion disk 12TB&lt;/p&gt;\n\n&lt;p&gt;Disk sentinel shows disk like st12000ne0008&lt;/p&gt;\n\n&lt;p&gt;Did anybody already shucking this disk? Can i expect problems like PWDIS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az0uxj", "is_robot_indexable": true, "report_reasons": null, "author": "forplan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az0uxj/before_disk_shucking_with_ironwolf_inside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az0uxj/before_disk_shucking_with_ironwolf_inside/", "subreddit_subscribers": 734420, "created_utc": 1708797614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. \n\nI went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn't apply to my build), or the onboard SATA controller is terrible.\n\nThe motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can't find the controllers used on this motherboard.\n\nMy question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I'm unaware of, as opposed to using the onboard SATA ports?", "author_fullname": "t2_74e8mz3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New unRAID build, need advice for onboard SATA ports vs  LSI/HBA PCIe card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpmz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708872583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ca.pcpartpicker.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. &lt;/p&gt;\n\n&lt;p&gt;I went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn&amp;#39;t apply to my build), or the onboard SATA controller is terrible.&lt;/p&gt;\n\n&lt;p&gt;The motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can&amp;#39;t find the controllers used on this motherboard.&lt;/p&gt;\n\n&lt;p&gt;My question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I&amp;#39;m unaware of, as opposed to using the onboard SATA ports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpmz7", "is_robot_indexable": true, "report_reasons": null, "author": "JakeHa0991", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpmz7/new_unraid_build_need_advice_for_onboard_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "subreddit_subscribers": 734420, "created_utc": 1708872583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Help! I am trying to understand how SnapRAID works.\n\nI have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?", "author_fullname": "t2_e4owahyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnapRAID Cloned Disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgdp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708840213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708839802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help! I am trying to understand how SnapRAID works.&lt;/p&gt;\n\n&lt;p&gt;I have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgdp2", "is_robot_indexable": true, "report_reasons": null, "author": "x5KSAM", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "subreddit_subscribers": 734420, "created_utc": 1708839802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. \n\nHowever, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.", "author_fullname": "t2_4kz0kk4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy and best method to clean VHS tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azg3ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708838838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. &lt;/p&gt;\n\n&lt;p&gt;However, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azg3ig", "is_robot_indexable": true, "report_reasons": null, "author": "harshcloud", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "subreddit_subscribers": 734420, "created_utc": 1708838838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know it can be found easily on a desktop browser by using the network tab on inspect element, but apparently the Amazon 3D viewer can only be viewed on mobile phones. The button won\u2019t appear even on mobile browsers.\n\nI have access to both Android and iOS devices. Google wasn\u2019t able to help me. Is there a way to get this file from the app?\n", "author_fullname": "t2_njkbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to extract a 3D model from Amazon mobile app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1az9vtr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p_LsJCsEoMKI1PilEki20NTNFiTK2iLYRTaigNetu3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708820090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it can be found easily on a desktop browser by using the network tab on inspect element, but apparently the Amazon 3D viewer can only be viewed on mobile phones. The button won\u2019t appear even on mobile browsers.&lt;/p&gt;\n\n&lt;p&gt;I have access to both Android and iOS devices. Google wasn\u2019t able to help me. Is there a way to get this file from the app?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mht3a8pgjmkc1.jpeg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?auto=webp&amp;s=185275c6fc53ce522d3d5b01de227b2d822edd78", "width": 1179, "height": 1394}, "resolutions": [{"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f945ce555eb8eada9180997c86f4577ed5d1df4", "width": 108, "height": 127}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f507582675312b81f1676e345e6d2ab50e67472", "width": 216, "height": 255}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bb1642f65bc7de163881f75bc835637da9db4ed", "width": 320, "height": 378}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=842e72badbb80e3e0fa41ac08e7ce13045d44afa", "width": 640, "height": 756}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca35f300346f389a90732542ed49794151f3b9", "width": 960, "height": 1135}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ca7903283d6b46986dd5aaafe0c6e7c8b6b4741", "width": 1080, "height": 1276}], "variants": {}, "id": "w4V-ynmE8IG6AShLabZtEMULa3nIkvonUxRcw-AAcKo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az9vtr", "is_robot_indexable": true, "report_reasons": null, "author": "esteban_agpa", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az9vtr/how_to_extract_a_3d_model_from_amazon_mobile_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mht3a8pgjmkc1.jpeg", "subreddit_subscribers": 734420, "created_utc": 1708820090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to convert my home VHS collection to DVD, but my Sansui VRDVD4005 is no longer able to read disks. I've had it for quite a while and it is not in bad condition by any means. Unfortunately, the literature on this device is not very extensive, and the error code I have received does not seem to apply (or at least I think). The error code it has spit out (at least once) is C-104 which seems to mean the disk is incompatible with the device. However, I have used this device before to convert my tapes. I do have a few disks from when I converted years back that can be read, but it does not want to take new disks at all. Is it possible it cannot take newer disks that record at a higher speed? All disks have been bought in the USA, so it shouldn't be a region issue. The manual states it can record on DVD-R ver 2.0 (4.7 GB). The disks I have used successfully in the past were 8x speed, but only some can be read while others cannot be. Same brand and everything. The disks I have been trying to use are 16x.\n\nI have tried swapping the DVD drive with a new model that was supposedly compatible with it and could write on disks that are capable of writing at a higher speed, but the disks are still not detected.\n\nAny other ideas on how to get this working again?", "author_fullname": "t2_p7alv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sansui VHS to DVD not detecting new disks, anyone have experience with this machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az7b50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708813508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to convert my home VHS collection to DVD, but my Sansui VRDVD4005 is no longer able to read disks. I&amp;#39;ve had it for quite a while and it is not in bad condition by any means. Unfortunately, the literature on this device is not very extensive, and the error code I have received does not seem to apply (or at least I think). The error code it has spit out (at least once) is C-104 which seems to mean the disk is incompatible with the device. However, I have used this device before to convert my tapes. I do have a few disks from when I converted years back that can be read, but it does not want to take new disks at all. Is it possible it cannot take newer disks that record at a higher speed? All disks have been bought in the USA, so it shouldn&amp;#39;t be a region issue. The manual states it can record on DVD-R ver 2.0 (4.7 GB). The disks I have used successfully in the past were 8x speed, but only some can be read while others cannot be. Same brand and everything. The disks I have been trying to use are 16x.&lt;/p&gt;\n\n&lt;p&gt;I have tried swapping the DVD drive with a new model that was supposedly compatible with it and could write on disks that are capable of writing at a higher speed, but the disks are still not detected.&lt;/p&gt;\n\n&lt;p&gt;Any other ideas on how to get this working again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az7b50", "is_robot_indexable": true, "report_reasons": null, "author": "Exquisite_Poupon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az7b50/sansui_vhs_to_dvd_not_detecting_new_disks_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az7b50/sansui_vhs_to_dvd_not_detecting_new_disks_anyone/", "subreddit_subscribers": 734420, "created_utc": 1708813508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Quick question:\n\nI needed to test a drive thoroughly, following a failure of another, so I needed to know if the replacement is OK.\n\nFrom everything that I read about testing drives on this reddit I figured that badblocks is the way to go, but unfortunately for the love of mine I could not make it work, neither on a VM in Windows, nor as a live usb. I was running into errors with permissions (that supposedly my user couldn't do sudo commands), unrecognisable directories (even though I checked what the drive directory was in the drives), or badblocks command executing and returning 'completed' result in seconds (on 8TB drive).\n\nBecause I was fed up with this, instead I performed what follows:\n\n1. Quick SMART check using HDDScan app\n2. h2testw test for entire drive (AFAIK this is windows-like equivalent of badblocks, it fills the drive with data and then verifies it) - that took like good 20 hours and came up clean\n3. Extended SMART test using HDDScan - came up clean\n\nIn the meantime I was checking the CrystalDisk info for SMART parameters, after everything I threw at it everything is OK, no reallocated sectors, no pending, no uncorrectable.\n\n&amp;#x200B;\n\nCan I assume that the drive is fine, or should I still perform badblocks, because the aforementioned tests may not be enough to detect a fault?", "author_fullname": "t2_38zesk7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this enough testing, or is badblocks necessary still?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2y3m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708802699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick question:&lt;/p&gt;\n\n&lt;p&gt;I needed to test a drive thoroughly, following a failure of another, so I needed to know if the replacement is OK.&lt;/p&gt;\n\n&lt;p&gt;From everything that I read about testing drives on this reddit I figured that badblocks is the way to go, but unfortunately for the love of mine I could not make it work, neither on a VM in Windows, nor as a live usb. I was running into errors with permissions (that supposedly my user couldn&amp;#39;t do sudo commands), unrecognisable directories (even though I checked what the drive directory was in the drives), or badblocks command executing and returning &amp;#39;completed&amp;#39; result in seconds (on 8TB drive).&lt;/p&gt;\n\n&lt;p&gt;Because I was fed up with this, instead I performed what follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Quick SMART check using HDDScan app&lt;/li&gt;\n&lt;li&gt;h2testw test for entire drive (AFAIK this is windows-like equivalent of badblocks, it fills the drive with data and then verifies it) - that took like good 20 hours and came up clean&lt;/li&gt;\n&lt;li&gt;Extended SMART test using HDDScan - came up clean&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In the meantime I was checking the CrystalDisk info for SMART parameters, after everything I threw at it everything is OK, no reallocated sectors, no pending, no uncorrectable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can I assume that the drive is fine, or should I still perform badblocks, because the aforementioned tests may not be enough to detect a fault?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az2y3m", "is_robot_indexable": true, "report_reasons": null, "author": "lightbringer0209", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az2y3m/is_this_enough_testing_or_is_badblocks_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az2y3m/is_this_enough_testing_or_is_badblocks_necessary/", "subreddit_subscribers": 734420, "created_utc": 1708802699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17's pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn't support Live photos according to some comments. \n\nI can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don't know what apps to use as of now. Another problem seems to be that I have \\~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they're backed up, but I know that's probably difficult.\n\nDoes anyone do something similar on their devices? Please let me know. thank you!", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up photos (including Live Photos) from iPhone to Android?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1azqv6a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17&amp;#39;s pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn&amp;#39;t support Live photos according to some comments. &lt;/p&gt;\n\n&lt;p&gt;I can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don&amp;#39;t know what apps to use as of now. Another problem seems to be that I have ~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they&amp;#39;re backed up, but I know that&amp;#39;s probably difficult.&lt;/p&gt;\n\n&lt;p&gt;Does anyone do something similar on their devices? Please let me know. thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqv6a", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "subreddit_subscribers": 734420, "created_utc": 1708875765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background:  \nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the \"2\" different types of medium).   \n\n\nMain part  \nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don't experience any performance issues so i wouldn't go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  \n\n\nMain question  \nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  \n\n\nThanks for reading and appreciate all the advice!  \n\n\n&amp;#x200B;\n\nTLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can't seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.", "author_fullname": "t2_31744y31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "12TB drives and rebuild time with RAID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpunm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708873159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background:&lt;br/&gt;\nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the &amp;quot;2&amp;quot; different types of medium).   &lt;/p&gt;\n\n&lt;p&gt;Main part&lt;br/&gt;\nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don&amp;#39;t experience any performance issues so i wouldn&amp;#39;t go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  &lt;/p&gt;\n\n&lt;p&gt;Main question&lt;br/&gt;\nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  &lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and appreciate all the advice!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can&amp;#39;t seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpunm", "is_robot_indexable": true, "report_reasons": null, "author": "WietseTDX", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "subreddit_subscribers": 734420, "created_utc": 1708873159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  \n\n\nSo that:\n\nIf I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.  \n&lt;BUT&gt;  \nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  \n\n\nIf it is \"doable\" how does one do it?  \n\n\nThe purpose would be to have a \"hidden\" cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  \n\n\n&amp;#x200B;", "author_fullname": "t2_3kfdots", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to have CD-Audio on a BDXL disc and data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpb90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708871677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  &lt;/p&gt;\n\n&lt;p&gt;So that:&lt;/p&gt;\n\n&lt;p&gt;If I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.&lt;br/&gt;\n&amp;lt;BUT&amp;gt;&lt;br/&gt;\nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  &lt;/p&gt;\n\n&lt;p&gt;If it is &amp;quot;doable&amp;quot; how does one do it?  &lt;/p&gt;\n\n&lt;p&gt;The purpose would be to have a &amp;quot;hidden&amp;quot; cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpb90", "is_robot_indexable": true, "report_reasons": null, "author": "trseeker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "subreddit_subscribers": 734420, "created_utc": 1708871677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "NetworkInfrastructureException('Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\\\\n&lt;html&gt;\\\\n  &lt;head&gt;\\\\n    &lt;title&gt;503 Backend fetch failed&lt;/title&gt;\\\\n  &lt;/head&gt;\\\\n  &lt;body&gt;\\\\n    &lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\\\\n    &lt;p&gt;Backend fetch failed&lt;/p&gt;\\\\n    &lt;h3&gt;Guru Meditation:&lt;/h3&gt;\\\\n    &lt;p&gt;XID: 802841683&lt;/p&gt;\\\\n    &lt;hr&gt;\\\\n    &lt;p&gt;Varnish cache server&lt;/p&gt;\\\\n  &lt;/body&gt;\\\\n&lt;/html&gt;\\\\n')\u2026 (Copy note to see full error)\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1659, in Start\n\nhydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nDuring handling of the above exception, another exception occurred:\n\n&amp;#x200B;\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\importing\\\\[ClientImportFileSeeds.py](https://ClientImportFileSeeds.py)\", line 1378, in WorkOnURL\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1977, in WaitUntilDone\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1698, in Start\n\nhydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\nI've only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don't know how to fix, internet searches aren't yielding relevant results. Help is appreciated.", "author_fullname": "t2_4p22esrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Hydrus Downloader, need help getting passed 503 to download from kemono", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azos8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708870187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NetworkInfrastructureException(&amp;#39;Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;\\n&amp;lt;html&amp;gt;\\n  &amp;lt;head&amp;gt;\\n    &amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;\\n  &amp;lt;/head&amp;gt;\\n  &amp;lt;body&amp;gt;\\n    &amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;\\n    &amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;\\n    &amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;\\n    &amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;\\n    &amp;lt;hr&amp;gt;\\n    &amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;\\n  &amp;lt;/body&amp;gt;\\n&amp;lt;/html&amp;gt;\\n&amp;#39;)\u2026 (Copy note to see full error)&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1659, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;During handling of the above exception, another exception occurred:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\importing\\&lt;a href=\"https://ClientImportFileSeeds.py\"&gt;ClientImportFileSeeds.py&lt;/a&gt;&amp;quot;, line 1378, in WorkOnURL&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1977, in WaitUntilDone&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1698, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don&amp;#39;t know how to fix, internet searches aren&amp;#39;t yielding relevant results. Help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azos8e", "is_robot_indexable": true, "report_reasons": null, "author": "Vynsyx", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "subreddit_subscribers": 734420, "created_utc": 1708870187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkyd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9c2bi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "japan", "selftext": "# Introduction\n\nFor anybody that has a 1999 copy of Kodansha's CD-ROM version of '[Encyclopedia of Japan](https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan)', here's instructions on how to install and run it on Windows 11 (x64).\n\nSurprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.\n\nWhilst I have made a backup ISO file, ~~I unfortunately can't lawfully distribute it. As such, please don't ask.~~\n\n**Edit: The ISO file has been uploaded [here](https://archive.org/details/EOJ10) based on [this](https://archive.org/about/dmca.php) statement. Do not use this software if you have no legal right to do so.**\n\nFor those without a copy, if you would like to access its contents, you can do so via [JapanKnowledge](https://japanknowledge.com/en/contents/eoj/).\n\n# Installation\n\nFollow the steps below:\n\n1. Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of `Internet Explorer 5`.\n2. Attempt to launch the application. If you receive an error message concerning `MCI32.OCX`, proceed with steps 3-7.\n3. Type `Command Prompt` into the taskbar `Search` box; this is located next to the `Start` menu.\n4. In the search results, right-click `Command Prompt` and select `Run as administrator`.\n5. Copy and paste the following command: `regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX`.\n6. If the command doesn't instantly execute, press the `Enter/Return` key.\n7. Confirm that `MCI32.OCX` has been registered, and then close `Command Prompt`.\n8. Launch the encyclopedia application.\n\n# Missing Media\n\nWhen a media file is unable to load, you will see an `X` instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.\n\n# Fix\n\nFollow the steps below:\n\n1. `Open` the CD-ROM.\n2. Go to: `[CD DRIVE LETTER]:\\eoj`.\n3. Copy the: `[CD DRIVE LETTER]:\\eoj\\media` folder.\n4. Paste the entire folder to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan`.\n5. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n6. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n7. Copy: `Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0`.\n8. Paste this into the top bar and press `Enter/Return`.\n9. Right-click `MEDIA_PATH` and select `Modify`. Change the `Value data` to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media`.\n10. Click `OK` and close.\n\n# Unable to Print\n\nDuring testing, the `Print` button did not function.\n\n# Fix\n\nIn Windows 11, use `Ctrl + P` to print pages.\n\nTo remove the header and footer, follow the steps below:\n\n1. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n2. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n3. Copy: `Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup`.\n4. Paste this into the top bar and press `Enter/Return`.\n5. In the empty space below `(Default)`, right-click and select `New` \\-&gt; `String Value`. Type `header` and press `Enter/Return`.\n6. Repeat the step above, but type `footer` instead.\n7. Close `Registry Editor`.\n\nNote: This is a system-wide change. You will need to right-click the values and select `Delete` to reverse the change.\n\n# Uninstallation\n\nWhen uninstalling, the uninstaller may ask if you would like to delete `Mci32.ocx` and `Comdlg32.OCX`. Click `No` or `No to All`. Copies of these files should remain in: `C:\\Windows\\SysWOW64`.", "author_fullname": "t2_noxre9enh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/japan", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayrh2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 130, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 130, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708809657.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708770653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;For anybody that has a 1999 copy of Kodansha&amp;#39;s CD-ROM version of &amp;#39;&lt;a href=\"https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan\"&gt;Encyclopedia of Japan&lt;/a&gt;&amp;#39;, here&amp;#39;s instructions on how to install and run it on Windows 11 (x64).&lt;/p&gt;\n\n&lt;p&gt;Surprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.&lt;/p&gt;\n\n&lt;p&gt;Whilst I have made a backup ISO file, &lt;del&gt;I unfortunately can&amp;#39;t lawfully distribute it. As such, please don&amp;#39;t ask.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit: The ISO file has been uploaded &lt;a href=\"https://archive.org/details/EOJ10\"&gt;here&lt;/a&gt; based on &lt;a href=\"https://archive.org/about/dmca.php\"&gt;this&lt;/a&gt; statement. Do not use this software if you have no legal right to do so.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For those without a copy, if you would like to access its contents, you can do so via &lt;a href=\"https://japanknowledge.com/en/contents/eoj/\"&gt;JapanKnowledge&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Installation&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of &lt;code&gt;Internet Explorer 5&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Attempt to launch the application. If you receive an error message concerning &lt;code&gt;MCI32.OCX&lt;/code&gt;, proceed with steps 3-7.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Command Prompt&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Command Prompt&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy and paste the following command: &lt;code&gt;regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;If the command doesn&amp;#39;t instantly execute, press the &lt;code&gt;Enter/Return&lt;/code&gt; key.&lt;/li&gt;\n&lt;li&gt;Confirm that &lt;code&gt;MCI32.OCX&lt;/code&gt; has been registered, and then close &lt;code&gt;Command Prompt&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Launch the encyclopedia application.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Missing Media&lt;/h1&gt;\n\n&lt;p&gt;When a media file is unable to load, you will see an &lt;code&gt;X&lt;/code&gt; instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;Open&lt;/code&gt; the CD-ROM.&lt;/li&gt;\n&lt;li&gt;Go to: &lt;code&gt;[CD DRIVE LETTER]:\\eoj&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy the: &lt;code&gt;[CD DRIVE LETTER]:\\eoj\\media&lt;/code&gt; folder.&lt;/li&gt;\n&lt;li&gt;Paste the entire folder to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Right-click &lt;code&gt;MEDIA_PATH&lt;/code&gt; and select &lt;code&gt;Modify&lt;/code&gt;. Change the &lt;code&gt;Value data&lt;/code&gt; to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Click &lt;code&gt;OK&lt;/code&gt; and close.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Unable to Print&lt;/h1&gt;\n\n&lt;p&gt;During testing, the &lt;code&gt;Print&lt;/code&gt; button did not function.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;In Windows 11, use &lt;code&gt;Ctrl + P&lt;/code&gt; to print pages.&lt;/p&gt;\n\n&lt;p&gt;To remove the header and footer, follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;In the empty space below &lt;code&gt;(Default)&lt;/code&gt;, right-click and select &lt;code&gt;New&lt;/code&gt; -&amp;gt; &lt;code&gt;String Value&lt;/code&gt;. Type &lt;code&gt;header&lt;/code&gt; and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Repeat the step above, but type &lt;code&gt;footer&lt;/code&gt; instead.&lt;/li&gt;\n&lt;li&gt;Close &lt;code&gt;Registry Editor&lt;/code&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Note: This is a system-wide change. You will need to right-click the values and select &lt;code&gt;Delete&lt;/code&gt; to reverse the change.&lt;/p&gt;\n\n&lt;h1&gt;Uninstallation&lt;/h1&gt;\n\n&lt;p&gt;When uninstalling, the uninstaller may ask if you would like to delete &lt;code&gt;Mci32.ocx&lt;/code&gt; and &lt;code&gt;Comdlg32.OCX&lt;/code&gt;. Click &lt;code&gt;No&lt;/code&gt; or &lt;code&gt;No to All&lt;/code&gt;. Copies of these files should remain in: &lt;code&gt;C:\\Windows\\SysWOW64&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh2u", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1ayrh2k", "is_robot_indexable": true, "report_reasons": null, "author": "YokaiZukan", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 825157, "created_utc": 1708770653.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708857334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkyd5", "is_robot_indexable": true, "report_reasons": null, "author": "zombieshavebrains", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1ayrh2k", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkyd5/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 734420, "created_utc": 1708857334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If in its original packaging, is it safe to do this?\n\n[View Poll](https://www.reddit.com/poll/1ayzimr)", "author_fullname": "t2_57pge7h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shipping a Qnap 4 bay with 4 x 6tb drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayzimr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708794333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If in its original packaging, is it safe to do this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1ayzimr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1ayzimr", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Bookkeeper", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1708967134013, "options": [{"text": "Yes", "id": "27202975"}, {"text": "No", "id": "27202976"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 50, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1ayzimr/shipping_a_qnap_4_bay_with_4_x_6tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/1ayzimr/shipping_a_qnap_4_bay_with_4_x_6tb_drives/", "subreddit_subscribers": 734420, "created_utc": 1708794333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After the ownership shakeup, the availability of the newer PDFs became a lot more difficult.", "author_fullname": "t2_10cn75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a CURRENT MAKE magazine archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpipa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708872256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After the ownership shakeup, the availability of the newer PDFs became a lot more difficult.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpipa", "is_robot_indexable": true, "report_reasons": null, "author": "baconlayer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpipa/does_anyone_have_a_current_make_magazine_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpipa/does_anyone_have_a_current_make_magazine_archive/", "subreddit_subscribers": 734420, "created_utc": 1708872256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a windows tool that I can set up for specific folders I want to copy every time when I\u2019m doing a backup to a usb stick? Basically I want to make a usb drive with only my essential data, but I don\u2019t want to go and grab it from the various folders every time, because sometimes they may get updated. ", "author_fullname": "t2_t2xqj88b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows tool for usb copying specified folders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azamjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708822090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a windows tool that I can set up for specific folders I want to copy every time when I\u2019m doing a backup to a usb stick? Basically I want to make a usb drive with only my essential data, but I don\u2019t want to go and grab it from the various folders every time, because sometimes they may get updated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azamjv", "is_robot_indexable": true, "report_reasons": null, "author": "Signal_Inside3436", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azamjv/windows_tool_for_usb_copying_specified_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azamjv/windows_tool_for_usb_copying_specified_folders/", "subreddit_subscribers": 734420, "created_utc": 1708822090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello all, looking to get protection for my slimmer 3.5\" SATA hard disk drive. I have the Seagate Ironwolf 4TB, and it's roughly 20mm high. I worry it will shake around in storage cases that were designed for 26mm drives. Are there protective storage cases specifically designed for these drives? Should I just get a piece of foam for extra padding and insert it along with the 20mm drive?\n\nI was looking at ORICO hard drive carrying cases for context. I also saw some brands selling aluminum storage boxes with multiple foam-padded bays.", "author_fullname": "t2_11bhal9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protection for 20mm HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az2p0j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708802074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, looking to get protection for my slimmer 3.5&amp;quot; SATA hard disk drive. I have the Seagate Ironwolf 4TB, and it&amp;#39;s roughly 20mm high. I worry it will shake around in storage cases that were designed for 26mm drives. Are there protective storage cases specifically designed for these drives? Should I just get a piece of foam for extra padding and insert it along with the 20mm drive?&lt;/p&gt;\n\n&lt;p&gt;I was looking at ORICO hard drive carrying cases for context. I also saw some brands selling aluminum storage boxes with multiple foam-padded bays.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az2p0j", "is_robot_indexable": true, "report_reasons": null, "author": "TowerTV", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1az2p0j/protection_for_20mm_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az2p0j/protection_for_20mm_hdds/", "subreddit_subscribers": 734420, "created_utc": 1708802074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking about building a NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azq8ju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_2qx2v4jl", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datacurator", "selftext": "I used Drobos in the past to backup and archive my data.  Lesson learned - do not rely on proprietary systems.  I'm now considering building my own NAS and need a little advice.  As far as software, I'm undecided between Unraid and TrueNAS but leaning toward Unraid because it seems a little easier to set up and manage.  As far as hardware, I already have lots of SATA drives (5 x 14TB, 10 x 10TB, 10 x 8TB, 6 x 6TB, plus a few other scattered sizes) so I think I would like to stick with those instead of reinvesting in SAS drives.  Beyond that, I don't really know.  I kind of like the idea of a desktop setup because I've built several Windows/Linux PCs before and am familiar with the process.  I don't know anything about rack-mounted homelabs and wouldn't know where to begin.  But at the same time I recognize that a desktop setup isn't going to accommodate as many drives or be as expandable as a rack system so I am wondering if climbing that learning curve would be worth the while.\n\nMy purposes for the NAS would be 1) backup of my main PCs hard drives and SSDs, 2) media player (Plex, Jellyfin, etc) 3) file server 4) maybe some VMs.  Budget: maybe $5000.  I wouldn't need to buy any drives at least to start out since as mentioned I already have a lot of drives lying around.\n\nAdvice please?\n\nXposted to r/DataHoarder and r/datacurator.  Thanks!", "author_fullname": "t2_2qx2v4jl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking about building a NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datacurator", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azq80e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708874113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datacurator", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Drobos in the past to backup and archive my data.  Lesson learned - do not rely on proprietary systems.  I&amp;#39;m now considering building my own NAS and need a little advice.  As far as software, I&amp;#39;m undecided between Unraid and TrueNAS but leaning toward Unraid because it seems a little easier to set up and manage.  As far as hardware, I already have lots of SATA drives (5 x 14TB, 10 x 10TB, 10 x 8TB, 6 x 6TB, plus a few other scattered sizes) so I think I would like to stick with those instead of reinvesting in SAS drives.  Beyond that, I don&amp;#39;t really know.  I kind of like the idea of a desktop setup because I&amp;#39;ve built several Windows/Linux PCs before and am familiar with the process.  I don&amp;#39;t know anything about rack-mounted homelabs and wouldn&amp;#39;t know where to begin.  But at the same time I recognize that a desktop setup isn&amp;#39;t going to accommodate as many drives or be as expandable as a rack system so I am wondering if climbing that learning curve would be worth the while.&lt;/p&gt;\n\n&lt;p&gt;My purposes for the NAS would be 1) backup of my main PCs hard drives and SSDs, 2) media player (Plex, Jellyfin, etc) 3) file server 4) maybe some VMs.  Budget: maybe $5000.  I wouldn&amp;#39;t need to buy any drives at least to start out since as mentioned I already have a lot of drives lying around.&lt;/p&gt;\n\n&lt;p&gt;Advice please?&lt;/p&gt;\n\n&lt;p&gt;Xposted to &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt; and &lt;a href=\"/r/datacurator\"&gt;r/datacurator&lt;/a&gt;.  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3hgix", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1azq80e", "is_robot_indexable": true, "report_reasons": null, "author": "kydar1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datacurator/comments/1azq80e/thinking_about_building_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datacurator/comments/1azq80e/thinking_about_building_a_nas/", "subreddit_subscribers": 17859, "created_utc": 1708874113.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708874149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datacurator", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/datacurator/comments/1azq80e/thinking_about_building_a_nas/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azq8ju", "is_robot_indexable": true, "report_reasons": null, "author": "kydar1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1azq80e", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1azq8ju/thinking_about_building_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datacurator/comments/1azq80e/thinking_about_building_a_nas/", "subreddit_subscribers": 734420, "created_utc": 1708874149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fc92z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good \"Storage media 101\" kind of video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_1azrigy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xA9Xq7hb6Q0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storage Media Life Expectancy: SSDs, HDDs &amp;amp; More!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Storage Media Life Expectancy: SSDs, HDDs &amp; More!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xA9Xq7hb6Q0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storage Media Life Expectancy: SSDs, HDDs &amp;amp; More!\"&gt;&lt;/iframe&gt;", "author_name": "ExplainingComputers", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xA9Xq7hb6Q0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ExplainingComputers"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xA9Xq7hb6Q0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storage Media Life Expectancy: SSDs, HDDs &amp;amp; More!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1azrigy", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oFMRz1uBDAtZQX3Q-YGmJV6Bba9rKXvmvn-zpb3ZbBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708877377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=xA9Xq7hb6Q0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?auto=webp&amp;s=2d51b297d3948118df0d37297ccde5e4538bf20c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3aea2c56a5853ebb03a9c8b593ca53eba163295e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9075a334005a91cf6cf247ff7d9c14080d76ce3a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75c2e1d31bc3adaccd9ab1f3bfd8c8d6f904f904", "width": 320, "height": 240}], "variants": {}, "id": "__mzULFT7h1UTg5sKKQExNpAqGNYf0AJNsLFpk_Jzhw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1azrigy", "is_robot_indexable": true, "report_reasons": null, "author": "dstarr3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azrigy/good_storage_media_101_kind_of_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=xA9Xq7hb6Q0", "subreddit_subscribers": 734420, "created_utc": 1708877377.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Storage Media Life Expectancy: SSDs, HDDs &amp; More!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xA9Xq7hb6Q0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storage Media Life Expectancy: SSDs, HDDs &amp;amp; More!\"&gt;&lt;/iframe&gt;", "author_name": "ExplainingComputers", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xA9Xq7hb6Q0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ExplainingComputers"}}, "is_video": false}}], "before": null}}