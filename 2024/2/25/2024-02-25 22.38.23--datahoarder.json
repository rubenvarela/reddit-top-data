{"kind": "Listing", "data": {"after": "t3_1azamjv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those of you worried about not being able to play DivX/XviD AVI files in the future, please enjoy this screenshot of Kodi 20.4 playing a 20+ year old 160x120 RealMedia video. You don't appreciate how amazing FFMPEG is.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1azphye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 216, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 216, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k7NHtsX5zmn0xyCLmu2SzYLP1fcFr-Hdcb19-xxtQrA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708872198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yusjnrd1uqkc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?auto=webp&amp;s=eb0ccf81b97157c9a0462b7a46c38d253e72e5a3", "width": 3840, "height": 2160}, "resolutions": [{"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3203b1a3197b61f6a87a22f68898e9d09ffb12df", "width": 108, "height": 60}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc4bfe9138044832da7bab68f55a4ba62c4ab9b8", "width": 216, "height": 121}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f83f02327240f5d3c9bc663fa627bcb5aa92b36", "width": 320, "height": 180}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94f8d8600af239eee51bcf180b4845ab9b6c2a37", "width": 640, "height": 360}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=032cf24d07dfbb5b460a6e948a9e5c948977585c", "width": 960, "height": 540}, {"url": "https://preview.redd.it/yusjnrd1uqkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ec1e1669aa0d7024fd0751c57ccb8d038585841", "width": 1080, "height": 607}], "variants": {}, "id": "5Io6ryMNxvF535M3V7_qZkOZMvFgyw8QrPp5qz5Rf9U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1azphye", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azphye/for_those_of_you_worried_about_not_being_able_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yusjnrd1uqkc1.png", "subreddit_subscribers": 734492, "created_utc": 1708872198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/ to the article: ", "author_fullname": "t2_24c0105r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correct me if I'm wrong, but isn't 1 petabyte actually 1000 terabytes? This article is saying 1 petabyte relates to 200 terabytes. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwdqw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RDVJCAV-WCeBrpOuwnPFHiOg6U74nmJ02bi6Hlea_YU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708889045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/\"&gt;https://www.pcgamer.com/researchers-have-developed-a-very-big-disctm-that-can-store-up-to-200-terabytes-of-data-and-may-represent-a-return-to-optical-media-for-long-term-storage/&lt;/a&gt; to the article: &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/oomiusxh8skc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?auto=webp&amp;s=35e9a295883cc8bf230b8c4956e2c67508d187fa", "width": 1080, "height": 6345}, "resolutions": [{"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58e16eeb4cfc419e17002b33b7208a4487623c42", "width": 108, "height": 216}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed2602d5c297100f276e902d8321037c80ec9a82", "width": 216, "height": 432}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e4b86221b31c7e2181aaadb0a8be9b445a4306a", "width": 320, "height": 640}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b76bfbd9140cc231996cb3616467325630c8a55c", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7b0bd1dc99435934117e38917448ccb6492f652", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/oomiusxh8skc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a70f4322be2ecce917ab21991362747019f259a0", "width": 1080, "height": 2160}], "variants": {}, "id": "RbG64Tsu-Ka0QgRPSuO_KBjJrZopolGt3sqyDi8LeLI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azwdqw", "is_robot_indexable": true, "report_reasons": null, "author": "licidil95", "discussion_type": null, "num_comments": 30, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azwdqw/correct_me_if_im_wrong_but_isnt_1_petabyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/oomiusxh8skc1.jpeg", "subreddit_subscribers": 734492, "created_utc": 1708889045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "continue\n\n* [5,719,123 subtitles from opensubtitles.org](https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/) \\- subs 1 to 9180517\n* [opensubtitles.org dump - 1 million subtitles - 23 GB](https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/) \\- subs 9180519 to 9521948\n\n## opensubtitles.org.dump.9500000.to.9599999\n\nTODO i will add this part in about 10 days. now its 85% complete\n\n## opensubtitles.org.dump.9600000.to.9699999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;dn=opensubtitles.org.dump.9600000.to.9699999\n\n## opensubtitles.org.dump.9700000.to.9799999\n\n2GB = 100\\_000 subtitles = 100 sqlite files\n\n    magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;dn=opensubtitles.org.dump.9700000.to.9799999\n\n## download from github\n\nNOTE i will remove these files from github in some weeks, to keep the repo size below 10GB\n\n`ln` = create hardlinks\n\n    git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n    \n    mkdir opensubtitles.org.dump.9600000.to.9699999\n    ln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n      opensubtitles.org.dump.9600000.to.9699999\n    \n    mkdir opensubtitles.org.dump.9700000.to.9799999\n    ln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n      opensubtitles.org.dump.9700000.to.9799999\n\n## download from archive.org\n\nTODO upload to archive.org for long term storage\n\n## scraper\n\n[https://github.com/milahu/opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n\nmy latest version is still unreleased. it is based on my [aiohttp\\_chromium](https://github.com/milahu/aiohttp_chromium) to bypass cloudflare\n\ni have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com\n\n## problem of trust\n\none problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files\n\n## subtitles server\n\nTODO create a subtitles server to make this usable for thin clients (video players)\n\n* the biggest challenge is the database size of about 150GB\n* use metadata from subtitles\\_all.txt.gz from [https://dl.opensubtitles.org/addons/export/](https://dl.opensubtitles.org/addons/export/) \\- see also `subtitles_all.txt.gz-parse.py` in [opensubtitles-scraper](https://github.com/milahu/opensubtitles-scraper)\n* map movie filename to imdb id to subtitles - see also `get-subs.py`\n* map movie filename to movie name to subtitles\n* recode to utf8 - see also `repack.py`\n* remove ads - see also `opensubtitles-ads.txt` and `find_ads.py`\n* maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay", "author_fullname": "t2_naq15kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "subtitles from opensubtitles.org - subs 9500000 to 9799999", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azqwa4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;continue&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/\"&gt;5,719,123 subtitles from opensubtitles.org&lt;/a&gt; - subs 1 to 9180517&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/\"&gt;opensubtitles.org dump - 1 million subtitles - 23 GB&lt;/a&gt; - subs 9180519 to 9521948&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9500000.to.9599999&lt;/h2&gt;\n\n&lt;p&gt;TODO i will add this part in about 10 days. now its 85% complete&lt;/p&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9600000.to.9699999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:a76396daa3262f6d908b7e8ee47ab0958f8c7451&amp;amp;dn=opensubtitles.org.dump.9600000.to.9699999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9700000.to.9799999&lt;/h2&gt;\n\n&lt;p&gt;2GB = 100_000 subtitles = 100 sqlite files&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:de1c9696bfa0e6e4e65d5ed9e1bdf81b910cc7ef&amp;amp;dn=opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from github&lt;/h2&gt;\n\n&lt;p&gt;NOTE i will remove these files from github in some weeks, to keep the repo size below 10GB&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ln&lt;/code&gt; = create hardlinks&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/milahu/opensubtitles-scraper-new-subs\n\nmkdir opensubtitles.org.dump.9600000.to.9699999\nln opensubtitles-scraper-new-subs/shards/96xxxxx/* \\\n  opensubtitles.org.dump.9600000.to.9699999\n\nmkdir opensubtitles.org.dump.9700000.to.9799999\nln opensubtitles-scraper-new-subs/shards/97xxxxx/* \\\n  opensubtitles.org.dump.9700000.to.9799999\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;download from archive.org&lt;/h2&gt;\n\n&lt;p&gt;TODO upload to archive.org for long term storage&lt;/p&gt;\n\n&lt;h2&gt;scraper&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;https://github.com/milahu/opensubtitles-scraper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;my latest version is still unreleased. it is based on my &lt;a href=\"https://github.com/milahu/aiohttp_chromium\"&gt;aiohttp_chromium&lt;/a&gt; to bypass cloudflare&lt;/p&gt;\n\n&lt;p&gt;i have 2 VIP accounts (20 euros per year) so i can download 2000 subs per day. for continuous scraping, this is cheaper than a scraping service like zenrows.com&lt;/p&gt;\n\n&lt;h2&gt;problem of trust&lt;/h2&gt;\n\n&lt;p&gt;one problem with this project is: the files have no signatures, so i cannot prove the data integrity, and others will have to trust me that i dont modify the files&lt;/p&gt;\n\n&lt;h2&gt;subtitles server&lt;/h2&gt;\n\n&lt;p&gt;TODO create a subtitles server to make this usable for thin clients (video players)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the biggest challenge is the database size of about 150GB&lt;/li&gt;\n&lt;li&gt;use metadata from subtitles_all.txt.gz from &lt;a href=\"https://dl.opensubtitles.org/addons/export/\"&gt;https://dl.opensubtitles.org/addons/export/&lt;/a&gt; - see also &lt;code&gt;subtitles_all.txt.gz-parse.py&lt;/code&gt; in &lt;a href=\"https://github.com/milahu/opensubtitles-scraper\"&gt;opensubtitles-scraper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to imdb id to subtitles - see also &lt;code&gt;get-subs.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;map movie filename to movie name to subtitles&lt;/li&gt;\n&lt;li&gt;recode to utf8 - see also &lt;code&gt;repack.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;remove ads - see also &lt;code&gt;opensubtitles-ads.txt&lt;/code&gt; and &lt;code&gt;find_ads.py&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;maybe also scrape download counts and ratings from opensubtitles.org, but usually, i simply download all subtitles for a movie, and switch through the subtitle tracks until i find a good match. in rare cases i need to adjust the subs delay&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?auto=webp&amp;s=4addee391a8fcdd79d9dc8bb3be80840cfbb2292", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6d87b10c172c2af78696cd0a9017ecffcb12898", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f65d0b1b68d620588e0a5da27334b745e70c8dc", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50392b9b700a2ed49029d9fc031570a4f0e6318b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5df9dc84a8f8e25e12eef950e3fe12459f8c712", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e752c9a858437424fd7d6df3cb09459a0923e91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6DPPyreBMy-qci499kE-8u_62wyZvYybHyauUWfRhBc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=261adcbc7bfcb31a1aae5ed22a391bedcf898e0b", "width": 1080, "height": 540}], "variants": {}, "id": "xzPnR4eWjwiQ6_oTfaBERSRsup0-vUB8Zvj-2miVx3M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqwa4", "is_robot_indexable": true, "report_reasons": null, "author": "milahu2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/", "subreddit_subscribers": 734492, "created_utc": 1708875847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.\n\nAll I did was drag and drop in the windows explorer.\n\nIf I want to routinely back up my drive every few months, should I be using an application?\n\nI feel like dragging and dropping again after the first transfer is suboptimal. \n\nTIA.", "author_fullname": "t2_iaosze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys do your routine offline backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgjh5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708840359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got another drive to store a backup offline, but man that took like 8 hours to back up 3tb+.&lt;/p&gt;\n\n&lt;p&gt;All I did was drag and drop in the windows explorer.&lt;/p&gt;\n\n&lt;p&gt;If I want to routinely back up my drive every few months, should I be using an application?&lt;/p&gt;\n\n&lt;p&gt;I feel like dragging and dropping again after the first transfer is suboptimal. &lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgjh5", "is_robot_indexable": true, "report_reasons": null, "author": "TryTurningItOffAgain", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgjh5/how_do_you_guys_do_your_routine_offline_backups/", "subreddit_subscribers": 734492, "created_utc": 1708840359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8nd1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I think Google Drive does not like Stablebit Drivepool... Is this a known issue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "name": "t3_1azw154", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YYV1XJoUQO-m-Wsyn40f53rydCFd1DDPPTJoc3wvGmQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708888210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/i61o4c4w5skc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?auto=webp&amp;s=97b96e3f2993b01ae7e76def3205a8e6fd2c6fc0", "width": 1695, "height": 1271}, "resolutions": [{"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43e90b906e7518f7f4055d6862693fdffd7dbee1", "width": 108, "height": 80}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d13ce242077bc065a6e17b70ec6d257322eb3b4d", "width": 216, "height": 161}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc85c967e1950d5ab636c37a2526989cf526ee27", "width": 320, "height": 239}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e46c38f654a913cc4754921fe9561a58124e3fa6", "width": 640, "height": 479}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=793d28a666a910e9c154665967807279ed313994", "width": 960, "height": 719}, {"url": "https://preview.redd.it/i61o4c4w5skc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b2990d780870fb20cb637db75c632d7648da39a", "width": 1080, "height": 809}], "variants": {}, "id": "AtCgh6Mhawd7HhppuIDmeqPGPJigARGYgTcqhgdXPJU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azw154", "is_robot_indexable": true, "report_reasons": null, "author": "clavicon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azw154/i_think_google_drive_does_not_like_stablebit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/i61o4c4w5skc1.jpeg", "subreddit_subscribers": 734492, "created_utc": 1708888210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know that the best way is to preserve the original without re-encoding. But I'm actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don't even wanna know about Realmedia compatibility today.", "author_fullname": "t2_1e60y1rd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have videos in divx and xvid codec. Should I encode them to modern standards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkv62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708856980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that the best way is to preserve the original without re-encoding. But I&amp;#39;m actually worried that one day, these codec will be gone forever, which make typical media players unable to play. My personal experience is with Apple Quicktime mov file that is not playable from Jellyfin. Hence this fear. I don&amp;#39;t even wanna know about Realmedia compatibility today.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkv62", "is_robot_indexable": true, "report_reasons": null, "author": "skylinestar1986", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azkv62/i_have_videos_in_divx_and_xvid_codec_should_i/", "subreddit_subscribers": 734492, "created_utc": 1708856980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know it can be found easily on a desktop browser by using the network tab on inspect element, but apparently the Amazon 3D viewer can only be viewed on mobile phones. The button won\u2019t appear even on mobile browsers.\n\nI have access to both Android and iOS devices. Google wasn\u2019t able to help me. Is there a way to get this file from the app?\n", "author_fullname": "t2_njkbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to extract a 3D model from Amazon mobile app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1az9vtr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p_LsJCsEoMKI1PilEki20NTNFiTK2iLYRTaigNetu3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708820090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it can be found easily on a desktop browser by using the network tab on inspect element, but apparently the Amazon 3D viewer can only be viewed on mobile phones. The button won\u2019t appear even on mobile browsers.&lt;/p&gt;\n\n&lt;p&gt;I have access to both Android and iOS devices. Google wasn\u2019t able to help me. Is there a way to get this file from the app?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mht3a8pgjmkc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?auto=webp&amp;s=185275c6fc53ce522d3d5b01de227b2d822edd78", "width": 1179, "height": 1394}, "resolutions": [{"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f945ce555eb8eada9180997c86f4577ed5d1df4", "width": 108, "height": 127}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f507582675312b81f1676e345e6d2ab50e67472", "width": 216, "height": 255}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bb1642f65bc7de163881f75bc835637da9db4ed", "width": 320, "height": 378}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=842e72badbb80e3e0fa41ac08e7ce13045d44afa", "width": 640, "height": 756}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca35f300346f389a90732542ed49794151f3b9", "width": 960, "height": 1135}, {"url": "https://preview.redd.it/mht3a8pgjmkc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ca7903283d6b46986dd5aaafe0c6e7c8b6b4741", "width": 1080, "height": 1276}], "variants": {}, "id": "w4V-ynmE8IG6AShLabZtEMULa3nIkvonUxRcw-AAcKo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az9vtr", "is_robot_indexable": true, "report_reasons": null, "author": "esteban_agpa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az9vtr/how_to_extract_a_3d_model_from_amazon_mobile_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mht3a8pgjmkc1.jpeg", "subreddit_subscribers": 734492, "created_utc": 1708820090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I\u2019ve got a ton of drives, and lots of project backups from the last 15+ years. I\u2019m talking many many terabytes across multiple drives. Lots of these backups have duplicate folders, some of those duplicates may or may not have a few unique files or folders in them. And some of the drives may have corrupted files (when copying files from old drives to new ones sometimes Windows freezes up on certain files, so I don\u2019t know if they are corrupt or what\u2026)\n\nI know.. I regret not backing things up properly all these years. It\u2019s all haphazard and disorganized\n\nSo I\u2019m looking for the best way to somehow consolidate all these folders and files onto one or more drives, skipping the duplicate and corrupt files, so I have everything in one place (that I can then backup properly)\n\nI\u2019m on Windows 10. What would be my best course of action?\n\nThank you!", "author_fullname": "t2_49mql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consolidate multiple drives with duplicate and (maybe) corrupt files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azt8rl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708883081.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708881577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve got a ton of drives, and lots of project backups from the last 15+ years. I\u2019m talking many many terabytes across multiple drives. Lots of these backups have duplicate folders, some of those duplicates may or may not have a few unique files or folders in them. And some of the drives may have corrupted files (when copying files from old drives to new ones sometimes Windows freezes up on certain files, so I don\u2019t know if they are corrupt or what\u2026)&lt;/p&gt;\n\n&lt;p&gt;I know.. I regret not backing things up properly all these years. It\u2019s all haphazard and disorganized&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m looking for the best way to somehow consolidate all these folders and files onto one or more drives, skipping the duplicate and corrupt files, so I have everything in one place (that I can then backup properly)&lt;/p&gt;\n\n&lt;p&gt;I\u2019m on Windows 10. What would be my best course of action?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azt8rl", "is_robot_indexable": true, "report_reasons": null, "author": "un-sub", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azt8rl/consolidate_multiple_drives_with_duplicate_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azt8rl/consolidate_multiple_drives_with_duplicate_and/", "subreddit_subscribers": 734492, "created_utc": 1708881577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background:  \nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the \"2\" different types of medium).   \n\n\nMain part  \nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don't experience any performance issues so i wouldn't go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  \n\n\nMain question  \nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  \n\n\nThanks for reading and appreciate all the advice!  \n\n\n&amp;#x200B;\n\nTLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can't seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.", "author_fullname": "t2_31744y31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "12TB drives and rebuild time with RAID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpunm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708873159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background:&lt;br/&gt;\nFirst time poster here and getting my feet wet with data hoarding. Currently bought a 8 bay Synology for moving my heaps of data of my pc and onto a system that can be backed up more easily. I started of with 3 12TB drives in RAID 5 and recently upgraded to 5 12TB drives. I currently have a friend with a NAS who wants to exchange backups (for the offsite backup). Onsite i have the NAS which i also backup weekly to a hard drive as cold storage, to almost live by the 3-2-1 rule (i would for example need to change the onsite backup to a SSD instead of HDD to achieve the &amp;quot;2&amp;quot; different types of medium).   &lt;/p&gt;\n\n&lt;p&gt;Main part&lt;br/&gt;\nWith the two added drives i want to use one for the offsite backup of my friend and non important storage. This leaves 4 drives for my storage pool, i wanted to put it into a RAID 6 config for the extra redundancy and the easy transform form RAID 5. When i started googling i saw that people advised RAID 10 due to the rebuild time of a RAID 5/6 config with 10TB+ drives and the chance of a drive dying in the rebuild process. RAID 10 does provide better performance but currently i don&amp;#39;t experience any performance issues so i wouldn&amp;#39;t go to RAID 10 for the performance. The reason i would go RAID 10 is when a drive dies it is a lot less hard on the remaining drives to rebuild the array and lower change of a drive dying in the rebuild process, but comes with the risk of only being able to lose one drive.  &lt;/p&gt;\n\n&lt;p&gt;Main question&lt;br/&gt;\nI would want to know what your guys opinion on this is. I am looking for reliability, should i go with RAID 6, RAID 10 or stick with RAID 5 (and increase storage capacity) because i ensure my backups are done regularly (offsite\u00a0and\u00a0onsite).  &lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and appreciate all the advice!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR: I have 4 12 TB drives which i want to put into a RAID 5, 6 or 10 config. Can&amp;#39;t seem to decide between RAID 5/6 and RAID 10 due to the rebuild times of RAID 5/6 with the 12TB drives. I am looking for reliability and do not seek more performance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpunm", "is_robot_indexable": true, "report_reasons": null, "author": "WietseTDX", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpunm/12tb_drives_and_rebuild_time_with_raid/", "subreddit_subscribers": 734492, "created_utc": 1708873159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nHey,   \n\ni am not from the IT field, albeit my statistical (testing)  understanding is profound enough to handle data properly, the problem  is, i dont know how to obtain this data in the first place.   \n\nI visit a simple forum/\"news\" website (not reddit) on a daily basis  and i came to notice, that on certain topics the user activity  (comments, \"upvotes\") is way out of proportion + other discrepancies  compared to other articles and even way beyond earlier point in times of  said topic (\\~ before end of 2023).   \n\n**My suspicion comes from the following:** \n\n&amp;#x200B;\n\n1) mean most upvoted comment of the day (across all threads, including said topic before 2024) = \\~100     \n\n\n2) the comment on the #1comment  usually follow the Zipf's law, so:     \n \n\nif  #1 = 100   then\n\n\\#1.1= 50   \n\n\\#1.2= 25   \n\n\\#1.3 = 12   \n\nunder the most upvoted comment.  Regular halving, a pattern you maybe know from reddit.   \n\n&amp;#x200B;\n\n 3)  the upvotes also accumulate over hours until saturation, as user  come by over time and click on the article, they may like or comment on  it, sure some dynamic patterns like for sunday evening its all  alittle  faster and higher numbers compared to a noon wednesday thread, but you  get the idea.   \n\n&amp;#x200B;\n\n**now the suspicious topics that get a thread maybe once a week:** \n\n\\- mean most upvoted comment = 1000   \n\n\\- the comments on the #1 comment still show usual real user  behavior: 50 / 25 etc. (it looks like there is a mass upvote on the #1  comment but they dont even go through comment-comments, so it displays  the \"normal\" numbers of real users like in other articles for all  others)   \n\n\\- the 1000+ upvotes accumulate in under 10min after post of thread.   \n\n\\- sometimes the number of comments in general explode in the same time frame, often just \"yeah totally!\" non specifc bot phrases. So if a normal  article has like 100 comments, it has like 1500 comments.   \n\n \\---   \n\nthe forum itself is quit \"old\", just using F12 and html i can  identify every passage for the like and comment section, so no  complicated java scripts where you cant really extract single objects of  the websites UI from each other.   \n\n\\---   \n\ni would really love to use it as a small data science project and  also have something in my hands to back up my claims before i contact  the owner.   The problem is i dont even know where to start and maybe you have some terminology of buzzwords i could search for to find what i want. \n\n&amp;#x200B;", "author_fullname": "t2_4wsr1tsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "log suspicion general user activity on a forum", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azv91z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708886374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,   &lt;/p&gt;\n\n&lt;p&gt;i am not from the IT field, albeit my statistical (testing)  understanding is profound enough to handle data properly, the problem  is, i dont know how to obtain this data in the first place.   &lt;/p&gt;\n\n&lt;p&gt;I visit a simple forum/&amp;quot;news&amp;quot; website (not reddit) on a daily basis  and i came to notice, that on certain topics the user activity  (comments, &amp;quot;upvotes&amp;quot;) is way out of proportion + other discrepancies  compared to other articles and even way beyond earlier point in times of  said topic (~ before end of 2023).   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My suspicion comes from the following:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) mean most upvoted comment of the day (across all threads, including said topic before 2024) = ~100     &lt;/p&gt;\n\n&lt;p&gt;2) the comment on the #1comment  usually follow the Zipf&amp;#39;s law, so:     &lt;/p&gt;\n\n&lt;p&gt;if  #1 = 100   then&lt;/p&gt;\n\n&lt;p&gt;#1.1= 50   &lt;/p&gt;\n\n&lt;p&gt;#1.2= 25   &lt;/p&gt;\n\n&lt;p&gt;#1.3 = 12   &lt;/p&gt;\n\n&lt;p&gt;under the most upvoted comment.  Regular halving, a pattern you maybe know from reddit.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;3)  the upvotes also accumulate over hours until saturation, as user  come by over time and click on the article, they may like or comment on  it, sure some dynamic patterns like for sunday evening its all  alittle  faster and higher numbers compared to a noon wednesday thread, but you  get the idea.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;now the suspicious topics that get a thread maybe once a week:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;- mean most upvoted comment = 1000   &lt;/p&gt;\n\n&lt;p&gt;- the comments on the #1 comment still show usual real user  behavior: 50 / 25 etc. (it looks like there is a mass upvote on the #1  comment but they dont even go through comment-comments, so it displays  the &amp;quot;normal&amp;quot; numbers of real users like in other articles for all  others)   &lt;/p&gt;\n\n&lt;p&gt;- the 1000+ upvotes accumulate in under 10min after post of thread.   &lt;/p&gt;\n\n&lt;p&gt;- sometimes the number of comments in general explode in the same time frame, often just &amp;quot;yeah totally!&amp;quot; non specifc bot phrases. So if a normal  article has like 100 comments, it has like 1500 comments.   &lt;/p&gt;\n\n&lt;p&gt;---   &lt;/p&gt;\n\n&lt;p&gt;the forum itself is quit &amp;quot;old&amp;quot;, just using F12 and html i can  identify every passage for the like and comment section, so no  complicated java scripts where you cant really extract single objects of  the websites UI from each other.   &lt;/p&gt;\n\n&lt;p&gt;---   &lt;/p&gt;\n\n&lt;p&gt;i would really love to use it as a small data science project and  also have something in my hands to back up my claims before i contact  the owner.   The problem is i dont even know where to start and maybe you have some terminology of buzzwords i could search for to find what i want. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azv91z", "is_robot_indexable": true, "report_reasons": null, "author": "Stffnpeter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azv91z/log_suspicion_general_user_activity_on_a_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azv91z/log_suspicion_general_user_activity_on_a_forum/", "subreddit_subscribers": 734492, "created_utc": 1708886374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to download the full image, and not the parts o. I checked the network tab, some of the images I was able to get some from there. But for the rest, I see them as small images, which together will provide me with a full image  \nmage  \nage  \n\n\nhttps://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;format=png&amp;auto=webp&amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b\n\nIs there a way to download the full image, and not the small parts of a full image?", "author_fullname": "t2_b2929", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download images from the website stored as multiple small images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pbfkt847xrkc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 69, "x": 108, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0279e52efbd3fac977043864d2e44fac962825"}, {"y": 139, "x": 216, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1df4e9dd4d74a5de388e4a241744bb294737baa5"}, {"y": 206, "x": 320, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4bfd7832accb11256c6106bdd69bf75a4df6b6c6"}, {"y": 412, "x": 640, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13218cb5fd4393ddf01355e490f83f45468c33cd"}, {"y": 619, "x": 960, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2b57873ef01381c4d58a81400af0aa8693f71bab"}, {"y": 696, "x": 1080, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7b3359944c716fd58278de0e79a8dfa4ecd39c"}], "s": {"y": 720, "x": 1116, "u": "https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;format=png&amp;auto=webp&amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b"}, "id": "pbfkt847xrkc1"}}, "name": "t3_1azuudk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gn8qHHoZZ4RmfzFibwYEAM6KB4am957Qqlw25ZP-sFc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708885400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to download the full image, and not the parts o. I checked the network tab, some of the images I was able to get some from there. But for the rest, I see them as small images, which together will provide me with a full image&lt;br/&gt;\nmage&lt;br/&gt;\nage  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b\"&gt;https://preview.redd.it/pbfkt847xrkc1.png?width=1116&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55c4988d8bb0fc8fa21d640c5a27c45065fe368b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there a way to download the full image, and not the small parts of a full image?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azuudk", "is_robot_indexable": true, "report_reasons": null, "author": "rip777", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azuudk/how_to_download_images_from_the_website_stored_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azuudk/how_to_download_images_from_the_website_stored_as/", "subreddit_subscribers": 734492, "created_utc": 1708885400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi people,  \nI'm a beginner recently subscribed to Hetzner Storage Box.  \nI'm finding it hard to mount it to my Hetzner VPS.   \nI'm stuck on this for the past week.  \nThe info regarding Hetzner Storage box and the mounting options are so rare in the internet.  \n\n\nAppreciate the help. Thanks ", "author_fullname": "t2_1nxbeabb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with mounting Hetzner Storage box to VPS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpv9n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708873202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people,&lt;br/&gt;\nI&amp;#39;m a beginner recently subscribed to Hetzner Storage Box.&lt;br/&gt;\nI&amp;#39;m finding it hard to mount it to my Hetzner VPS.&lt;br/&gt;\nI&amp;#39;m stuck on this for the past week.&lt;br/&gt;\nThe info regarding Hetzner Storage box and the mounting options are so rare in the internet.  &lt;/p&gt;\n\n&lt;p&gt;Appreciate the help. Thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpv9n", "is_robot_indexable": true, "report_reasons": null, "author": "BossZkie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpv9n/need_help_with_mounting_hetzner_storage_box_to_vps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpv9n/need_help_with_mounting_hetzner_storage_box_to_vps/", "subreddit_subscribers": 734492, "created_utc": 1708873202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. \n\nI went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn't apply to my build), or the onboard SATA controller is terrible.\n\nThe motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can't find the controllers used on this motherboard.\n\nMy question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I'm unaware of, as opposed to using the onboard SATA ports?", "author_fullname": "t2_74e8mz3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New unRAID build, need advice for onboard SATA ports vs  LSI/HBA PCIe card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpmz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708872583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ca.pcpartpicker.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to build an unRAID system with the goal of hosting my Plex server. Keep in mind that the only thing I bought as of today is the CPU, which gives me some flexibility. &lt;/p&gt;\n\n&lt;p&gt;I went with a motherboard having 8 SATA 6Gbps ports for simplicity, but I recently came across some posts indicating that many people go with LSI/HBA cards. So I did some research and learned that happens for a few reasons: to have more ports available (doesn&amp;#39;t apply to my build), or the onboard SATA controller is terrible.&lt;/p&gt;\n\n&lt;p&gt;The motherboard I chose is the ASRock Z690 Pro RS. Looking at yhe manuals, I can&amp;#39;t find the controllers used on this motherboard.&lt;/p&gt;\n\n&lt;p&gt;My question is, does someone know what controllers are on this motherboard? Is my approach to SATA ports okay? Are there other benefits of getting an LSI/HBA card that I&amp;#39;m unaware of, as opposed to using the onboard SATA ports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpmz7", "is_robot_indexable": true, "report_reasons": null, "author": "JakeHa0991", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpmz7/new_unraid_build_need_advice_for_onboard_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ca.pcpartpicker.com/user/Jake09/saved/#view=r7rcMp", "subreddit_subscribers": 734492, "created_utc": 1708872583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  \n\n\nSo that:\n\nIf I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.  \n&lt;BUT&gt;  \nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  \n\n\nIf it is \"doable\" how does one do it?  \n\n\nThe purpose would be to have a \"hidden\" cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  \n\n\n&amp;#x200B;", "author_fullname": "t2_3kfdots", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to have CD-Audio on a BDXL disc and data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azpb90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708871677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excuse me if this is the wrong place for this question.  Is it possible to have CD-Audio on a BDXL disc and data? Meaning can the data co-exist on the same disc?  &lt;/p&gt;\n\n&lt;p&gt;So that:&lt;/p&gt;\n\n&lt;p&gt;If I were to put the disc in a bdxl-drive the audio CD portion is accessible as well as the data.&lt;br/&gt;\n&amp;lt;BUT&amp;gt;&lt;br/&gt;\nIf I were to put the disc in a regular CD drive it would only play the CD-audio.  &lt;/p&gt;\n\n&lt;p&gt;If it is &amp;quot;doable&amp;quot; how does one do it?  &lt;/p&gt;\n\n&lt;p&gt;The purpose would be to have a &amp;quot;hidden&amp;quot; cache of data, that looks like a bunch of audio CDs, and acts like a bunch of audio CDs.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azpb90", "is_robot_indexable": true, "report_reasons": null, "author": "trseeker", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azpb90/is_it_possible_to_have_cdaudio_on_a_bdxl_disc_and/", "subreddit_subscribers": 734492, "created_utc": 1708871677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently trying to downsize and no longer pay the fee for my monthly 2TB Google Drive plan. I moved and am now currently/temporarily stuck on a data plan. I am not the owner of the house/person who pays for the internet plan, so it's not optional to get off it until I move out much later this year. My GDrive is currently sitting at about 1.75TB and would much rather store it physically and quit paying for a service I can't really use anyways. We're on a 1.25TB cap so downloading it all in one go really isn't an option. I could theoretically do it over time but I'd really prefer to just avoid the headache of that. Is there a service that'll do this for me or am I better of just sticking with the bill for now?\n\nSorry if this is a stupid question, as much as I'm more of a hoarder than the typical PC owner, I really haven't delved in to this stuff much before.", "author_fullname": "t2_20duihb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Service that will download my Google Drive to a hard disk then mail it to me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azlleh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708859777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently trying to downsize and no longer pay the fee for my monthly 2TB Google Drive plan. I moved and am now currently/temporarily stuck on a data plan. I am not the owner of the house/person who pays for the internet plan, so it&amp;#39;s not optional to get off it until I move out much later this year. My GDrive is currently sitting at about 1.75TB and would much rather store it physically and quit paying for a service I can&amp;#39;t really use anyways. We&amp;#39;re on a 1.25TB cap so downloading it all in one go really isn&amp;#39;t an option. I could theoretically do it over time but I&amp;#39;d really prefer to just avoid the headache of that. Is there a service that&amp;#39;ll do this for me or am I better of just sticking with the bill for now?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is a stupid question, as much as I&amp;#39;m more of a hoarder than the typical PC owner, I really haven&amp;#39;t delved in to this stuff much before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azlleh", "is_robot_indexable": true, "report_reasons": null, "author": "NovaResonance", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azlleh/service_that_will_download_my_google_drive_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azlleh/service_that_will_download_my_google_drive_to_a/", "subreddit_subscribers": 734492, "created_utc": 1708859777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azkyd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9c2bi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "japan", "selftext": "# Introduction\n\nFor anybody that has a 1999 copy of Kodansha's CD-ROM version of '[Encyclopedia of Japan](https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan)', here's instructions on how to install and run it on Windows 11 (x64).\n\nSurprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.\n\nWhilst I have made a backup ISO file, ~~I unfortunately can't lawfully distribute it. As such, please don't ask.~~\n\n**Edit: The ISO file has been uploaded [here](https://archive.org/details/EOJ10) based on [this](https://archive.org/about/dmca.php) statement. Do not use this software if you have no legal right to do so.**\n\nFor those without a copy, if you would like to access its contents, you can do so via [JapanKnowledge](https://japanknowledge.com/en/contents/eoj/).\n\n# Installation\n\nFollow the steps below:\n\n1. Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of `Internet Explorer 5`.\n2. Attempt to launch the application. If you receive an error message concerning `MCI32.OCX`, proceed with steps 3-7.\n3. Type `Command Prompt` into the taskbar `Search` box; this is located next to the `Start` menu.\n4. In the search results, right-click `Command Prompt` and select `Run as administrator`.\n5. Copy and paste the following command: `regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX`.\n6. If the command doesn't instantly execute, press the `Enter/Return` key.\n7. Confirm that `MCI32.OCX` has been registered, and then close `Command Prompt`.\n8. Launch the encyclopedia application.\n\n# Missing Media\n\nWhen a media file is unable to load, you will see an `X` instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.\n\n# Fix\n\nFollow the steps below:\n\n1. `Open` the CD-ROM.\n2. Go to: `[CD DRIVE LETTER]:\\eoj`.\n3. Copy the: `[CD DRIVE LETTER]:\\eoj\\media` folder.\n4. Paste the entire folder to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan`.\n5. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n6. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n7. Copy: `Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0`.\n8. Paste this into the top bar and press `Enter/Return`.\n9. Right-click `MEDIA_PATH` and select `Modify`. Change the `Value data` to: `C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media`.\n10. Click `OK` and close.\n\n# Unable to Print\n\nDuring testing, the `Print` button did not function.\n\n# Fix\n\nIn Windows 11, use `Ctrl + P` to print pages.\n\nTo remove the header and footer, follow the steps below:\n\n1. Type `Registry Editor` into the taskbar `Search` box; this is located next to the `Start` menu.\n2. In the search results, right-click `Registry Editor` and select `Run as administrator`.\n3. Copy: `Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup`.\n4. Paste this into the top bar and press `Enter/Return`.\n5. In the empty space below `(Default)`, right-click and select `New` \\-&gt; `String Value`. Type `header` and press `Enter/Return`.\n6. Repeat the step above, but type `footer` instead.\n7. Close `Registry Editor`.\n\nNote: This is a system-wide change. You will need to right-click the values and select `Delete` to reverse the change.\n\n# Uninstallation\n\nWhen uninstalling, the uninstaller may ask if you would like to delete `Mci32.ocx` and `Comdlg32.OCX`. Click `No` or `No to All`. Copies of these files should remain in: `C:\\Windows\\SysWOW64`.", "author_fullname": "t2_noxre9enh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kodansha's 'Encyclopedia of Japan' CD-ROM (1999) - Windows 11 Installation Instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/japan", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ayrh2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 130, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 130, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708809657.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708770653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;For anybody that has a 1999 copy of Kodansha&amp;#39;s CD-ROM version of &amp;#39;&lt;a href=\"https://en.wikipedia.org/wiki/Kodansha_Encyclopedia_of_Japan\"&gt;Encyclopedia of Japan&lt;/a&gt;&amp;#39;, here&amp;#39;s instructions on how to install and run it on Windows 11 (x64).&lt;/p&gt;\n\n&lt;p&gt;Surprisingly, I was able to acquire a factory-sealed copy, for approximately \u00a58,250, yesterday.&lt;/p&gt;\n\n&lt;p&gt;Whilst I have made a backup ISO file, &lt;del&gt;I unfortunately can&amp;#39;t lawfully distribute it. As such, please don&amp;#39;t ask.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit: The ISO file has been uploaded &lt;a href=\"https://archive.org/details/EOJ10\"&gt;here&lt;/a&gt; based on &lt;a href=\"https://archive.org/about/dmca.php\"&gt;this&lt;/a&gt; statement. Do not use this software if you have no legal right to do so.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For those without a copy, if you would like to access its contents, you can do so via &lt;a href=\"https://japanknowledge.com/en/contents/eoj/\"&gt;JapanKnowledge&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;Installation&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Insert the CD-ROM, and install the application without changing the default installation directory. It is not necessary to install the included copy of &lt;code&gt;Internet Explorer 5&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Attempt to launch the application. If you receive an error message concerning &lt;code&gt;MCI32.OCX&lt;/code&gt;, proceed with steps 3-7.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Command Prompt&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Command Prompt&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy and paste the following command: &lt;code&gt;regsvr32 C:\\Windows\\SysWOW64\\MCI32.OCX&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;If the command doesn&amp;#39;t instantly execute, press the &lt;code&gt;Enter/Return&lt;/code&gt; key.&lt;/li&gt;\n&lt;li&gt;Confirm that &lt;code&gt;MCI32.OCX&lt;/code&gt; has been registered, and then close &lt;code&gt;Command Prompt&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Launch the encyclopedia application.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Missing Media&lt;/h1&gt;\n\n&lt;p&gt;When a media file is unable to load, you will see an &lt;code&gt;X&lt;/code&gt; instead of the media. This is because the software was originally designed to load media from the CD-ROM, which means that the CD-ROM must always be inserted in order to view those files.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;Follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;code&gt;Open&lt;/code&gt; the CD-ROM.&lt;/li&gt;\n&lt;li&gt;Go to: &lt;code&gt;[CD DRIVE LETTER]:\\eoj&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy the: &lt;code&gt;[CD DRIVE LETTER]:\\eoj\\media&lt;/code&gt; folder.&lt;/li&gt;\n&lt;li&gt;Paste the entire folder to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Uniscope\\Kodansha Encyclopedia of Japan\\1.0&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Right-click &lt;code&gt;MEDIA_PATH&lt;/code&gt; and select &lt;code&gt;Modify&lt;/code&gt;. Change the &lt;code&gt;Value data&lt;/code&gt; to: &lt;code&gt;C:\\Program Files (x86)\\Kodansha Encyclopedia of Japan\\media&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Click &lt;code&gt;OK&lt;/code&gt; and close.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Unable to Print&lt;/h1&gt;\n\n&lt;p&gt;During testing, the &lt;code&gt;Print&lt;/code&gt; button did not function.&lt;/p&gt;\n\n&lt;h1&gt;Fix&lt;/h1&gt;\n\n&lt;p&gt;In Windows 11, use &lt;code&gt;Ctrl + P&lt;/code&gt; to print pages.&lt;/p&gt;\n\n&lt;p&gt;To remove the header and footer, follow the steps below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Type &lt;code&gt;Registry Editor&lt;/code&gt; into the taskbar &lt;code&gt;Search&lt;/code&gt; box; this is located next to the &lt;code&gt;Start&lt;/code&gt; menu.&lt;/li&gt;\n&lt;li&gt;In the search results, right-click &lt;code&gt;Registry Editor&lt;/code&gt; and select &lt;code&gt;Run as administrator&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Copy: &lt;code&gt;Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\PageSetup&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Paste this into the top bar and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;In the empty space below &lt;code&gt;(Default)&lt;/code&gt;, right-click and select &lt;code&gt;New&lt;/code&gt; -&amp;gt; &lt;code&gt;String Value&lt;/code&gt;. Type &lt;code&gt;header&lt;/code&gt; and press &lt;code&gt;Enter/Return&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Repeat the step above, but type &lt;code&gt;footer&lt;/code&gt; instead.&lt;/li&gt;\n&lt;li&gt;Close &lt;code&gt;Registry Editor&lt;/code&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Note: This is a system-wide change. You will need to right-click the values and select &lt;code&gt;Delete&lt;/code&gt; to reverse the change.&lt;/p&gt;\n\n&lt;h1&gt;Uninstallation&lt;/h1&gt;\n\n&lt;p&gt;When uninstalling, the uninstaller may ask if you would like to delete &lt;code&gt;Mci32.ocx&lt;/code&gt; and &lt;code&gt;Comdlg32.OCX&lt;/code&gt;. Click &lt;code&gt;No&lt;/code&gt; or &lt;code&gt;No to All&lt;/code&gt;. Copies of these files should remain in: &lt;code&gt;C:\\Windows\\SysWOW64&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh2u", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1ayrh2k", "is_robot_indexable": true, "report_reasons": null, "author": "YokaiZukan", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 825414, "created_utc": 1708770653.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708857334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.japan", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iwN9zxT6Uhkn2C4TyPsHYlo6-bc81mmFbAkoCf0703c.jpg?auto=webp&amp;s=2c8e24d8c9881da3476c3557f2c03a0ae2598ea4", "width": 50, "height": 39}, "resolutions": [], "variants": {}, "id": "IXQnJ7fzMbZbZVznbHAi-3muAls33KoXSzVH8LwT3c0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azkyd5", "is_robot_indexable": true, "report_reasons": null, "author": "zombieshavebrains", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1ayrh2k", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azkyd5/kodanshas_encyclopedia_of_japan_cdrom_1999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/japan/comments/1ayrh2k/kodanshas_encyclopedia_of_japan_cdrom_1999/", "subreddit_subscribers": 734492, "created_utc": 1708857334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Help! I am trying to understand how SnapRAID works.\n\nI have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?", "author_fullname": "t2_e4owahyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnapRAID Cloned Disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azgdp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708840213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708839802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help! I am trying to understand how SnapRAID works.&lt;/p&gt;\n\n&lt;p&gt;I have 4 data disks and 2 parity disks. Say sometime later, lighting  strikes and I lose all 6 disks. But before it struck, I had cloned all 6  disks with rsync (including content files). If I insert all 6 backup  disks into a new PC, redownload SnapRAID and reconfigure to point to the disks, and run a scrub, would the scrub complete with no errors? Or would I lose my 2 disk protection and need to regenerate the content/parity files, thereby making backing up the parity disks useless?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azgdp2", "is_robot_indexable": true, "report_reasons": null, "author": "x5KSAM", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azgdp2/snapraid_cloned_disks/", "subreddit_subscribers": 734492, "created_utc": 1708839802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. \n\nHowever, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.", "author_fullname": "t2_4kz0kk4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy and best method to clean VHS tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azg3ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708838838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context: I recently purchase an upscaler, sansui vcr and elgato capture card to start digitizing my families home VHS tape collection. I have recorded probably about two videos so far and it went by smooth. After two videos I got bad playback and purchased a vcr head cleaner. &lt;/p&gt;\n\n&lt;p&gt;However, I still have the problem of some tapes getting the vcr more dirty and want to ensure the longevity of the tapes and less strenuous on the digitizing process. The electronic vcr tape cleaners seem to be rare and not sure the other method to go about cleaning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azg3ig", "is_robot_indexable": true, "report_reasons": null, "author": "harshcloud", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azg3ig/easy_and_best_method_to_clean_vhs_tapes/", "subreddit_subscribers": 734492, "created_utc": 1708838838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to convert my home VHS collection to DVD, but my Sansui VRDVD4005 is no longer able to read disks. I've had it for quite a while and it is not in bad condition by any means. Unfortunately, the literature on this device is not very extensive, and the error code I have received does not seem to apply (or at least I think). The error code it has spit out (at least once) is C-104 which seems to mean the disk is incompatible with the device. However, I have used this device before to convert my tapes. I do have a few disks from when I converted years back that can be read, but it does not want to take new disks at all. Is it possible it cannot take newer disks that record at a higher speed? All disks have been bought in the USA, so it shouldn't be a region issue. The manual states it can record on DVD-R ver 2.0 (4.7 GB). The disks I have used successfully in the past were 8x speed, but only some can be read while others cannot be. Same brand and everything. The disks I have been trying to use are 16x.\n\nI have tried swapping the DVD drive with a new model that was supposedly compatible with it and could write on disks that are capable of writing at a higher speed, but the disks are still not detected.\n\nAny other ideas on how to get this working again?", "author_fullname": "t2_p7alv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sansui VHS to DVD not detecting new disks, anyone have experience with this machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1az7b50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708813508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to convert my home VHS collection to DVD, but my Sansui VRDVD4005 is no longer able to read disks. I&amp;#39;ve had it for quite a while and it is not in bad condition by any means. Unfortunately, the literature on this device is not very extensive, and the error code I have received does not seem to apply (or at least I think). The error code it has spit out (at least once) is C-104 which seems to mean the disk is incompatible with the device. However, I have used this device before to convert my tapes. I do have a few disks from when I converted years back that can be read, but it does not want to take new disks at all. Is it possible it cannot take newer disks that record at a higher speed? All disks have been bought in the USA, so it shouldn&amp;#39;t be a region issue. The manual states it can record on DVD-R ver 2.0 (4.7 GB). The disks I have used successfully in the past were 8x speed, but only some can be read while others cannot be. Same brand and everything. The disks I have been trying to use are 16x.&lt;/p&gt;\n\n&lt;p&gt;I have tried swapping the DVD drive with a new model that was supposedly compatible with it and could write on disks that are capable of writing at a higher speed, but the disks are still not detected.&lt;/p&gt;\n\n&lt;p&gt;Any other ideas on how to get this working again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1az7b50", "is_robot_indexable": true, "report_reasons": null, "author": "Exquisite_Poupon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1az7b50/sansui_vhs_to_dvd_not_detecting_new_disks_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1az7b50/sansui_vhs_to_dvd_not_detecting_new_disks_anyone/", "subreddit_subscribers": 734492, "created_utc": 1708813508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I wanna build my first NAS and now I am on the lookout for drives but I have no idea what to go for. Need something reliable but not to expensive. Are there any problems with refurbished HGST Ultrastar from Amazon cause they look really cheap. Thx for the help\n", "author_fullname": "t2_4dbidl7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First build", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azxip1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708891780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I wanna build my first NAS and now I am on the lookout for drives but I have no idea what to go for. Need something reliable but not to expensive. Are there any problems with refurbished HGST Ultrastar from Amazon cause they look really cheap. Thx for the help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azxip1", "is_robot_indexable": true, "report_reasons": null, "author": "erasor954", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azxip1/first_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azxip1/first_build/", "subreddit_subscribers": 734492, "created_utc": 1708891780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need font management advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwo9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_uo1tmrw", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "fonts", "selftext": "Hi folks\n\nOver the last two decades I've accumulated about 20k fonts from various sources on multiple machines running different OSes. I just dumped all of them into a single location. Currently everything is classified alphabetically. \n\nI'd like to split this collection into: serif, sans-serif, display,  symbols, etc. And also Commercial (Adobe, Linotype, etc) and Open Source. Get rid of duplicates or bad fonts, etc. \n\nIn other words just clean it up in a more meaningful way than just alphabetically. \n\nIf I try importing the A folder into Apple Font Book for example, either Font Book beach balls forever or asks me to manually click Activate a thousand times over. \n\nAnyone know of a program (macOS, Windows or Linux) or script that can automate this?\n\nThanks!\n\n", "author_fullname": "t2_uo1tmrw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need font management advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/fonts", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azwjtk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708889449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.fonts", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks&lt;/p&gt;\n\n&lt;p&gt;Over the last two decades I&amp;#39;ve accumulated about 20k fonts from various sources on multiple machines running different OSes. I just dumped all of them into a single location. Currently everything is classified alphabetically. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to split this collection into: serif, sans-serif, display,  symbols, etc. And also Commercial (Adobe, Linotype, etc) and Open Source. Get rid of duplicates or bad fonts, etc. &lt;/p&gt;\n\n&lt;p&gt;In other words just clean it up in a more meaningful way than just alphabetically. &lt;/p&gt;\n\n&lt;p&gt;If I try importing the A folder into Apple Font Book for example, either Font Book beach balls forever or asks me to manually click Activate a thousand times over. &lt;/p&gt;\n\n&lt;p&gt;Anyone know of a program (macOS, Windows or Linux) or script that can automate this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qurp", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1azwjtk", "is_robot_indexable": true, "report_reasons": null, "author": "heliomedia", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/fonts/comments/1azwjtk/need_font_management_advice/", "subreddit_subscribers": 36347, "created_utc": 1708889449.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1708889748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.fonts", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azwo9w", "is_robot_indexable": true, "report_reasons": null, "author": "heliomedia", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1azwjtk", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azwo9w/need_font_management_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/fonts/comments/1azwjtk/need_font_management_advice/", "subreddit_subscribers": 734492, "created_utc": 1708889748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17's pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn't support Live photos according to some comments. \n\nI can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don't know what apps to use as of now. Another problem seems to be that I have \\~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they're backed up, but I know that's probably difficult.\n\nDoes anyone do something similar on their devices? Please let me know. thank you!", "author_fullname": "t2_3lurx74i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up photos (including Live Photos) from iPhone to Android?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azqv6a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708875765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I have a Pixel XL that I want to use to backup my iPhone on iOS 17&amp;#39;s pictures, including Live Photos, RAW photos, and DV HDR videos. I have seen other posts but it seems Resilio sync doesn&amp;#39;t support Live photos according to some comments. &lt;/p&gt;\n\n&lt;p&gt;I can self host a container that would help get the photos off my iPhone and onto the Pixel, but I don&amp;#39;t know what apps to use as of now. Another problem seems to be that I have ~500GB of photos on my iPhone right now which I guess I would have to transfer manually. I would also prefer a solution that would delete the photos once they&amp;#39;re backed up, but I know that&amp;#39;s probably difficult.&lt;/p&gt;\n\n&lt;p&gt;Does anyone do something similar on their devices? Please let me know. thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azqv6a", "is_robot_indexable": true, "report_reasons": null, "author": "LavaCreeperBOSSB", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azqv6a/backing_up_photos_including_live_photos_from/", "subreddit_subscribers": 734492, "created_utc": 1708875765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "NetworkInfrastructureException('Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\\\\n&lt;html&gt;\\\\n  &lt;head&gt;\\\\n    &lt;title&gt;503 Backend fetch failed&lt;/title&gt;\\\\n  &lt;/head&gt;\\\\n  &lt;body&gt;\\\\n    &lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\\\\n    &lt;p&gt;Backend fetch failed&lt;/p&gt;\\\\n    &lt;h3&gt;Guru Meditation:&lt;/h3&gt;\\\\n    &lt;p&gt;XID: 802841683&lt;/p&gt;\\\\n    &lt;hr&gt;\\\\n    &lt;p&gt;Varnish cache server&lt;/p&gt;\\\\n  &lt;/body&gt;\\\\n&lt;/html&gt;\\\\n')\u2026 (Copy note to see full error)\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1659, in Start\n\nhydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nDuring handling of the above exception, another exception occurred:\n\n&amp;#x200B;\n\nTraceback (most recent call last):\n\n  File \"hydrus\\\\client\\\\importing\\\\[ClientImportFileSeeds.py](https://ClientImportFileSeeds.py)\", line 1378, in WorkOnURL\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1977, in WaitUntilDone\n\n  File \"hydrus\\\\client\\\\networking\\\\[ClientNetworkingJobs.py](https://ClientNetworkingJobs.py)\", line 1698, in Start\n\nhydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n\n  &lt;head&gt;\n\n&lt;title&gt;503 Backend fetch failed&lt;/title&gt;\n\n  &lt;/head&gt;\n\n  &lt;body&gt;\n\n&lt;h1&gt;Error 503 Backend fetch failed&lt;/h1&gt;\n\n&lt;p&gt;Backend fetch failed&lt;/p&gt;\n\n&lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n\n&lt;p&gt;XID: 802841683&lt;/p&gt;\n\n&lt;hr&gt;\n\n&lt;p&gt;Varnish cache server&lt;/p&gt;\n\n  &lt;/body&gt;\n\n&lt;/html&gt;\n\nI've only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don't know how to fix, internet searches aren't yielding relevant results. Help is appreciated.", "author_fullname": "t2_4p22esrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Hydrus Downloader, need help getting passed 503 to download from kemono", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azos8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708870187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NetworkInfrastructureException(&amp;#39;Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;\\n&amp;lt;html&amp;gt;\\n  &amp;lt;head&amp;gt;\\n    &amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;\\n  &amp;lt;/head&amp;gt;\\n  &amp;lt;body&amp;gt;\\n    &amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;\\n    &amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;\\n    &amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;\\n    &amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;\\n    &amp;lt;hr&amp;gt;\\n    &amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;\\n  &amp;lt;/body&amp;gt;\\n&amp;lt;/html&amp;gt;\\n&amp;#39;)\u2026 (Copy note to see full error)&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1659, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.ShouldReattemptNetworkException: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;During handling of the above exception, another exception occurred:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\importing\\&lt;a href=\"https://ClientImportFileSeeds.py\"&gt;ClientImportFileSeeds.py&lt;/a&gt;&amp;quot;, line 1378, in WorkOnURL&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1977, in WaitUntilDone&lt;/p&gt;\n\n&lt;p&gt;File &amp;quot;hydrus\\client\\networking\\&lt;a href=\"https://ClientNetworkingJobs.py\"&gt;ClientNetworkingJobs.py&lt;/a&gt;&amp;quot;, line 1698, in Start&lt;/p&gt;\n\n&lt;p&gt;hydrus.core.HydrusExceptions.NetworkInfrastructureException: Ran out of reattempts on this error: 503: &amp;lt;!DOCTYPE html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;title&amp;gt;503 Backend fetch failed&amp;lt;/title&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/head&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h1&amp;gt;Error 503 Backend fetch failed&amp;lt;/h1&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Backend fetch failed&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;h3&amp;gt;Guru Meditation:&amp;lt;/h3&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;XID: 802841683&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;hr&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;p&amp;gt;Varnish cache server&amp;lt;/p&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/body&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/html&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only just started using this, specifically through the URL Downloader. The first session of links went  fine, then afterwards it took a break for a few hours before starting again, running into this error that I don&amp;#39;t know how to fix, internet searches aren&amp;#39;t yielding relevant results. Help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azos8e", "is_robot_indexable": true, "report_reasons": null, "author": "Vynsyx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azos8e/new_to_hydrus_downloader_need_help_getting_passed/", "subreddit_subscribers": 734492, "created_utc": 1708870187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I am running OpenMediaVault with MergerFS/Snapraid and a bunch of different sized disks (2x 8TB, 1x 6TB, 1x 4TB, 2x 3TB) - WD Reds and HGST DC drives.\n\nI am long thinking about moving to something more \"standard\", considering something like 4x or 5x 14TB drives with a RAIDZ-1 or RAIDZ-2 config. Also I'd want to have SAS drives (mostly because they should show a steady LED on the disk caddies on my SuperMicro server, and well, because of higher reliablity as well - but its mostly about those lights). \n\nThe question is, how the switch to SAS will impact the power consumption of my setup? Currently my rack is idling around 200 Watts, but I may soon put my additional R430 into production use and  I have a Tesla P4 underway, so my consumption may soon rise to close to 500 Watts. ", "author_fullname": "t2_gww3b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is replacing my SATA disks with SAS ones a good idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azy4qx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708893246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I am running OpenMediaVault with MergerFS/Snapraid and a bunch of different sized disks (2x 8TB, 1x 6TB, 1x 4TB, 2x 3TB) - WD Reds and HGST DC drives.&lt;/p&gt;\n\n&lt;p&gt;I am long thinking about moving to something more &amp;quot;standard&amp;quot;, considering something like 4x or 5x 14TB drives with a RAIDZ-1 or RAIDZ-2 config. Also I&amp;#39;d want to have SAS drives (mostly because they should show a steady LED on the disk caddies on my SuperMicro server, and well, because of higher reliablity as well - but its mostly about those lights). &lt;/p&gt;\n\n&lt;p&gt;The question is, how the switch to SAS will impact the power consumption of my setup? Currently my rack is idling around 200 Watts, but I may soon put my additional R430 into production use and  I have a Tesla P4 underway, so my consumption may soon rise to close to 500 Watts. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1azy4qx", "is_robot_indexable": true, "report_reasons": null, "author": "rudeer_poke", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azy4qx/is_replacing_my_sata_disks_with_sas_ones_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azy4qx/is_replacing_my_sata_disks_with_sas_ones_a_good/", "subreddit_subscribers": 734492, "created_utc": 1708893246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a windows tool that I can set up for specific folders I want to copy every time when I\u2019m doing a backup to a usb stick? Basically I want to make a usb drive with only my essential data, but I don\u2019t want to go and grab it from the various folders every time, because sometimes they may get updated. ", "author_fullname": "t2_t2xqj88b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows tool for usb copying specified folders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1azamjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708822090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a windows tool that I can set up for specific folders I want to copy every time when I\u2019m doing a backup to a usb stick? Basically I want to make a usb drive with only my essential data, but I don\u2019t want to go and grab it from the various folders every time, because sometimes they may get updated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1azamjv", "is_robot_indexable": true, "report_reasons": null, "author": "Signal_Inside3436", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1azamjv/windows_tool_for_usb_copying_specified_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1azamjv/windows_tool_for_usb_copying_specified_folders/", "subreddit_subscribers": 734492, "created_utc": 1708822090.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}