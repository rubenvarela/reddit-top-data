{"kind": "Listing", "data": {"after": "t3_1b1kk3i", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7yh1jlaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectation from junior engineer ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1f95l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": "transparent", "ups": 258, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 258, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d5AIo9QplSBYUvU60BHuhMPiUDvcXhSj9_24MbBuhkY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709049403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nyexsfsbh5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?auto=webp&amp;s=e0dc582c5437ab84712de4c1f5bf3dae4f548af4", "width": 1080, "height": 1136}, "resolutions": [{"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce47bc55d3a2cf60ad25cf0aa6a705f973d7a20", "width": 108, "height": 113}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90df9f4bf8c3189fea6ce58af72c64f77ea8d9b3", "width": 216, "height": 227}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0164bca5cbc2a79751460b58bfbf3c67c7bcd139", "width": 320, "height": 336}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=546933cf5dcd3d47afa647e7780de370cfe2b418", "width": 640, "height": 673}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eefd08a2fde80381bcb448ac8b88caad7739e8e2", "width": 960, "height": 1009}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=691d799253e5195fcf86563d672feaf51f7015cc", "width": 1080, "height": 1136}], "variants": {}, "id": "XTde57wjEcOWnaAgTdf11yPwKaQ4UtAouBLs8_9PIgk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1f95l", "is_robot_indexable": true, "report_reasons": null, "author": "Foot_Straight", "discussion_type": null, "num_comments": 101, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1f95l/expectation_from_junior_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nyexsfsbh5lc1.png", "subreddit_subscribers": 164246, "created_utc": 1709049403.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, ingestr is an open-source command-line application that allows ingesting &amp; copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)\n\nIt does a few things that make it the easiest alternative out there:\n\n&amp;#x200B;\n\n* \u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs\n* \u2795 incremental loading: create+replace, delete+insert, append\n* \ud83d\udc0d single-command installation: pip install ingestr\n\nWe built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.\n\nSome common use-cases ingestr solve are:\n\n&amp;#x200B;\n\n* Migrating data from legacy systems to modern databases for better analysis\n* Syncing data between your application's database and your analytics platform in batches or incrementally\n* Backing up your databases to ensure data safety\n* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases\n* Facilitating real-time data transfer for applications that require immediate updates\n\nWe\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0[https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)", "author_fullname": "t2_153gh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source CLI tool to ingest/copy data between any databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18mfk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709029438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, ingestr is an open-source command-line application that allows ingesting &amp;amp; copying data between two databases without any code: &lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It does a few things that make it the easiest alternative out there:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs&lt;/li&gt;\n&lt;li&gt;\u2795 incremental loading: create+replace, delete+insert, append&lt;/li&gt;\n&lt;li&gt;\ud83d\udc0d single-command installation: pip install ingestr&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.&lt;/p&gt;\n\n&lt;p&gt;Some common use-cases ingestr solve are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Migrating data from legacy systems to modern databases for better analysis&lt;/li&gt;\n&lt;li&gt;Syncing data between your application&amp;#39;s database and your analytics platform in batches or incrementally&lt;/li&gt;\n&lt;li&gt;Backing up your databases to ensure data safety&lt;/li&gt;\n&lt;li&gt;Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases&lt;/li&gt;\n&lt;li&gt;Facilitating real-time data transfer for applications that require immediate updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0&lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?auto=webp&amp;s=71ed11c21a77b630e601e5051f4772431462627b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04bda8eab9d0ae0614648424f7d053daef1a08e6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=49188a34f55d47bd66a68f0f88c20fb3ba364186", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08276d6a94f8f7be274a822eeb3a2826c0d29076", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da5f26047549b5b649fe8d98e70f833e5829aa1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b2d7565df4a4182a23ea538152715a64554bf04", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d57305cc005ba7fa2adfb13086d6f70916ef5c0", "width": 1080, "height": 540}], "variants": {}, "id": "XDsKdm5QbGo1ePzc9WAtlkw6WpXhnmXv5XfGicn_zpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b18mfk", "is_robot_indexable": true, "report_reasons": null, "author": "karakanb", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "subreddit_subscribers": 164246, "created_utc": 1709029438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Walmart saw spike in sales of beer during hurricanes.\n\nApparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)\n\nWhat\u2019s your favorite data insight story?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People buy beer during hurricanes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ckh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709042551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Walmart saw spike in sales of beer during hurricanes.&lt;/p&gt;\n\n&lt;p&gt;Apparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)&lt;/p&gt;\n\n&lt;p&gt;What\u2019s your favorite data insight story?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ckh8", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "subreddit_subscribers": 164246, "created_utc": 1709042551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5a55k9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the reasons why ADF is love/hate type of thing - a task that would be super easy in Python such as skipping a for loop block if a condition is not met, turns out into a creative hack that takes many hours to get right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1gmf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jA_LNyv0BCZbxetNQpaMxQenEHPQvRu3lH9TbA7DnTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709052668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rhfpex8nq5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?auto=webp&amp;s=f50af32a38a82268aaeacfbba92f41f97685fbbd", "width": 1515, "height": 861}, "resolutions": [{"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=587c1c97d4263e0f0c5c0831adac22d35e8db4c8", "width": 108, "height": 61}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d86464f6a9fe7ce768086594dc2b9a2048f8a4d3", "width": 216, "height": 122}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=036fa90d1e66d8966bfa8aaa01ece070c4b86e10", "width": 320, "height": 181}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed5695d954a3169c76fea2e650d7b011bf889162", "width": 640, "height": 363}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16d2b1fe9bf607f6f075b6267494c38d1eef469", "width": 960, "height": 545}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbb31ad543c4846964e2800599f9056ee9f3a7aa", "width": 1080, "height": 613}], "variants": {}, "id": "sK0y8seZr8DTVPJEbWf_dGHmEkWW2nm1w0iE7Iv3kH8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1gmf6", "is_robot_indexable": true, "report_reasons": null, "author": "koteikin", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1gmf6/one_of_the_reasons_why_adf_is_lovehate_type_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rhfpex8nq5lc1.png", "subreddit_subscribers": 164246, "created_utc": 1709052668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.\n\nNow start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.\n\nWhich means it's main work is to perform **transform and load within DWH itself.**\n\nIs my understanding correct?", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DBT's main strength to help engineers execute transformation and load within data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b16o2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709021228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.&lt;/p&gt;\n\n&lt;p&gt;Now start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.&lt;/p&gt;\n\n&lt;p&gt;Which means it&amp;#39;s main work is to perform &lt;strong&gt;transform and load within DWH itself.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is my understanding correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b16o2d", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "subreddit_subscribers": 164246, "created_utc": 1709021228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.\n\nThe API's data is updated daily around 9pm, so my first approach was to request the data each day by `created_date` and drop partitioned files in the S3 bucket.\n\nHowever, I realized that the number of rows increases for previous days.  For example,\n\n* On 2024-02-25 the initial request returned 9000 rows\n* Yesterday, I got back 9500 rows for 2024-02-25\n* Today, I got back 10000 rows for 2024-02-25\n\nIt seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, \\~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it's still theoretically possible data could change as far back as a year.\n\nThe only \"solution\" I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I'd have to rebuild all the tables in my DW everyday, so I don't feel like this is a good solution.\n\nWhat would be a best practice approach for capturing all of these changes in the full data set?", "author_fullname": "t2_j12b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice approach for ingesting data from an API where many rows will be added/changed/deleted over time without any indication/warning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1kx6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709064570.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709062951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The API&amp;#39;s data is updated daily around 9pm, so my first approach was to request the data each day by &lt;code&gt;created_date&lt;/code&gt; and drop partitioned files in the S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;However, I realized that the number of rows increases for previous days.  For example,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On 2024-02-25 the initial request returned 9000 rows&lt;/li&gt;\n&lt;li&gt;Yesterday, I got back 9500 rows for 2024-02-25&lt;/li&gt;\n&lt;li&gt;Today, I got back 10000 rows for 2024-02-25&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, ~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it&amp;#39;s still theoretically possible data could change as far back as a year.&lt;/p&gt;\n\n&lt;p&gt;The only &amp;quot;solution&amp;quot; I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I&amp;#39;d have to rebuild all the tables in my DW everyday, so I don&amp;#39;t feel like this is a good solution.&lt;/p&gt;\n\n&lt;p&gt;What would be a best practice approach for capturing all of these changes in the full data set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1kx6o", "is_robot_indexable": true, "report_reasons": null, "author": "Eatsleeptren", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "subreddit_subscribers": 164246, "created_utc": 1709062951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like turtles, `(Jim, likes, apples)` `(Jim, age, 35)` `(Jim, knows, Sam)`\n\nHas anyone used these kinds of stores? What\u2019d you use it for?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone actually employed the use of a Triple Store?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1to5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709093656.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709084664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like turtles, &lt;code&gt;(Jim, likes, apples)&lt;/code&gt; &lt;code&gt;(Jim, age, 35)&lt;/code&gt; &lt;code&gt;(Jim, knows, Sam)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone used these kinds of stores? What\u2019d you use it for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1to5o", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "subreddit_subscribers": 164246, "created_utc": 1709084664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience in AWS services related to data engineering. I've been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.\n\nWhen I'm trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I'm thinking of where it's a good idea to move my stack to azure. \n\nAppreciate your ideas on this", "author_fullname": "t2_6b96x89c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS or Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18xcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709030597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience in AWS services related to data engineering. I&amp;#39;ve been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.&lt;/p&gt;\n\n&lt;p&gt;When I&amp;#39;m trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I&amp;#39;m thinking of where it&amp;#39;s a good idea to move my stack to azure. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your ideas on this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b18xcc", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Buy-4947", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18xcc/aws_or_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18xcc/aws_or_azure/", "subreddit_subscribers": 164246, "created_utc": 1709030597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to test a SQL database (its procedures and so on)?    \n\n\nAnd what is the best way to do it?", "author_fullname": "t2_lwhyoz1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way/tool for testing SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1e5lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709046689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to test a SQL database (its procedures and so on)?    &lt;/p&gt;\n\n&lt;p&gt;And what is the best way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1e5lm", "is_robot_indexable": true, "report_reasons": null, "author": "alexcrav", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "subreddit_subscribers": 164246, "created_utc": 1709046689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title says all. Want to understand where is community gearing towards", "author_fullname": "t2_3x0urbjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your KPIs/OKRs for this year ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1wec1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709092400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says all. Want to understand where is community gearing towards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1wec1", "is_robot_indexable": true, "report_reasons": null, "author": "educationruinedme1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1wec1/what_are_your_kpisokrs_for_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1wec1/what_are_your_kpisokrs_for_this_year/", "subreddit_subscribers": 164246, "created_utc": 1709092400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. \n\nI just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. \n\nThe project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. \n\nSo far I've got databases for:  \n\n\n1. Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.\n2. Models - lists last run, columns &amp; descriptions for all models \n3. I was working on trying to bring in the compiled SQL code into a code block, but Notion's API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm building a DBT Docs -&gt; Notion tool for my company. Any interest in having me write a guide or sharing the source code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1i3vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709056179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. &lt;/p&gt;\n\n&lt;p&gt;I just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. &lt;/p&gt;\n\n&lt;p&gt;The project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. &lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve got databases for:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.&lt;/li&gt;\n&lt;li&gt;Models - lists last run, columns &amp;amp; descriptions for all models &lt;/li&gt;\n&lt;li&gt;I was working on trying to bring in the compiled SQL code into a code block, but Notion&amp;#39;s API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1i3vh", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "subreddit_subscribers": 164246, "created_utc": 1709056179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Data Engineering Community!\n\nI'm currently working on a Spark Streaming project aimed at processing telemetry JSON messages. The primary goal is to normalize these messages for further analytics and operations. Given the nature of telemetry data, we're dealing with a high volume of messages that come in a variety of formats and structures, making the normalization process quite challenging.\n\nOne of the key requirements of this project is to maintain low latency (less 10 sec) from ingestion to processed output. This is critical as the data is time-sensitive and needs to be actionable almost in real-time. However, I've encountered several challenges, especially when it comes to handling complex joins and dealing with various issues related to data quality and structure.\n\nHere are a few points I'm grappling with:\n\n1. **Normalization Complexity**: The telemetry messages vary significantly in structure and content. What are some effective strategies for dynamically normalizing these messages into a unified format that's suitable for downstream processing and analysis?\n2. **Spark Streaming Limitations**: I've implemented an approach that handles and pivots different parts of a JSON message using various DataFrames. These are then joined to construct a normalized message. However, some of these DataFrames can be empty, necessitating a left join that doesn't function properly without watermarks. Unfortunately, watermarks are not suitable for my solution as they could introduce performance issues. My objective is to join DataFrames within a JSON message without logically needing to use watermarks, which seems unnecessary and potentially problematic in this context.\n3. **Handling Joins with Low Latency**: Some of the normalization logic requires enriching the telemetry data by joining it with reference datasets. However, these joins are proving to be a bottleneck, especially when trying to keep the processing latency low. How can I optimize these operations, considering the distributed nature of Spark?\n4. **Tooling and Techniques**: Are there specific Spark Streaming features, external tools, or techniques that you've found particularly useful for these types of challenges?\n5. **Examples and Case Studies**: If you've worked on similar projects, could you share your approach, especially how you managed to balance the trade-offs between normalization complexity, data quality, and latency requirements?\n\nAny advice, insights, or references to relevant resources would be greatly appreciated. I'm looking for strategies to improve the efficiency and effectiveness of our Spark Streaming jobs, particularly around the areas of data normalization and processing latency.\n\nThank you in advance for your help and suggestions!", "author_fullname": "t2_2xzylww1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Normalizing Telemetry JSON Messages in Spark Streaming with Low Latency: Handling Complex Joins and Data Issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1x5o5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709094713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Data Engineering Community!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a Spark Streaming project aimed at processing telemetry JSON messages. The primary goal is to normalize these messages for further analytics and operations. Given the nature of telemetry data, we&amp;#39;re dealing with a high volume of messages that come in a variety of formats and structures, making the normalization process quite challenging.&lt;/p&gt;\n\n&lt;p&gt;One of the key requirements of this project is to maintain low latency (less 10 sec) from ingestion to processed output. This is critical as the data is time-sensitive and needs to be actionable almost in real-time. However, I&amp;#39;ve encountered several challenges, especially when it comes to handling complex joins and dealing with various issues related to data quality and structure.&lt;/p&gt;\n\n&lt;p&gt;Here are a few points I&amp;#39;m grappling with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Normalization Complexity&lt;/strong&gt;: The telemetry messages vary significantly in structure and content. What are some effective strategies for dynamically normalizing these messages into a unified format that&amp;#39;s suitable for downstream processing and analysis?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Spark Streaming Limitations&lt;/strong&gt;: I&amp;#39;ve implemented an approach that handles and pivots different parts of a JSON message using various DataFrames. These are then joined to construct a normalized message. However, some of these DataFrames can be empty, necessitating a left join that doesn&amp;#39;t function properly without watermarks. Unfortunately, watermarks are not suitable for my solution as they could introduce performance issues. My objective is to join DataFrames within a JSON message without logically needing to use watermarks, which seems unnecessary and potentially problematic in this context.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handling Joins with Low Latency&lt;/strong&gt;: Some of the normalization logic requires enriching the telemetry data by joining it with reference datasets. However, these joins are proving to be a bottleneck, especially when trying to keep the processing latency low. How can I optimize these operations, considering the distributed nature of Spark?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tooling and Techniques&lt;/strong&gt;: Are there specific Spark Streaming features, external tools, or techniques that you&amp;#39;ve found particularly useful for these types of challenges?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Examples and Case Studies&lt;/strong&gt;: If you&amp;#39;ve worked on similar projects, could you share your approach, especially how you managed to balance the trade-offs between normalization complexity, data quality, and latency requirements?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice, insights, or references to relevant resources would be greatly appreciated. I&amp;#39;m looking for strategies to improve the efficiency and effectiveness of our Spark Streaming jobs, particularly around the areas of data normalization and processing latency.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help and suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1x5o5", "is_robot_indexable": true, "report_reasons": null, "author": "vk_pro", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1x5o5/best_practices_for_normalizing_telemetry_json/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1x5o5/best_practices_for_normalizing_telemetry_json/", "subreddit_subscribers": 164246, "created_utc": 1709094713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,   \nI am reading the system design book by Alex Xu and cannot understand the design. \n\nThis is a click aggregation system. We are reading data from the log and writing it to 1(Kafka topic). Then there's an aggregation performed and it writes to 2(Kafka topic). From 2 we write it to DB. I don't understand why we need the 2nd Kafka topic, why can't we directly aggregate and write it to db. In the book it says - **To achieve end-to-end exactly once semantics (atomic commit) we need it.**  \n Can someone please explain what that means?   \n\n\nhttps://preview.redd.it/jtncia1729lc1.png?width=650&amp;format=png&amp;auto=webp&amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System Design Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jtncia1729lc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/jtncia1729lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b6fe84292d8f88b33d44e6382fffe24c8931caa"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/jtncia1729lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d7819da718acf840922697f997d6b5a6b7263cc"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/jtncia1729lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4c48e9a19bf3e7786010aedb5b680342b53158"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/jtncia1729lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e452954393d2d4762d8587cc6fbbc12f73a1c444"}], "s": {"y": 343, "x": 650, "u": "https://preview.redd.it/jtncia1729lc1.png?width=650&amp;format=png&amp;auto=webp&amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae"}, "id": "jtncia1729lc1"}}, "name": "t3_1b1wl4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nGmftjutpHr4knW0vvYvI9X5roKybztjaSHh3fbIFKM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709092961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nI am reading the system design book by Alex Xu and cannot understand the design. &lt;/p&gt;\n\n&lt;p&gt;This is a click aggregation system. We are reading data from the log and writing it to 1(Kafka topic). Then there&amp;#39;s an aggregation performed and it writes to 2(Kafka topic). From 2 we write it to DB. I don&amp;#39;t understand why we need the 2nd Kafka topic, why can&amp;#39;t we directly aggregate and write it to db. In the book it says - &lt;strong&gt;To achieve end-to-end exactly once semantics (atomic commit) we need it.&lt;/strong&gt;&lt;br/&gt;\n Can someone please explain what that means?   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jtncia1729lc1.png?width=650&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae\"&gt;https://preview.redd.it/jtncia1729lc1.png?width=650&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1wl4e", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1wl4e/system_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1wl4e/system_design_question/", "subreddit_subscribers": 164246, "created_utc": 1709092961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Boost your career growth by actively participating in Engineering Tech Talks. \n\nYou will learn:\n\n- How to make the most of it\n\n- Benefits like Instance Recognition\n\n- Growth opportunities \n\n- Bonus on how to keep up with it\n\n\nI do think tech talks have helped me grow, learning new, building confidence and getting better feedback and reviews.\n\n\nhttps://www.junaideffendi.com/p/why-tech-talks-are-important\n\n\nLet me know, hows your tech talk looks like.", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importance of Tech Talks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1r1ah", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709077553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Boost your career growth by actively participating in Engineering Tech Talks. &lt;/p&gt;\n\n&lt;p&gt;You will learn:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How to make the most of it&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Benefits like Instance Recognition&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Growth opportunities &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Bonus on how to keep up with it&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do think tech talks have helped me grow, learning new, building confidence and getting better feedback and reviews.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.junaideffendi.com/p/why-tech-talks-are-important\"&gt;https://www.junaideffendi.com/p/why-tech-talks-are-important&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know, hows your tech talk looks like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?auto=webp&amp;s=d0e959e009f175e9a4b188daf8cf43721c0d20bb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b100ca3491448c1a56c408a541d51c40219088a9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6b07655440c14388ebc73d5a4877af4c19008b3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf9e25c8bca90557a5192d990b00297ca085cf69", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90facde25a76bfcdb0353da376b949ec728644dd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce7635e62cf4eb0f8edf09722e66d177c2f27b0e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d05eebe7ce34b24d552dc9f25e00b4c9ebec10ca", "width": 1080, "height": 540}], "variants": {}, "id": "kKy5jqFgwBnl99q_MFxIOnQbbGG6JB9r3FVS4DQBXQU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1r1ah", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1r1ah/importance_of_tech_talks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1r1ah/importance_of_tech_talks/", "subreddit_subscribers": 164246, "created_utc": 1709077553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!! \n\nI\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.\n\nI\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.\n\nAny insights or suggestions would be greatly appreciated. Thank you!\n\nFeel free to let me know if you need any further adjustments or have any specific preferences!", "author_fullname": "t2_8fi5ln6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on backfilling airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1goj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.&lt;/p&gt;\n\n&lt;p&gt;Any insights or suggestions would be greatly appreciated. Thank you!&lt;/p&gt;\n\n&lt;p&gt;Feel free to let me know if you need any further adjustments or have any specific preferences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1goj4", "is_robot_indexable": true, "report_reasons": null, "author": "sailor_Moon_Pie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "subreddit_subscribers": 164246, "created_utc": 1709052808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jnd3h0a4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharding Redis 101: Using Pixel Art To Explain The Basics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1el4v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1b1el4v", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/LHPXFmXwb3nzKUcnt9skH2TOZyJQEV5gUaa6WJQcnD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709047767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/OUNimTqgkZM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?auto=webp&amp;s=f5f162840cbbc19dfd9555c40dd3cdeeef61bf4c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=673d90c0d2d95230a6ca4c9c0beca2fa7c04fd89", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b23acaf341ff6bbb773b557d242b5681e0dddc3f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26b7b3baa6e01e47a2b7e29206f6652c230f02cd", "width": 320, "height": 240}], "variants": {}, "id": "WcjcUtEvWDXzwRXdZ5uUMo1KfN_fjy369prwUT8YECI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1el4v", "is_robot_indexable": true, "report_reasons": null, "author": "schematical", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1el4v/sharding_redis_101_using_pixel_art_to_explain_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/OUNimTqgkZM", "subreddit_subscribers": 164246, "created_utc": 1709047767.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2m4jmqrn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability: The Next Frontier of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1c0iu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/92p5nuuO28iKEKJTddI8-afwtfhls7XSUT8vwmntpuM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709041001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?auto=webp&amp;s=23b87a3c3434a563f208b5ce9d60ae71cb25eb4c", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d6d3e6cebcee1463c51a65006b6bdaff91447ec", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=54235a755f7b3e0b34af63bd0bfd3e4306f5174b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe1bc4229063a081e7143bdd9c2c603eb10bba8c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64b707a5757ce60317484ebd1b65082e2efc0d31", "width": 640, "height": 336}], "variants": {}, "id": "JGhC9b9qhHNJ4eh7-qMHVcYJNdZfz1K-5VoSz-ZvhQY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1c0iu", "is_robot_indexable": true, "report_reasons": null, "author": "michaellyamm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1c0iu/data_observability_the_next_frontier_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "subreddit_subscribers": 164246, "created_utc": 1709041001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I'm thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.", "author_fullname": "t2_h0hkehfsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion suggestion in azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1a3ni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709035013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I&amp;#39;m thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1a3ni", "is_robot_indexable": true, "report_reasons": null, "author": "timetravel_looper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "subreddit_subscribers": 164246, "created_utc": 1709035013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we can connect Microsoft 365 business central SQL Database!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b17ax0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709023867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b17ax0", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "subreddit_subscribers": 164246, "created_utc": 1709023867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting query timeouts and appropriate alerts for Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1b15yi4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oojNEsC-H3Ge4vQ7WjyWaiJvolq7cwbkK8qZJfrqjeQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709018452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?auto=webp&amp;s=83f5ede9df0770bc92fe9880edcb22ddc85750c3", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a9dcda060ee14aa03003fea8f51eb6e9e8411b8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a3acabcc936daa3d1ab0acb3f87eee4007c3f5", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebdb2e183c0fffcf7993fbcc10996816e0fb550d", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e75db1f02946562a52ac283ab5d47e5d80151b40", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9adda2cae88e7de944026539799f038d7d0de755", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=293187c6fe2dbf3bbcf054286fcbe7ee6a9470e8", "width": 1080, "height": 720}], "variants": {}, "id": "ZhjudSQ1egdhKR93zH-Y4GUy7HyAkIjZEYYCTBXg5fE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b15yi4", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b15yi4/setting_query_timeouts_and_appropriate_alerts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "subreddit_subscribers": 164246, "created_utc": 1709018452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am being asked to learn pyspark. ( my manager said it will be easier to pick Databricks if you get good with pyspark- so Databricks may be the next step in this learning process) \nPlease suggest some of learning paths and certification and practice methods that you experts considered to be the best from your experience. \n\nMy background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, \n\nSome basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) \n\nI have tried installing python using anaconda distribution and have meddled with some python programming doing pandas, some web crawling spider - all following some step by step tutorial. But don\u2019t remember much as I have not continued practising.\n\n", "author_fullname": "t2_6cuqg1k5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning plan recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1xshy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709096723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am being asked to learn pyspark. ( my manager said it will be easier to pick Databricks if you get good with pyspark- so Databricks may be the next step in this learning process) \nPlease suggest some of learning paths and certification and practice methods that you experts considered to be the best from your experience. &lt;/p&gt;\n\n&lt;p&gt;My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, &lt;/p&gt;\n\n&lt;p&gt;Some basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) &lt;/p&gt;\n\n&lt;p&gt;I have tried installing python using anaconda distribution and have meddled with some python programming doing pandas, some web crawling spider - all following some step by step tutorial. But don\u2019t remember much as I have not continued practising.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1xshy", "is_robot_indexable": true, "report_reasons": null, "author": "sneekeeei", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1xshy/learning_plan_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1xshy/learning_plan_recommendations/", "subreddit_subscribers": 164246, "created_utc": 1709096723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the expensive cloud services (for de/analytics) and the counter service that you guys use?\n", "author_fullname": "t2_fta9agm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expensive cloud services and the counters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1w3n5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709091513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the expensive cloud services (for de/analytics) and the counter service that you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1w3n5", "is_robot_indexable": true, "report_reasons": null, "author": "luqmancrit69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1w3n5/expensive_cloud_services_and_the_counters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1w3n5/expensive_cloud_services_and_the_counters/", "subreddit_subscribers": 164246, "created_utc": 1709091513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?", "author_fullname": "t2_itoec5nn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for monitoring ADF pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ug0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709086794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ug0t", "is_robot_indexable": true, "report_reasons": null, "author": "CocoaDependent1664", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "subreddit_subscribers": 164246, "created_utc": 1709086794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI'm looking for advice and insights on modernizing my data warehouse architecture. Here's where I currently stand:\n\n* Kimball methodology with a star schema.\n* My ETL processes are managed by Talend Open Studio open source - has been discontinued\n   * I import jobs once a day\n* Tableau serves as my primary tool for reporting.\n* PostgreSQL is my  DBMS.\n\nHere's what I'm aiming for in the modernization process:\n\n1. **Data Volume:** I process around 1-2 million rows daily.\n2. **Real-Time Needs:** I'm aiming for near-real-time data processing.\n3. **New ETL Tool:** I'm considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I'd love to hear your thoughts and any advice you might have.\n4. **Database Consideration:** ClickHouse has caught my attention for its potential benefits.\n\nAny advice, experiences, or alternative suggestions are welcome!\n\nThank you", "author_fullname": "t2_9fw69uas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on Modernizing Data Warehouse Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1me42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709066459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for advice and insights on modernizing my data warehouse architecture. Here&amp;#39;s where I currently stand:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimball methodology with a star schema.&lt;/li&gt;\n&lt;li&gt;My ETL processes are managed by Talend Open Studio open source - has been discontinued\n\n&lt;ul&gt;\n&lt;li&gt;I import jobs once a day&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tableau serves as my primary tool for reporting.&lt;/li&gt;\n&lt;li&gt;PostgreSQL is my  DBMS.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m aiming for in the modernization process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Volume:&lt;/strong&gt; I process around 1-2 million rows daily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Real-Time Needs:&lt;/strong&gt; I&amp;#39;m aiming for near-real-time data processing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;New ETL Tool:&lt;/strong&gt; I&amp;#39;m considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I&amp;#39;d love to hear your thoughts and any advice you might have.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Database Consideration:&lt;/strong&gt; ClickHouse has caught my attention for its potential benefits.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice, experiences, or alternative suggestions are welcome!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1me42", "is_robot_indexable": true, "report_reasons": null, "author": "Ahmouu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "subreddit_subscribers": 164246, "created_utc": 1709066459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nI've only ever been a software engineer.\n\nI've always found data to be interesting, but I really have no idea what it's like to do the work of any data role.\n\nI watched some videos on what a data engineer does, and I want to experience it.\n\nDoes anyone have any suggestions on what I can try?\n\nAlso, let's say I do want to become a data engineer, what data job should I apply to for my first data role? There seem to be so many titles. I'm sure I will be told that titles don't matter, but it would be great if I could get some exact title to search for on LinkedIn.\n\nThank you.", "author_fullname": "t2_n7ndn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I experience what working in data is like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1kk3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709062069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only ever been a software engineer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always found data to be interesting, but I really have no idea what it&amp;#39;s like to do the work of any data role.&lt;/p&gt;\n\n&lt;p&gt;I watched some videos on what a data engineer does, and I want to experience it.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any suggestions on what I can try?&lt;/p&gt;\n\n&lt;p&gt;Also, let&amp;#39;s say I do want to become a data engineer, what data job should I apply to for my first data role? There seem to be so many titles. I&amp;#39;m sure I will be told that titles don&amp;#39;t matter, but it would be great if I could get some exact title to search for on LinkedIn.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1kk3i", "is_robot_indexable": true, "report_reasons": null, "author": "fossdeep", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1kk3i/how_do_i_experience_what_working_in_data_is_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1kk3i/how_do_i_experience_what_working_in_data_is_like/", "subreddit_subscribers": 164246, "created_utc": 1709062069.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}