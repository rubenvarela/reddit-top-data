{"kind": "Listing", "data": {"after": "t3_1b1me42", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7yh1jlaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectation from junior engineer ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1f95l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "ups": 212, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 212, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d5AIo9QplSBYUvU60BHuhMPiUDvcXhSj9_24MbBuhkY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709049403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nyexsfsbh5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?auto=webp&amp;s=e0dc582c5437ab84712de4c1f5bf3dae4f548af4", "width": 1080, "height": 1136}, "resolutions": [{"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce47bc55d3a2cf60ad25cf0aa6a705f973d7a20", "width": 108, "height": 113}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90df9f4bf8c3189fea6ce58af72c64f77ea8d9b3", "width": 216, "height": 227}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0164bca5cbc2a79751460b58bfbf3c67c7bcd139", "width": 320, "height": 336}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=546933cf5dcd3d47afa647e7780de370cfe2b418", "width": 640, "height": 673}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eefd08a2fde80381bcb448ac8b88caad7739e8e2", "width": 960, "height": 1009}, {"url": "https://preview.redd.it/nyexsfsbh5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=691d799253e5195fcf86563d672feaf51f7015cc", "width": 1080, "height": 1136}], "variants": {}, "id": "XTde57wjEcOWnaAgTdf11yPwKaQ4UtAouBLs8_9PIgk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1f95l", "is_robot_indexable": true, "report_reasons": null, "author": "Foot_Straight", "discussion_type": null, "num_comments": 97, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1f95l/expectation_from_junior_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nyexsfsbh5lc1.png", "subreddit_subscribers": 164205, "created_utc": 1709049403.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, ingestr is an open-source command-line application that allows ingesting &amp; copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)\n\nIt does a few things that make it the easiest alternative out there:\n\n&amp;#x200B;\n\n* \u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs\n* \u2795 incremental loading: create+replace, delete+insert, append\n* \ud83d\udc0d single-command installation: pip install ingestr\n\nWe built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.\n\nSome common use-cases ingestr solve are:\n\n&amp;#x200B;\n\n* Migrating data from legacy systems to modern databases for better analysis\n* Syncing data between your application's database and your analytics platform in batches or incrementally\n* Backing up your databases to ensure data safety\n* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases\n* Facilitating real-time data transfer for applications that require immediate updates\n\nWe\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0[https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)", "author_fullname": "t2_153gh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source CLI tool to ingest/copy data between any databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18mfk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709029438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, ingestr is an open-source command-line application that allows ingesting &amp;amp; copying data between two databases without any code: &lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It does a few things that make it the easiest alternative out there:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u2728 copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs&lt;/li&gt;\n&lt;li&gt;\u2795 incremental loading: create+replace, delete+insert, append&lt;/li&gt;\n&lt;li&gt;\ud83d\udc0d single-command installation: pip install ingestr&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We built ingestr because we believe for 80% of the cases out there people shouldn\u2019t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.&lt;/p&gt;\n\n&lt;p&gt;Some common use-cases ingestr solve are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Migrating data from legacy systems to modern databases for better analysis&lt;/li&gt;\n&lt;li&gt;Syncing data between your application&amp;#39;s database and your analytics platform in batches or incrementally&lt;/li&gt;\n&lt;li&gt;Backing up your databases to ensure data safety&lt;/li&gt;\n&lt;li&gt;Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases&lt;/li&gt;\n&lt;li&gt;Facilitating real-time data transfer for applications that require immediate updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019d love to hear your feedback, and make sure to give us a star on GitHub if you like it! \ud83d\ude80\u00a0&lt;a href=\"https://github.com/bruin-data/ingestr\"&gt;https://github.com/bruin-data/ingestr&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?auto=webp&amp;s=71ed11c21a77b630e601e5051f4772431462627b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04bda8eab9d0ae0614648424f7d053daef1a08e6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=49188a34f55d47bd66a68f0f88c20fb3ba364186", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08276d6a94f8f7be274a822eeb3a2826c0d29076", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da5f26047549b5b649fe8d98e70f833e5829aa1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b2d7565df4a4182a23ea538152715a64554bf04", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/grPZ1KgMfJL8J0i7zujp2oQuVR4SijYMgAfL2LDjL9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d57305cc005ba7fa2adfb13086d6f70916ef5c0", "width": 1080, "height": 540}], "variants": {}, "id": "XDsKdm5QbGo1ePzc9WAtlkw6WpXhnmXv5XfGicn_zpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b18mfk", "is_robot_indexable": true, "report_reasons": null, "author": "karakanb", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/", "subreddit_subscribers": 164205, "created_utc": 1709029438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Walmart saw spike in sales of beer during hurricanes.\n\nApparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)\n\nWhat\u2019s your favorite data insight story?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People buy beer during hurricanes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ckh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709042551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Walmart saw spike in sales of beer during hurricanes.&lt;/p&gt;\n\n&lt;p&gt;Apparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)&lt;/p&gt;\n\n&lt;p&gt;What\u2019s your favorite data insight story?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ckh8", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ckh8/people_buy_beer_during_hurricanes/", "subreddit_subscribers": 164205, "created_utc": 1709042551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5a55k9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the reasons why ADF is love/hate type of thing - a task that would be super easy in Python such as skipping a for loop block if a condition is not met, turns out into a creative hack that takes many hours to get right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1gmf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jA_LNyv0BCZbxetNQpaMxQenEHPQvRu3lH9TbA7DnTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709052668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rhfpex8nq5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?auto=webp&amp;s=f50af32a38a82268aaeacfbba92f41f97685fbbd", "width": 1515, "height": 861}, "resolutions": [{"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=587c1c97d4263e0f0c5c0831adac22d35e8db4c8", "width": 108, "height": 61}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d86464f6a9fe7ce768086594dc2b9a2048f8a4d3", "width": 216, "height": 122}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=036fa90d1e66d8966bfa8aaa01ece070c4b86e10", "width": 320, "height": 181}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed5695d954a3169c76fea2e650d7b011bf889162", "width": 640, "height": 363}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16d2b1fe9bf607f6f075b6267494c38d1eef469", "width": 960, "height": 545}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbb31ad543c4846964e2800599f9056ee9f3a7aa", "width": 1080, "height": 613}], "variants": {}, "id": "sK0y8seZr8DTVPJEbWf_dGHmEkWW2nm1w0iE7Iv3kH8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1gmf6", "is_robot_indexable": true, "report_reasons": null, "author": "koteikin", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1gmf6/one_of_the_reasons_why_adf_is_lovehate_type_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rhfpex8nq5lc1.png", "subreddit_subscribers": 164205, "created_utc": 1709052668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.\n\nNow start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.\n\nWhich means it's main work is to perform **transform and load within DWH itself.**\n\nIs my understanding correct?", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DBT's main strength to help engineers execute transformation and load within data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b16o2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709021228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.&lt;/p&gt;\n\n&lt;p&gt;Now start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.&lt;/p&gt;\n\n&lt;p&gt;Which means it&amp;#39;s main work is to perform &lt;strong&gt;transform and load within DWH itself.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is my understanding correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b16o2d", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b16o2d/is_dbts_main_strength_to_help_engineers_execute/", "subreddit_subscribers": 164205, "created_utc": 1709021228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am leaving my company at the end of the week. They leave me with \\~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  \n\n\nI have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y'all recommend for me to go for next? I'm up in the air about going \"tool-specific\" like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.\n\n&amp;#x200B;\n\nP.S. I will be doing personal projects on the side as well. I know it's foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.", "author_fullname": "t2_mkh2olbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Best\" Next DE Certification to get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b13uiy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709011093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am leaving my company at the end of the week. They leave me with ~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  &lt;/p&gt;\n\n&lt;p&gt;I have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner &amp;amp; the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y&amp;#39;all recommend for me to go for next? I&amp;#39;m up in the air about going &amp;quot;tool-specific&amp;quot; like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. I will be doing personal projects on the side as well. I know it&amp;#39;s foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b13uiy", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid-Second-8434", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b13uiy/best_next_de_certification_to_get/", "subreddit_subscribers": 164205, "created_utc": 1709011093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi team,\n\nhope you are crushing it this week.\n\nI have a newbie question here which i am sure have been asked many times as well and here goes.\n\nQuestion is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify's api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?\n\nWe are trying to understand if using an additional service like an integration tool justify the cost in the long run.\n\nThe other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.\n\nI am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook's api, twitter's api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?\n\n&amp;#x200B;\n\nthank you for your wisdom on this troubling thought.", "author_fullname": "t2_6mulsm33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using data integration tools like fivetran, airbyte etc vs building it from the ground up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b12old", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709007380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi team,&lt;/p&gt;\n\n&lt;p&gt;hope you are crushing it this week.&lt;/p&gt;\n\n&lt;p&gt;I have a newbie question here which i am sure have been asked many times as well and here goes.&lt;/p&gt;\n\n&lt;p&gt;Question is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify&amp;#39;s api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?&lt;/p&gt;\n\n&lt;p&gt;We are trying to understand if using an additional service like an integration tool justify the cost in the long run.&lt;/p&gt;\n\n&lt;p&gt;The other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.&lt;/p&gt;\n\n&lt;p&gt;I am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook&amp;#39;s api, twitter&amp;#39;s api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thank you for your wisdom on this troubling thought.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b12old", "is_robot_indexable": true, "report_reasons": null, "author": "simon_chia", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b12old/using_data_integration_tools_like_fivetran/", "subreddit_subscribers": 164205, "created_utc": 1709007380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.\n\nThe API's data is updated daily around 9pm, so my first approach was to request the data each day by `created_date` and drop partitioned files in the S3 bucket.\n\nHowever, I realized that the number of rows increases for previous days.  For example,\n\n* On 2024-02-25 the initial request returned 9000 rows\n* Yesterday, I got back 9500 rows for 2024-02-25\n* Today, I got back 10000 rows for 2024-02-25\n\nIt seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, \\~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it's still theoretically possible data could change as far back as a year.\n\nThe only \"solution\" I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I'd have to rebuild all the tables in my DW everyday, so I don't feel like this is a good solution.\n\nWhat would be a best practice approach for capturing all of these changes in the full data set?", "author_fullname": "t2_j12b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice approach for ingesting data from an API where many rows will be added/changed/deleted over time without any indication/warning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1kx6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709064570.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709062951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The API&amp;#39;s data is updated daily around 9pm, so my first approach was to request the data each day by &lt;code&gt;created_date&lt;/code&gt; and drop partitioned files in the S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;However, I realized that the number of rows increases for previous days.  For example,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On 2024-02-25 the initial request returned 9000 rows&lt;/li&gt;\n&lt;li&gt;Yesterday, I got back 9500 rows for 2024-02-25&lt;/li&gt;\n&lt;li&gt;Today, I got back 10000 rows for 2024-02-25&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, ~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it&amp;#39;s still theoretically possible data could change as far back as a year.&lt;/p&gt;\n\n&lt;p&gt;The only &amp;quot;solution&amp;quot; I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I&amp;#39;d have to rebuild all the tables in my DW everyday, so I don&amp;#39;t feel like this is a good solution.&lt;/p&gt;\n\n&lt;p&gt;What would be a best practice approach for capturing all of these changes in the full data set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1kx6o", "is_robot_indexable": true, "report_reasons": null, "author": "Eatsleeptren", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "subreddit_subscribers": 164205, "created_utc": 1709062951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience in AWS services related to data engineering. I've been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.\n\nWhen I'm trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I'm thinking of where it's a good idea to move my stack to azure. \n\nAppreciate your ideas on this", "author_fullname": "t2_6b96x89c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS or Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b18xcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709030597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience in AWS services related to data engineering. I&amp;#39;ve been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.&lt;/p&gt;\n\n&lt;p&gt;When I&amp;#39;m trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I&amp;#39;m thinking of where it&amp;#39;s a good idea to move my stack to azure. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your ideas on this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b18xcc", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Buy-4947", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b18xcc/aws_or_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b18xcc/aws_or_azure/", "subreddit_subscribers": 164205, "created_utc": 1709030597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to test a SQL database (its procedures and so on)?    \n\n\nAnd what is the best way to do it?", "author_fullname": "t2_lwhyoz1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way/tool for testing SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1e5lm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709046689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to test a SQL database (its procedures and so on)?    &lt;/p&gt;\n\n&lt;p&gt;And what is the best way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1e5lm", "is_robot_indexable": true, "report_reasons": null, "author": "alexcrav", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1e5lm/what_is_the_best_waytool_for_testing_sql/", "subreddit_subscribers": 164205, "created_utc": 1709046689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm running a Spark cluster on Azure Databricks. While notebooks in the browser work, I'd prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can't access certain JVM-dependent functions like `df.rdd.getNumPartitions()` but that's ok.\n\nIs it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I'd like to avoid.", "author_fullname": "t2_jkgy1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyway to run Scala code in notebooks using Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b11qvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709004562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a Spark cluster on Azure Databricks. While notebooks in the browser work, I&amp;#39;d prefer using an IDE like IntelliJ. I successfully set up Databricks Connect and run PySpark code in DataSpell using  Jupyter notebooks. However, I understand I can&amp;#39;t access certain JVM-dependent functions like &lt;code&gt;df.rdd.getNumPartitions()&lt;/code&gt; but that&amp;#39;s ok.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to achieve the same functionality with Scala in IntelliJ? I explored Zeppelin notebooks, but they require local Scala and Spark setup, which I&amp;#39;d like to avoid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b11qvo", "is_robot_indexable": true, "report_reasons": null, "author": "napsterv", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b11qvo/anyway_to_run_scala_code_in_notebooks_using/", "subreddit_subscribers": 164205, "created_utc": 1709004562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like turtles, `(Jim, likes, apples)` `(Jim, age, 35)` `(Jim, knows, Sam)`\n\nHas anyone used these kinds of stores? What\u2019s you use it for?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone actually employed the use of a Triple Store?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1to5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709084664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like turtles, &lt;code&gt;(Jim, likes, apples)&lt;/code&gt; &lt;code&gt;(Jim, age, 35)&lt;/code&gt; &lt;code&gt;(Jim, knows, Sam)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone used these kinds of stores? What\u2019s you use it for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1to5o", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "subreddit_subscribers": 164205, "created_utc": 1709084664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Boost your career growth by actively participating in Engineering Tech Talks. \n\nYou will learn:\n\n- How to make the most of it\n\n- Benefits like Instance Recognition\n\n- Growth opportunities \n\n- Bonus on how to keep up with it\n\n\nI do think tech talks have helped me grow, learning new, building confidence and getting better feedback and reviews.\n\n\nhttps://www.junaideffendi.com/p/why-tech-talks-are-important\n\n\nLet me know, hows your tech talk looks like.", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importance of Tech Talks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1r1ah", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709077553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Boost your career growth by actively participating in Engineering Tech Talks. &lt;/p&gt;\n\n&lt;p&gt;You will learn:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How to make the most of it&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Benefits like Instance Recognition&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Growth opportunities &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Bonus on how to keep up with it&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do think tech talks have helped me grow, learning new, building confidence and getting better feedback and reviews.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.junaideffendi.com/p/why-tech-talks-are-important\"&gt;https://www.junaideffendi.com/p/why-tech-talks-are-important&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know, hows your tech talk looks like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?auto=webp&amp;s=d0e959e009f175e9a4b188daf8cf43721c0d20bb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b100ca3491448c1a56c408a541d51c40219088a9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6b07655440c14388ebc73d5a4877af4c19008b3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf9e25c8bca90557a5192d990b00297ca085cf69", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90facde25a76bfcdb0353da376b949ec728644dd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce7635e62cf4eb0f8edf09722e66d177c2f27b0e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/UYqo488wtX4M0lLMGU4y8W-suLJATPRljtzKx3dpOPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d05eebe7ce34b24d552dc9f25e00b4c9ebec10ca", "width": 1080, "height": 540}], "variants": {}, "id": "kKy5jqFgwBnl99q_MFxIOnQbbGG6JB9r3FVS4DQBXQU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1r1ah", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1r1ah/importance_of_tech_talks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1r1ah/importance_of_tech_talks/", "subreddit_subscribers": 164205, "created_utc": 1709077553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. \n\nI just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. \n\nThe project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. \n\nSo far I've got databases for:  \n\n\n1. Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.\n2. Models - lists last run, columns &amp; descriptions for all models \n3. I was working on trying to bring in the compiled SQL code into a code block, but Notion's API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm building a DBT Docs -&gt; Notion tool for my company. Any interest in having me write a guide or sharing the source code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1i3vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709056179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. &lt;/p&gt;\n\n&lt;p&gt;I just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. &lt;/p&gt;\n\n&lt;p&gt;The project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. &lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve got databases for:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.&lt;/li&gt;\n&lt;li&gt;Models - lists last run, columns &amp;amp; descriptions for all models &lt;/li&gt;\n&lt;li&gt;I was working on trying to bring in the compiled SQL code into a code block, but Notion&amp;#39;s API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1i3vh", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "subreddit_subscribers": 164205, "created_utc": 1709056179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My department has this huge initiative to migrate all of our onprem processes to the cloud. We're an SSIS shop, so we've been asked to convert all of our packages and SQL stored procedures to a hybrid of ADF pipelines and databricks notebooks. Since we're using databricks, they want us to use delta lake as our replacement to SQL Server (Azure SQL Server is not an option).\n\nI have pretty limited exposure to databricks and spark, but another team we work with suggested we - \n\n* convert our tables to parquet files using an ADF pipeline\n* create a delta lake table based off of the parquet files in databricks.\n\nI've already done the above two steps, but I'm wondering if there's going to be any issues with this, or if there's a preferred method that's widely used?", "author_fullname": "t2_5w4jf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating onprem SQL Server tables to azure databricks delta lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1hv4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709055593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My department has this huge initiative to migrate all of our onprem processes to the cloud. We&amp;#39;re an SSIS shop, so we&amp;#39;ve been asked to convert all of our packages and SQL stored procedures to a hybrid of ADF pipelines and databricks notebooks. Since we&amp;#39;re using databricks, they want us to use delta lake as our replacement to SQL Server (Azure SQL Server is not an option).&lt;/p&gt;\n\n&lt;p&gt;I have pretty limited exposure to databricks and spark, but another team we work with suggested we - &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;convert our tables to parquet files using an ADF pipeline&lt;/li&gt;\n&lt;li&gt;create a delta lake table based off of the parquet files in databricks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve already done the above two steps, but I&amp;#39;m wondering if there&amp;#39;s going to be any issues with this, or if there&amp;#39;s a preferred method that&amp;#39;s widely used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1hv4r", "is_robot_indexable": true, "report_reasons": null, "author": "lozyk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1hv4r/migrating_onprem_sql_server_tables_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1hv4r/migrating_onprem_sql_server_tables_to_azure/", "subreddit_subscribers": 164205, "created_utc": 1709055593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!! \n\nI\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.\n\nI\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.\n\nAny insights or suggestions would be greatly appreciated. Thank you!\n\nFeel free to let me know if you need any further adjustments or have any specific preferences!", "author_fullname": "t2_8fi5ln6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on backfilling airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1goj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.&lt;/p&gt;\n\n&lt;p&gt;Any insights or suggestions would be greatly appreciated. Thank you!&lt;/p&gt;\n\n&lt;p&gt;Feel free to let me know if you need any further adjustments or have any specific preferences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1goj4", "is_robot_indexable": true, "report_reasons": null, "author": "sailor_Moon_Pie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "subreddit_subscribers": 164205, "created_utc": 1709052808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jnd3h0a4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharding Redis 101: Using Pixel Art To Explain The Basics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1el4v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1b1el4v", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/LHPXFmXwb3nzKUcnt9skH2TOZyJQEV5gUaa6WJQcnD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709047767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/OUNimTqgkZM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?auto=webp&amp;s=f5f162840cbbc19dfd9555c40dd3cdeeef61bf4c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=673d90c0d2d95230a6ca4c9c0beca2fa7c04fd89", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b23acaf341ff6bbb773b557d242b5681e0dddc3f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DrhPPfcSao-4nKIHZIyBB1DyDV0OVeot7vqRmbmzHJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26b7b3baa6e01e47a2b7e29206f6652c230f02cd", "width": 320, "height": 240}], "variants": {}, "id": "WcjcUtEvWDXzwRXdZ5uUMo1KfN_fjy369prwUT8YECI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1el4v", "is_robot_indexable": true, "report_reasons": null, "author": "schematical", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1el4v/sharding_redis_101_using_pixel_art_to_explain_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/OUNimTqgkZM", "subreddit_subscribers": 164205, "created_utc": 1709047767.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OUNimTqgkZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Scale Redis Horizontally To Save Money Using Sharding On AWS ElastiCache\"&gt;&lt;/iframe&gt;", "author_name": "Schematical", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OUNimTqgkZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Schematical"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2m4jmqrn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Observability: The Next Frontier of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1c0iu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/92p5nuuO28iKEKJTddI8-afwtfhls7XSUT8vwmntpuM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709041001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?auto=webp&amp;s=23b87a3c3434a563f208b5ce9d60ae71cb25eb4c", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d6d3e6cebcee1463c51a65006b6bdaff91447ec", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=54235a755f7b3e0b34af63bd0bfd3e4306f5174b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe1bc4229063a081e7143bdd9c2c603eb10bba8c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CiIb74DNXa_ciWTa9qUm_xD-44uVMxm9gkvrIy689tE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64b707a5757ce60317484ebd1b65082e2efc0d31", "width": 640, "height": 336}], "variants": {}, "id": "JGhC9b9qhHNJ4eh7-qMHVcYJNdZfz1K-5VoSz-ZvhQY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1c0iu", "is_robot_indexable": true, "report_reasons": null, "author": "michaellyamm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1c0iu/data_observability_the_next_frontier_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering", "subreddit_subscribers": 164205, "created_utc": 1709041001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lw1vuj1gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Your Data Consultancy Might Be One Password Away from Disaster And 7 Ways To Fix It", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1byww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wGSkyvfT43b4Yve25V_aPZ2A5z8HTTxyO-BaUf2Mq-g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709040873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/why-your-data-consultancy-might-be-one-password-away-from-disaster-and-7-ways-to-fix-it/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?auto=webp&amp;s=e57d3b8c461698c7565e7ab66a9ad77fbae92bb7", "width": 2258, "height": 1528}, "resolutions": [{"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1000c9b5487d759bb6a5327802759702c2a24d1", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18c1462cab6b72ead48f2350f0612f4a7a4d87e5", "width": 216, "height": 146}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=54811d8c433a4566191e3acb3e6f9a8ab96e3110", "width": 320, "height": 216}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7a9970d6d5eb9abeab108c6c5043e8c08796506", "width": 640, "height": 433}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f09aa6bd7e54bd57c1d742ee8c8f8634b3be050c", "width": 960, "height": 649}, {"url": "https://external-preview.redd.it/FIVv7WuJug2v4jVpgYlABB3kg0kSXlnRVi2nBBIT_OI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=906f8128fb1a75f1769203f3ac4244f3f87e7736", "width": 1080, "height": 730}], "variants": {}, "id": "R-cgpqSW0siFKko5wr6GGJDyV0tyhJdCGKizuk-hOhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b1byww", "is_robot_indexable": true, "report_reasons": null, "author": "Distinct-Economics24", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1byww/why_your_data_consultancy_might_be_one_password/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/why-your-data-consultancy-might-be-one-password-away-from-disaster-and-7-ways-to-fix-it/", "subreddit_subscribers": 164205, "created_utc": 1709040873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I'm thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.", "author_fullname": "t2_h0hkehfsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion suggestion in azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1a3ni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709035013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I&amp;#39;m thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1a3ni", "is_robot_indexable": true, "report_reasons": null, "author": "timetravel_looper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1a3ni/data_ingestion_suggestion_in_azure/", "subreddit_subscribers": 164205, "created_utc": 1709035013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we can connect Microsoft 365 business central SQL Database!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b17ax0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709023867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. \nConsequently, please tell me how to establish connectivity with the Business Central SQL Database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b17ax0", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b17ax0/how_we_can_connect_microsoft_365_business_central/", "subreddit_subscribers": 164205, "created_utc": 1709023867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting query timeouts and appropriate alerts for Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1b15yi4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oojNEsC-H3Ge4vQ7WjyWaiJvolq7cwbkK8qZJfrqjeQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709018452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?auto=webp&amp;s=83f5ede9df0770bc92fe9880edcb22ddc85750c3", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a9dcda060ee14aa03003fea8f51eb6e9e8411b8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a3acabcc936daa3d1ab0acb3f87eee4007c3f5", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebdb2e183c0fffcf7993fbcc10996816e0fb550d", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e75db1f02946562a52ac283ab5d47e5d80151b40", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9adda2cae88e7de944026539799f038d7d0de755", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/IFZ61Gk4vn9DGpTXxFYpO22-5j4pVm49gW1UHwpP9uk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=293187c6fe2dbf3bbcf054286fcbe7ee6a9470e8", "width": 1080, "height": 720}], "variants": {}, "id": "ZhjudSQ1egdhKR93zH-Y4GUy7HyAkIjZEYYCTBXg5fE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b15yi4", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b15yi4/setting_query_timeouts_and_appropriate_alerts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@sahil_singla/how-to-not-run-a-12-000-query-on-snowflake-b80c456b790c", "subreddit_subscribers": 164205, "created_utc": 1709018452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anybody here have experience with designing huge enterprise-wide dimensional models? I have extensive experience in designing dimensional models on the data mart level for BI tools. I often see mentions of dimensional modeling being used at the core of a DWH for an enterprise-wide (whole business) model. I struggle with understanding how that is designed. \n\nIn my experience when talking about an enterprise-wide model we talk about a layer that contains cleansed data that all reporting, ML, and analytics use cases will be building on top of. That means that this model needs to be flexible (support many use cases, some of them not yet known) and is hard to change once designed (many dependencies). Without exact reporting requirements and the ability to easily change the model (e.g. grain of a fact table) later, how can a dimensional model be successful as an enterprise-wide layer? Thanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enterprise-wide dimensional model, does it work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b14do2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709012864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anybody here have experience with designing huge enterprise-wide dimensional models? I have extensive experience in designing dimensional models on the data mart level for BI tools. I often see mentions of dimensional modeling being used at the core of a DWH for an enterprise-wide (whole business) model. I struggle with understanding how that is designed. &lt;/p&gt;\n\n&lt;p&gt;In my experience when talking about an enterprise-wide model we talk about a layer that contains cleansed data that all reporting, ML, and analytics use cases will be building on top of. That means that this model needs to be flexible (support many use cases, some of them not yet known) and is hard to change once designed (many dependencies). Without exact reporting requirements and the ability to easily change the model (e.g. grain of a fact table) later, how can a dimensional model be successful as an enterprise-wide layer? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b14do2", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b14do2/enterprisewide_dimensional_model_does_it_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b14do2/enterprisewide_dimensional_model_does_it_work/", "subreddit_subscribers": 164205, "created_utc": 1709012864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?", "author_fullname": "t2_itoec5nn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for monitoring ADF pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b1ug0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709086794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ug0t", "is_robot_indexable": true, "report_reasons": null, "author": "CocoaDependent1664", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "subreddit_subscribers": 164205, "created_utc": 1709086794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI'm looking for advice and insights on modernizing my data warehouse architecture. Here's where I currently stand:\n\n* Kimball methodology with a star schema.\n* My ETL processes are managed by Talend Open Studio open source - has been discontinued\n   * I import jobs once a day\n* Tableau serves as my primary tool for reporting.\n* PostgreSQL is my  DBMS.\n\nHere's what I'm aiming for in the modernization process:\n\n1. **Data Volume:** I process around 1-2 million rows daily.\n2. **Real-Time Needs:** I'm aiming for near-real-time data processing.\n3. **New ETL Tool:** I'm considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I'd love to hear your thoughts and any advice you might have.\n4. **Database Consideration:** ClickHouse has caught my attention for its potential benefits.\n\nAny advice, experiences, or alternative suggestions are welcome!\n\nThank you", "author_fullname": "t2_9fw69uas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on Modernizing Data Warehouse Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1me42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709066459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for advice and insights on modernizing my data warehouse architecture. Here&amp;#39;s where I currently stand:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimball methodology with a star schema.&lt;/li&gt;\n&lt;li&gt;My ETL processes are managed by Talend Open Studio open source - has been discontinued\n\n&lt;ul&gt;\n&lt;li&gt;I import jobs once a day&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tableau serves as my primary tool for reporting.&lt;/li&gt;\n&lt;li&gt;PostgreSQL is my  DBMS.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m aiming for in the modernization process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Volume:&lt;/strong&gt; I process around 1-2 million rows daily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Real-Time Needs:&lt;/strong&gt; I&amp;#39;m aiming for near-real-time data processing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;New ETL Tool:&lt;/strong&gt; I&amp;#39;m considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I&amp;#39;d love to hear your thoughts and any advice you might have.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Database Consideration:&lt;/strong&gt; ClickHouse has caught my attention for its potential benefits.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice, experiences, or alternative suggestions are welcome!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1me42", "is_robot_indexable": true, "report_reasons": null, "author": "Ahmouu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "subreddit_subscribers": 164205, "created_utc": 1709066459.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}