{"kind": "Listing", "data": {"after": "t3_1b1me42", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5a55k9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the reasons why ADF is love/hate type of thing - a task that would be super easy in Python such as skipping a for loop block if a condition is not met, turns out into a creative hack that takes many hours to get right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1gmf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jA_LNyv0BCZbxetNQpaMxQenEHPQvRu3lH9TbA7DnTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709052668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rhfpex8nq5lc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?auto=webp&amp;s=f50af32a38a82268aaeacfbba92f41f97685fbbd", "width": 1515, "height": 861}, "resolutions": [{"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=587c1c97d4263e0f0c5c0831adac22d35e8db4c8", "width": 108, "height": 61}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d86464f6a9fe7ce768086594dc2b9a2048f8a4d3", "width": 216, "height": 122}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=036fa90d1e66d8966bfa8aaa01ece070c4b86e10", "width": 320, "height": 181}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed5695d954a3169c76fea2e650d7b011bf889162", "width": 640, "height": 363}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16d2b1fe9bf607f6f075b6267494c38d1eef469", "width": 960, "height": 545}, {"url": "https://preview.redd.it/rhfpex8nq5lc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bbb31ad543c4846964e2800599f9056ee9f3a7aa", "width": 1080, "height": 613}], "variants": {}, "id": "sK0y8seZr8DTVPJEbWf_dGHmEkWW2nm1w0iE7Iv3kH8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1gmf6", "is_robot_indexable": true, "report_reasons": null, "author": "koteikin", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1gmf6/one_of_the_reasons_why_adf_is_lovehate_type_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rhfpex8nq5lc1.png", "subreddit_subscribers": 164378, "created_utc": 1709052668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, DataEngineers on Reddit! \ud83d\ude80 So, I'm making this move from Data Analyst to Data Engineering and could use some advice. Right now, I'm rocking the intermediate level in Postgre SQL. My game plan is to dive into the Data Engineer track on DataCamp, focusing on Postgre and Python.\n\nOnce I've got that down, I'm eyeing AWS and Azure certifications to level up my cloud skills. Do you think this combo will be enough to score a Data Engineering role, or am I missing something crucial? Any friendly suggestions or thoughts? Cheers! \ud83e\udd13 #DataEngineeringJourney", "author_fullname": "t2_sn6a5x3cf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Analyst to Data Engineer!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ztkv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709103720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, DataEngineers on Reddit! \ud83d\ude80 So, I&amp;#39;m making this move from Data Analyst to Data Engineering and could use some advice. Right now, I&amp;#39;m rocking the intermediate level in Postgre SQL. My game plan is to dive into the Data Engineer track on DataCamp, focusing on Postgre and Python.&lt;/p&gt;\n\n&lt;p&gt;Once I&amp;#39;ve got that down, I&amp;#39;m eyeing AWS and Azure certifications to level up my cloud skills. Do you think this combo will be enough to score a Data Engineering role, or am I missing something crucial? Any friendly suggestions or thoughts? Cheers! \ud83e\udd13 #DataEngineeringJourney&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b1ztkv", "is_robot_indexable": true, "report_reasons": null, "author": "Mega-Byte-69", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1ztkv/transitioning_from_analyst_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ztkv/transitioning_from_analyst_to_data_engineer/", "subreddit_subscribers": 164378, "created_utc": 1709103720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like turtles, `(Jim, likes, apples)` `(Jim, age, 35)` `(Jim, knows, Sam)`\n\nHas anyone used these kinds of stores? What\u2019d you use it for?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone actually employed the use of a Triple Store?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1to5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709093656.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709084664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like turtles, &lt;code&gt;(Jim, likes, apples)&lt;/code&gt; &lt;code&gt;(Jim, age, 35)&lt;/code&gt; &lt;code&gt;(Jim, knows, Sam)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone used these kinds of stores? What\u2019d you use it for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1to5o", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1to5o/has_anyone_actually_employed_the_use_of_a_triple/", "subreddit_subscribers": 164378, "created_utc": 1709084664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.\n\nThe API's data is updated daily around 9pm, so my first approach was to request the data each day by `created_date` and drop partitioned files in the S3 bucket.\n\nHowever, I realized that the number of rows increases for previous days.  For example,\n\n* On 2024-02-25 the initial request returned 9000 rows\n* Yesterday, I got back 9500 rows for 2024-02-25\n* Today, I got back 10000 rows for 2024-02-25\n\nIt seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, \\~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it's still theoretically possible data could change as far back as a year.\n\nThe only \"solution\" I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I'd have to rebuild all the tables in my DW everyday, so I don't feel like this is a good solution.\n\nWhat would be a best practice approach for capturing all of these changes in the full data set?", "author_fullname": "t2_j12b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice approach for ingesting data from an API where many rows will be added/changed/deleted over time without any indication/warning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1kx6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709064570.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709062951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The API&amp;#39;s data is updated daily around 9pm, so my first approach was to request the data each day by &lt;code&gt;created_date&lt;/code&gt; and drop partitioned files in the S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;However, I realized that the number of rows increases for previous days.  For example,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On 2024-02-25 the initial request returned 9000 rows&lt;/li&gt;\n&lt;li&gt;Yesterday, I got back 9500 rows for 2024-02-25&lt;/li&gt;\n&lt;li&gt;Today, I got back 10000 rows for 2024-02-25&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, ~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it&amp;#39;s still theoretically possible data could change as far back as a year.&lt;/p&gt;\n\n&lt;p&gt;The only &amp;quot;solution&amp;quot; I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I&amp;#39;d have to rebuild all the tables in my DW everyday, so I don&amp;#39;t feel like this is a good solution.&lt;/p&gt;\n\n&lt;p&gt;What would be a best practice approach for capturing all of these changes in the full data set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1kx6o", "is_robot_indexable": true, "report_reasons": null, "author": "Eatsleeptren", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1kx6o/best_practice_approach_for_ingesting_data_from_an/", "subreddit_subscribers": 164378, "created_utc": 1709062951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the SQL patterns you use on a regular basis and why?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite SQL patterns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b26kf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709128402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the SQL patterns you use on a regular basis and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b26kf9", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b26kf9/favorite_sql_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b26kf9/favorite_sql_patterns/", "subreddit_subscribers": 164378, "created_utc": 1709128402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nJust wrapped up a project where I built a system to predict rental prices using data from Rightmove. I really dived into Data Engineering, ML Engineering, and MLOps, all thanks to the free Data Talk Clubs courses I took. I am self taught in Data Engineering and ML in general (Finance graduate). I would really appreciate any constructive feedback on this project. \n\n**Quick features:**\n\n* Production Web Scraping with monitoring\n* RandomForest Rental Prediction model with feature engineering. Engineered the walk score algorithm (based on what I could find online) \n* MLOps with model, data quality and data drift monitoring. \n\n**Tech Stack:**\n\n* Infrastructure: Terraform, Docker Compose, AWS, and GCP.\n* Model serving with FastAPI and visual insights via Streamlit and Grafana.\n* Experiment tracking with MLFlow.\n\nI really tried to mesh everything I could from these courses together. I am not sure if I followed industry standards. Feel free to be as harsh and as honest as you like. All I care about is that the feedback is actionable. Thank you. \n\n&amp;#x200B;\n\n[System Diagram](https://preview.redd.it/hkikvujy9blc1.png?width=4913&amp;format=png&amp;auto=webp&amp;s=8df8f6e916fd98a48ca89b7467e4b4d917b7a7bd)\n\nGithub: [https://github.com/alexandergirardet/london\\_rightmove](https://github.com/alexandergirardet/london_rightmove)", "author_fullname": "t2_7kiysnlez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rental Price Prediction ML/Data system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hkikvujy9blc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b4ecf2fe0ef73b5dc27d2555c54346cf414dfaa"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7b791dda412d51718c47c7fc75c4eedca8e99bf"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bd0aee1e01699207466451210f864605d526d41"}, {"y": 423, "x": 640, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b12ae52d47ce9bcaf8f6bdd732b5877662d12ff"}, {"y": 634, "x": 960, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f2c241e29f1baec345d13a994aff4fda15cde66"}, {"y": 713, "x": 1080, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d42da4d411d269eb458b1a57ffa8bca7536c8b0d"}], "s": {"y": 3248, "x": 4913, "u": "https://preview.redd.it/hkikvujy9blc1.png?width=4913&amp;format=png&amp;auto=webp&amp;s=8df8f6e916fd98a48ca89b7467e4b4d917b7a7bd"}, "id": "hkikvujy9blc1"}}, "name": "t3_1b23thj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pF2HR7JNASG7SvVBIwqpZ18vFo5gCEZW6EK8QLX6ATU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1709119579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Just wrapped up a project where I built a system to predict rental prices using data from Rightmove. I really dived into Data Engineering, ML Engineering, and MLOps, all thanks to the free Data Talk Clubs courses I took. I am self taught in Data Engineering and ML in general (Finance graduate). I would really appreciate any constructive feedback on this project. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Production Web Scraping with monitoring&lt;/li&gt;\n&lt;li&gt;RandomForest Rental Prediction model with feature engineering. Engineered the walk score algorithm (based on what I could find online) &lt;/li&gt;\n&lt;li&gt;MLOps with model, data quality and data drift monitoring. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Infrastructure: Terraform, Docker Compose, AWS, and GCP.&lt;/li&gt;\n&lt;li&gt;Model serving with FastAPI and visual insights via Streamlit and Grafana.&lt;/li&gt;\n&lt;li&gt;Experiment tracking with MLFlow.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I really tried to mesh everything I could from these courses together. I am not sure if I followed industry standards. Feel free to be as harsh and as honest as you like. All I care about is that the feedback is actionable. Thank you. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hkikvujy9blc1.png?width=4913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8df8f6e916fd98a48ca89b7467e4b4d917b7a7bd\"&gt;System Diagram&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/alexandergirardet/london_rightmove\"&gt;https://github.com/alexandergirardet/london_rightmove&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?auto=webp&amp;s=1ae21a88a44fcec76fcc4fcd3fbc82f4c54cfa78", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=601bdb1e3403eaa8c338c8cd4e4eacd6cc9a43ac", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=690b982c78a6f130040da7605f55e82e5bfa2893", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e2b21acf899b7251354347bfa50b80c024f5834", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e5857ffe0f3f70e26bebecd6220980fe9ad7fab", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=026227c8bdb54856ecf4a89daaaee393a7ff6bf8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_VYKNb9XM32VNo31NViqbvuQWQ_oQZyIQv97uSIZ36E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1729cd0fd7b0c72b3c2d1761f87b556ed476fa1d", "width": 1080, "height": 540}], "variants": {}, "id": "H1SGab16E_VzBuxBMY1UzihuMtP-QTbjKRH4HFsHyyY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b23thj", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Bobcat_7458", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b23thj/rental_price_prediction_mldata_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b23thj/rental_price_prediction_mldata_system/", "subreddit_subscribers": 164378, "created_utc": 1709119579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is using Airbyte for ELT, and having the option to easily add a Dbt transformation with each connection was probably the main point of choosing Airbyte, but they're deprecating this feature by the end of March and focusing on the EL process. We're looking for a data orchestrator to help schedule our airbyte connections (these are created and triggered by using Airbyte's API's) followed by the Dbt transformations, and the recommended tools by Airbyte are Airflow, Dagster, Prefect and Kestra. Which of these would you recommend?\n\nP.S. I think it's obvious that I'm a junior DE that's still very new to the field, I'd appreciate any guidance.", "author_fullname": "t2_fkh83nm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte + Airflow, Dagster, Prefect or Kestra?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b24nxy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709122497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is using Airbyte for ELT, and having the option to easily add a Dbt transformation with each connection was probably the main point of choosing Airbyte, but they&amp;#39;re deprecating this feature by the end of March and focusing on the EL process. We&amp;#39;re looking for a data orchestrator to help schedule our airbyte connections (these are created and triggered by using Airbyte&amp;#39;s API&amp;#39;s) followed by the Dbt transformations, and the recommended tools by Airbyte are Airflow, Dagster, Prefect and Kestra. Which of these would you recommend?&lt;/p&gt;\n\n&lt;p&gt;P.S. I think it&amp;#39;s obvious that I&amp;#39;m a junior DE that&amp;#39;s still very new to the field, I&amp;#39;d appreciate any guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b24nxy", "is_robot_indexable": true, "report_reasons": null, "author": "Whatinthetabuleh", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b24nxy/airbyte_airflow_dagster_prefect_or_kestra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b24nxy/airbyte_airflow_dagster_prefect_or_kestra/", "subreddit_subscribers": 164378, "created_utc": 1709122497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title says all. Want to understand where is community gearing towards", "author_fullname": "t2_3x0urbjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your KPIs/OKRs for this year ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1wec1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709092400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says all. Want to understand where is community gearing towards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1wec1", "is_robot_indexable": true, "report_reasons": null, "author": "educationruinedme1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1wec1/what_are_your_kpisokrs_for_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1wec1/what_are_your_kpisokrs_for_this_year/", "subreddit_subscribers": 164378, "created_utc": 1709092400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zero to Hero: Mastering Change Data Capture For Remarkable Database Integrations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1b27q0s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ycJDX51KGt-CxBXVSFmRxnoz0uvD4GgdsxwjC2QQ2CU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709131557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datagibberish.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datagibberish.com/p/cdc-zero-to-hero?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6Yprr-_9ZZ177Pzj549GKTWYY4_RTOWLuJ9LsDuo7AY.jpg?auto=webp&amp;s=0be439f660d0bfd5718272bbd3f5c44f51ad10d1", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/6Yprr-_9ZZ177Pzj549GKTWYY4_RTOWLuJ9LsDuo7AY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60cbd1a1eeee95e619272d080f6972b2dc9c0356", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/6Yprr-_9ZZ177Pzj549GKTWYY4_RTOWLuJ9LsDuo7AY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38ca2aefb06f6f2d40c2900172a9f6189dbb6783", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/6Yprr-_9ZZ177Pzj549GKTWYY4_RTOWLuJ9LsDuo7AY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b8ec0c152bfd228b7e6fe3d6902b09609ea7633", "width": 320, "height": 320}], "variants": {}, "id": "IVo0FWHiDAfejzfRVaj0bCPCxjaTH9Zcu4cj0zREdp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b27q0s", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b27q0s/zero_to_hero_mastering_change_data_capture_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datagibberish.com/p/cdc-zero-to-hero?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 164378, "created_utc": 1709131557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community,\n\nI wanted to ask if there are still relational DBMS / MPP systems around where predicate pushdown optimization is not a thing.  From my experience, the times when this optimization was not present are long gone, most optimizers are doing pretty good job at applying the filters as early as possible. Whats your experience guys?  Anyone knows any relatively modern data processing engine where it does matter in terms of performance where to put filter condition for example (WHERE clause vs.  INNER JOIN clause)?", "author_fullname": "t2_8xpzmwy0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predicate pushdown optimization in modern data processing engines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1z2zh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709101019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community,&lt;/p&gt;\n\n&lt;p&gt;I wanted to ask if there are still relational DBMS / MPP systems around where predicate pushdown optimization is not a thing.  From my experience, the times when this optimization was not present are long gone, most optimizers are doing pretty good job at applying the filters as early as possible. Whats your experience guys?  Anyone knows any relatively modern data processing engine where it does matter in terms of performance where to put filter condition for example (WHERE clause vs.  INNER JOIN clause)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1z2zh", "is_robot_indexable": true, "report_reasons": null, "author": "HatShort14", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1z2zh/predicate_pushdown_optimization_in_modern_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1z2zh/predicate_pushdown_optimization_in_modern_data/", "subreddit_subscribers": 164378, "created_utc": 1709101019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Data Engineering Community!\n\nI'm currently working on a Spark Streaming project aimed at processing telemetry JSON messages. The primary goal is to normalize these messages for further analytics and operations. Given the nature of telemetry data, we're dealing with a high volume of messages that come in a variety of formats and structures, making the normalization process quite challenging.\n\nOne of the key requirements of this project is to maintain low latency (less 10 sec) from ingestion to processed output. This is critical as the data is time-sensitive and needs to be actionable almost in real-time. However, I've encountered several challenges, especially when it comes to handling complex joins and dealing with various issues related to data quality and structure.\n\nHere are a few points I'm grappling with:\n\n1. **Normalization Complexity**: The telemetry messages vary significantly in structure and content. What are some effective strategies for dynamically normalizing these messages into a unified format that's suitable for downstream processing and analysis?\n2. **Spark Streaming Limitations**: I've implemented an approach that handles and pivots different parts of a JSON message using various DataFrames. These are then joined to construct a normalized message. However, some of these DataFrames can be empty, necessitating a left join that doesn't function properly without watermarks. Unfortunately, watermarks are not suitable for my solution as they could introduce performance issues. My objective is to join DataFrames within a JSON message without logically needing to use watermarks, which seems unnecessary and potentially problematic in this context.\n3. **Handling Joins with Low Latency**: Some of the normalization logic requires enriching the telemetry data by joining it with reference datasets. However, these joins are proving to be a bottleneck, especially when trying to keep the processing latency low. How can I optimize these operations, considering the distributed nature of Spark?\n4. **Tooling and Techniques**: Are there specific Spark Streaming features, external tools, or techniques that you've found particularly useful for these types of challenges?\n5. **Examples and Case Studies**: If you've worked on similar projects, could you share your approach, especially how you managed to balance the trade-offs between normalization complexity, data quality, and latency requirements?\n\nAny advice, insights, or references to relevant resources would be greatly appreciated. I'm looking for strategies to improve the efficiency and effectiveness of our Spark Streaming jobs, particularly around the areas of data normalization and processing latency.\n\nThank you in advance for your help and suggestions!", "author_fullname": "t2_2xzylww1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Normalizing Telemetry JSON Messages in Spark Streaming with Low Latency: Handling Complex Joins and Data Issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1x5o5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709094713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Data Engineering Community!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a Spark Streaming project aimed at processing telemetry JSON messages. The primary goal is to normalize these messages for further analytics and operations. Given the nature of telemetry data, we&amp;#39;re dealing with a high volume of messages that come in a variety of formats and structures, making the normalization process quite challenging.&lt;/p&gt;\n\n&lt;p&gt;One of the key requirements of this project is to maintain low latency (less 10 sec) from ingestion to processed output. This is critical as the data is time-sensitive and needs to be actionable almost in real-time. However, I&amp;#39;ve encountered several challenges, especially when it comes to handling complex joins and dealing with various issues related to data quality and structure.&lt;/p&gt;\n\n&lt;p&gt;Here are a few points I&amp;#39;m grappling with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Normalization Complexity&lt;/strong&gt;: The telemetry messages vary significantly in structure and content. What are some effective strategies for dynamically normalizing these messages into a unified format that&amp;#39;s suitable for downstream processing and analysis?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Spark Streaming Limitations&lt;/strong&gt;: I&amp;#39;ve implemented an approach that handles and pivots different parts of a JSON message using various DataFrames. These are then joined to construct a normalized message. However, some of these DataFrames can be empty, necessitating a left join that doesn&amp;#39;t function properly without watermarks. Unfortunately, watermarks are not suitable for my solution as they could introduce performance issues. My objective is to join DataFrames within a JSON message without logically needing to use watermarks, which seems unnecessary and potentially problematic in this context.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handling Joins with Low Latency&lt;/strong&gt;: Some of the normalization logic requires enriching the telemetry data by joining it with reference datasets. However, these joins are proving to be a bottleneck, especially when trying to keep the processing latency low. How can I optimize these operations, considering the distributed nature of Spark?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tooling and Techniques&lt;/strong&gt;: Are there specific Spark Streaming features, external tools, or techniques that you&amp;#39;ve found particularly useful for these types of challenges?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Examples and Case Studies&lt;/strong&gt;: If you&amp;#39;ve worked on similar projects, could you share your approach, especially how you managed to balance the trade-offs between normalization complexity, data quality, and latency requirements?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice, insights, or references to relevant resources would be greatly appreciated. I&amp;#39;m looking for strategies to improve the efficiency and effectiveness of our Spark Streaming jobs, particularly around the areas of data normalization and processing latency.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help and suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1x5o5", "is_robot_indexable": true, "report_reasons": null, "author": "vk_pro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1x5o5/best_practices_for_normalizing_telemetry_json/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1x5o5/best_practices_for_normalizing_telemetry_json/", "subreddit_subscribers": 164378, "created_utc": 1709094713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,   \nI am reading the system design book by Alex Xu and cannot understand the design. \n\nThis is a click aggregation system. We are reading data from the log and writing it to 1(Kafka topic). Then there's an aggregation performed and it writes to 2(Kafka topic). From 2 we write it to DB. I don't understand why we need the 2nd Kafka topic, why can't we directly aggregate and write it to db. In the book it says - **To achieve end-to-end exactly once semantics (atomic commit) we need it.**  \n Can someone please explain what that means?   \n\n\nhttps://preview.redd.it/jtncia1729lc1.png?width=650&amp;format=png&amp;auto=webp&amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System Design Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jtncia1729lc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/jtncia1729lc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b6fe84292d8f88b33d44e6382fffe24c8931caa"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/jtncia1729lc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d7819da718acf840922697f997d6b5a6b7263cc"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/jtncia1729lc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4c48e9a19bf3e7786010aedb5b680342b53158"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/jtncia1729lc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e452954393d2d4762d8587cc6fbbc12f73a1c444"}], "s": {"y": 343, "x": 650, "u": "https://preview.redd.it/jtncia1729lc1.png?width=650&amp;format=png&amp;auto=webp&amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae"}, "id": "jtncia1729lc1"}}, "name": "t3_1b1wl4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nGmftjutpHr4knW0vvYvI9X5roKybztjaSHh3fbIFKM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709092961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nI am reading the system design book by Alex Xu and cannot understand the design. &lt;/p&gt;\n\n&lt;p&gt;This is a click aggregation system. We are reading data from the log and writing it to 1(Kafka topic). Then there&amp;#39;s an aggregation performed and it writes to 2(Kafka topic). From 2 we write it to DB. I don&amp;#39;t understand why we need the 2nd Kafka topic, why can&amp;#39;t we directly aggregate and write it to db. In the book it says - &lt;strong&gt;To achieve end-to-end exactly once semantics (atomic commit) we need it.&lt;/strong&gt;&lt;br/&gt;\n Can someone please explain what that means?   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jtncia1729lc1.png?width=650&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae\"&gt;https://preview.redd.it/jtncia1729lc1.png?width=650&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12bac9015b3e71787f63d2cad131e5e5db2f08ae&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1wl4e", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b1wl4e/system_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1wl4e/system_design_question/", "subreddit_subscribers": 164378, "created_utc": 1709092961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. \n\nI just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. \n\nThe project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. \n\nSo far I've got databases for:  \n\n\n1. Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.\n2. Models - lists last run, columns &amp; descriptions for all models \n3. I was working on trying to bring in the compiled SQL code into a code block, but Notion's API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm building a DBT Docs -&gt; Notion tool for my company. Any interest in having me write a guide or sharing the source code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1i3vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709056179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. &lt;/p&gt;\n\n&lt;p&gt;I just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. &lt;/p&gt;\n\n&lt;p&gt;The project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. &lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve got databases for:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.&lt;/li&gt;\n&lt;li&gt;Models - lists last run, columns &amp;amp; descriptions for all models &lt;/li&gt;\n&lt;li&gt;I was working on trying to bring in the compiled SQL code into a code block, but Notion&amp;#39;s API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1i3vh", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1i3vh/im_building_a_dbt_docs_notion_tool_for_my_company/", "subreddit_subscribers": 164378, "created_utc": 1709056179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there - I have an option to either go for a DBA internship in a big tech company which might lead to job/give me a kickstart in data career OR take an expensive Data Engineering bootcamp which would teach latest tech stack like Apache Nifi, Spark, Hive, Pig, Kafka, etc. and offer job/internship placement.\n\nWhich path should I take? I am a recent grad with Engineering Major. My ultimate goal is to become Data Engineer but due to no Data related experience, I am not really able to land a relevant role.\n\nWould appreciate any guidance/comments!", "author_fullname": "t2_lm8mxlfu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance needed: DBA internship or DE bootcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b28f8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709133382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there - I have an option to either go for a DBA internship in a big tech company which might lead to job/give me a kickstart in data career OR take an expensive Data Engineering bootcamp which would teach latest tech stack like Apache Nifi, Spark, Hive, Pig, Kafka, etc. and offer job/internship placement.&lt;/p&gt;\n\n&lt;p&gt;Which path should I take? I am a recent grad with Engineering Major. My ultimate goal is to become Data Engineer but due to no Data related experience, I am not really able to land a relevant role.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any guidance/comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b28f8d", "is_robot_indexable": true, "report_reasons": null, "author": "NetIllustrious6586", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b28f8d/guidance_needed_dba_internship_or_de_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b28f8d/guidance_needed_dba_internship_or_de_bootcamp/", "subreddit_subscribers": 164378, "created_utc": 1709133382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python ETL pipeline with Airbyte and Pathway", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b27h4g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1709130889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pathway.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://pathway.com/developers/showcases/etl-python-airbyte", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b27h4g", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b27h4g/python_etl_pipeline_with_airbyte_and_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pathway.com/developers/showcases/etl-python-airbyte", "subreddit_subscribers": 164378, "created_utc": 1709130889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Role of Interoperability in End-to-End Data Governance: As Implemented by Data Developer Platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1b25xlc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M4cxEA9NVhD8DRIWHM8Ajz_2HN3nPEhqfhlSZm6P2xc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709126512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/role-of-interoperability-in-end-to", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?auto=webp&amp;s=4d9c8cccb71946e01306c967d477a205f10d362b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=314e4e94f444969dad82dfb98e383bc69dc54fa4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=507d19878f183c99abec4797771193cc2ebb1877", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=addb80b38d493b4a5b902d5c4ae744678e03ed23", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2f2868977ec9a9690a4c46182abf87e3221485c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=175f2aefcfdf5427d8de757de3db14e4290c829d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RRw0A9DFPXOPeSrK3f3BXSpbqYgmyA6qGabnFAxNhN8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5dd448f1e944e3f2f54c55c18830c600312592eb", "width": 1080, "height": 540}], "variants": {}, "id": "6rcHi_tEisOpy7ufTkqiEjSOuRr6NREOrmx6JJA2zck"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b25xlc", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b25xlc/role_of_interoperability_in_endtoend_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/role-of-interoperability-in-end-to", "subreddit_subscribers": 164378, "created_utc": 1709126512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am being asked to learn pyspark. ( my manager said it will be easier to pick Databricks if you get good with pyspark- so Databricks may be the next step in this learning process) \nPlease suggest some of learning paths and certification and practice methods that you experts considered to be the best from your experience. \n\nMy background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, \n\nSome basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) \n\nI have tried installing python using anaconda distribution and have meddled with some python programming doing pandas, some web crawling spider - all following some step by step tutorial. But don\u2019t remember much as I have not continued practising.\n\n", "author_fullname": "t2_6cuqg1k5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning plan recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1xshy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709096723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am being asked to learn pyspark. ( my manager said it will be easier to pick Databricks if you get good with pyspark- so Databricks may be the next step in this learning process) \nPlease suggest some of learning paths and certification and practice methods that you experts considered to be the best from your experience. &lt;/p&gt;\n\n&lt;p&gt;My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, &lt;/p&gt;\n\n&lt;p&gt;Some basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) &lt;/p&gt;\n\n&lt;p&gt;I have tried installing python using anaconda distribution and have meddled with some python programming doing pandas, some web crawling spider - all following some step by step tutorial. But don\u2019t remember much as I have not continued practising.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1xshy", "is_robot_indexable": true, "report_reasons": null, "author": "sneekeeei", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1xshy/learning_plan_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1xshy/learning_plan_recommendations/", "subreddit_subscribers": 164378, "created_utc": 1709096723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the expensive cloud services (for de/analytics) and the counter service that you guys use?\n", "author_fullname": "t2_fta9agm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expensive cloud services and the counters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1w3n5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709091513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the expensive cloud services (for de/analytics) and the counter service that you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1w3n5", "is_robot_indexable": true, "report_reasons": null, "author": "luqmancrit69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1w3n5/expensive_cloud_services_and_the_counters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1w3n5/expensive_cloud_services_and_the_counters/", "subreddit_subscribers": 164378, "created_utc": 1709091513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!! \n\nI\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.\n\nI\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.\n\nAny insights or suggestions would be greatly appreciated. Thank you!\n\nFeel free to let me know if you need any further adjustments or have any specific preferences!", "author_fullname": "t2_8fi5ln6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on backfilling airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1goj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709052808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working with a DAG named \u2018tpa\u2019, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I\u2019m familiar with the Airflow CLI command, but I\u2019m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, \u2018tpa\u2019 has a dependency on another pipeline named \u2018st\u2019, which also takes about an hour to execute.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I\u2019m uncertain if this is the best approach.&lt;/p&gt;\n\n&lt;p&gt;Any insights or suggestions would be greatly appreciated. Thank you!&lt;/p&gt;\n\n&lt;p&gt;Feel free to let me know if you need any further adjustments or have any specific preferences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b1goj4", "is_robot_indexable": true, "report_reasons": null, "author": "sailor_Moon_Pie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1goj4/need_advice_on_backfilling_airflow/", "subreddit_subscribers": 164378, "created_utc": 1709052808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are in the process of building out a new data warehouse, and I've been reading Kimball on dimensional modeling. Does anyone know of any good sources or have any advice on optimizing our data model specifically with PowerBI's vertipaq engine in mind?", "author_fullname": "t2_49ipr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing data model for PowerBI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b27wbs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709132026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are in the process of building out a new data warehouse, and I&amp;#39;ve been reading Kimball on dimensional modeling. Does anyone know of any good sources or have any advice on optimizing our data model specifically with PowerBI&amp;#39;s vertipaq engine in mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b27wbs", "is_robot_indexable": true, "report_reasons": null, "author": "thomasutra", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b27wbs/optimizing_data_model_for_powerbi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b27wbs/optimizing_data_model_for_powerbi/", "subreddit_subscribers": 164378, "created_utc": 1709132026.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Either I\u2019m losing my ability to google, or there really is just a complete lack of comprehensive Prefect testing documentation that covers more than assert my_flow() == 42. I feel like I need to join the Freemasons to learn the ancient secrets for this. \n\nBasic stuff I get, but mocking dependencies? Mocking out datetime without freezegun causing everything to error out?\n\nHas anyone successfully unit tested anything in prefect more advanced than a hello world flow?", "author_fullname": "t2_1eht5os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on unit testing Prefect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b26bx0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709127703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Either I\u2019m losing my ability to google, or there really is just a complete lack of comprehensive Prefect testing documentation that covers more than assert my_flow() == 42. I feel like I need to join the Freemasons to learn the ancient secrets for this. &lt;/p&gt;\n\n&lt;p&gt;Basic stuff I get, but mocking dependencies? Mocking out datetime without freezegun causing everything to error out?&lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully unit tested anything in prefect more advanced than a hello world flow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b26bx0", "is_robot_indexable": true, "report_reasons": null, "author": "mbsquad24", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b26bx0/need_advice_on_unit_testing_prefect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b26bx0/need_advice_on_unit_testing_prefect/", "subreddit_subscribers": 164378, "created_utc": 1709127703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm relatively new to the world of data. So not 100% sure this is the right place to ask this question.\n\nI've been following quite a few American and European experts on LinkedIn as part of my education. And there seems to be a renewed interest in (conceptual) data modeling. They're talking about how data teams are not often in sync with business needs and conceptual models can bridge this gap.\n\nWhile I'm familiar with ER diagrams, none of the resources I've used as relative novice put any stress on this.\n\nIs it applicable only when it comes to enterprise use cases? Where there might be the need to have this layer that business and data teams can use?\n\n(I'm also posting this on other related subs to get more feedback, just fyi).", "author_fullname": "t2_us2vwkpwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a renewed focus on (conceptual) data modeling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b22oie", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709115232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m relatively new to the world of data. So not 100% sure this is the right place to ask this question.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been following quite a few American and European experts on LinkedIn as part of my education. And there seems to be a renewed interest in (conceptual) data modeling. They&amp;#39;re talking about how data teams are not often in sync with business needs and conceptual models can bridge this gap.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m familiar with ER diagrams, none of the resources I&amp;#39;ve used as relative novice put any stress on this.&lt;/p&gt;\n\n&lt;p&gt;Is it applicable only when it comes to enterprise use cases? Where there might be the need to have this layer that business and data teams can use?&lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m also posting this on other related subs to get more feedback, just fyi).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b22oie", "is_robot_indexable": true, "report_reasons": null, "author": "sama-3lli3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b22oie/is_there_a_renewed_focus_on_conceptual_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b22oie/is_there_a_renewed_focus_on_conceptual_data/", "subreddit_subscribers": 164378, "created_utc": 1709115232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any ideas on [https://news.ycombinator.com/item?id=39535743](https://news.ycombinator.com/item?id=39535743)", "author_fullname": "t2_czm7z9qa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source data platform alternative equivalent to Palantir Foundry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b221tr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709112675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any ideas on &lt;a href=\"https://news.ycombinator.com/item?id=39535743\"&gt;https://news.ycombinator.com/item?id=39535743&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b221tr", "is_robot_indexable": true, "report_reasons": null, "author": "leokster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b221tr/open_source_data_platform_alternative_equivalent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b221tr/open_source_data_platform_alternative_equivalent/", "subreddit_subscribers": 164378, "created_utc": 1709112675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?", "author_fullname": "t2_itoec5nn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for monitoring ADF pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1ug0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709086794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1ug0t", "is_robot_indexable": true, "report_reasons": null, "author": "CocoaDependent1664", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1ug0t/best_practice_for_monitoring_adf_pipelines/", "subreddit_subscribers": 164378, "created_utc": 1709086794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI'm looking for advice and insights on modernizing my data warehouse architecture. Here's where I currently stand:\n\n* Kimball methodology with a star schema.\n* My ETL processes are managed by Talend Open Studio open source - has been discontinued\n   * I import jobs once a day\n* Tableau serves as my primary tool for reporting.\n* PostgreSQL is my  DBMS.\n\nHere's what I'm aiming for in the modernization process:\n\n1. **Data Volume:** I process around 1-2 million rows daily.\n2. **Real-Time Needs:** I'm aiming for near-real-time data processing.\n3. **New ETL Tool:** I'm considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I'd love to hear your thoughts and any advice you might have.\n4. **Database Consideration:** ClickHouse has caught my attention for its potential benefits.\n\nAny advice, experiences, or alternative suggestions are welcome!\n\nThank you", "author_fullname": "t2_9fw69uas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on Modernizing Data Warehouse Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b1me42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709066459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for advice and insights on modernizing my data warehouse architecture. Here&amp;#39;s where I currently stand:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kimball methodology with a star schema.&lt;/li&gt;\n&lt;li&gt;My ETL processes are managed by Talend Open Studio open source - has been discontinued\n\n&lt;ul&gt;\n&lt;li&gt;I import jobs once a day&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tableau serves as my primary tool for reporting.&lt;/li&gt;\n&lt;li&gt;PostgreSQL is my  DBMS.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m aiming for in the modernization process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Volume:&lt;/strong&gt; I process around 1-2 million rows daily.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Real-Time Needs:&lt;/strong&gt; I&amp;#39;m aiming for near-real-time data processing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;New ETL Tool:&lt;/strong&gt; I&amp;#39;m considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I&amp;#39;d love to hear your thoughts and any advice you might have.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Database Consideration:&lt;/strong&gt; ClickHouse has caught my attention for its potential benefits.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice, experiences, or alternative suggestions are welcome!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b1me42", "is_robot_indexable": true, "report_reasons": null, "author": "Ahmouu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b1me42/advice_on_modernizing_data_warehouse_architecture/", "subreddit_subscribers": 164378, "created_utc": 1709066459.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}