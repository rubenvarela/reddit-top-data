{"kind": "Listing", "data": {"after": "t3_1apl1l7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked as a data professional with different job titles for about 10 years now and I'm noticing a pattern that I haven't seen explored before, and I'm interested in some structure.  I see two types of data engineering roles/teams and I'll try to describe them as best I can in a couple bullets.  \n\n\nType 1: the converted software engineer\n\n* Comfortable in Scala/Rust/Spark/kubernetes\n* Handles not just deployment but serving production systems\n* Highly interested in optimizations as these save the company real money or just to make the problems tractable. \n* More likely to use streaming architectures.\n* Further removed from the business problems.\n\n&amp;#x200B;\n\nType 2: the senior data scientist\n\n* I've built a fancy model, now what?\n* Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well\n* Likely don't have replicated environments.\n* Passes off data artifacts/exposes data products to the SE team for integration into core platform\n* Focused on solving problems for the business, which necessitates data engineering.\n\n&amp;#x200B;\n\nI could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?", "author_fullname": "t2_xi7dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering vs Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap40zg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707755384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked as a data professional with different job titles for about 10 years now and I&amp;#39;m noticing a pattern that I haven&amp;#39;t seen explored before, and I&amp;#39;m interested in some structure.  I see two types of data engineering roles/teams and I&amp;#39;ll try to describe them as best I can in a couple bullets.  &lt;/p&gt;\n\n&lt;p&gt;Type 1: the converted software engineer&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Comfortable in Scala/Rust/Spark/kubernetes&lt;/li&gt;\n&lt;li&gt;Handles not just deployment but serving production systems&lt;/li&gt;\n&lt;li&gt;Highly interested in optimizations as these save the company real money or just to make the problems tractable. &lt;/li&gt;\n&lt;li&gt;More likely to use streaming architectures.&lt;/li&gt;\n&lt;li&gt;Further removed from the business problems.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Type 2: the senior data scientist&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve built a fancy model, now what?&lt;/li&gt;\n&lt;li&gt;Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well&lt;/li&gt;\n&lt;li&gt;Likely don&amp;#39;t have replicated environments.&lt;/li&gt;\n&lt;li&gt;Passes off data artifacts/exposes data products to the SE team for integration into core platform&lt;/li&gt;\n&lt;li&gt;Focused on solving problems for the business, which necessitates data engineering.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap40zg", "is_robot_indexable": true, "report_reasons": null, "author": "apple_pie_52", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/", "subreddit_subscribers": 160335, "created_utc": 1707755384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the dbt features you use and love most?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite DBT features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap8onr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707766517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the dbt features you use and love most?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap8onr", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap8onr/favorite_dbt_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap8onr/favorite_dbt_features/", "subreddit_subscribers": 160335, "created_utc": 1707766517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m exhausted of the Iceberg vs Delta Lake arguments. They\u2019ve devolved to nitpicking minor features, none of which bring long-term value to Data Engineering.\n\nGoogle and Microsoft appear to be backing OneTable (renamed to XTable). What are your thoughts?\n\nhttps://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065", "author_fullname": "t2_z15jq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on OneTable/XTable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap2xop", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707752677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m exhausted of the Iceberg vs Delta Lake arguments. They\u2019ve devolved to nitpicking minor features, none of which bring long-term value to Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;Google and Microsoft appear to be backing OneTable (renamed to XTable). What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065\"&gt;https://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap2xop", "is_robot_indexable": true, "report_reasons": null, "author": "Data_cruncher", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap2xop/thoughts_on_onetablextable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap2xop/thoughts_on_onetablextable/", "subreddit_subscribers": 160335, "created_utc": 1707752677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One part of my app acts as a pseudo data-pipeline, where I ETL a single player's data at a time (I have to due to the nature of the API I'm pulling data from).\n\nI am currently using pandas for all of this, where I pull the data from the API, throw it in a dataframe, transform it, and run \\`pandas.to\\_sql()\\` to store it.\n\nThe problem is that this is memory intensive, and I'm running into RAM issues when running this in a docker container.\n\nWhat would be a good memory efficient tool to work with moving datasets through an application?\n\nTo clarify: I am using pandas as a tool to efficiently pass around this dataset that I add data to, transform it, and store it in the database. I would need this replacement tool to do the same thing in python. I also have only 1 server, so a distributed system like Spark isn't an option for me", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using pandas in a data-intensive application - what the best, RAM-efficient, alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aphlml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707789616.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707789155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One part of my app acts as a pseudo data-pipeline, where I ETL a single player&amp;#39;s data at a time (I have to due to the nature of the API I&amp;#39;m pulling data from).&lt;/p&gt;\n\n&lt;p&gt;I am currently using pandas for all of this, where I pull the data from the API, throw it in a dataframe, transform it, and run `pandas.to_sql()` to store it.&lt;/p&gt;\n\n&lt;p&gt;The problem is that this is memory intensive, and I&amp;#39;m running into RAM issues when running this in a docker container.&lt;/p&gt;\n\n&lt;p&gt;What would be a good memory efficient tool to work with moving datasets through an application?&lt;/p&gt;\n\n&lt;p&gt;To clarify: I am using pandas as a tool to efficiently pass around this dataset that I add data to, transform it, and store it in the database. I would need this replacement tool to do the same thing in python. I also have only 1 server, so a distributed system like Spark isn&amp;#39;t an option for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aphlml", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aphlml/using_pandas_in_a_dataintensive_application_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aphlml/using_pandas_in_a_dataintensive_application_what/", "subreddit_subscribers": 160335, "created_utc": 1707789155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "These days it seems like every solution is cloud-based for data engineering. What are some instances in which moving to the cloud does not make sense.", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are instances where data storage, ETL, analytics, etc don't make sense on the cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1apkimj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707797667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These days it seems like every solution is cloud-based for data engineering. What are some instances in which moving to the cloud does not make sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1apkimj", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1apkimj/what_are_instances_where_data_storage_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1apkimj/what_are_instances_where_data_storage_etl/", "subreddit_subscribers": 160335, "created_utc": 1707797667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Aspirants,\n\nLast week, I cleared DP-203 with 920 score after 2 weeks of studying. I have made a post about it and the resources used here: [https://www.reddit.com/r/dataengineering/comments/1andkrm/passed\\_dp203\\_last\\_week\\_details\\_for\\_aspirants\\_in/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nThis will be my second and last post related to DP-203. In this post I focus on how you should go about studying as in my preparation I came across so many resources to study but no concrete plan or strategy which only led to confusion.\n\n**Section A: Build Your Knowledge Base**\n\n1. Start with synapse documentation.  \nUnderstand spark pools, difference between serverless and dedicated sql pools. Managed and external tables. Dynamic Data Masking. TDE. Row Level Security and Column Level Security.\n2. Azure Stream Analytics documentation will be up next.  \nBe sure to understand 5 types of windows as atleast 1 question will be on that.\n3. Azure Data Factory Documentation\n4. MS Learn Path  \nSCD Types. 1 questions atleast will come on it.\n\nNote: There is no way you are going to go through all the pages in documentations and next to impossible to remember them. My advise would be to skim through it and try to build a high level knowledge base. You do not need to be expert who knows everything.\n\n**Section B: Real Preparation Starts Here, Build Knowledge required for actual exam.**  \nNote: Below two points are going to be very vital for you to pass the exam as 70-80% questions will be from Dump.\n\n1. Exam Topics free questions.  \nGo through the discussions and the links provided, this is where your best learning is going to happen. There will be some questions with divided answers and you are going to just have to trust your gut. \n2. [https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5](https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5)  \nGo through this video. Many questions will overlap with Exam Topics but still highly recommened watching this video as questions will come from this as well, especially the case study.\n\n**Section C: Important must-do topics**\n\n1. SCD Types: [https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types](https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types)\n2. Replication Zones: [https://youtu.be/K9epl86BGOk](https://youtu.be/K9epl86BGOk)  \nAbove channel is a gold mine for this certification as well as for aspiring Azure Data Engineers. Highly recommend going through his videos, especially security, access tier and ADF ones.\n3. Window Types in Stream Analytics: [https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions)\n4. Data Masking: [https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql](https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql)\n\n**Section D: Final Tips for Exam**\n\n1. Do give Microsoft practice assessment. \n2. Try to navigate MS learn when studying, giving above assessment, etc. It will be useful when giving exam, you can get couple of answers by going through it.\n3. Know the answers to previously asked questions and the logic for them thoroughly.\n4. Plan how you are going to give exam. I had 41 questions and 100 minutes. I thought I'll be attempting 15 questions every 30 minutes but as it turned out most questions were from dumps so ended up marking 30 questions inside first 20 minutes :)   \nPoint is you will have lot of time if you have done the dumps thoroughly, so mark questions you are not sure about for review, and at the end go through MS learn.  \n\n\nIf anyone has any questions, feel free to reach out to me or comment here. I'll be glad to help.", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Study advise for DP-203 Aspirants.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap22x7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707750407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Aspirants,&lt;/p&gt;\n\n&lt;p&gt;Last week, I cleared DP-203 with 920 score after 2 weeks of studying. I have made a post about it and the resources used here: &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This will be my second and last post related to DP-203. In this post I focus on how you should go about studying as in my preparation I came across so many resources to study but no concrete plan or strategy which only led to confusion.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Section A: Build Your Knowledge Base&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Start with synapse documentation.&lt;br/&gt;\nUnderstand spark pools, difference between serverless and dedicated sql pools. Managed and external tables. Dynamic Data Masking. TDE. Row Level Security and Column Level Security.&lt;/li&gt;\n&lt;li&gt;Azure Stream Analytics documentation will be up next.&lt;br/&gt;\nBe sure to understand 5 types of windows as atleast 1 question will be on that.&lt;/li&gt;\n&lt;li&gt;Azure Data Factory Documentation&lt;/li&gt;\n&lt;li&gt;MS Learn Path&lt;br/&gt;\nSCD Types. 1 questions atleast will come on it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Note: There is no way you are going to go through all the pages in documentations and next to impossible to remember them. My advise would be to skim through it and try to build a high level knowledge base. You do not need to be expert who knows everything.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Section B: Real Preparation Starts Here, Build Knowledge required for actual exam.&lt;/strong&gt;&lt;br/&gt;\nNote: Below two points are going to be very vital for you to pass the exam as 70-80% questions will be from Dump.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Exam Topics free questions.&lt;br/&gt;\nGo through the discussions and the links provided, this is where your best learning is going to happen. There will be some questions with divided answers and you are going to just have to trust your gut. &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5\"&gt;https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&lt;/a&gt;&lt;br/&gt;\nGo through this video. Many questions will overlap with Exam Topics but still highly recommened watching this video as questions will come from this as well, especially the case study.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Section C: Important must-do topics&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SCD Types: &lt;a href=\"https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types\"&gt;https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Replication Zones: &lt;a href=\"https://youtu.be/K9epl86BGOk\"&gt;https://youtu.be/K9epl86BGOk&lt;/a&gt;&lt;br/&gt;\nAbove channel is a gold mine for this certification as well as for aspiring Azure Data Engineers. Highly recommend going through his videos, especially security, access tier and ADF ones.&lt;/li&gt;\n&lt;li&gt;Window Types in Stream Analytics: &lt;a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions\"&gt;https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Data Masking: &lt;a href=\"https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql\"&gt;https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Section D: Final Tips for Exam&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do give Microsoft practice assessment. &lt;/li&gt;\n&lt;li&gt;Try to navigate MS learn when studying, giving above assessment, etc. It will be useful when giving exam, you can get couple of answers by going through it.&lt;/li&gt;\n&lt;li&gt;Know the answers to previously asked questions and the logic for them thoroughly.&lt;/li&gt;\n&lt;li&gt;Plan how you are going to give exam. I had 41 questions and 100 minutes. I thought I&amp;#39;ll be attempting 15 questions every 30 minutes but as it turned out most questions were from dumps so ended up marking 30 questions inside first 20 minutes :)&lt;br/&gt;\nPoint is you will have lot of time if you have done the dumps thoroughly, so mark questions you are not sure about for review, and at the end go through MS learn.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If anyone has any questions, feel free to reach out to me or comment here. I&amp;#39;ll be glad to help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?auto=webp&amp;s=9bbf148fcd0f0b8b6fd5549050b351928e70b845", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cba2396bca6523f443570ee351863f6ce61485eb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92392c8a86e0431966f0e26cb979c564da2e37fd", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13b784ad47410ac26c659b9de5295ad6e6f32249", "width": 320, "height": 240}], "variants": {}, "id": "t-X48R81FbV2hHKHXQHfnFoap_VzKiNwCda90e43_Hs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ap22x7", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap22x7/study_advise_for_dp203_aspirants/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap22x7/study_advise_for_dp203_aspirants/", "subreddit_subscribers": 160335, "created_utc": 1707750407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to transition into data engineering and am looking for advice. \n\nIn a few months, I'll graduate with a PhD in physics. I've decided that staying in academia isn't for me, and thought some of my skills might translate well to data engineering. I've been applying to positions for about a month now, and haven't had any luck. I'm seeking feedback. \n\nMy research requires me to use Python to interact with datasets almost daily. I'm confident writing code for data acquisition, complex calculations, as well as data-visualization.\n\nI lack any experience with common cloud-based systems (AWS, Azure, etc). I also don't have SQL experience (although I've been learning on my own time). \n\nAre there any obvious steps I could take to get a foot in the door? Is it worth completing AWS certifications, or am I just throwing money away? \n\nAny advice would be appreciated.", "author_fullname": "t2_4ebp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone trying to transition into data engineering from academia?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap269w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707750659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to transition into data engineering and am looking for advice. &lt;/p&gt;\n\n&lt;p&gt;In a few months, I&amp;#39;ll graduate with a PhD in physics. I&amp;#39;ve decided that staying in academia isn&amp;#39;t for me, and thought some of my skills might translate well to data engineering. I&amp;#39;ve been applying to positions for about a month now, and haven&amp;#39;t had any luck. I&amp;#39;m seeking feedback. &lt;/p&gt;\n\n&lt;p&gt;My research requires me to use Python to interact with datasets almost daily. I&amp;#39;m confident writing code for data acquisition, complex calculations, as well as data-visualization.&lt;/p&gt;\n\n&lt;p&gt;I lack any experience with common cloud-based systems (AWS, Azure, etc). I also don&amp;#39;t have SQL experience (although I&amp;#39;ve been learning on my own time). &lt;/p&gt;\n\n&lt;p&gt;Are there any obvious steps I could take to get a foot in the door? Is it worth completing AWS certifications, or am I just throwing money away? &lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ap269w", "is_robot_indexable": true, "report_reasons": null, "author": "ianmgull", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap269w/advice_for_someone_trying_to_transition_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap269w/advice_for_someone_trying_to_transition_into_data/", "subreddit_subscribers": 160335, "created_utc": 1707750659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a data platform and am looking to learn from people who have bought or implemented a new BI tool (Metabase, Looker, Superset, Tableau, anything!) in the last \\~6 months.\n\n(Edit: to clarify, I'm building a data tool product/company, rather than implementing BI for my team.)\n\nI'm more specifically looking to understand what problems you set out to solve, what triggered the decision to adopt a new tool in the first place, what options you looked at, and what made you select that product.\n\nIf you were involved in picking a BI tool recently and are also open to jumping onto a \\~15min call to walk me through your experience, I'd be 1) extremely thankful and 2) also happy to compensate you for your time!", "author_fullname": "t2_md5ib1c5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you choose your BI setup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aoz9ic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707767731.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707742134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a data platform and am looking to learn from people who have bought or implemented a new BI tool (Metabase, Looker, Superset, Tableau, anything!) in the last ~6 months.&lt;/p&gt;\n\n&lt;p&gt;(Edit: to clarify, I&amp;#39;m building a data tool product/company, rather than implementing BI for my team.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m more specifically looking to understand what problems you set out to solve, what triggered the decision to adopt a new tool in the first place, what options you looked at, and what made you select that product.&lt;/p&gt;\n\n&lt;p&gt;If you were involved in picking a BI tool recently and are also open to jumping onto a ~15min call to walk me through your experience, I&amp;#39;d be 1) extremely thankful and 2) also happy to compensate you for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aoz9ic", "is_robot_indexable": true, "report_reasons": null, "author": "AdImaginary8024", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aoz9ic/how_did_you_choose_your_bi_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aoz9ic/how_did_you_choose_your_bi_setup/", "subreddit_subscribers": 160335, "created_utc": 1707742134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is considering migrating from AWS Postgres RDS to either Redshift/Snowflake. \n\nCurrently, we have two ways of ingesting 3rd party providers data:\n\n* Fivetran, which can be connected to both Redshift/Snowflake, \n* and an EC2 instance with Crons, which can be refactored,\n\n I am unsure if a direct connection via the EC2 instance with all the crons there is the best approach for the ingestion. If it fails, we will stop receiving the data from these sources completely.\n\nI was wondering if you could have a better approach for the ingestion or anything to keep in mind on the transition.", "author_fullname": "t2_126mha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best approach to 3rd ingest data into Redshift / Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aoygvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707739412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is considering migrating from AWS Postgres RDS to either Redshift/Snowflake. &lt;/p&gt;\n\n&lt;p&gt;Currently, we have two ways of ingesting 3rd party providers data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fivetran, which can be connected to both Redshift/Snowflake, &lt;/li&gt;\n&lt;li&gt;&lt;p&gt;and an EC2 instance with Crons, which can be refactored,&lt;/p&gt;\n\n&lt;p&gt;I am unsure if a direct connection via the EC2 instance with all the crons there is the best approach for the ingestion. If it fails, we will stop receiving the data from these sources completely.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was wondering if you could have a better approach for the ingestion or anything to keep in mind on the transition.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aoygvj", "is_robot_indexable": true, "report_reasons": null, "author": "Peivol", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aoygvj/best_approach_to_3rd_ingest_data_into_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aoygvj/best_approach_to_3rd_ingest_data_into_redshift/", "subreddit_subscribers": 160335, "created_utc": 1707739412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi everyone :)\n\nI've been working on a cool project in the past 1.5 months and I was wondering if you'd like to try it. It's called Merlinn, and it's for fellow engineers and anyone who use observability tools. It's an LLM agent designed to speed up incident resolution and minimize the Mean Time to Resolution (MTTR).\n\nWhat it does is it basically connects to your observability tools and data sources and tries to investigate production alerts &amp; incidents on its own, and provide key findings in seconds directly to Slack. You can learn more about it in this website: [https://merlinn.co](https://merlinn.co/)\n\nI'd really love to get some feedback on that and talk about how you investigate and resolve incidents &amp; alerts in your organization. I plan on building more integrations (OpenTelemetry, Prometheus, Google Cloud Logging, etc) and I'd love to talk with the community about observability.", "author_fullname": "t2_n9em3bcw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I developed a cool new LLM agent that helps with investigating and resolving alerts faster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aoy4oo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707738198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a cool project in the past 1.5 months and I was wondering if you&amp;#39;d like to try it. It&amp;#39;s called Merlinn, and it&amp;#39;s for fellow engineers and anyone who use observability tools. It&amp;#39;s an LLM agent designed to speed up incident resolution and minimize the Mean Time to Resolution (MTTR).&lt;/p&gt;\n\n&lt;p&gt;What it does is it basically connects to your observability tools and data sources and tries to investigate production alerts &amp;amp; incidents on its own, and provide key findings in seconds directly to Slack. You can learn more about it in this website: &lt;a href=\"https://merlinn.co/\"&gt;https://merlinn.co&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really love to get some feedback on that and talk about how you investigate and resolve incidents &amp;amp; alerts in your organization. I plan on building more integrations (OpenTelemetry, Prometheus, Google Cloud Logging, etc) and I&amp;#39;d love to talk with the community about observability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aoy4oo", "is_robot_indexable": true, "report_reasons": null, "author": "Old_Cauliflower6316", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aoy4oo/i_developed_a_cool_new_llm_agent_that_helps_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aoy4oo/i_developed_a_cool_new_llm_agent_that_helps_with/", "subreddit_subscribers": 160335, "created_utc": 1707738198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on thoughts. With the Snowflake release of Dynamic Tables, and the ability to have them update on a schedule, triggered from the last table in the DAG, is there still a place for dbt on Snowflake?\n\nI get that it comes with docs, and a nice viz for lineage, but these things aren\u2019t hard to pull together in other ways.\n\nKeen to hear others thoughts.", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt still relevant on Snowflake with Dynamic Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aovhqs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707727196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on thoughts. With the Snowflake release of Dynamic Tables, and the ability to have them update on a schedule, triggered from the last table in the DAG, is there still a place for dbt on Snowflake?&lt;/p&gt;\n\n&lt;p&gt;I get that it comes with docs, and a nice viz for lineage, but these things aren\u2019t hard to pull together in other ways.&lt;/p&gt;\n\n&lt;p&gt;Keen to hear others thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aovhqs", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aovhqs/is_dbt_still_relevant_on_snowflake_with_dynamic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aovhqs/is_dbt_still_relevant_on_snowflake_with_dynamic/", "subreddit_subscribers": 160335, "created_utc": 1707727196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " My team has some Python scripts which extract data from several API sources and save as csv files. The data from each source produces several staging tables. So for example we extract several data objects from the Salesforce API and same for Stripe etc and I'm not sure how I'd structure this in a bucket. I was thinking something like this:\n\ns3://{bucketname}/{API\\_source}/{dataobject or tablename}/{yyyy-mm-dd}\n\nor\n\ns3://{bucketname}/{API\\_source}/{yyyy-mm-dd}/{dataobject or tablename}\n\nNot sure which of these two is better or if there is a better way altogether. We have no need for an additional 'layer' in the form of a separate bucket because we have minimal pre-processing.", "author_fullname": "t2_hxekotykt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to properly structure and partition an S3 bucket for raw data storage from several sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1apdkil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707778238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has some Python scripts which extract data from several API sources and save as csv files. The data from each source produces several staging tables. So for example we extract several data objects from the Salesforce API and same for Stripe etc and I&amp;#39;m not sure how I&amp;#39;d structure this in a bucket. I was thinking something like this:&lt;/p&gt;\n\n&lt;p&gt;s3://{bucketname}/{API_source}/{dataobject or tablename}/{yyyy-mm-dd}&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;s3://{bucketname}/{API_source}/{yyyy-mm-dd}/{dataobject or tablename}&lt;/p&gt;\n\n&lt;p&gt;Not sure which of these two is better or if there is a better way altogether. We have no need for an additional &amp;#39;layer&amp;#39; in the form of a separate bucket because we have minimal pre-processing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1apdkil", "is_robot_indexable": true, "report_reasons": null, "author": "NotGuiltySparkk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1apdkil/how_to_properly_structure_and_partition_an_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1apdkil/how_to_properly_structure_and_partition_an_s3/", "subreddit_subscribers": 160335, "created_utc": 1707778238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been prototyping a simple data tool for small teams. The problem I'm trying to validate is: setting up a data stack and doing basic analytics is hard for growing teams. \n\nThere are [too many moving parts](https://twitter.com/eladgil/status/1649861867576385537) (managing ingestion with Fivetran/Meltano/Airbyte, managing a warehouse, setting up Airflow, dbt, and then finally building out your model and metrics for dashboards) and this puts people off. \n\nI spoke with a few somewhat-large teams (Series A/B/C startups) and realized most data stacks are built in a rush, with poor UX for the folks building it. Essentially, teams tend to avoid building out a data stack for as long as possible. \n\nI'm trying to do a few things at the same time, so I thought I'd turn to this community for feedback. I think:\n\n1. Building a tool which spins up a warehouse (you pick between Bigquery, Redshift, Snowflake) lets you pick connectors directly (from fivetran/metlano/airbyte) and start ingesting data right away\n2. Comes with pre-built metrics layer for the most commonly used metrics (mostly marketing, sales, product metrics) \n3. You can extend by bringing in or building metrics with SQL that sync (bi-directionally) with dbt\n4. Has basic features like reporting, alerts, campaign drill-downs built into it. \n\nMy hypothesis is that this might allow small and growing teams to handle their own data needs by themselves, and once they have a data engineer onboard will still be built on top of tools they are familiar with. \n\nI'm hesitant to paste a link here, because this really isn't a promotion or shill, rather I just want to get feedback, and help think through common gotcha's. I know there are a billion tools out there that do some part of this, but I'm targeting: \n\n1. Super-modern UX (like Vercel for analytics) \n2. Go to exclusively teams that are just starting out with their data-stacks\n3. Eventually build a cheaper, better UX platform that helps you build, manage, and run anything data-related on your team\n\nOpen to your feedback here or via DMs. Would love to learn what I'm doing wrong (or right) ", "author_fullname": "t2_jds3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback on building data platform as a service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap90en", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1707767288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been prototyping a simple data tool for small teams. The problem I&amp;#39;m trying to validate is: setting up a data stack and doing basic analytics is hard for growing teams. &lt;/p&gt;\n\n&lt;p&gt;There are &lt;a href=\"https://twitter.com/eladgil/status/1649861867576385537\"&gt;too many moving parts&lt;/a&gt; (managing ingestion with Fivetran/Meltano/Airbyte, managing a warehouse, setting up Airflow, dbt, and then finally building out your model and metrics for dashboards) and this puts people off. &lt;/p&gt;\n\n&lt;p&gt;I spoke with a few somewhat-large teams (Series A/B/C startups) and realized most data stacks are built in a rush, with poor UX for the folks building it. Essentially, teams tend to avoid building out a data stack for as long as possible. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to do a few things at the same time, so I thought I&amp;#39;d turn to this community for feedback. I think:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Building a tool which spins up a warehouse (you pick between Bigquery, Redshift, Snowflake) lets you pick connectors directly (from fivetran/metlano/airbyte) and start ingesting data right away&lt;/li&gt;\n&lt;li&gt;Comes with pre-built metrics layer for the most commonly used metrics (mostly marketing, sales, product metrics) &lt;/li&gt;\n&lt;li&gt;You can extend by bringing in or building metrics with SQL that sync (bi-directionally) with dbt&lt;/li&gt;\n&lt;li&gt;Has basic features like reporting, alerts, campaign drill-downs built into it. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My hypothesis is that this might allow small and growing teams to handle their own data needs by themselves, and once they have a data engineer onboard will still be built on top of tools they are familiar with. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hesitant to paste a link here, because this really isn&amp;#39;t a promotion or shill, rather I just want to get feedback, and help think through common gotcha&amp;#39;s. I know there are a billion tools out there that do some part of this, but I&amp;#39;m targeting: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Super-modern UX (like Vercel for analytics) &lt;/li&gt;\n&lt;li&gt;Go to exclusively teams that are just starting out with their data-stacks&lt;/li&gt;\n&lt;li&gt;Eventually build a cheaper, better UX platform that helps you build, manage, and run anything data-related on your team&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Open to your feedback here or via DMs. Would love to learn what I&amp;#39;m doing wrong (or right) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gZtwRFTdTjVvN5MidYDGlbQpBxsFK2nnwxJOv5BL9D0.jpg?auto=webp&amp;s=a52b7664a0a6cdc0baabbd748e5d383577482cff", "width": 140, "height": 86}, "resolutions": [{"url": "https://external-preview.redd.it/gZtwRFTdTjVvN5MidYDGlbQpBxsFK2nnwxJOv5BL9D0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cc7450e847783aae707948d4ddb8aee792c6808", "width": 108, "height": 66}], "variants": {}, "id": "CCMEE5mjEXNzmtN7_L3L1qiCpCkb4utxHPPcTV9mF90"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap90en", "is_robot_indexable": true, "report_reasons": null, "author": "peekkk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap90en/feedback_on_building_data_platform_as_a_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap90en/feedback_on_building_data_platform_as_a_service/", "subreddit_subscribers": 160335, "created_utc": 1707767288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Previous Post](https://www.reddit.com/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/)\n\n Hello DBT Enthusiasts!\n\nAfter some great discussions from my last post, I'm curious about another aspect of DBT: **DBT Packages**. I've recently gone through the package list again, after having gained significantly more experience in DBT and found the dbt\\_artifacts package that now allows me to report on our \"dbt exposures\". It provides the ability to report on things like test &amp; documentation coverage which is something I'm interested in lately.   \n\n\nI'd like to know:\n\n1. **Which DBT Packages are essential in your projects?**\n2. **Any challenges or limitations with these packages?**\n3. **Have you created custom DBT packages? If so, what prompted this?**  \n\n\nI'll go first:\n\n1. dbt\\_artifacts &amp; dbt\\_codegen most notably.\n   1. Artifacts I've explained above, and codegen is great for adding new resources like sources, models or yaml files.\n2. Sharing &amp; documenting which macros the company uses, and for what. I know this is solved by a Notion wiki or something, but that's work. \n3. No, but I did create a model that shows column-level descriptions from a dbt\\_artifacts asset. The gets extracted from dbt\\_artifacts' \"models\" table. I thought about trying to contribute it to the project myself, but it uses a db-specific function so wouldn't work for everyone.\n\nYour insights will help us all better understand the practical use of these packages and potentially inspire improvements or new creations!\n\nExcited to hear your thoughts!  \n\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another question for DBT Users -&gt; Who's Using DBT Packages and How?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap88lt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707765469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1aj3fac/people_who_use_dbt_data_build_tool_what_are_you/\"&gt;Previous Post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hello DBT Enthusiasts!&lt;/p&gt;\n\n&lt;p&gt;After some great discussions from my last post, I&amp;#39;m curious about another aspect of DBT: &lt;strong&gt;DBT Packages&lt;/strong&gt;. I&amp;#39;ve recently gone through the package list again, after having gained significantly more experience in DBT and found the dbt_artifacts package that now allows me to report on our &amp;quot;dbt exposures&amp;quot;. It provides the ability to report on things like test &amp;amp; documentation coverage which is something I&amp;#39;m interested in lately.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Which DBT Packages are essential in your projects?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Any challenges or limitations with these packages?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Have you created custom DBT packages? If so, what prompted this?&lt;/strong&gt;&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ll go first:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dbt_artifacts &amp;amp; dbt_codegen most notably.\n\n&lt;ol&gt;\n&lt;li&gt;Artifacts I&amp;#39;ve explained above, and codegen is great for adding new resources like sources, models or yaml files.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Sharing &amp;amp; documenting which macros the company uses, and for what. I know this is solved by a Notion wiki or something, but that&amp;#39;s work. &lt;/li&gt;\n&lt;li&gt;No, but I did create a model that shows column-level descriptions from a dbt_artifacts asset. The gets extracted from dbt_artifacts&amp;#39; &amp;quot;models&amp;quot; table. I thought about trying to contribute it to the project myself, but it uses a db-specific function so wouldn&amp;#39;t work for everyone.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Your insights will help us all better understand the practical use of these packages and potentially inspire improvements or new creations!&lt;/p&gt;\n\n&lt;p&gt;Excited to hear your thoughts!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap88lt", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap88lt/another_question_for_dbt_users_whos_using_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap88lt/another_question_for_dbt_users_whos_using_dbt/", "subreddit_subscribers": 160335, "created_utc": 1707765469.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Column and table names occasionally change, sometimes on tables inside the organization we do not maintain. Our current solution is we have a piece of regex code that finds all of the GitHub repositories that use whatever field or table you are looking for and provides the code that field is in and we tell everyone to update their code based on who usually owns/works on that repo. \n\nMy supervisor doesn't like this because it is manual and wants the DE team to be responsible on fixing things when these changes occur. I don't like writing a regex replace on an entire code base because I find it tedious, prone to error, and prefer to leave coding changes up to the product owner. Am I being stubborn? Are there other ways to manage this?\n\nWe have an on-prem oracle DB, sloowwwlly moving over to snowflake, and code in python/SQL orchestrated by Airflow. \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_f1q7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing SQL Table Changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap3flw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707753939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Column and table names occasionally change, sometimes on tables inside the organization we do not maintain. Our current solution is we have a piece of regex code that finds all of the GitHub repositories that use whatever field or table you are looking for and provides the code that field is in and we tell everyone to update their code based on who usually owns/works on that repo. &lt;/p&gt;\n\n&lt;p&gt;My supervisor doesn&amp;#39;t like this because it is manual and wants the DE team to be responsible on fixing things when these changes occur. I don&amp;#39;t like writing a regex replace on an entire code base because I find it tedious, prone to error, and prefer to leave coding changes up to the product owner. Am I being stubborn? Are there other ways to manage this?&lt;/p&gt;\n\n&lt;p&gt;We have an on-prem oracle DB, sloowwwlly moving over to snowflake, and code in python/SQL orchestrated by Airflow. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ap3flw", "is_robot_indexable": true, "report_reasons": null, "author": "machinegunke11y", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap3flw/managing_sql_table_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap3flw/managing_sql_table_changes/", "subreddit_subscribers": 160335, "created_utc": 1707753939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!\n\ndlt (data load tool) library added **Databricks** and **Azure** **Synapse** destinations. Now you too can benefit from schema inference, evolution, management, column level lineage and data contracts.\n\n**How to try?**\n\nSimply run **pip install dlt** and you're halfway there. For the detailed setup, check out the docs:\n\n* [Databricks](https://dlthub.com/docs/dlt-ecosystem/destinations/databricks). Do you already have the dlt \u201cdelta live tables\u201d Installed? see [Here](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21)\n* [Azure synapse](https://dlthub.com/docs/dlt-ecosystem/destinations/synapse)\n* Want to try on duckdb? here's a colab [notebook](https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM)\n\n**I'm eager to hear your thoughts:**\n\n* Have you worked with dlt (data load tool) and Databricks before?\n* Did you try running data load tool on databricks but got delta live tables already installed? this [guide](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21) might help\n* How do you currently take data from apis into Databricks or Synapse? Do you use python or something else?\n* Any pain points you think this integration could solve?\n\n**Community-powered**\n\nFinally, shoutout to Evan and his colleagues from [swishbi.com](http://swishbi.com/) for their hard work on the Databricks integration. Collaborations like these are what push the envelope forward in our field.\n\nDo you have some asks from dlt, or interesting stories or use cases you want to tell the world about? Tell us in the  [\\#sharing-and-contributing](https://dlthub.com/community) slack channel\n\nLooking forward to your insights and discussions!", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dlt (Data Load Tool) adds Databricks and Azure Synapse destinations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap0ygf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707747301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;dlt (data load tool) library added &lt;strong&gt;Databricks&lt;/strong&gt; and &lt;strong&gt;Azure&lt;/strong&gt; &lt;strong&gt;Synapse&lt;/strong&gt; destinations. Now you too can benefit from schema inference, evolution, management, column level lineage and data contracts.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to try?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Simply run &lt;strong&gt;pip install dlt&lt;/strong&gt; and you&amp;#39;re halfway there. For the detailed setup, check out the docs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/databricks\"&gt;Databricks&lt;/a&gt;. Do you already have the dlt \u201cdelta live tables\u201d Installed? see &lt;a href=\"https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21\"&gt;Here&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/synapse\"&gt;Azure synapse&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Want to try on duckdb? here&amp;#39;s a colab &lt;a href=\"https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM\"&gt;notebook&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m eager to hear your thoughts:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have you worked with dlt (data load tool) and Databricks before?&lt;/li&gt;\n&lt;li&gt;Did you try running data load tool on databricks but got delta live tables already installed? this &lt;a href=\"https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21\"&gt;guide&lt;/a&gt; might help&lt;/li&gt;\n&lt;li&gt;How do you currently take data from apis into Databricks or Synapse? Do you use python or something else?&lt;/li&gt;\n&lt;li&gt;Any pain points you think this integration could solve?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Community-powered&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Finally, shoutout to Evan and his colleagues from &lt;a href=\"http://swishbi.com/\"&gt;swishbi.com&lt;/a&gt; for their hard work on the Databricks integration. Collaborations like these are what push the envelope forward in our field.&lt;/p&gt;\n\n&lt;p&gt;Do you have some asks from dlt, or interesting stories or use cases you want to tell the world about? Tell us in the  &lt;a href=\"https://dlthub.com/community\"&gt;#sharing-and-contributing&lt;/a&gt; slack channel&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and discussions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ap0ygf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap0ygf/dlt_data_load_tool_adds_databricks_and_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap0ygf/dlt_data_load_tool_adds_databricks_and_azure/", "subreddit_subscribers": 160335, "created_utc": 1707747301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m managing a table for streaming data in databricks\nand I want to split it into two: \n1. hot table for the last 7 days\n2. \u2060cold table for historical data. \n\nCurrently, I'm moving data from the hot table to the cold table daily and then deleting it from the hot table. \nThis process is scheduled daily. \n\nIs there a more efficient approach than this?\n\nI can't use the merge option due to the large volume of data", "author_fullname": "t2_ptil4bof4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage Hot and Cold Tables for Streaming Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aoue6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707722542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m managing a table for streaming data in databricks\nand I want to split it into two: \n1. hot table for the last 7 days\n2. \u2060cold table for historical data. &lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m moving data from the hot table to the cold table daily and then deleting it from the hot table. \nThis process is scheduled daily. &lt;/p&gt;\n\n&lt;p&gt;Is there a more efficient approach than this?&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t use the merge option due to the large volume of data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aoue6j", "is_robot_indexable": true, "report_reasons": null, "author": "HousingStriking3770", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aoue6j/how_to_manage_hot_and_cold_tables_for_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aoue6j/how_to_manage_hot_and_cold_tables_for_streaming/", "subreddit_subscribers": 160335, "created_utc": 1707722542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a task to load data from Salesforce to Snowflake using ADF. But so far I can see many people are using Blob storage for staging. So can we use ADF to copy data from Salesforce to Snowflake directly without staging data in Blob storage? because using Blog storage requires to use SaS token which my IT department is trying to avoid. #ADF #Snowflake #Salesforce #Azure #Blob", "author_fullname": "t2_hi267s7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we use ADF to copy data from Salesforce to Snowflake directly without staging data in Blob storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1apktdn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707798612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a task to load data from Salesforce to Snowflake using ADF. But so far I can see many people are using Blob storage for staging. So can we use ADF to copy data from Salesforce to Snowflake directly without staging data in Blob storage? because using Blog storage requires to use SaS token which my IT department is trying to avoid. #ADF #Snowflake #Salesforce #Azure #Blob&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1apktdn", "is_robot_indexable": true, "report_reasons": null, "author": "JK_1975", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1apktdn/can_we_use_adf_to_copy_data_from_salesforce_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1apktdn/can_we_use_adf_to_copy_data_from_salesforce_to/", "subreddit_subscribers": 160335, "created_utc": 1707798612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am graduating with a CS degree soon and having a hard time finding a SWE engineering internship. It seems like DE internships are just as competitive. What are some ideas for roles suitable for a CS graduate that enable you to transition to DE? I have been applying to data analyst, business analyst/intelligence, IT internship ect. Not sure what the best path forward to start would be , since I\u2019m striking out with software/data engineering internships as of now. I am trying to find roles that are doable for me to get in this rough market but aren\u2019t dead end roles that won\u2019t be progressing my technical skills to reach one of my end goals (Data engineering, software engineering, cloud engineering). Don\u2019t really have the luxury of applying for a year after graduation, bills have to be paid. If anyone has role suggestions I haven\u2019t listed that would great. Thanks ", "author_fullname": "t2_126xnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Viable starting paths to DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aouc2b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1707722769.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707722313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am graduating with a CS degree soon and having a hard time finding a SWE engineering internship. It seems like DE internships are just as competitive. What are some ideas for roles suitable for a CS graduate that enable you to transition to DE? I have been applying to data analyst, business analyst/intelligence, IT internship ect. Not sure what the best path forward to start would be , since I\u2019m striking out with software/data engineering internships as of now. I am trying to find roles that are doable for me to get in this rough market but aren\u2019t dead end roles that won\u2019t be progressing my technical skills to reach one of my end goals (Data engineering, software engineering, cloud engineering). Don\u2019t really have the luxury of applying for a year after graduation, bills have to be paid. If anyone has role suggestions I haven\u2019t listed that would great. Thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aouc2b", "is_robot_indexable": true, "report_reasons": null, "author": "Kylerhanley", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aouc2b/viable_starting_paths_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aouc2b/viable_starting_paths_to_de/", "subreddit_subscribers": 160335, "created_utc": 1707722313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI work in the data team for a very large organization in Europe. Despite the size and influence of the organization, we are actually quite behind other companies in terms of data architecture, the tools we use and so on. I guess a typical issue for a large old organization.\n\nTo put it in context, I work for a team that does analytics for the website and mobile app. The website is split per \u201cscope\u201d and there are one or multiple analysts assigned to each \u201cscope\u201d and usually that is their sole focus and they are not really aware of what other analysts are doing, what queries they use, how exactly they measure their KPIs and so on. I think you get the idea. We work in a very decentralized way and would like to move to a more centralized setup with one \u201csource of truth\u201d.\n\nWe are already working on documenting KPIs and aligning how each team calculates them so we do it the same way across the department. Baby steps at the moment.\n\nBut that\u2019s not why I am here. I have been assigned to lead a team that owns the data and transforms it for final use by the analysts. Basically transforming all the raw data into final tables (some people call it the \u201cgold layer\u201d).\n\nCurrently, each analyst would basically dig into the raw data and find whatever they need and schedule a query that would feed new data everyday to a table and that table would be connected to their dashboard. \n\nNot only is it ridiculously expensive, but also very inefficient. We work with millions of sessions per day so you can imagine how much data we have.\n\nI have spent quite some time looking for resources online but there is so little focusing on modeling event data. I have very little data engineering/analytics engineering experience so any resources or tips would be highly appreciated. \n\nWe only use BigQuery and dbt at the moment.\nLooking into adding fivetran to the stack to simplify data streams from all the third party apps (currently we have many scripts for each 3rd party app and eqch script is owned by a different person so not centralized at all)\n\nThank you!", "author_fullname": "t2_59sa4mgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for data modeling and building a data architecture for event data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1apkw7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707798861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I work in the data team for a very large organization in Europe. Despite the size and influence of the organization, we are actually quite behind other companies in terms of data architecture, the tools we use and so on. I guess a typical issue for a large old organization.&lt;/p&gt;\n\n&lt;p&gt;To put it in context, I work for a team that does analytics for the website and mobile app. The website is split per \u201cscope\u201d and there are one or multiple analysts assigned to each \u201cscope\u201d and usually that is their sole focus and they are not really aware of what other analysts are doing, what queries they use, how exactly they measure their KPIs and so on. I think you get the idea. We work in a very decentralized way and would like to move to a more centralized setup with one \u201csource of truth\u201d.&lt;/p&gt;\n\n&lt;p&gt;We are already working on documenting KPIs and aligning how each team calculates them so we do it the same way across the department. Baby steps at the moment.&lt;/p&gt;\n\n&lt;p&gt;But that\u2019s not why I am here. I have been assigned to lead a team that owns the data and transforms it for final use by the analysts. Basically transforming all the raw data into final tables (some people call it the \u201cgold layer\u201d).&lt;/p&gt;\n\n&lt;p&gt;Currently, each analyst would basically dig into the raw data and find whatever they need and schedule a query that would feed new data everyday to a table and that table would be connected to their dashboard. &lt;/p&gt;\n\n&lt;p&gt;Not only is it ridiculously expensive, but also very inefficient. We work with millions of sessions per day so you can imagine how much data we have.&lt;/p&gt;\n\n&lt;p&gt;I have spent quite some time looking for resources online but there is so little focusing on modeling event data. I have very little data engineering/analytics engineering experience so any resources or tips would be highly appreciated. &lt;/p&gt;\n\n&lt;p&gt;We only use BigQuery and dbt at the moment.\nLooking into adding fivetran to the stack to simplify data streams from all the third party apps (currently we have many scripts for each 3rd party app and eqch script is owned by a different person so not centralized at all)&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1apkw7y", "is_robot_indexable": true, "report_reasons": null, "author": "Several_Percentage_5", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/", "subreddit_subscribers": 160335, "created_utc": 1707798861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS Learning Path\n\nHello everyone,\n\nI have been preparing for the AWS Cloud Practitioner certification for about 2 weeks and I have also created an account in AWS and I am using free tier. I am the totally new in AWS but I have been almost 4 years experience in a software industry in data related positions. Mostly data scientist but lately data engineer. So, I'm familiar with the concepts. I want to improve myself on AWS.\n\nAny resources, advice or a road map would be really great. \n\nAlso, I like to get my hands dirty, so I'm open to project ideas that I can do while learning AWS.", "author_fullname": "t2_nfbsqoqx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Learning Path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap7eqf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707763465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS Learning Path&lt;/p&gt;\n\n&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have been preparing for the AWS Cloud Practitioner certification for about 2 weeks and I have also created an account in AWS and I am using free tier. I am the totally new in AWS but I have been almost 4 years experience in a software industry in data related positions. Mostly data scientist but lately data engineer. So, I&amp;#39;m familiar with the concepts. I want to improve myself on AWS.&lt;/p&gt;\n\n&lt;p&gt;Any resources, advice or a road map would be really great. &lt;/p&gt;\n\n&lt;p&gt;Also, I like to get my hands dirty, so I&amp;#39;m open to project ideas that I can do while learning AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ap7eqf", "is_robot_indexable": true, "report_reasons": null, "author": "satyriconw", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap7eqf/aws_learning_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap7eqf/aws_learning_path/", "subreddit_subscribers": 160335, "created_utc": 1707763465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have an upcoming senior data engineer interview with Atlassian. Anybody has any recent interview experience with Atlassian? If yes, please share more details about the process and questions asked. Thanks in advance!", "author_fullname": "t2_o1ex5zsz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Atlassian Senior Data Engineer interview process and questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap6e48", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707761050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have an upcoming senior data engineer interview with Atlassian. Anybody has any recent interview experience with Atlassian? If yes, please share more details about the process and questions asked. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1ap6e48", "is_robot_indexable": true, "report_reasons": null, "author": "tank3190", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap6e48/atlassian_senior_data_engineer_interview_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ap6e48/atlassian_senior_data_engineer_interview_process/", "subreddit_subscribers": 160335, "created_utc": 1707761050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rethinking Serverless: The Price of Convenience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1ap58sg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WeMezRxlZh-HmLTjds5dkbTgXYS6PUKQ2rg5y8H0DIw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1707758293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/sync-computing/rethinking-serverless-the-price-of-convenience-9b9e29549d3b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?auto=webp&amp;s=7456914991ca35c39c6c4c487524552911535e37", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7e6d1f2866d127f23d477a6706ac01a53d8b5ba", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f0cb9db0f04dac76165ffa7932072dfa2a0cbea", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2a240378ea1a379b2157b9edb42779cf3d7fa41", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b8f7c6dfe2b750d56671b26d8e3bbcf6ac36243", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8336a4eee44066a7ddd6145902604c56732a0784", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=132d67b274d72a5aea3dc9dcd6d489f8fde715e6", "width": 1080, "height": 565}], "variants": {}, "id": "kQOOGqlUn0sGgVecv_vx2mlQcdE_rS48JopeFXI6yUg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ap58sg", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ap58sg/rethinking_serverless_the_price_of_convenience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/sync-computing/rethinking-serverless-the-price-of-convenience-9b9e29549d3b", "subreddit_subscribers": 160335, "created_utc": 1707758293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, i am using aws glue to load big query data into dynamic dataframe but it is taking time. Is there a way to directly load big query data into spark dataframe using glue?", "author_fullname": "t2_3p3vfvzt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading big query data to spark dataframe.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aoy0ku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707737768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i am using aws glue to load big query data into dynamic dataframe but it is taking time. Is there a way to directly load big query data into spark dataframe using glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aoy0ku", "is_robot_indexable": true, "report_reasons": null, "author": "Dr_Fida", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aoy0ku/loading_big_query_data_to_spark_dataframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aoy0ku/loading_big_query_data_to_spark_dataframe/", "subreddit_subscribers": 160335, "created_utc": 1707737768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I guess if you use Kubernetes then you have this out the box, never used Kubernetes myself though.\n\nI\u2019ve grown fond of the idea of having a network wide dns resolver at the service level. I\u2019d like to have my reverse proxy mention services by name, not IP:port. Any service that needs to use another service could similarly just use its name, To take things further, the name can automatically go to a load balancer for that service- assuming multiple instances are running.\n\nI\u2019ve been reading that Consul does this. Does anyone find that these features are often a big help in development? Or are they often a big hassle to keep up?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use a service mesh, manage IPs yourself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1apl1l7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1707799331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I guess if you use Kubernetes then you have this out the box, never used Kubernetes myself though.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve grown fond of the idea of having a network wide dns resolver at the service level. I\u2019d like to have my reverse proxy mention services by name, not IP:port. Any service that needs to use another service could similarly just use its name, To take things further, the name can automatically go to a load balancer for that service- assuming multiple instances are running.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been reading that Consul does this. Does anyone find that these features are often a big help in development? Or are they often a big hassle to keep up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1apl1l7", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1apl1l7/do_you_use_a_service_mesh_manage_ips_yourself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1apl1l7/do_you_use_a_service_mesh_manage_ips_yourself/", "subreddit_subscribers": 160335, "created_utc": 1707799331.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}