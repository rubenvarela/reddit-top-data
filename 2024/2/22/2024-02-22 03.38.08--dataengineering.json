{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\n**TLDR:** Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference &amp; modelling) package in Rust.\n\nWith DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.\n\nWe wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.\n\nLooking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated \ud83d\ude03)\n\n***EDIT: adding clarification***\n\n**Quary will be easier to build on top of**\n\nBecause of our Rust core, we can expose JS, Python, and other bindings, making it easier to build additional tooling on top of Quary. For example, we've built our \"cloud\" offering on a WASM compilation.\n\n**Column-Level Lineage**\n\nThe Quary core contains column-level lineage directly and this enables us to offer unique capabilities. For instance:\n\n**Automated Inference and Documentation**\n\nOur system can intelligently infer tests and generate documentation, significantly reducing your manual workload.\n\n**Testing Efficiency**: By avoiding inferrable tests, we can skip tests that Quary knows to be true. In our template for example, we can skip around 1/3 of tests.\n\n**Quary is better for handling sensitive data like PII.**\n\nBecause we can compile to WASM, our \"cloud offering\" interacts with your data warehouse from the client. Data doesn't flow through any of our servers, protecting your information.\n\n**Speed-up for developer experience**\n\nWhile the impact on database performance is minimal, one of the true benefits of Quary's fast core is developer experience. The fast core allows us to build experiences where we can provide faster feedback.\n\nhttps://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;format=pjpg&amp;auto=webp&amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source DBT core alternative written in Rust (30x faster)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3kisasoefwjc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b268eb8ba9f4e906d5aff5f16f58945a88b06e0b"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af417f90677c04df9ae254e038c926930b7de974"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a7f7b68171bfd3c6c10a566df5c1775268f6d0f"}, {"y": 392, "x": 640, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=adc7e1478f6ee2a0e55cfebc81e89dd6e2801f3c"}, {"y": 588, "x": 960, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd118e81cb6c6ce854522191cb0693b7babc4a26"}, {"y": 662, "x": 1080, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38628b6d703ed6b02580f66bb9b5819575da209b"}], "s": {"y": 2216, "x": 3612, "u": "https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;format=pjpg&amp;auto=webp&amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5"}, "id": "3kisasoefwjc1"}}, "name": "t3_1aw7368", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 142, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 142, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VZi9tcyvi9b-tgRfYixpPxqpV_kseg8ktYRf428RVvo.jpg", "edited": 1708542681.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708503956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference &amp;amp; modelling) package in Rust.&lt;/p&gt;\n\n&lt;p&gt;With DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.&lt;/p&gt;\n\n&lt;p&gt;We wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated \ud83d\ude03)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT: adding clarification&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quary will be easier to build on top of&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Because of our Rust core, we can expose JS, Python, and other bindings, making it easier to build additional tooling on top of Quary. For example, we&amp;#39;ve built our &amp;quot;cloud&amp;quot; offering on a WASM compilation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Column-Level Lineage&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Quary core contains column-level lineage directly and this enables us to offer unique capabilities. For instance:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Automated Inference and Documentation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our system can intelligently infer tests and generate documentation, significantly reducing your manual workload.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Testing Efficiency&lt;/strong&gt;: By avoiding inferrable tests, we can skip tests that Quary knows to be true. In our template for example, we can skip around 1/3 of tests.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quary is better for handling sensitive data like PII.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Because we can compile to WASM, our &amp;quot;cloud offering&amp;quot; interacts with your data warehouse from the client. Data doesn&amp;#39;t flow through any of our servers, protecting your information.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Speed-up for developer experience&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While the impact on database performance is minimal, one of the true benefits of Quary&amp;#39;s fast core is developer experience. The fast core allows us to build experiences where we can provide faster feedback.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5\"&gt;https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38666d5d348282797edbc19aa69d0df040fc3ed5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1aw7368", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw7368/open_source_dbt_core_alternative_written_in_rust/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw7368/open_source_dbt_core_alternative_written_in_rust/", "subreddit_subscribers": 162684, "created_utc": 1708503956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered \"only a few of those applicant are good\". I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? ", "author_fullname": "t2_13mcsa6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What divides good from average?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw4uaq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708495611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a post earlier about Data Engineering roles having way too much applicant and most of the people answered &amp;quot;only a few of those applicant are good&amp;quot;. I am a Java/Scala developer at the moment and I want to transition to full time data engineer roles and I want to know how can i seperate myself from the group and be regarded as one of the good ones. What should be in my portfolio etc. ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aw4uaq", "is_robot_indexable": true, "report_reasons": null, "author": "Chediras", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw4uaq/what_divides_good_from_average/", "subreddit_subscribers": 162684, "created_utc": 1708495611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Asking as a junior DE. But answers from all levels of experience are very welcome!", "author_fullname": "t2_ecnqt6o1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those who have worked in both, which do you prefer: start ups or big corporations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awe533", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708527707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Asking as a junior DE. But answers from all levels of experience are very welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1awe533", "is_robot_indexable": true, "report_reasons": null, "author": "silentwardrbe", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awe533/for_those_who_have_worked_in_both_which_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awe533/for_those_who_have_worked_in_both_which_do_you/", "subreddit_subscribers": 162684, "created_utc": 1708527707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here goes our latest blog on \"Moving a Billion Postgres Rows on a $100 Budget\". The blog challenges the status quo of current ETL tools. Can optimization beat out costly solutions? We believe it can. [https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget](https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget)", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving a Billion Postgres Rows on a $100 Budget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awl4l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708544193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here goes our latest blog on &amp;quot;Moving a Billion Postgres Rows on a $100 Budget&amp;quot;. The blog challenges the status quo of current ETL tools. Can optimization beat out costly solutions? We believe it can. &lt;a href=\"https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget\"&gt;https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?auto=webp&amp;s=901a3fef2ad2ec5fc91e4fdcc1aa2b9c9d181839", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cdd3a09193ae2c9d5957ef7003f8d42d18b99d5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6b7351a3508eb11fc40ae59e66e342c086a5cd4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=354734ab50abfcb021ca091c59f969cc638336bf", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=00b9cfaf69985efbaf65237c7898600b79dc7fa8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b3449d5d08f8079aa565147b7467a7461b6743a", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23b36fe32654d8485cffaab1b40007b3260e8228", "width": 1080, "height": 567}], "variants": {}, "id": "KlZ1TNrcyE7zTR-BQ60BVMHR_YAiJHCIdwnFQOSzFlg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awl4l4", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awl4l4/moving_a_billion_postgres_rows_on_a_100_budget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awl4l4/moving_a_billion_postgres_rows_on_a_100_budget/", "subreddit_subscribers": 162684, "created_utc": 1708544193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this has been said before, but I'm starting to understand why people in this thread say that data engineering is not really an \"entry level\" job. \n\nJust like certain other roles like infrastructure engineering, database admin, devops engineer(perhaps?), etc. \n\nThere aren't many \"junior infrastructure engineers\", most of the time you start working another job, and then you just so happen to end up managing some infrastructure. \n\nI think the reason for this is because companies for the most part see DE, and similar as a cost center and run skeleton crews. \n\nDuring the job search, I've only met with \"directors/vps\" that are usually more focused on analytics or BI, and only know a little about DE. The department usually only had 1 or 2 DEs tops. \n\nUnless it's a bigger company with a significant amount of data to manage, most companies only hire experienced DEs because you can't really bring in an inexperienced DE for business critical data, along with other things like data security. And right now, big companies aren't hiring much. \n\nI'm wondering if there's a way for us to make this information more visible so that people interested in DE could have an easier time setting their expectations?", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There should be more visibility as to which tech jobs aren't usually \"entry level'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awjm6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708540639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this has been said before, but I&amp;#39;m starting to understand why people in this thread say that data engineering is not really an &amp;quot;entry level&amp;quot; job. &lt;/p&gt;\n\n&lt;p&gt;Just like certain other roles like infrastructure engineering, database admin, devops engineer(perhaps?), etc. &lt;/p&gt;\n\n&lt;p&gt;There aren&amp;#39;t many &amp;quot;junior infrastructure engineers&amp;quot;, most of the time you start working another job, and then you just so happen to end up managing some infrastructure. &lt;/p&gt;\n\n&lt;p&gt;I think the reason for this is because companies for the most part see DE, and similar as a cost center and run skeleton crews. &lt;/p&gt;\n\n&lt;p&gt;During the job search, I&amp;#39;ve only met with &amp;quot;directors/vps&amp;quot; that are usually more focused on analytics or BI, and only know a little about DE. The department usually only had 1 or 2 DEs tops. &lt;/p&gt;\n\n&lt;p&gt;Unless it&amp;#39;s a bigger company with a significant amount of data to manage, most companies only hire experienced DEs because you can&amp;#39;t really bring in an inexperienced DE for business critical data, along with other things like data security. And right now, big companies aren&amp;#39;t hiring much. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if there&amp;#39;s a way for us to make this information more visible so that people interested in DE could have an easier time setting their expectations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awjm6r", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1awjm6r/there_should_be_more_visibility_as_to_which_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awjm6r/there_should_be_more_visibility_as_to_which_tech/", "subreddit_subscribers": 162684, "created_utc": 1708540639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The data I\u2019m working with is best suited to an OLAP database due to its focus as a historic dataset that researchers query, but we\u2019re looking at ways to allow external researchers to get data that we expose by connecting to an API. I haven\u2019t quite been able to find the best practices for where API layers connect - directly to the data warehouse, a separate OLTP database, a static file export of the most recent data? Interested in what others do for allowing external consumers to get at the data without spending too much on snowflake credits or server costs. ", "author_fullname": "t2_2ivpdou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use for your API layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awfxbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708531967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The data I\u2019m working with is best suited to an OLAP database due to its focus as a historic dataset that researchers query, but we\u2019re looking at ways to allow external researchers to get data that we expose by connecting to an API. I haven\u2019t quite been able to find the best practices for where API layers connect - directly to the data warehouse, a separate OLTP database, a static file export of the most recent data? Interested in what others do for allowing external consumers to get at the data without spending too much on snowflake credits or server costs. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awfxbw", "is_robot_indexable": true, "report_reasons": null, "author": "Butterhero_", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awfxbw/what_do_you_use_for_your_api_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awfxbw/what_do_you_use_for_your_api_layer/", "subreddit_subscribers": 162684, "created_utc": 1708531967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So yeah, if you do use tests, model &amp; column descriptions, how do you handle duplicates? Like if my \"customer id\" is referenced in 3 models, do you write the same definition, or even a slightly different one in each of the referenced models? ", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Users -&gt; Do you write column descriptions for EVERYTHING? I started to do it, and realized how many duplicates there would be as we follow the source -&gt; staging -&gt; int -&gt; mart which duplicates columns in each layer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awfnk5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708531335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So yeah, if you do use tests, model &amp;amp; column descriptions, how do you handle duplicates? Like if my &amp;quot;customer id&amp;quot; is referenced in 3 models, do you write the same definition, or even a slightly different one in each of the referenced models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awfnk5", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awfnk5/dbt_users_do_you_write_column_descriptions_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awfnk5/dbt_users_do_you_write_column_descriptions_for/", "subreddit_subscribers": 162684, "created_utc": 1708531335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Brews to Bytes: Demystifying Snowflake's Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1awg4q1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QCl0HzPKsmrXVJ1Ij8kun5-8uWrEygoGjNnIWyXh-B4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708532454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/datagibberish/p/snowlfake-beer-warehouse?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?auto=webp&amp;s=d0fa66413f72f534ea89b673a0f1b75232582d50", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81b88bd5724ae2659a6f3e152ef596cb771360a8", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ecd42c8c87bfadae02fe61556d19161b332e615", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc52b17b7b0bbac94688deaf79795ea77f6842c", "width": 320, "height": 320}], "variants": {}, "id": "XCr_QSWw4e-JVVoEL_6R08oYeVhxC2PpPDkMUQRCPiI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awg4q1", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1awg4q1/from_brews_to_bytes_demystifying_snowflakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/datagibberish/p/snowlfake-beer-warehouse?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "subreddit_subscribers": 162684, "created_utc": 1708532454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\nI am looking for practical walkthrough on how, step-by-step, one does normalization and implements a star schema data warehouse using Python or SQL. For such cases, people mostly throwing books and theory at you, Ive read those but looking for real implementation. In the past I was on fairly small project, where I would get everything in one table (which later stays as fact), create synthetic keys there, and then strip down distinct values along with keys to dim table with qualitative attributes as a view but I think thats not how every other, bigger project goes so I wonder. Is there any video/tutorial like that, or my approach seem correct for most of projects", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dimensional modelling implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw9q6b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708515205.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708514508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello\nI am looking for practical walkthrough on how, step-by-step, one does normalization and implements a star schema data warehouse using Python or SQL. For such cases, people mostly throwing books and theory at you, Ive read those but looking for real implementation. In the past I was on fairly small project, where I would get everything in one table (which later stays as fact), create synthetic keys there, and then strip down distinct values along with keys to dim table with qualitative attributes as a view but I think thats not how every other, bigger project goes so I wonder. Is there any video/tutorial like that, or my approach seem correct for most of projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw9q6b", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw9q6b/dimensional_modelling_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw9q6b/dimensional_modelling_implementation/", "subreddit_subscribers": 162684, "created_utc": 1708514508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.\n\nI am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hard real time time series database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awp36v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708553629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.&lt;/p&gt;\n\n&lt;p&gt;I am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awp36v", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "subreddit_subscribers": 162684, "created_utc": 1708553629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to develop an in house data lineage application. I am able to get the data about relationships between tables but struggling with visualization of data. So far, I have tried using pyvis and d3 hierarchy trees. The problem I\u2019m facing in pyvis is flexibility to add on click node functions and problem with trees is that I cannot deal with multiple parent nodes(Since it is for hierarchical data). Has anybody worked on a similar project? If yes, then please help \n", "author_fullname": "t2_eb80kwgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lineage Solution: Snowflake, Django, D3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awa1zz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708515711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to develop an in house data lineage application. I am able to get the data about relationships between tables but struggling with visualization of data. So far, I have tried using pyvis and d3 hierarchy trees. The problem I\u2019m facing in pyvis is flexibility to add on click node functions and problem with trees is that I cannot deal with multiple parent nodes(Since it is for hierarchical data). Has anybody worked on a similar project? If yes, then please help &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awa1zz", "is_robot_indexable": true, "report_reasons": null, "author": "No-Caregiver-1204", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awa1zz/data_lineage_solution_snowflake_django_d3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awa1zz/data_lineage_solution_snowflake_django_d3/", "subreddit_subscribers": 162684, "created_utc": 1708515711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a bit stuck and need some input from my fellow reddit engineers. :D\n\nWe are redesigning our (central) data platform and I'm trying to identify database candidates that we could use to serve req/rep-style workloads related to personalization and \"data apps\". I looked at the obvious candidates - Firebolt, Druid, MongoDB, Postgres - but neither convinced me as the \"obviously right choice\". Are there others or am I missing something?\n\nThe characteristics that I need are:\n\n* **Throughput-optimized ingest**: We currently produce around 12TB/day of clickstream data and a couple GB/day of transaction and marketing data. I can filter this down in our stream layer (Kafka+Flink) but I still expect &gt;10M updates/inserts a day if not more.\n* **low-latency point queries**: Queries will ask for data on a specific userID in order to personalize user experience, trigger custom offers, etc. This is user-facing so it should have low latency to not affect UX negatively. Currently there is no latency SLA, but lower is certainly better.\n* **rollup/aggregation**: We expect queries to ask for aggregates, e.g., number of orders for this userID over the last 30 days, number of product/page views in the current session, etc.\n\nFor context: *Our current setup is segment based. We have a batch pipeline that spins at regular intervals, computes statistics for each segment using a warehouse (redshift) and then sends the updates to an OLTP database (postgres) for serving. We would like to change the grain from segment-level to user-level and are now checking what implications this would have for our platform/infra.*\n\n&amp;#x200B;", "author_fullname": "t2_frdb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for personalization and \"data apps\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw762n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708504284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit stuck and need some input from my fellow reddit engineers. :D&lt;/p&gt;\n\n&lt;p&gt;We are redesigning our (central) data platform and I&amp;#39;m trying to identify database candidates that we could use to serve req/rep-style workloads related to personalization and &amp;quot;data apps&amp;quot;. I looked at the obvious candidates - Firebolt, Druid, MongoDB, Postgres - but neither convinced me as the &amp;quot;obviously right choice&amp;quot;. Are there others or am I missing something?&lt;/p&gt;\n\n&lt;p&gt;The characteristics that I need are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Throughput-optimized ingest&lt;/strong&gt;: We currently produce around 12TB/day of clickstream data and a couple GB/day of transaction and marketing data. I can filter this down in our stream layer (Kafka+Flink) but I still expect &amp;gt;10M updates/inserts a day if not more.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;low-latency point queries&lt;/strong&gt;: Queries will ask for data on a specific userID in order to personalize user experience, trigger custom offers, etc. This is user-facing so it should have low latency to not affect UX negatively. Currently there is no latency SLA, but lower is certainly better.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;rollup/aggregation&lt;/strong&gt;: We expect queries to ask for aggregates, e.g., number of orders for this userID over the last 30 days, number of product/page views in the current session, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For context: &lt;em&gt;Our current setup is segment based. We have a batch pipeline that spins at regular intervals, computes statistics for each segment using a warehouse (redshift) and then sends the updates to an OLTP database (postgres) for serving. We would like to change the grain from segment-level to user-level and are now checking what implications this would have for our platform/infra.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aw762n", "is_robot_indexable": true, "report_reasons": null, "author": "FirefoxMetzger", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw762n/choosing_a_database_for_personalization_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw762n/choosing_a_database_for_personalization_and_data/", "subreddit_subscribers": 162684, "created_utc": 1708504284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nFirst post on here - looking for some help. I'm developing a data analytics platform at my company right now and I've run into a question, \"should I be streaming data?\"\n\nHere is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions. \n\nThe proof-of-concept I developed runs the [telegraf](https://www.influxdata.com/time-series-platform/telegraf/) agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second. \n\nMy question is this, now that I am moving into developing something for production, I'm wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be \"real-time\" and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else? \n\nI have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated. \n\nThank you all in advance!", "author_fullname": "t2_4ghysex8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Streaming...what the heck should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awq1ao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708555849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;First post on here - looking for some help. I&amp;#39;m developing a data analytics platform at my company right now and I&amp;#39;ve run into a question, &amp;quot;should I be streaming data?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Here is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions. &lt;/p&gt;\n\n&lt;p&gt;The proof-of-concept I developed runs the &lt;a href=\"https://www.influxdata.com/time-series-platform/telegraf/\"&gt;telegraf&lt;/a&gt; agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second. &lt;/p&gt;\n\n&lt;p&gt;My question is this, now that I am moving into developing something for production, I&amp;#39;m wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be &amp;quot;real-time&amp;quot; and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else? &lt;/p&gt;\n\n&lt;p&gt;I have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awq1ao", "is_robot_indexable": true, "report_reasons": null, "author": "full_nelson0510", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "subreddit_subscribers": 162684, "created_utc": 1708555849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nWhat are the benefits of a materialized view over writing truncate a table with Airflow (for example) with a schedule ? \n", "author_fullname": "t2_94h4vsep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Materialized view vs regular table with Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awjd9c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708540073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;What are the benefits of a materialized view over writing truncate a table with Airflow (for example) with a schedule ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awjd9c", "is_robot_indexable": true, "report_reasons": null, "author": "GressiDawn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awjd9c/materialized_view_vs_regular_table_with_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awjd9c/materialized_view_vs_regular_table_with_airflow/", "subreddit_subscribers": 162684, "created_utc": 1708540073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI'm a software developer diving in a new project in work for analytics in Azure and I need suggestions.  \nI have experience with airflow,sql, python and so on.  \nWe are going to create analytics solution for our customers.\n\nwe have raw data layer as parquet in azure blob storage. we use synapse to extract the data.  \n\n\nWe need to transform the data for other layers (silver,gold) and I find azure stack disappointing.  \n\n\n* Synapse and ADF are mostly no code solutions and I'm looking for something or combination that I can create scheduling and transform in code.  \n\n* for scheduling - synapse scheduling is lacking and airflow managed is very limited. ADF also seems no code.\n* databricks seems very pricey and more than we need.\n\n&amp;#x200B;\n\nfor the transformation - I know we need pyspark as we have a lot of data. this is also a code solution that satisfy me.\n\n&amp;#x200B;\n\nI see a PR with json file instead of code and I know it will slow development and won't be maintainable.  \n\n\nanyone has recommendation for stack that involve code ? coming from more software oriented I believe in more coding solutions as I worked with Apache NiFi and it was not maintainable.  \n\n\nThanks  \n", "author_fullname": "t2_atpck6ua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure stack for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw80uf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708507794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer diving in a new project in work for analytics in Azure and I need suggestions.&lt;br/&gt;\nI have experience with airflow,sql, python and so on.&lt;br/&gt;\nWe are going to create analytics solution for our customers.&lt;/p&gt;\n\n&lt;p&gt;we have raw data layer as parquet in azure blob storage. we use synapse to extract the data.  &lt;/p&gt;\n\n&lt;p&gt;We need to transform the data for other layers (silver,gold) and I find azure stack disappointing.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Synapse and ADF are mostly no code solutions and I&amp;#39;m looking for something or combination that I can create scheduling and transform in code.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;for scheduling - synapse scheduling is lacking and airflow managed is very limited. ADF also seems no code.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;databricks seems very pricey and more than we need.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;for the transformation - I know we need pyspark as we have a lot of data. this is also a code solution that satisfy me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I see a PR with json file instead of code and I know it will slow development and won&amp;#39;t be maintainable.  &lt;/p&gt;\n\n&lt;p&gt;anyone has recommendation for stack that involve code ? coming from more software oriented I believe in more coding solutions as I worked with Apache NiFi and it was not maintainable.  &lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw80uf", "is_robot_indexable": true, "report_reasons": null, "author": "pythondeveloper77", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw80uf/azure_stack_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw80uf/azure_stack_for_de/", "subreddit_subscribers": 162684, "created_utc": 1708507794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I'm trying to weigh the benefits of various tools to figure out what the best one would be for our use case.\n\nOur team is regularly tasked with setting up jobs that move data from 50-60 separate PostgreSQL databases to a single destination PostgreSQL database. Problem is, no one on the team is a trained data engineer. The current way we move data from Source A to Destination B is by writing Airflow jobs but the process to create regularly recurring transfers tends to be prohibitively time intensive, and it seems like there's a better way than writing new airflow scripts every time we need to set up a new job.\n\nI'm looking to push for the adoption of a tool that is specifically meant for this purpose (data modeling and recurring transfers). So far I've seen people recommend DBT and Fivetran, but I don't want to overengineer it, considering our use case is fairly limited in scope. What would make the most sense for us to pursue?", "author_fullname": "t2_104vvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What ETL/ELT tool to use for this use-case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awegrq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708528490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I&amp;#39;m trying to weigh the benefits of various tools to figure out what the best one would be for our use case.&lt;/p&gt;\n\n&lt;p&gt;Our team is regularly tasked with setting up jobs that move data from 50-60 separate PostgreSQL databases to a single destination PostgreSQL database. Problem is, no one on the team is a trained data engineer. The current way we move data from Source A to Destination B is by writing Airflow jobs but the process to create regularly recurring transfers tends to be prohibitively time intensive, and it seems like there&amp;#39;s a better way than writing new airflow scripts every time we need to set up a new job.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to push for the adoption of a tool that is specifically meant for this purpose (data modeling and recurring transfers). So far I&amp;#39;ve seen people recommend DBT and Fivetran, but I don&amp;#39;t want to overengineer it, considering our use case is fairly limited in scope. What would make the most sense for us to pursue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awegrq", "is_robot_indexable": true, "report_reasons": null, "author": "LonghornMorgs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awegrq/what_etlelt_tool_to_use_for_this_usecase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awegrq/what_etlelt_tool_to_use_for_this_usecase/", "subreddit_subscribers": 162684, "created_utc": 1708528490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We use bitbucket for our CI/CD deployments.   \nHave tried airbyte via UI and would like to check if anyone has tried CI/CD deployment? Is it possible wit bitbucket CI/CD pipeline?", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte CI/CD deployments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awebe6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708528131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We use bitbucket for our CI/CD deployments.&lt;br/&gt;\nHave tried airbyte via UI and would like to check if anyone has tried CI/CD deployment? Is it possible wit bitbucket CI/CD pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awebe6", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awebe6/airbyte_cicd_deployments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awebe6/airbyte_cicd_deployments/", "subreddit_subscribers": 162684, "created_utc": 1708528131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nI plan to take one of the above AWS certifications but I am confused about which one to take. I have worked as an ETL Developer for almost 3 years and now want to switch to DE.   \nPlease help me choose the right AWS certification from above, which will benefit me in the long term.", "author_fullname": "t2_skmlxs64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is more appropriate for DE? AWS Data Engineer Associate or AWS Solutions Architect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awe5pd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708527747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I plan to take one of the above AWS certifications but I am confused about which one to take. I have worked as an ETL Developer for almost 3 years and now want to switch to DE.&lt;br/&gt;\nPlease help me choose the right AWS certification from above, which will benefit me in the long term.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awe5pd", "is_robot_indexable": true, "report_reasons": null, "author": "Over_Student_2522", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awe5pd/which_is_more_appropriate_for_de_aws_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awe5pd/which_is_more_appropriate_for_de_aws_data/", "subreddit_subscribers": 162684, "created_utc": 1708527747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have loaded 5 million documents having 2 text fields to an open search index.\n\nI want to use open search sql to filter about 1 million distinct values of a field.\n\nIs open search a good tool for such bulk searches or should i consider a cache instead?", "author_fullname": "t2_kfvc08j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opensearch question ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awaq1v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708518041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have loaded 5 million documents having 2 text fields to an open search index.&lt;/p&gt;\n\n&lt;p&gt;I want to use open search sql to filter about 1 million distinct values of a field.&lt;/p&gt;\n\n&lt;p&gt;Is open search a good tool for such bulk searches or should i consider a cache instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awaq1v", "is_robot_indexable": true, "report_reasons": null, "author": "RepulsiveCry8412", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awaq1v/opensearch_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awaq1v/opensearch_question/", "subreddit_subscribers": 162684, "created_utc": 1708518041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For data lake including rbacs for project folder, synapse, data factory, power bi. In past i've used yaml but this time they want in biceps. Does biceps will support or any limitation/complexity is there? What about synapse analytics iac deployment in bicep with roles, user creation etc and can we use sql script for schemas, objects creation? Advise synapse data artifacts deployment best approach/", "author_fullname": "t2_gw1qtave", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data analytics iac deployment through Azure bicep ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awa35p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708515841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For data lake including rbacs for project folder, synapse, data factory, power bi. In past i&amp;#39;ve used yaml but this time they want in biceps. Does biceps will support or any limitation/complexity is there? What about synapse analytics iac deployment in bicep with roles, user creation etc and can we use sql script for schemas, objects creation? Advise synapse data artifacts deployment best approach/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awa35p", "is_robot_indexable": true, "report_reasons": null, "author": "efor007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awa35p/data_analytics_iac_deployment_through_azure_bicep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awa35p/data_analytics_iac_deployment_through_azure_bicep/", "subreddit_subscribers": 162684, "created_utc": 1708515841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building up tables and shema in my data warehouse (let's say Redshift). And this build-up process is in very early stage right now.\n\nand no matter how long I give a careful thought on how to set the schema right, at some point I have to change schema (add or subtract columns) .\n\nIt's like this:\n\n\\--------------------------------------------------------------------------------------\n\n|date              | base\\_trade\\_volume | quote\\_trade\\_volume |\n\n| 2023-01-02 | 200,000                       | 400,000,000               |\n\n| 2023-01-03 | 300,000                       | 450,000,000               |\n\n|    ....               |            ....                     |        ...                           |\n\n\\--------------------------------------------------------------------------------------\n\nThis is a table called 'user\\_trade\\_statistics'. and is the very first version of it.\n\n&amp;#x200B;\n\nand all of sudden, there's a need for 'trade\\_count' column. so I have to reprocess whole data to add new column to existing table.\n\n\\-------------------------------------------------------------------------------------------------------------\n\n|date              | base\\_trade\\_volume | quote\\_trade\\_volume |. trade\\_count\n\n| 2023-01-02 | 200,000                       | 400,000,000               |.  120\n\n| 2023-01-03 | 300,000                       | 450,000,000               |.  130\n\n|    ....               |            ....                     |        ...                           |.  .......\n\n\\-------------------------------------------------------------------------------------------------------------\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe problem here is I have to use lots of resources every time to reprocess for the changing schema. and this is going to happen quite frequently until the warehouse gets mature.\n\nI just want to know how you guys these kind of problems.\n\n&amp;#x200B;\n\np.s. This problem has nothing to do with 'slow changing dimension(SCD)'\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_h4q1lnfou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deal with frequently changing schema in early stage of building data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aw9rlc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708515750.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708514657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building up tables and shema in my data warehouse (let&amp;#39;s say Redshift). And this build-up process is in very early stage right now.&lt;/p&gt;\n\n&lt;p&gt;and no matter how long I give a careful thought on how to set the schema right, at some point I have to change schema (add or subtract columns) .&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;--------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;|date              | base_trade_volume | quote_trade_volume |&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-02 | 200,000                       | 400,000,000               |&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-03 | 300,000                       | 450,000,000               |&lt;/p&gt;\n\n&lt;p&gt;|    ....               |            ....                     |        ...                           |&lt;/p&gt;\n\n&lt;p&gt;--------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;This is a table called &amp;#39;user_trade_statistics&amp;#39;. and is the very first version of it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and all of sudden, there&amp;#39;s a need for &amp;#39;trade_count&amp;#39; column. so I have to reprocess whole data to add new column to existing table.&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;|date              | base_trade_volume | quote_trade_volume |. trade_count&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-02 | 200,000                       | 400,000,000               |.  120&lt;/p&gt;\n\n&lt;p&gt;| 2023-01-03 | 300,000                       | 450,000,000               |.  130&lt;/p&gt;\n\n&lt;p&gt;|    ....               |            ....                     |        ...                           |.  .......&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The problem here is I have to use lots of resources every time to reprocess for the changing schema. and this is going to happen quite frequently until the warehouse gets mature.&lt;/p&gt;\n\n&lt;p&gt;I just want to know how you guys these kind of problems.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;p.s. This problem has nothing to do with &amp;#39;slow changing dimension(SCD)&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aw9rlc", "is_robot_indexable": true, "report_reasons": null, "author": "DonkeyThin8833", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aw9rlc/how_do_you_deal_with_frequently_changing_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aw9rlc/how_do_you_deal_with_frequently_changing_schema/", "subreddit_subscribers": 162684, "created_utc": 1708514657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The dynamic world of data wrangling, shedding light on its techniques, distinguishing it from ETL processes and uncovering the trends shaping its trajectory in 2024. \n\n[https://www.dasca.org/world-of-big-data/article/the-art-of-data-wrangling-in-2024-techniques-and-trends](https://www.dasca.org/world-of-big-data/article/the-art-of-data-wrangling-in-2024-techniques-and-trends)", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Art of Data Wrangling in 2024: Techniques and Trends", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awbz56", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708521933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The dynamic world of data wrangling, shedding light on its techniques, distinguishing it from ETL processes and uncovering the trends shaping its trajectory in 2024. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dasca.org/world-of-big-data/article/the-art-of-data-wrangling-in-2024-techniques-and-trends\"&gt;https://www.dasca.org/world-of-big-data/article/the-art-of-data-wrangling-in-2024-techniques-and-trends&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZnyYGYrWCEtbmSXB8oDNvWRZW-djUwe98lI1WtlW4Fk.jpg?auto=webp&amp;s=c3804eeb91601ba8602083cbc04c036e2eda2f1d", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/ZnyYGYrWCEtbmSXB8oDNvWRZW-djUwe98lI1WtlW4Fk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=716d82775e810cc312111695e076e3c1cdfb05f8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ZnyYGYrWCEtbmSXB8oDNvWRZW-djUwe98lI1WtlW4Fk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a19beec9cb6ccee91dd0a483b0e6e5941215c84d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ZnyYGYrWCEtbmSXB8oDNvWRZW-djUwe98lI1WtlW4Fk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9acc0e8c19c5ef812142cde49ff5ceae94a4fcb1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ZnyYGYrWCEtbmSXB8oDNvWRZW-djUwe98lI1WtlW4Fk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6c70ae7817412367fc6993e9bdb15ec6175b505", "width": 640, "height": 336}], "variants": {}, "id": "GnNSv05l5aF3AMGceHxYmU5bdiOnSDtx6JDC87xpbFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awbz56", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awbz56/the_art_of_data_wrangling_in_2024_techniques_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awbz56/the_art_of_data_wrangling_in_2024_techniques_and/", "subreddit_subscribers": 162684, "created_utc": 1708521933.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}