{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here goes our latest blog on \"Moving a Billion Postgres Rows on a $100 Budget\". The blog challenges the status quo of current ETL tools. Can optimization beat out costly solutions? We believe it can. [https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget](https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget)", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving a Billion Postgres Rows on a $100 Budget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awl4l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708544193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here goes our latest blog on &amp;quot;Moving a Billion Postgres Rows on a $100 Budget&amp;quot;. The blog challenges the status quo of current ETL tools. Can optimization beat out costly solutions? We believe it can. &lt;a href=\"https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget\"&gt;https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?auto=webp&amp;s=901a3fef2ad2ec5fc91e4fdcc1aa2b9c9d181839", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cdd3a09193ae2c9d5957ef7003f8d42d18b99d5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6b7351a3508eb11fc40ae59e66e342c086a5cd4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=354734ab50abfcb021ca091c59f969cc638336bf", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=00b9cfaf69985efbaf65237c7898600b79dc7fa8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b3449d5d08f8079aa565147b7467a7461b6743a", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/HXOS8u4e_a9hnLZOUHZXFlGELdUHt_Aq1ecr82qaYTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23b36fe32654d8485cffaab1b40007b3260e8228", "width": 1080, "height": 567}], "variants": {}, "id": "KlZ1TNrcyE7zTR-BQ60BVMHR_YAiJHCIdwnFQOSzFlg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awl4l4", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awl4l4/moving_a_billion_postgres_rows_on_a_100_budget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awl4l4/moving_a_billion_postgres_rows_on_a_100_budget/", "subreddit_subscribers": 162789, "created_utc": 1708544193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this has been said before, but I'm starting to understand why people in this thread say that data engineering is not really an \"entry level\" job. \n\nJust like certain other roles like infrastructure engineering, database admin, devops engineer(perhaps?), etc. \n\nThere aren't many \"junior infrastructure engineers\", most of the time you start working another job, and then you just so happen to end up managing some infrastructure. \n\nI think the reason for this is because companies for the most part see DE, and similar as a cost center and run skeleton crews. \n\nDuring the job search, I've only met with \"directors/vps\" that are usually more focused on analytics or BI, and only know a little about DE. The department usually only had 1 or 2 DEs tops. \n\nUnless it's a bigger company with a significant amount of data to manage, most companies only hire experienced DEs because you can't really bring in an inexperienced DE for business critical data, along with other things like data security. And right now, big companies aren't hiring much. \n\nI'm wondering if there's a way for us to make this information more visible so that people interested in DE could have an easier time setting their expectations?", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There should be more visibility as to which tech jobs aren't usually \"entry level'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awjm6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708540639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this has been said before, but I&amp;#39;m starting to understand why people in this thread say that data engineering is not really an &amp;quot;entry level&amp;quot; job. &lt;/p&gt;\n\n&lt;p&gt;Just like certain other roles like infrastructure engineering, database admin, devops engineer(perhaps?), etc. &lt;/p&gt;\n\n&lt;p&gt;There aren&amp;#39;t many &amp;quot;junior infrastructure engineers&amp;quot;, most of the time you start working another job, and then you just so happen to end up managing some infrastructure. &lt;/p&gt;\n\n&lt;p&gt;I think the reason for this is because companies for the most part see DE, and similar as a cost center and run skeleton crews. &lt;/p&gt;\n\n&lt;p&gt;During the job search, I&amp;#39;ve only met with &amp;quot;directors/vps&amp;quot; that are usually more focused on analytics or BI, and only know a little about DE. The department usually only had 1 or 2 DEs tops. &lt;/p&gt;\n\n&lt;p&gt;Unless it&amp;#39;s a bigger company with a significant amount of data to manage, most companies only hire experienced DEs because you can&amp;#39;t really bring in an inexperienced DE for business critical data, along with other things like data security. And right now, big companies aren&amp;#39;t hiring much. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if there&amp;#39;s a way for us to make this information more visible so that people interested in DE could have an easier time setting their expectations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awjm6r", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1awjm6r/there_should_be_more_visibility_as_to_which_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awjm6r/there_should_be_more_visibility_as_to_which_tech/", "subreddit_subscribers": 162789, "created_utc": 1708540639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The data I\u2019m working with is best suited to an OLAP database due to its focus as a historic dataset that researchers query, but we\u2019re looking at ways to allow external researchers to get data that we expose by connecting to an API. I haven\u2019t quite been able to find the best practices for where API layers connect - directly to the data warehouse, a separate OLTP database, a static file export of the most recent data? Interested in what others do for allowing external consumers to get at the data without spending too much on snowflake credits or server costs. ", "author_fullname": "t2_2ivpdou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use for your API layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awfxbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708531967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The data I\u2019m working with is best suited to an OLAP database due to its focus as a historic dataset that researchers query, but we\u2019re looking at ways to allow external researchers to get data that we expose by connecting to an API. I haven\u2019t quite been able to find the best practices for where API layers connect - directly to the data warehouse, a separate OLTP database, a static file export of the most recent data? Interested in what others do for allowing external consumers to get at the data without spending too much on snowflake credits or server costs. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awfxbw", "is_robot_indexable": true, "report_reasons": null, "author": "Butterhero_", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awfxbw/what_do_you_use_for_your_api_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awfxbw/what_do_you_use_for_your_api_layer/", "subreddit_subscribers": 162789, "created_utc": 1708531967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not particularly experience in this so I have a feeling 'doing data engineering' is quite an annoying way to phrase it, but i'm too inexperienced to word it any other way. But is there an industry you'd stay away from in terms of data engineering? I have manufacturing/supply chain as highly complex in my head.\n\nIs there a particular cloud platform that makes it easier? Would on-premise make it harder?", "author_fullname": "t2_k47n2ise", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an industry that stands out as highly complex in terms of 'doing data engineering'?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax4agw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708601544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not particularly experience in this so I have a feeling &amp;#39;doing data engineering&amp;#39; is quite an annoying way to phrase it, but i&amp;#39;m too inexperienced to word it any other way. But is there an industry you&amp;#39;d stay away from in terms of data engineering? I have manufacturing/supply chain as highly complex in my head.&lt;/p&gt;\n\n&lt;p&gt;Is there a particular cloud platform that makes it easier? Would on-premise make it harder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax4agw", "is_robot_indexable": true, "report_reasons": null, "author": "PlayfulMonk4943", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax4agw/is_there_an_industry_that_stands_out_as_highly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax4agw/is_there_an_industry_that_stands_out_as_highly/", "subreddit_subscribers": 162789, "created_utc": 1708601544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Brews to Bytes: Demystifying Snowflake's Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1awg4q1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "transparent", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QCl0HzPKsmrXVJ1Ij8kun5-8uWrEygoGjNnIWyXh-B4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708532454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/datagibberish/p/snowlfake-beer-warehouse?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?auto=webp&amp;s=d0fa66413f72f534ea89b673a0f1b75232582d50", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81b88bd5724ae2659a6f3e152ef596cb771360a8", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ecd42c8c87bfadae02fe61556d19161b332e615", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/sdpc8pPNZ3A0T01qaGLyWu4l0nDk7Qru2GIS5ALiQAE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc52b17b7b0bbac94688deaf79795ea77f6842c", "width": 320, "height": 320}], "variants": {}, "id": "XCr_QSWw4e-JVVoEL_6R08oYeVhxC2PpPDkMUQRCPiI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1awg4q1", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1awg4q1/from_brews_to_bytes_demystifying_snowflakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/datagibberish/p/snowlfake-beer-warehouse?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true", "subreddit_subscribers": 162789, "created_utc": 1708532454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So yeah, if you do use tests, model &amp; column descriptions, how do you handle duplicates? Like if my \"customer id\" is referenced in 3 models, do you write the same definition, or even a slightly different one in each of the referenced models? ", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Users -&gt; Do you write column descriptions for EVERYTHING? I started to do it, and realized how many duplicates there would be as we follow the source -&gt; staging -&gt; int -&gt; mart which duplicates columns in each layer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awfnk5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708531335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So yeah, if you do use tests, model &amp;amp; column descriptions, how do you handle duplicates? Like if my &amp;quot;customer id&amp;quot; is referenced in 3 models, do you write the same definition, or even a slightly different one in each of the referenced models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awfnk5", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awfnk5/dbt_users_do_you_write_column_descriptions_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awfnk5/dbt_users_do_you_write_column_descriptions_for/", "subreddit_subscribers": 162789, "created_utc": 1708531335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys, \n\nI am a Junior DE with less than 1 year of experience, got laid off and found another job in 6 months and in this role , I am the only inhouse DE.\n\nThe tech stack is Azure DevOps , Databricks , Snowflake and use Data Vault \n\nPlease let me know suggestions or tips or what I need to make sure when working with this tech stack so everything runs smoothly", "author_fullname": "t2_nz18mmwjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for Junior DE who is joining taking a new role with me as the only in-house DE.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax1lip", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708590902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys, &lt;/p&gt;\n\n&lt;p&gt;I am a Junior DE with less than 1 year of experience, got laid off and found another job in 6 months and in this role , I am the only inhouse DE.&lt;/p&gt;\n\n&lt;p&gt;The tech stack is Azure DevOps , Databricks , Snowflake and use Data Vault &lt;/p&gt;\n\n&lt;p&gt;Please let me know suggestions or tips or what I need to make sure when working with this tech stack so everything runs smoothly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ax1lip", "is_robot_indexable": true, "report_reasons": null, "author": "GreyArea1985", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax1lip/advice_for_junior_de_who_is_joining_taking_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax1lip/advice_for_junior_de_who_is_joining_taking_a_new/", "subreddit_subscribers": 162789, "created_utc": 1708590902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.\n\nI am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hard real time time series database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awp36v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708553629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.&lt;/p&gt;\n\n&lt;p&gt;I am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awp36v", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "subreddit_subscribers": 162789, "created_utc": 1708553629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using Databricks Delta Tables. In one of the table I have 900M rows and now insert/update operation became too much slow.\n\nI have applied ZORDER and Partition column but I am not seeing much improvement after applying those things.\n\nI am trying to figure out what other optimization technique I can use to improve insert speed.\n\nThis is how my insert code looks like. I am filtering unique column based on id and then only insert necessary rows.\n```python\n    inserts_df = df.join(\n        spark.sql(f\"SELECT id FROM {view}\"),\n        on=[\"id\"],\n        how=\"left_anti\"\n    )\n    \n    if inserts_df.count() &gt; 0:\n        inserts_df.write.format(\"delta\").partitionBy(\"date\").mode(\"append\").saveAsTable(table_path)\n    else: \n        print(\"batch is already inserted\")\n```\nIf I try to insert around 15000 rows in one shot then it takes around 300 seconds to insert.  \n\n\nUpdate:\n\nHere is my view\n\n```python\nview = \"transactions_view\";  \nlarge_view = spark.read.format(\"delta\").table(table_path).select([\"id\"])  \nlarge_view.createOrReplaceTempView(view)\n```", "author_fullname": "t2_469rhx49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to optimize databricks table having 900M rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awx1nu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708583518.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708574843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using Databricks Delta Tables. In one of the table I have 900M rows and now insert/update operation became too much slow.&lt;/p&gt;\n\n&lt;p&gt;I have applied ZORDER and Partition column but I am not seeing much improvement after applying those things.&lt;/p&gt;\n\n&lt;p&gt;I am trying to figure out what other optimization technique I can use to improve insert speed.&lt;/p&gt;\n\n&lt;p&gt;This is how my insert code looks like. I am filtering unique column based on id and then only insert necessary rows.\n```python\n    inserts_df = df.join(\n        spark.sql(f&amp;quot;SELECT id FROM {view}&amp;quot;),\n        on=[&amp;quot;id&amp;quot;],\n        how=&amp;quot;left_anti&amp;quot;\n    )&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;if inserts_df.count() &amp;gt; 0:\n    inserts_df.write.format(&amp;quot;delta&amp;quot;).partitionBy(&amp;quot;date&amp;quot;).mode(&amp;quot;append&amp;quot;).saveAsTable(table_path)\nelse: \n    print(&amp;quot;batch is already inserted&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```\nIf I try to insert around 15000 rows in one shot then it takes around 300 seconds to insert.  &lt;/p&gt;\n\n&lt;p&gt;Update:&lt;/p&gt;\n\n&lt;p&gt;Here is my view&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;python\nview = &amp;quot;transactions_view&amp;quot;;  \nlarge_view = spark.read.format(&amp;quot;delta&amp;quot;).table(table_path).select([&amp;quot;id&amp;quot;])  \nlarge_view.createOrReplaceTempView(view)\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awx1nu", "is_robot_indexable": true, "report_reasons": null, "author": "sarjuhansaliya", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awx1nu/how_to_optimize_databricks_table_having_900m_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awx1nu/how_to_optimize_databricks_table_having_900m_rows/", "subreddit_subscribers": 162789, "created_utc": 1708574843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a side project and trying to allow users to make their own queries and also create visualizations for various metrics (\"Most Releases in past (month, 3 months, 6 months, etc) by Y\" where Y is some dimensional attribute.   \n\n\nMost of the my time working on the job, I am creating the pipeline, writing some tests, and thats pretty much it. On a few projects I have also created a Looker dashboard, but the end result is simply some LookML and a dashboard in Looker thats used by internal stakeholders. In this case, you could consider this a \"data product\", in that I am trying to externalize the data through a web application. I was looking for some advice on the best way of actually creating things non-technical end users could see and visualize. I was thinking of creating a simple Django web application and embedding some Looker or Tableau dashboards but maybe there are better ideas.", "author_fullname": "t2_e0uj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Externalizing data via a web app...are embedded dashboards the answer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awzza0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708584688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a side project and trying to allow users to make their own queries and also create visualizations for various metrics (&amp;quot;Most Releases in past (month, 3 months, 6 months, etc) by Y&amp;quot; where Y is some dimensional attribute.   &lt;/p&gt;\n\n&lt;p&gt;Most of the my time working on the job, I am creating the pipeline, writing some tests, and thats pretty much it. On a few projects I have also created a Looker dashboard, but the end result is simply some LookML and a dashboard in Looker thats used by internal stakeholders. In this case, you could consider this a &amp;quot;data product&amp;quot;, in that I am trying to externalize the data through a web application. I was looking for some advice on the best way of actually creating things non-technical end users could see and visualize. I was thinking of creating a simple Django web application and embedding some Looker or Tableau dashboards but maybe there are better ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awzza0", "is_robot_indexable": true, "report_reasons": null, "author": "sinuspane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awzza0/externalizing_data_via_a_web_appare_embedded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awzza0/externalizing_data_via_a_web_appare_embedded/", "subreddit_subscribers": 162789, "created_utc": 1708584688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nFirst post on here - looking for some help. I'm developing a data analytics platform at my company right now and I've run into a question, \"should I be streaming data?\"\n\nHere is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions. \n\nThe proof-of-concept I developed runs the [telegraf](https://www.influxdata.com/time-series-platform/telegraf/) agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second. \n\nMy question is this, now that I am moving into developing something for production, I'm wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be \"real-time\" and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else? \n\nI have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated. \n\nThank you all in advance!", "author_fullname": "t2_4ghysex8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Streaming...what the heck should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awq1ao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708555849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;First post on here - looking for some help. I&amp;#39;m developing a data analytics platform at my company right now and I&amp;#39;ve run into a question, &amp;quot;should I be streaming data?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Here is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions. &lt;/p&gt;\n\n&lt;p&gt;The proof-of-concept I developed runs the &lt;a href=\"https://www.influxdata.com/time-series-platform/telegraf/\"&gt;telegraf&lt;/a&gt; agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second. &lt;/p&gt;\n\n&lt;p&gt;My question is this, now that I am moving into developing something for production, I&amp;#39;m wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be &amp;quot;real-time&amp;quot; and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else? &lt;/p&gt;\n\n&lt;p&gt;I have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awq1ao", "is_robot_indexable": true, "report_reasons": null, "author": "full_nelson0510", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "subreddit_subscribers": 162789, "created_utc": 1708555849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is Oracle Data Integrator (ODI) a kind of orchestrator ? Or is it just a ETL/ELT tool?\n\nHow do you use ODI in your tasks?", "author_fullname": "t2_rvp7wzgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I classify the Oracle Data Integrator (ODI)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax4nas", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708602867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is Oracle Data Integrator (ODI) a kind of orchestrator ? Or is it just a ETL/ELT tool?&lt;/p&gt;\n\n&lt;p&gt;How do you use ODI in your tasks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax4nas", "is_robot_indexable": true, "report_reasons": null, "author": "nivlek_miroma", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax4nas/how_can_i_classify_the_oracle_data_integrator_odi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax4nas/how_can_i_classify_the_oracle_data_integrator_odi/", "subreddit_subscribers": 162789, "created_utc": 1708602867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a debate within my team about when it is appropriate to create mini dimensions in a dimensional model. I always thought - based on experience and documentation I've read, that the reasons to create minidimensions are:\n\n* attributes that are frequently consumed together (and usually also separately from the main dimension attributes). For example, I might have a large CONTRACT dimension. I might want to isolate CONTRACT attributes such as RETIRED\\_CUSTOMER\\_FLAG and VULNERABLE\\_CUSTOMER\\_FLAG, if these two attributes are frequently used together in reports, especially if they are used in reports without any other CONTRACT attributes. Thus, I avoid joining the larger CONTRACT dimension for a report that only requires these flags.\n* attributes that change frequently. I might have a CLIENT\\_IN\\_DEBT flag, which I want to keep separate from the main CLIENT dimension, because I noticed that this flag changes frequently, and I do not want to have to insert a new line in the CLIENT dimension each time this happens. Instead the matrix aspect of the minidimension helps me here.\n\nOn the other hand, I have seen people who tend to overuse minidimensions: they create 2-3 minidimensions for each dimension. In these minidimensions, they put attributes with low cardinality. Even for attributes with low cardinality, the minidim can grow quite big if you have 6-7 attributes already. They claim it is more efficient this way, though I can't really see how.\n\nWhat's your reason for using minidimensions? How often do you use them? Would love some documentation if you can provide (other than Kimball).\n\nTHANK YOU!\n\n\\*If not clear what I mean by dimensions/minidimensions: [https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity](https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity)", "author_fullname": "t2_3dghdpvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you create minidimensions in a dimensional model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ax719q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708610364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a debate within my team about when it is appropriate to create mini dimensions in a dimensional model. I always thought - based on experience and documentation I&amp;#39;ve read, that the reasons to create minidimensions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;attributes that are frequently consumed together (and usually also separately from the main dimension attributes). For example, I might have a large CONTRACT dimension. I might want to isolate CONTRACT attributes such as RETIRED_CUSTOMER_FLAG and VULNERABLE_CUSTOMER_FLAG, if these two attributes are frequently used together in reports, especially if they are used in reports without any other CONTRACT attributes. Thus, I avoid joining the larger CONTRACT dimension for a report that only requires these flags.&lt;/li&gt;\n&lt;li&gt;attributes that change frequently. I might have a CLIENT_IN_DEBT flag, which I want to keep separate from the main CLIENT dimension, because I noticed that this flag changes frequently, and I do not want to have to insert a new line in the CLIENT dimension each time this happens. Instead the matrix aspect of the minidimension helps me here.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On the other hand, I have seen people who tend to overuse minidimensions: they create 2-3 minidimensions for each dimension. In these minidimensions, they put attributes with low cardinality. Even for attributes with low cardinality, the minidim can grow quite big if you have 6-7 attributes already. They claim it is more efficient this way, though I can&amp;#39;t really see how.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your reason for using minidimensions? How often do you use them? Would love some documentation if you can provide (other than Kimball).&lt;/p&gt;\n\n&lt;p&gt;THANK YOU!&lt;/p&gt;\n\n&lt;p&gt;*If not clear what I mean by dimensions/minidimensions: &lt;a href=\"https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity\"&gt;https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ax719q", "is_robot_indexable": true, "report_reasons": null, "author": "MaLinChao", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax719q/when_do_you_create_minidimensions_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax719q/when_do_you_create_minidimensions_in_a/", "subreddit_subscribers": 162789, "created_utc": 1708610364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Folks,\n\n&amp;#x200B;\n\nI just attended a data infrastructure conference, and more than half of the presentations mentioned ontologies.  \n\n\nI just want to survey this group on its thoughts on the topic.  \n\n\nI personally kinda hate them and think they are antiquated in the age of LLMs and AI generally.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_ddwwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "General Thoughts on Ontologies, Knowledge Graphs, SPARQL, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax69pr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708608106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I just attended a data infrastructure conference, and more than half of the presentations mentioned ontologies.  &lt;/p&gt;\n\n&lt;p&gt;I just want to survey this group on its thoughts on the topic.  &lt;/p&gt;\n\n&lt;p&gt;I personally kinda hate them and think they are antiquated in the age of LLMs and AI generally.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax69pr", "is_robot_indexable": true, "report_reasons": null, "author": "Jimmyfatz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax69pr/general_thoughts_on_ontologies_knowledge_graphs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax69pr/general_thoughts_on_ontologies_knowledge_graphs/", "subreddit_subscribers": 162789, "created_utc": 1708608106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nI know the subject  may look weird to some users here, well it is probably weird indeed. But I will try to explain.\n\nA bit of off topic but needed contextual info: I'm not a data engineer, I've been working as a data analyst, and not the most advanced one (technically-wise) and not in the most advanced environment (not an IT or analytics company). I've been tasked with, what i think, is a data engineering task (there are no developers/data engineers in my company so they came to me), and I have a really rough idea about what i am going to do.  So, basically, I'm a noob, in a desperate need of help. I usually managed to overcome by just googling things, but, to my dismay, i found nothing this time.\n\nSo... the task - my bosses want a cube in Excel. Well, what they exactly want is \"an OLAP cube wired up  into Excel so that you could import it into a pivot table and arbitrarily slice and dice data by dragging and dropping columns\".  I do realize it's a really dated format for frontend, and that there are some really neat BI tools, dashboards or even in-browser solutions, but, as i said, my bosses want exactly it like that (really used to work with excel pivot tables and nothing else) and i come from a non tech-savvy place, to say the least.\n\nI have 2 data sources - an ERP and a CRM. I know some Python and Numpy/Pandas, so I've created a small script that uses an API connection to automatically get data from CRM, dropping some useless data, adding some fields etc. I'm using PyCharm as IDE. Now, ERP is being provided to us by another company, and they have some devs, so they should be able to do the same thing, but from the ERP side of things.\n\nAs you could probably imagine, volumes of data in case of my company would be quite small. No more than 2 million rows with like up to 10 columns.\n\nSo the very first step - having a dedicated (server) machine with some raw data automatically uploaded into its memory as CSV files or stored as Pandas dataframes - is done or at least it's clear to me how it's done. But i will need to look into some orchestrators, so that my python script(s) will be executed every n hours.\n\nBut what comes next - is quite a challenge. As i conceptualize it, it's either:\n\n\\- CSV/Pandas Dataframes -&gt; MS SQL DB -&gt; MS SQL SSAS -&gt; Excel\n\n\\- CSV/Pandas Dataframes -&gt; DuckDB -&gt; Excel\n\nSo, am I aware of sqlite, but it's OLTP, apparently? And there are few additional -  maybe complicated - steps so that an OLAP cube on Excel side may connect to a DB like that, but i'm not sure i want to go through that. I think CSV/Pandas into sqlite into MS SQL SSAS may be an additional option though, not sure?\n\nAfter educating myself a bit about DuckDB, I'm in favor of sticking to it, as it's been marketed as an OLAP solution for data analytics. The part about transforming Pandas dataframes into DB objects, writing queries to tables, fetching queries results etc - is covered quite extensively in documentation and on forums - i should be alright doing all that.\n\nThe most mysterious part comes next, and it's basically why I'm creating this thread.\n\nHow do I connect DuckDB to Excel? How Excel connection wizard - or whatever it's called - might \"see\" a DuckDB database (or should i say DuckDB datawarehouse)? Will it even be able to?\n\nThanks for reading all that, hope somebody might guide me through, would be really grateful.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_1svoqb4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB into Excel Pivot table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax3j11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708599437.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708598694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I know the subject  may look weird to some users here, well it is probably weird indeed. But I will try to explain.&lt;/p&gt;\n\n&lt;p&gt;A bit of off topic but needed contextual info: I&amp;#39;m not a data engineer, I&amp;#39;ve been working as a data analyst, and not the most advanced one (technically-wise) and not in the most advanced environment (not an IT or analytics company). I&amp;#39;ve been tasked with, what i think, is a data engineering task (there are no developers/data engineers in my company so they came to me), and I have a really rough idea about what i am going to do.  So, basically, I&amp;#39;m a noob, in a desperate need of help. I usually managed to overcome by just googling things, but, to my dismay, i found nothing this time.&lt;/p&gt;\n\n&lt;p&gt;So... the task - my bosses want a cube in Excel. Well, what they exactly want is &amp;quot;an OLAP cube wired up  into Excel so that you could import it into a pivot table and arbitrarily slice and dice data by dragging and dropping columns&amp;quot;.  I do realize it&amp;#39;s a really dated format for frontend, and that there are some really neat BI tools, dashboards or even in-browser solutions, but, as i said, my bosses want exactly it like that (really used to work with excel pivot tables and nothing else) and i come from a non tech-savvy place, to say the least.&lt;/p&gt;\n\n&lt;p&gt;I have 2 data sources - an ERP and a CRM. I know some Python and Numpy/Pandas, so I&amp;#39;ve created a small script that uses an API connection to automatically get data from CRM, dropping some useless data, adding some fields etc. I&amp;#39;m using PyCharm as IDE. Now, ERP is being provided to us by another company, and they have some devs, so they should be able to do the same thing, but from the ERP side of things.&lt;/p&gt;\n\n&lt;p&gt;As you could probably imagine, volumes of data in case of my company would be quite small. No more than 2 million rows with like up to 10 columns.&lt;/p&gt;\n\n&lt;p&gt;So the very first step - having a dedicated (server) machine with some raw data automatically uploaded into its memory as CSV files or stored as Pandas dataframes - is done or at least it&amp;#39;s clear to me how it&amp;#39;s done. But i will need to look into some orchestrators, so that my python script(s) will be executed every n hours.&lt;/p&gt;\n\n&lt;p&gt;But what comes next - is quite a challenge. As i conceptualize it, it&amp;#39;s either:&lt;/p&gt;\n\n&lt;p&gt;- CSV/Pandas Dataframes -&amp;gt; MS SQL DB -&amp;gt; MS SQL SSAS -&amp;gt; Excel&lt;/p&gt;\n\n&lt;p&gt;- CSV/Pandas Dataframes -&amp;gt; DuckDB -&amp;gt; Excel&lt;/p&gt;\n\n&lt;p&gt;So, am I aware of sqlite, but it&amp;#39;s OLTP, apparently? And there are few additional -  maybe complicated - steps so that an OLAP cube on Excel side may connect to a DB like that, but i&amp;#39;m not sure i want to go through that. I think CSV/Pandas into sqlite into MS SQL SSAS may be an additional option though, not sure?&lt;/p&gt;\n\n&lt;p&gt;After educating myself a bit about DuckDB, I&amp;#39;m in favor of sticking to it, as it&amp;#39;s been marketed as an OLAP solution for data analytics. The part about transforming Pandas dataframes into DB objects, writing queries to tables, fetching queries results etc - is covered quite extensively in documentation and on forums - i should be alright doing all that.&lt;/p&gt;\n\n&lt;p&gt;The most mysterious part comes next, and it&amp;#39;s basically why I&amp;#39;m creating this thread.&lt;/p&gt;\n\n&lt;p&gt;How do I connect DuckDB to Excel? How Excel connection wizard - or whatever it&amp;#39;s called - might &amp;quot;see&amp;quot; a DuckDB database (or should i say DuckDB datawarehouse)? Will it even be able to?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading all that, hope somebody might guide me through, would be really grateful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ax3j11", "is_robot_indexable": true, "report_reasons": null, "author": "whoooooaaaaaaaaaaaa", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax3j11/duckdb_into_excel_pivot_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax3j11/duckdb_into_excel_pivot_table/", "subreddit_subscribers": 162789, "created_utc": 1708598694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nWhat are the benefits of a materialized view over writing truncate a table with Airflow (for example) with a schedule ? \n", "author_fullname": "t2_94h4vsep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Materialized view vs regular table with Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awjd9c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708540073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;What are the benefits of a materialized view over writing truncate a table with Airflow (for example) with a schedule ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awjd9c", "is_robot_indexable": true, "report_reasons": null, "author": "GressiDawn", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awjd9c/materialized_view_vs_regular_table_with_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awjd9c/materialized_view_vs_regular_table_with_airflow/", "subreddit_subscribers": 162789, "created_utc": 1708540073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I'm trying to weigh the benefits of various tools to figure out what the best one would be for our use case.\n\nOur team is regularly tasked with setting up jobs that move data from 50-60 separate PostgreSQL databases to a single destination PostgreSQL database. Problem is, no one on the team is a trained data engineer. The current way we move data from Source A to Destination B is by writing Airflow jobs but the process to create regularly recurring transfers tends to be prohibitively time intensive, and it seems like there's a better way than writing new airflow scripts every time we need to set up a new job.\n\nI'm looking to push for the adoption of a tool that is specifically meant for this purpose (data modeling and recurring transfers). So far I've seen people recommend DBT and Fivetran, but I don't want to overengineer it, considering our use case is fairly limited in scope. What would make the most sense for us to pursue?", "author_fullname": "t2_104vvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What ETL/ELT tool to use for this use-case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awegrq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708528490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I&amp;#39;m trying to weigh the benefits of various tools to figure out what the best one would be for our use case.&lt;/p&gt;\n\n&lt;p&gt;Our team is regularly tasked with setting up jobs that move data from 50-60 separate PostgreSQL databases to a single destination PostgreSQL database. Problem is, no one on the team is a trained data engineer. The current way we move data from Source A to Destination B is by writing Airflow jobs but the process to create regularly recurring transfers tends to be prohibitively time intensive, and it seems like there&amp;#39;s a better way than writing new airflow scripts every time we need to set up a new job.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to push for the adoption of a tool that is specifically meant for this purpose (data modeling and recurring transfers). So far I&amp;#39;ve seen people recommend DBT and Fivetran, but I don&amp;#39;t want to overengineer it, considering our use case is fairly limited in scope. What would make the most sense for us to pursue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awegrq", "is_robot_indexable": true, "report_reasons": null, "author": "LonghornMorgs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awegrq/what_etlelt_tool_to_use_for_this_usecase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awegrq/what_etlelt_tool_to_use_for_this_usecase/", "subreddit_subscribers": 162789, "created_utc": 1708528490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as the title states I'm taking on a new role with some clients I used to manage 8+ years ago.\n\nThese clients use sophisticated rate shoppers that analyze the package economics (dimensions, size, weight etc) and picks the most optimal (primarily price but some include on-time performance) parcel carrier to tender the package to.\n\nMy job is to come in and try to maximize our volume and by extension increase our revenue and package market share via optimizing the rate shopper.\n\nMy initial thoughts are to see where our existing volume falls within a rate card.   A rate card is a simple excel file that has individual weights listed ie 1lb, 2lb, 3lb, (up to 150lbs) down the rows and zones (essentially distance) across the columns.   Some carriers have 10 to 16 zones so it's not apples to apples.  As an example a 5lb zone 9 = rate\n\nMy original thought is to place existing volumes into each weight and zone to see if any patterns standout.  I think we would need ALL the volume to see where we're missing.\n\nAny other guidance or strategies, AI solution, that could help analyze the data.\n\n&amp;#x200B;\n\nTy!\n\nEDIT: PS I love playing with data but I'm an amateur \n\n&amp;#x200B;", "author_fullname": "t2_33ybrs28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taking a new role that requires analytics - my org isn't the best at this - Transportation and Logistics Industry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ax8eyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708613975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title states I&amp;#39;m taking on a new role with some clients I used to manage 8+ years ago.&lt;/p&gt;\n\n&lt;p&gt;These clients use sophisticated rate shoppers that analyze the package economics (dimensions, size, weight etc) and picks the most optimal (primarily price but some include on-time performance) parcel carrier to tender the package to.&lt;/p&gt;\n\n&lt;p&gt;My job is to come in and try to maximize our volume and by extension increase our revenue and package market share via optimizing the rate shopper.&lt;/p&gt;\n\n&lt;p&gt;My initial thoughts are to see where our existing volume falls within a rate card.   A rate card is a simple excel file that has individual weights listed ie 1lb, 2lb, 3lb, (up to 150lbs) down the rows and zones (essentially distance) across the columns.   Some carriers have 10 to 16 zones so it&amp;#39;s not apples to apples.  As an example a 5lb zone 9 = rate&lt;/p&gt;\n\n&lt;p&gt;My original thought is to place existing volumes into each weight and zone to see if any patterns standout.  I think we would need ALL the volume to see where we&amp;#39;re missing.&lt;/p&gt;\n\n&lt;p&gt;Any other guidance or strategies, AI solution, that could help analyze the data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ty!&lt;/p&gt;\n\n&lt;p&gt;EDIT: PS I love playing with data but I&amp;#39;m an amateur &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax8eyh", "is_robot_indexable": true, "report_reasons": null, "author": "tylerhill11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax8eyh/taking_a_new_role_that_requires_analytics_my_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax8eyh/taking_a_new_role_that_requires_analytics_my_org/", "subreddit_subscribers": 162789, "created_utc": 1708613975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a Data Engineer. I worked as a freelancer for past 4 months. Before that I lost my job due to layoffs and was having overall experience of 2 yrs. Now I got an offer from a company for which I have said that I have overall experience of 2.4 yrs ( Including Freelancing work). How can I show that I really did freelancing work. I have no idea I am new to this. Please guide.", "author_fullname": "t2_i159isc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to show my freelancing experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ax8av0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708613669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a Data Engineer. I worked as a freelancer for past 4 months. Before that I lost my job due to layoffs and was having overall experience of 2 yrs. Now I got an offer from a company for which I have said that I have overall experience of 2.4 yrs ( Including Freelancing work). How can I show that I really did freelancing work. I have no idea I am new to this. Please guide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ax8av0", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Zookeepergame256", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax8av0/how_to_show_my_freelancing_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax8av0/how_to_show_my_freelancing_experience/", "subreddit_subscribers": 162789, "created_utc": 1708613669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_579zp4ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Awesome Open Source Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax5xal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Qf21_qHt4hF6GhEG3IDa9tYHd5_yBKbNLn2ptvhVPFE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708607012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/pracdata/awesome-open-source-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?auto=webp&amp;s=1e826c0c4c2f10e3fdab1519962fe1b2e16d9966", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5c722dd50754be678a9f98a080b389df4318ef5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79cf4cb9fa8e84790de88c1ca4f774ad7d1e5ad9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=44be72a8902261109e36b7d507f0d7cada0cf54f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=571efbe36091f5616320704eb296430d79dd7387", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c31945bc32fa1424129159c7551129148103d16", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=084abacb4fd06282455bd9c0f3e52313b2d0f75c", "width": 1080, "height": 540}], "variants": {}, "id": "jGtX2uBGRqXYeivLwDl4nSO9BxdbH0qX33SH34ixZIc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ax5xal", "is_robot_indexable": true, "report_reasons": null, "author": "ithoughtful", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax5xal/awesome_open_source_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/pracdata/awesome-open-source-data-engineering", "subreddit_subscribers": 162789, "created_utc": 1708607012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use case that where webhook send a http post request to a cloud functions endpoint to trigger a function which takes response, encode it and publish the Pub/Sub. Its about 200 message per second at most and latency is ignoreable. Then messages will subscribed into BigQuery. \n\nSo there are 2 approaches suggested, one is considered HTTP functions (foreground, use functions framework) the other is Event-driven functions (background, do not use functions framework)\n\nAs far as i understand both can triggered by and process http post request. There are explicit quotas are mentioned about background one and not for foreground one.\n\nWhich function type i need to use for that usecase, couldn\u2019t distinct basically.\n\nFurthermore any comments on whether to\nuse http function or https one, webhook is able to send both. Also do you foresee any connection polling limit issue on the Pub/Sub side, as Cloud Function will open a client for any message or does itself handle after publishing one by one.\n\nThank you for any comment and help, bests.", "author_fullname": "t2_iqv7zs94b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Cloud Functions type to choose on this usecase?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax2jg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708594807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use case that where webhook send a http post request to a cloud functions endpoint to trigger a function which takes response, encode it and publish the Pub/Sub. Its about 200 message per second at most and latency is ignoreable. Then messages will subscribed into BigQuery. &lt;/p&gt;\n\n&lt;p&gt;So there are 2 approaches suggested, one is considered HTTP functions (foreground, use functions framework) the other is Event-driven functions (background, do not use functions framework)&lt;/p&gt;\n\n&lt;p&gt;As far as i understand both can triggered by and process http post request. There are explicit quotas are mentioned about background one and not for foreground one.&lt;/p&gt;\n\n&lt;p&gt;Which function type i need to use for that usecase, couldn\u2019t distinct basically.&lt;/p&gt;\n\n&lt;p&gt;Furthermore any comments on whether to\nuse http function or https one, webhook is able to send both. Also do you foresee any connection polling limit issue on the Pub/Sub side, as Cloud Function will open a client for any message or does itself handle after publishing one by one.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any comment and help, bests.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ax2jg1", "is_robot_indexable": true, "report_reasons": null, "author": "thinkfl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax2jg1/which_cloud_functions_type_to_choose_on_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax2jg1/which_cloud_functions_type_to_choose_on_this/", "subreddit_subscribers": 162789, "created_utc": 1708594807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, \n\ndlt (pip install dlt, not delta live tables) cofounder here. We have some pipelines running on different orchestrators (like git actions), and some running on gcp cloud function as webhook (no orchestrator you could say).\n\nI have been looking into achieving a single pane of glass for these pipelines, and after very little consideration I came up with the following  \n\n\n\\- **Alert** failures to slack (could optionally alert success too, to not have to wonder if job is running or done). I am thinking here to perhaps store the historic run time of the runs and query it periodically during the pipeline run to generate a possible lateness alert.  \n\\- **Monitor** tables by row counts over time to see if all the runs were succesful and if there is any obvious anomaly. We do this by storing the run's time and row count at load time.  \n\\- Since I am discussing Slack data with personal information, we store the **data schemas and annotate which fields are personal info**, for compliance and possibility of deletion. We use views for reporting, so if we nuke the data in the raw layer, it's gone.  \nI wrote up implementation details on our [blog](https://dlthub.com/docs/blog/single-pane-glass).\n\nWhat else would you track? what's important to you in such a case of running pipelines in different places? and, do you even have this problem? I am asking because it would be interesting to support your needs.", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about pipeline observability - is it a problem? what do you do as a solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax2ioz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708594722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;dlt (pip install dlt, not delta live tables) cofounder here. We have some pipelines running on different orchestrators (like git actions), and some running on gcp cloud function as webhook (no orchestrator you could say).&lt;/p&gt;\n\n&lt;p&gt;I have been looking into achieving a single pane of glass for these pipelines, and after very little consideration I came up with the following  &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Alert&lt;/strong&gt; failures to slack (could optionally alert success too, to not have to wonder if job is running or done). I am thinking here to perhaps store the historic run time of the runs and query it periodically during the pipeline run to generate a possible lateness alert.&lt;br/&gt;\n- &lt;strong&gt;Monitor&lt;/strong&gt; tables by row counts over time to see if all the runs were succesful and if there is any obvious anomaly. We do this by storing the run&amp;#39;s time and row count at load time.&lt;br/&gt;\n- Since I am discussing Slack data with personal information, we store the &lt;strong&gt;data schemas and annotate which fields are personal info&lt;/strong&gt;, for compliance and possibility of deletion. We use views for reporting, so if we nuke the data in the raw layer, it&amp;#39;s gone.&lt;br/&gt;\nI wrote up implementation details on our &lt;a href=\"https://dlthub.com/docs/blog/single-pane-glass\"&gt;blog&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;What else would you track? what&amp;#39;s important to you in such a case of running pipelines in different places? and, do you even have this problem? I am asking because it would be interesting to support your needs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?auto=webp&amp;s=38a16149be56faee49b736395bffc58dd48fbac6", "width": 1582, "height": 744}, "resolutions": [{"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0fb5a78c2fb335ab495a73532fefc3a0458a620", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d57c49a9ce8a67c509d951ce8610f000cf07b349", "width": 216, "height": 101}, {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0172052633123b31bcbfd373d648f422b1c5e596", "width": 320, "height": 150}, {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e01d02980980517ae1a90b44583f7f05a72c874", "width": 640, "height": 300}, {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9a22efeab0138a5456429ed503a28303710dcc3", "width": 960, "height": 451}, {"url": "https://external-preview.redd.it/DBRn8w0wDOqrEP_dBPKM7Tj0Njuo1jOaBLFIL0nijQM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9de65f1e84aec4676c658f0ef7481f7b14129b2e", "width": 1080, "height": 507}], "variants": {}, "id": "uvn9QMGfYHG4U6k_wojjcXg2Gh_ViMlofINJwhKbO0w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax2ioz", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax2ioz/question_about_pipeline_observability_is_it_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax2ioz/question_about_pipeline_observability_is_it_a/", "subreddit_subscribers": 162789, "created_utc": 1708594722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We use bitbucket for our CI/CD deployments.   \nHave tried airbyte via UI and would like to check if anyone has tried CI/CD deployment? Is it possible wit bitbucket CI/CD pipeline?", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte CI/CD deployments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awebe6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708528131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We use bitbucket for our CI/CD deployments.&lt;br/&gt;\nHave tried airbyte via UI and would like to check if anyone has tried CI/CD deployment? Is it possible wit bitbucket CI/CD pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awebe6", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awebe6/airbyte_cicd_deployments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awebe6/airbyte_cicd_deployments/", "subreddit_subscribers": 162789, "created_utc": 1708528131.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}