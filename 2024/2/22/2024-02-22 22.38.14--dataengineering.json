{"kind": "Listing", "data": {"after": "t3_1axit9z", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not particularly experience in this so I have a feeling 'doing data engineering' is quite an annoying way to phrase it, but i'm too inexperienced to word it any other way. But is there an industry you'd stay away from in terms of data engineering? I have manufacturing/supply chain as highly complex in my head.\n\nIs there a particular cloud platform that makes it easier? Would on-premise make it harder?", "author_fullname": "t2_k47n2ise", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an industry that stands out as highly complex in terms of 'doing data engineering'?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax4agw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708601544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not particularly experience in this so I have a feeling &amp;#39;doing data engineering&amp;#39; is quite an annoying way to phrase it, but i&amp;#39;m too inexperienced to word it any other way. But is there an industry you&amp;#39;d stay away from in terms of data engineering? I have manufacturing/supply chain as highly complex in my head.&lt;/p&gt;\n\n&lt;p&gt;Is there a particular cloud platform that makes it easier? Would on-premise make it harder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax4agw", "is_robot_indexable": true, "report_reasons": null, "author": "PlayfulMonk4943", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax4agw/is_there_an_industry_that_stands_out_as_highly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax4agw/is_there_an_industry_that_stands_out_as_highly/", "subreddit_subscribers": 162887, "created_utc": 1708601544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys, \n\nI am a Junior DE with less than 1 year of experience, got laid off and found another job in 6 months and in this role , I am the only inhouse DE.\n\nThe tech stack is Azure DevOps , Databricks , Snowflake and use Data Vault \n\nPlease let me know suggestions or tips or what I need to make sure when working with this tech stack so everything runs smoothly", "author_fullname": "t2_nz18mmwjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for Junior DE who is joining taking a new role with me as the only in-house DE.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax1lip", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708590902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys, &lt;/p&gt;\n\n&lt;p&gt;I am a Junior DE with less than 1 year of experience, got laid off and found another job in 6 months and in this role , I am the only inhouse DE.&lt;/p&gt;\n\n&lt;p&gt;The tech stack is Azure DevOps , Databricks , Snowflake and use Data Vault &lt;/p&gt;\n\n&lt;p&gt;Please let me know suggestions or tips or what I need to make sure when working with this tech stack so everything runs smoothly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ax1lip", "is_robot_indexable": true, "report_reasons": null, "author": "GreyArea1985", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax1lip/advice_for_junior_de_who_is_joining_taking_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax1lip/advice_for_junior_de_who_is_joining_taking_a_new/", "subreddit_subscribers": 162887, "created_utc": 1708590902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using Databricks Delta Tables. In one of the table I have 900M rows and now insert/update operation became too much slow.\n\nI have applied ZORDER and Partition column but I am not seeing much improvement after applying those things.\n\nI am trying to figure out what other optimization technique I can use to improve insert speed.\n\nThis is how my insert code looks like. I am filtering unique column based on id and then only insert necessary rows.\n```python\n    inserts_df = df.join(\n        spark.sql(f\"SELECT id FROM {view}\"),\n        on=[\"id\"],\n        how=\"left_anti\"\n    )\n    \n    if inserts_df.count() &gt; 0:\n        inserts_df.write.format(\"delta\").partitionBy(\"date\").mode(\"append\").saveAsTable(table_path)\n    else: \n        print(\"batch is already inserted\")\n```\nIf I try to insert around 15000 rows in one shot then it takes around 300 seconds to insert.  \n\n\nUpdate:\n\nHere is my view\n\n```python\nview = \"transactions_view\";  \nlarge_view = spark.read.format(\"delta\").table(table_path).select([\"id\"])  \nlarge_view.createOrReplaceTempView(view)\n```", "author_fullname": "t2_469rhx49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to optimize databricks table having 900M rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awx1nu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708583518.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708574843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using Databricks Delta Tables. In one of the table I have 900M rows and now insert/update operation became too much slow.&lt;/p&gt;\n\n&lt;p&gt;I have applied ZORDER and Partition column but I am not seeing much improvement after applying those things.&lt;/p&gt;\n\n&lt;p&gt;I am trying to figure out what other optimization technique I can use to improve insert speed.&lt;/p&gt;\n\n&lt;p&gt;This is how my insert code looks like. I am filtering unique column based on id and then only insert necessary rows.\n```python\n    inserts_df = df.join(\n        spark.sql(f&amp;quot;SELECT id FROM {view}&amp;quot;),\n        on=[&amp;quot;id&amp;quot;],\n        how=&amp;quot;left_anti&amp;quot;\n    )&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;if inserts_df.count() &amp;gt; 0:\n    inserts_df.write.format(&amp;quot;delta&amp;quot;).partitionBy(&amp;quot;date&amp;quot;).mode(&amp;quot;append&amp;quot;).saveAsTable(table_path)\nelse: \n    print(&amp;quot;batch is already inserted&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```\nIf I try to insert around 15000 rows in one shot then it takes around 300 seconds to insert.  &lt;/p&gt;\n\n&lt;p&gt;Update:&lt;/p&gt;\n\n&lt;p&gt;Here is my view&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;python\nview = &amp;quot;transactions_view&amp;quot;;  \nlarge_view = spark.read.format(&amp;quot;delta&amp;quot;).table(table_path).select([&amp;quot;id&amp;quot;])  \nlarge_view.createOrReplaceTempView(view)\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awx1nu", "is_robot_indexable": true, "report_reasons": null, "author": "sarjuhansaliya", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awx1nu/how_to_optimize_databricks_table_having_900m_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awx1nu/how_to_optimize_databricks_table_having_900m_rows/", "subreddit_subscribers": 162887, "created_utc": 1708574843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Share your favorite tips for writing better SQL, your pet peeves and best practices.", "author_fullname": "t2_svxm6jt4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your Top SQL Query Optimization tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axd7cy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708625364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Share your favorite tips for writing better SQL, your pet peeves and best practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axd7cy", "is_robot_indexable": true, "report_reasons": null, "author": "data_engineer_", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axd7cy/what_are_your_top_sql_query_optimization_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axd7cy/what_are_your_top_sql_query_optimization_tips/", "subreddit_subscribers": 162887, "created_utc": 1708625364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.\n\nI am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hard real time time series database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awp36v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708553629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking into time series databases for a usecase with hard real time constraints. It is about fully automated bidding for electricity prices and and controlling a power plant according to auction outcome.&lt;/p&gt;\n\n&lt;p&gt;I am looking into timescale, M3, starrocks. Am I missing a good option? Are there some experiences/suggestions for databases suiting such hard real-time constraints ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awp36v", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awp36v/hard_real_time_time_series_database/", "subreddit_subscribers": 162887, "created_utc": 1708553629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nFirst post on here - looking for some help. I'm developing a data analytics platform at my company right now and I've run into a question, \"should I be streaming data?\"\n\nHere is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions.\n\nThe proof-of-concept I developed runs the [telegraf](https://www.influxdata.com/time-series-platform/telegraf/) agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second.\n\nMy question is this, now that I am moving into developing something for production, I'm wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be \"real-time\" and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else?\n\nI have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated.\n\nThank you all in advance!\n\nEDIT: Thank you all for your insights and suggestions so far. I have some experience in more traditional data analytics and ML stuff,  but it is evident that I would benefit greatly from taking some courses in data engineering to help understand streaming architecture better. If anyone has suggestions on courses to fill in some gaps, I would appreciate the suggestions!", "author_fullname": "t2_4ghysex8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Streaming...what the heck should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awq1ao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708627817.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708555849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;First post on here - looking for some help. I&amp;#39;m developing a data analytics platform at my company right now and I&amp;#39;ve run into a question, &amp;quot;should I be streaming data?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Here is my use case: I am developing a data analytics platform that collects data from industrial equipment (Modbus) and will do a few specific things with that data. First, it will ingest the data into a database for long-term storage so our clients can use it for historical trending on things like energy consumption and peak load times, etc. Second, data will be displayed in a customer-facing, custom, dashboard. Third, we will use the data to alarm based on specific conditions.&lt;/p&gt;\n\n&lt;p&gt;The proof-of-concept I developed runs the &lt;a href=\"https://www.influxdata.com/time-series-platform/telegraf/\"&gt;telegraf&lt;/a&gt; agent once per second to collect the data and just dumps the data into a time-series database and the dashboard queries the database once per second. We are not dealing with a lot of data here. For each of our clients, we estimated around 7-12KB per second.&lt;/p&gt;\n\n&lt;p&gt;My question is this, now that I am moving into developing something for production, I&amp;#39;m wondering if a streaming data architecture is the proper approach. I want the dashboard and alarms to be &amp;quot;real-time&amp;quot; and I know the approach I took for the proof-of-concept is far from that. Is streaming data the way to go? If so, should I use Kafka/Amazon MSK, Amazon Kinesis, or something else?&lt;/p&gt;\n\n&lt;p&gt;I have no experience with streaming data and am struggling to wrap my head around the concepts and how they would apply to my use case. So any input is valued and appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thank you all for your insights and suggestions so far. I have some experience in more traditional data analytics and ML stuff,  but it is evident that I would benefit greatly from taking some courses in data engineering to help understand streaming architecture better. If anyone has suggestions on courses to fill in some gaps, I would appreciate the suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1awq1ao", "is_robot_indexable": true, "report_reasons": null, "author": "full_nelson0510", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awq1ao/data_streamingwhat_the_heck_should_i_do/", "subreddit_subscribers": 162887, "created_utc": 1708555849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data analyst fresh out of college, but I don't really generate business insights, I'm more on the ETL-database side of things. I'm working with legacy systems, most of it is MS Access/Excel (~20 TBs worth of .accdb/.xlsx files on a remote drive, yes I know it's bad) with some hints of SQL server (no admin/write privileges, only read). I still try to follow good ETL practices, having most of the transformations and filters at the SQL layer, then moving around data with pandas (pd.read_xxx and df.to_xxx all day). My pipelines are very amateurish, I've written several pipelines so far, all of them rely on pandas and pyodbc/sqlalchemy. \n\nI'm working on creating a mini ELT datalake for one of our reports that pulls from several Access files and tables. I have pipelines running from Access to SQLite on a daily basis, I have the tables split into several SQLite files. When it's time to run the report (usually once a month, occasionally adhoc with some extra transformations) I have some python code that reads and transforms the multiple SQLite files in parallel with concurrent.futures.ProcessPoolExecutor, then outputs the report into a lovely spreedsheet. It speeds up the process for this report by a few min, it's definitely my coolest project so far, and it's something I hope to implement for more\nreports as well.\n\nHow do seasoned, experienced engineers working with modern stacks perceive this kind of work? I would like to work with a major cloud service one day like AWS/Azure/GCP, I have some self-experience spinning up MySQL databases and hosting small websites in Azure, but no big data work.", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with legacy systems, how do seasoned engineers perceive this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axbzso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708623749.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708622522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analyst fresh out of college, but I don&amp;#39;t really generate business insights, I&amp;#39;m more on the ETL-database side of things. I&amp;#39;m working with legacy systems, most of it is MS Access/Excel (~20 TBs worth of .accdb/.xlsx files on a remote drive, yes I know it&amp;#39;s bad) with some hints of SQL server (no admin/write privileges, only read). I still try to follow good ETL practices, having most of the transformations and filters at the SQL layer, then moving around data with pandas (pd.read_xxx and df.to_xxx all day). My pipelines are very amateurish, I&amp;#39;ve written several pipelines so far, all of them rely on pandas and pyodbc/sqlalchemy. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on creating a mini ELT datalake for one of our reports that pulls from several Access files and tables. I have pipelines running from Access to SQLite on a daily basis, I have the tables split into several SQLite files. When it&amp;#39;s time to run the report (usually once a month, occasionally adhoc with some extra transformations) I have some python code that reads and transforms the multiple SQLite files in parallel with concurrent.futures.ProcessPoolExecutor, then outputs the report into a lovely spreedsheet. It speeds up the process for this report by a few min, it&amp;#39;s definitely my coolest project so far, and it&amp;#39;s something I hope to implement for more\nreports as well.&lt;/p&gt;\n\n&lt;p&gt;How do seasoned, experienced engineers working with modern stacks perceive this kind of work? I would like to work with a major cloud service one day like AWS/Azure/GCP, I have some self-experience spinning up MySQL databases and hosting small websites in Azure, but no big data work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axbzso", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axbzso/working_with_legacy_systems_how_do_seasoned/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axbzso/working_with_legacy_systems_how_do_seasoned/", "subreddit_subscribers": 162887, "created_utc": 1708622522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a side project and trying to allow users to make their own queries and also create visualizations for various metrics (\"Most Releases in past (month, 3 months, 6 months, etc) by Y\" where Y is some dimensional attribute.   \n\n\nMost of the my time working on the job, I am creating the pipeline, writing some tests, and thats pretty much it. On a few projects I have also created a Looker dashboard, but the end result is simply some LookML and a dashboard in Looker thats used by internal stakeholders. In this case, you could consider this a \"data product\", in that I am trying to externalize the data through a web application. I was looking for some advice on the best way of actually creating things non-technical end users could see and visualize. I was thinking of creating a simple Django web application and embedding some Looker or Tableau dashboards but maybe there are better ideas.", "author_fullname": "t2_e0uj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Externalizing data via a web app...are embedded dashboards the answer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1awzza0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708584688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a side project and trying to allow users to make their own queries and also create visualizations for various metrics (&amp;quot;Most Releases in past (month, 3 months, 6 months, etc) by Y&amp;quot; where Y is some dimensional attribute.   &lt;/p&gt;\n\n&lt;p&gt;Most of the my time working on the job, I am creating the pipeline, writing some tests, and thats pretty much it. On a few projects I have also created a Looker dashboard, but the end result is simply some LookML and a dashboard in Looker thats used by internal stakeholders. In this case, you could consider this a &amp;quot;data product&amp;quot;, in that I am trying to externalize the data through a web application. I was looking for some advice on the best way of actually creating things non-technical end users could see and visualize. I was thinking of creating a simple Django web application and embedding some Looker or Tableau dashboards but maybe there are better ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1awzza0", "is_robot_indexable": true, "report_reasons": null, "author": "sinuspane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1awzza0/externalizing_data_via_a_web_appare_embedded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1awzza0/externalizing_data_via_a_web_appare_embedded/", "subreddit_subscribers": 162887, "created_utc": 1708584688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently use Fivetran. It works great, no complaints (other than the price even with a heavy discount). The reliability for all the API connectors is top notch, but I feel like we're getting eaten alive on our first party data and our data volume is growing rapidly. We're going from Aurora (AWS) to Snowflake\n\nSo I'm looking at using something else just for the MySQL stuff (since SQL connectors are supported everywhere and should be reliable). For \"scale\" I don't mean petabyte scale, but we are looking at \\~2b monthly active rows on Fivetran currently, about 3.5b total events a month if you're looking at events based pricing.\n\nOptions I've seen:\n\nSelf host Airbyte\n\nCloudQuery (might need paid version for MySQL connector?)\n\nMeltano (not sure if this will work at this scale)\n\ndlt (same q as Meltano)\n\nUpsolver (saw them at Coalesce last year but never heard feedback from real users)\n\nAre there any other tools that you would recommend to look at?  Striim, Rivery, etc? Just want to make sure I don't leave any stone unturned before signing a gigantic renewal and wasting money (but also if it's gonna be a headache it's not worth it)", "author_fullname": "t2_ea1aq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are they any lesser known options for MySQL CDC at scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axb0hf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708622791.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708620285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently use Fivetran. It works great, no complaints (other than the price even with a heavy discount). The reliability for all the API connectors is top notch, but I feel like we&amp;#39;re getting eaten alive on our first party data and our data volume is growing rapidly. We&amp;#39;re going from Aurora (AWS) to Snowflake&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking at using something else just for the MySQL stuff (since SQL connectors are supported everywhere and should be reliable). For &amp;quot;scale&amp;quot; I don&amp;#39;t mean petabyte scale, but we are looking at ~2b monthly active rows on Fivetran currently, about 3.5b total events a month if you&amp;#39;re looking at events based pricing.&lt;/p&gt;\n\n&lt;p&gt;Options I&amp;#39;ve seen:&lt;/p&gt;\n\n&lt;p&gt;Self host Airbyte&lt;/p&gt;\n\n&lt;p&gt;CloudQuery (might need paid version for MySQL connector?)&lt;/p&gt;\n\n&lt;p&gt;Meltano (not sure if this will work at this scale)&lt;/p&gt;\n\n&lt;p&gt;dlt (same q as Meltano)&lt;/p&gt;\n\n&lt;p&gt;Upsolver (saw them at Coalesce last year but never heard feedback from real users)&lt;/p&gt;\n\n&lt;p&gt;Are there any other tools that you would recommend to look at?  Striim, Rivery, etc? Just want to make sure I don&amp;#39;t leave any stone unturned before signing a gigantic renewal and wasting money (but also if it&amp;#39;s gonna be a headache it&amp;#39;s not worth it)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axb0hf", "is_robot_indexable": true, "report_reasons": null, "author": "molodyets", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axb0hf/are_they_any_lesser_known_options_for_mysql_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axb0hf/are_they_any_lesser_known_options_for_mysql_cdc/", "subreddit_subscribers": 162887, "created_utc": 1708620285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_insol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can GPT4 + Duckdb write complex queries, inspect them, and provide an analysis of the result? Experimental but making progress. What datasets should i throw at this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_1axab7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/qlm538ubw5kc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1732, "scrubber_media_url": "https://v.redd.it/qlm538ubw5kc1/DASH_96.mp4", "dash_url": "https://v.redd.it/qlm538ubw5kc1/DASHPlaylist.mpd?a=1711233494%2CZmEyZDM0NzgxZjcwMWQ3ZWM4NGY1ZDNmZTNjZGVhYmMxYjM3ZWQyNDQ5MzQ0ZWM0OWE4ZDc0MWQ3MThlMDA4ZA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/qlm538ubw5kc1/HLSPlaylist.m3u8?a=1711233494%2CNDBjMDhkM2U4ZWRjM2RhMWExMGMwMmM5NTg3ZWZmYjdlMDk4NWM4NWM1YWMxZDdkZDk0OWRiYjBjYTc3ODFiNA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0a8c8e53163eb607877c86b114395f7d724b8c91", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708618620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/qlm538ubw5kc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?format=pjpg&amp;auto=webp&amp;s=b8755e0b6cd97ee4a9849261c6a7e7c7f82f6a37", "width": 3464, "height": 2160}, "resolutions": [{"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ff6ffcc66cc1238de2cb072b2b69883ffdcd0bbc", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fd19e151b444143d4147d4c35a47d8523f067fdb", "width": 216, "height": 134}, {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aee7bede43dbee331eaafaa416a09dfb4b7201ce", "width": 320, "height": 199}, {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f65b0ef6b65b3563c608b342d073f98477a539c1", "width": 640, "height": 399}, {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0c5bfc2dc48f7ee4cf40e7f47a82cb7afcb40b27", "width": 960, "height": 598}, {"url": "https://external-preview.redd.it/cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=34d7056d22b208889ca8b2dd674f1deeab04ab60", "width": 1080, "height": 673}], "variants": {}, "id": "cDhyZ2w3NGV3NWtjMcjbVPVRfNEBYWwiiKlxlcyWH-C4zGDoGblfxmJqTxwi"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1axab7a", "is_robot_indexable": true, "report_reasons": null, "author": "ashpreetbedi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axab7a/can_gpt4_duckdb_write_complex_queries_inspect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/qlm538ubw5kc1", "subreddit_subscribers": 162887, "created_utc": 1708618620.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/qlm538ubw5kc1/DASH_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1732, "scrubber_media_url": "https://v.redd.it/qlm538ubw5kc1/DASH_96.mp4", "dash_url": "https://v.redd.it/qlm538ubw5kc1/DASHPlaylist.mpd?a=1711233494%2CZmEyZDM0NzgxZjcwMWQ3ZWM4NGY1ZDNmZTNjZGVhYmMxYjM3ZWQyNDQ5MzQ0ZWM0OWE4ZDc0MWQ3MThlMDA4ZA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/qlm538ubw5kc1/HLSPlaylist.m3u8?a=1711233494%2CNDBjMDhkM2U4ZWRjM2RhMWExMGMwMmM5NTg3ZWZmYjdlMDk4NWM4NWM1YWMxZDdkZDk0OWRiYjBjYTc3ODFiNA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nI know the subject  may look weird to some users here, well it is probably weird indeed. But I will try to explain.\n\nA bit of off topic but needed contextual info: I'm not a data engineer, I've been working as a data analyst, and not the most advanced one (technically-wise) and not in the most advanced environment (not an IT or analytics company). I've been tasked with, what i think, is a data engineering task (there are no developers/data engineers in my company so they came to me), and I have a really rough idea about what i am going to do.  So, basically, I'm a noob, in a desperate need of help. I usually managed to overcome by just googling things, but, to my dismay, i found nothing this time.\n\nSo... the task - my bosses want a cube in Excel. Well, what they exactly want is \"an OLAP cube wired up  into Excel so that you could import it into a pivot table and arbitrarily slice and dice data by dragging and dropping columns\".  I do realize it's a really dated format for frontend, and that there are some really neat BI tools, dashboards or even in-browser solutions, but, as i said, my bosses want exactly it like that (really used to work with excel pivot tables and nothing else) and i come from a non tech-savvy place, to say the least.\n\nI have 2 data sources - an ERP and a CRM. I know some Python and Numpy/Pandas, so I've created a small script that uses an API connection to automatically get data from CRM, dropping some useless data, adding some fields etc. I'm using PyCharm as IDE. Now, ERP is being provided to us by another company, and they have some devs, so they should be able to do the same thing, but from the ERP side of things.\n\nAs you could probably imagine, volumes of data in case of my company would be quite small. No more than 2 million rows with like up to 10 columns.\n\nSo the very first step - having a dedicated (server) machine with some raw data automatically uploaded into its memory as CSV files or stored as Pandas dataframes - is done or at least it's clear to me how it's done. But i will need to look into some orchestrators, so that my python script(s) will be executed every n hours.\n\nBut what comes next - is quite a challenge. As i conceptualize it, it's either:\n\n\\- CSV/Pandas Dataframes -&gt; MS SQL DB -&gt; MS SQL SSAS -&gt; Excel\n\n\\- CSV/Pandas Dataframes -&gt; DuckDB -&gt; Excel\n\nSo, am I aware of sqlite, but it's OLTP, apparently? And there are few additional -  maybe complicated - steps so that an OLAP cube on Excel side may connect to a DB like that, but i'm not sure i want to go through that. I think CSV/Pandas into sqlite into MS SQL SSAS may be an additional option though, not sure?\n\nAfter educating myself a bit about DuckDB, I'm in favor of sticking to it, as it's been marketed as an OLAP solution for data analytics. The part about transforming Pandas dataframes into DB objects, writing queries to tables, fetching queries results etc - is covered quite extensively in documentation and on forums - i should be alright doing all that.\n\nThe most mysterious part comes next, and it's basically why I'm creating this thread.\n\nHow do I connect DuckDB to Excel? How Excel connection wizard - or whatever it's called - might \"see\" a DuckDB database (or should i say DuckDB datawarehouse)? Will it even be able to?\n\nThanks for reading all that, hope somebody might guide me through, would be really grateful.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_1svoqb4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB into Excel Pivot table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax3j11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708599437.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708598694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I know the subject  may look weird to some users here, well it is probably weird indeed. But I will try to explain.&lt;/p&gt;\n\n&lt;p&gt;A bit of off topic but needed contextual info: I&amp;#39;m not a data engineer, I&amp;#39;ve been working as a data analyst, and not the most advanced one (technically-wise) and not in the most advanced environment (not an IT or analytics company). I&amp;#39;ve been tasked with, what i think, is a data engineering task (there are no developers/data engineers in my company so they came to me), and I have a really rough idea about what i am going to do.  So, basically, I&amp;#39;m a noob, in a desperate need of help. I usually managed to overcome by just googling things, but, to my dismay, i found nothing this time.&lt;/p&gt;\n\n&lt;p&gt;So... the task - my bosses want a cube in Excel. Well, what they exactly want is &amp;quot;an OLAP cube wired up  into Excel so that you could import it into a pivot table and arbitrarily slice and dice data by dragging and dropping columns&amp;quot;.  I do realize it&amp;#39;s a really dated format for frontend, and that there are some really neat BI tools, dashboards or even in-browser solutions, but, as i said, my bosses want exactly it like that (really used to work with excel pivot tables and nothing else) and i come from a non tech-savvy place, to say the least.&lt;/p&gt;\n\n&lt;p&gt;I have 2 data sources - an ERP and a CRM. I know some Python and Numpy/Pandas, so I&amp;#39;ve created a small script that uses an API connection to automatically get data from CRM, dropping some useless data, adding some fields etc. I&amp;#39;m using PyCharm as IDE. Now, ERP is being provided to us by another company, and they have some devs, so they should be able to do the same thing, but from the ERP side of things.&lt;/p&gt;\n\n&lt;p&gt;As you could probably imagine, volumes of data in case of my company would be quite small. No more than 2 million rows with like up to 10 columns.&lt;/p&gt;\n\n&lt;p&gt;So the very first step - having a dedicated (server) machine with some raw data automatically uploaded into its memory as CSV files or stored as Pandas dataframes - is done or at least it&amp;#39;s clear to me how it&amp;#39;s done. But i will need to look into some orchestrators, so that my python script(s) will be executed every n hours.&lt;/p&gt;\n\n&lt;p&gt;But what comes next - is quite a challenge. As i conceptualize it, it&amp;#39;s either:&lt;/p&gt;\n\n&lt;p&gt;- CSV/Pandas Dataframes -&amp;gt; MS SQL DB -&amp;gt; MS SQL SSAS -&amp;gt; Excel&lt;/p&gt;\n\n&lt;p&gt;- CSV/Pandas Dataframes -&amp;gt; DuckDB -&amp;gt; Excel&lt;/p&gt;\n\n&lt;p&gt;So, am I aware of sqlite, but it&amp;#39;s OLTP, apparently? And there are few additional -  maybe complicated - steps so that an OLAP cube on Excel side may connect to a DB like that, but i&amp;#39;m not sure i want to go through that. I think CSV/Pandas into sqlite into MS SQL SSAS may be an additional option though, not sure?&lt;/p&gt;\n\n&lt;p&gt;After educating myself a bit about DuckDB, I&amp;#39;m in favor of sticking to it, as it&amp;#39;s been marketed as an OLAP solution for data analytics. The part about transforming Pandas dataframes into DB objects, writing queries to tables, fetching queries results etc - is covered quite extensively in documentation and on forums - i should be alright doing all that.&lt;/p&gt;\n\n&lt;p&gt;The most mysterious part comes next, and it&amp;#39;s basically why I&amp;#39;m creating this thread.&lt;/p&gt;\n\n&lt;p&gt;How do I connect DuckDB to Excel? How Excel connection wizard - or whatever it&amp;#39;s called - might &amp;quot;see&amp;quot; a DuckDB database (or should i say DuckDB datawarehouse)? Will it even be able to?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading all that, hope somebody might guide me through, would be really grateful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ax3j11", "is_robot_indexable": true, "report_reasons": null, "author": "whoooooaaaaaaaaaaaa", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax3j11/duckdb_into_excel_pivot_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax3j11/duckdb_into_excel_pivot_table/", "subreddit_subscribers": 162887, "created_utc": 1708598694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your thoughts on Data Vault modeling?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to Data Vault when not to Data Vault?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axd8vc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708625453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your thoughts on Data Vault modeling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axd8vc", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axd8vc/when_to_data_vault_when_not_to_data_vault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axd8vc/when_to_data_vault_when_not_to_data_vault/", "subreddit_subscribers": 162887, "created_utc": 1708625453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAm currently facing one big challenge at work. \n\nI joined a company 8 months ago. We are building a Saas product that mainly exposes some AI analysis results with a Frontend app. The system is mainly based on an in house ETL solution.   \nWe load the data in Postgres, transfer it to Elasticsearch for analysis systems, then we load the results back to Postgres for a back office app, and then  we serialize the results to Elasticsearch again in an easily queryable format. The results are exploited mainly for analytical workloads.  \n\n\nWe found the process really slow and painfull. Especially when we have to reprocess the data for some diverse reasons. For example, when we adjusts our AI models or when we change some of the labels associated with data, we have to reprocess the data in order to reflect the changes. Also we found Elasticsearch not to be resilient to some high writes load, causing the process to stop due to some 429 Too many requests response. Maybe we can handle this differently. Any advice would be welcomed.  \n\n\nThe solution we are trying to push these days, is to use only postgres for the whole analysis and reduce the data moving during analysis (since its heavily error prone and fragile). This way our pipeline would only analyse the data present and update the results in one place (postgres).\n\n  \nDo you think its possible to achieve this without denormalisation (as we doing today) ?  \n\n\nI would like to hear your taugths about this and how you would solve it ?   \n", "author_fullname": "t2_bimho7u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axcyhb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708632770.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708624812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Am currently facing one big challenge at work. &lt;/p&gt;\n\n&lt;p&gt;I joined a company 8 months ago. We are building a Saas product that mainly exposes some AI analysis results with a Frontend app. The system is mainly based on an in house ETL solution.&lt;br/&gt;\nWe load the data in Postgres, transfer it to Elasticsearch for analysis systems, then we load the results back to Postgres for a back office app, and then  we serialize the results to Elasticsearch again in an easily queryable format. The results are exploited mainly for analytical workloads.  &lt;/p&gt;\n\n&lt;p&gt;We found the process really slow and painfull. Especially when we have to reprocess the data for some diverse reasons. For example, when we adjusts our AI models or when we change some of the labels associated with data, we have to reprocess the data in order to reflect the changes. Also we found Elasticsearch not to be resilient to some high writes load, causing the process to stop due to some 429 Too many requests response. Maybe we can handle this differently. Any advice would be welcomed.  &lt;/p&gt;\n\n&lt;p&gt;The solution we are trying to push these days, is to use only postgres for the whole analysis and reduce the data moving during analysis (since its heavily error prone and fragile). This way our pipeline would only analyse the data present and update the results in one place (postgres).&lt;/p&gt;\n\n&lt;p&gt;Do you think its possible to achieve this without denormalisation (as we doing today) ?  &lt;/p&gt;\n\n&lt;p&gt;I would like to hear your taugths about this and how you would solve it ?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axcyhb", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Fudge9617", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axcyhb/alternatives_to_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axcyhb/alternatives_to_etl/", "subreddit_subscribers": 162887, "created_utc": 1708624812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nIntroducing version v0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our [website](https://squirrels-nest.github.io/)!\n\nFollowing our version v0.1.0 release in our previous post [here](https://www.reddit.com/r/dataengineering/comments/173k91u/made_a_python_package_for_creating_api_endpoints/), we've recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to v0.1.0, the new v0.2.0 version has incorporated the following features:\n\n* ***Flexible Lineage for Data Models***\n   * Create data models (in SQL or Python) as database views or federates. Federate models provide a \"ref\" function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you.\n   * You can query different database systems with different database views and join the results together in a federate model!\n* ***Consolidated Parameters***\n   * Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses.\n* ***Authentication***\n   * You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time!\n   * We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users.\n\nFeel free to give Squirrels a whirl by following our simple tutorial listed on our [tutorial page](https://squirrels-nest.github.io/docs/category/tutorial)! And here's the link to our GitHub repo: [https://github.com/squirrels-nest/squirrels](https://github.com/squirrels-nest/squirrels)\n\nAlso, in addition to commenting below, feel free to reach out to us via DM or on our new [Discord Server](https://discord.gg/cRDrRA7T)!\n\n**Target Audience:**\n\nData Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc.\n\n**Similar Projects:**\n\nWe've come across several questions regarding \"What's the difference between Squirrels and &lt;some other data tool&gt;?\". Below are a few of the points on some commonly asked about comparisons.\n\n* dbt:\n   * Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead)\n   * Does not support widget parameters (and does not need to for non-customer-facing data transformations)\n* cube.js:\n   * Data models are created in YAML (for Squirrels, they're created in SQL or Python)\n   * Cannot create a lineage of multiple data models under one API endpoint\n   * No support for specifying widget parameters properties to dynamically change dataset behavior\n* VulcanSQL\n   * No support for Python models and cannot create a lineage of multiple data models under one API endpoint\n   * Can specify parameters, but it's very \"free-form\" with limited control over the typing of parameters for the client (\"typing\" such as dropdown parameter, date parameter, etc.)\n   * Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source.\n\nThus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up.", "author_fullname": "t2_kjvfum79o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Version of Squirrels: An Open Source REST API Framework with Dynamic Queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax8yhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708615330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Introducing version v0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our &lt;a href=\"https://squirrels-nest.github.io/\"&gt;website&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;Following our version v0.1.0 release in our previous post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/173k91u/made_a_python_package_for_creating_api_endpoints/\"&gt;here&lt;/a&gt;, we&amp;#39;ve recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to v0.1.0, the new v0.2.0 version has incorporated the following features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Flexible Lineage for Data Models&lt;/em&gt;&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create data models (in SQL or Python) as database views or federates. Federate models provide a &amp;quot;ref&amp;quot; function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you.&lt;/li&gt;\n&lt;li&gt;You can query different database systems with different database views and join the results together in a federate model!&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Consolidated Parameters&lt;/em&gt;&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Authentication&lt;/em&gt;&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time!&lt;/li&gt;\n&lt;li&gt;We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Feel free to give Squirrels a whirl by following our simple tutorial listed on our &lt;a href=\"https://squirrels-nest.github.io/docs/category/tutorial\"&gt;tutorial page&lt;/a&gt;! And here&amp;#39;s the link to our GitHub repo: &lt;a href=\"https://github.com/squirrels-nest/squirrels\"&gt;https://github.com/squirrels-nest/squirrels&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, in addition to commenting below, feel free to reach out to us via DM or on our new &lt;a href=\"https://discord.gg/cRDrRA7T\"&gt;Discord Server&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Target Audience:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Data Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Similar Projects:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve come across several questions regarding &amp;quot;What&amp;#39;s the difference between Squirrels and &amp;lt;some other data tool&amp;gt;?&amp;quot;. Below are a few of the points on some commonly asked about comparisons.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dbt:\n\n&lt;ul&gt;\n&lt;li&gt;Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead)&lt;/li&gt;\n&lt;li&gt;Does not support widget parameters (and does not need to for non-customer-facing data transformations)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;cube.js:\n\n&lt;ul&gt;\n&lt;li&gt;Data models are created in YAML (for Squirrels, they&amp;#39;re created in SQL or Python)&lt;/li&gt;\n&lt;li&gt;Cannot create a lineage of multiple data models under one API endpoint&lt;/li&gt;\n&lt;li&gt;No support for specifying widget parameters properties to dynamically change dataset behavior&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;VulcanSQL\n\n&lt;ul&gt;\n&lt;li&gt;No support for Python models and cannot create a lineage of multiple data models under one API endpoint&lt;/li&gt;\n&lt;li&gt;Can specify parameters, but it&amp;#39;s very &amp;quot;free-form&amp;quot; with limited control over the typing of parameters for the client (&amp;quot;typing&amp;quot; such as dropdown parameter, date parameter, etc.)&lt;/li&gt;\n&lt;li&gt;Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ax8yhf", "is_robot_indexable": true, "report_reasons": null, "author": "squirrels-api", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax8yhf/new_version_of_squirrels_an_open_source_rest_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax8yhf/new_version_of_squirrels_an_open_source_rest_api/", "subreddit_subscribers": 162887, "created_utc": 1708615330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as the title states I'm taking on a new role with some clients I used to manage 8+ years ago.\n\nThese clients use sophisticated rate shoppers that analyze the package economics (dimensions, size, weight etc) and picks the most optimal (primarily price but some include on-time performance) parcel carrier to tender the package to.\n\nMy job is to come in and try to maximize our volume and by extension increase our revenue and package market share via optimizing the rate shopper.\n\nMy initial thoughts are to see where our existing volume falls within a rate card.   A rate card is a simple excel file that has individual weights listed ie 1lb, 2lb, 3lb, (up to 150lbs) down the rows and zones (essentially distance) across the columns.   Some carriers have 10 to 16 zones so it's not apples to apples.  As an example a 5lb zone 9 = rate\n\nMy original thought is to place existing volumes into each weight and zone to see if any patterns standout.  I think we would need ALL the volume to see where we're missing.\n\nAny other guidance or strategies, AI solution, that could help analyze the data.\n\n&amp;#x200B;\n\nTy!\n\nEDIT: PS I love playing with data but I'm an amateur \n\n&amp;#x200B;", "author_fullname": "t2_33ybrs28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taking a new role that requires analytics - my org isn't the best at this - Transportation and Logistics Industry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax8eyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708613975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title states I&amp;#39;m taking on a new role with some clients I used to manage 8+ years ago.&lt;/p&gt;\n\n&lt;p&gt;These clients use sophisticated rate shoppers that analyze the package economics (dimensions, size, weight etc) and picks the most optimal (primarily price but some include on-time performance) parcel carrier to tender the package to.&lt;/p&gt;\n\n&lt;p&gt;My job is to come in and try to maximize our volume and by extension increase our revenue and package market share via optimizing the rate shopper.&lt;/p&gt;\n\n&lt;p&gt;My initial thoughts are to see where our existing volume falls within a rate card.   A rate card is a simple excel file that has individual weights listed ie 1lb, 2lb, 3lb, (up to 150lbs) down the rows and zones (essentially distance) across the columns.   Some carriers have 10 to 16 zones so it&amp;#39;s not apples to apples.  As an example a 5lb zone 9 = rate&lt;/p&gt;\n\n&lt;p&gt;My original thought is to place existing volumes into each weight and zone to see if any patterns standout.  I think we would need ALL the volume to see where we&amp;#39;re missing.&lt;/p&gt;\n\n&lt;p&gt;Any other guidance or strategies, AI solution, that could help analyze the data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ty!&lt;/p&gt;\n\n&lt;p&gt;EDIT: PS I love playing with data but I&amp;#39;m an amateur &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax8eyh", "is_robot_indexable": true, "report_reasons": null, "author": "tylerhill11", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax8eyh/taking_a_new_role_that_requires_analytics_my_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax8eyh/taking_a_new_role_that_requires_analytics_my_org/", "subreddit_subscribers": 162887, "created_utc": 1708613975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_579zp4ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Awesome Open Source Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax5xal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Qf21_qHt4hF6GhEG3IDa9tYHd5_yBKbNLn2ptvhVPFE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708607012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/pracdata/awesome-open-source-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?auto=webp&amp;s=1e826c0c4c2f10e3fdab1519962fe1b2e16d9966", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5c722dd50754be678a9f98a080b389df4318ef5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79cf4cb9fa8e84790de88c1ca4f774ad7d1e5ad9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=44be72a8902261109e36b7d507f0d7cada0cf54f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=571efbe36091f5616320704eb296430d79dd7387", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c31945bc32fa1424129159c7551129148103d16", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/dQ5nfplg6DuDOPoZHn2Nv2sEEM-LMGu7iXxCPcM6jwE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=084abacb4fd06282455bd9c0f3e52313b2d0f75c", "width": 1080, "height": 540}], "variants": {}, "id": "jGtX2uBGRqXYeivLwDl4nSO9BxdbH0qX33SH34ixZIc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ax5xal", "is_robot_indexable": true, "report_reasons": null, "author": "ithoughtful", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax5xal/awesome_open_source_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/pracdata/awesome-open-source-data-engineering", "subreddit_subscribers": 162887, "created_utc": 1708607012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a Data Engineer. I worked as a freelancer for past 4 months. Before that I lost my job due to layoffs and was having overall experience of 2 yrs. Now I got an offer from a company for which I have said that I have overall experience of 2.4 yrs ( Including Freelancing work). How can I show that I really did freelancing work. I have no idea I am new to this. Please guide.", "author_fullname": "t2_i159isc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to show my freelancing experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax8av0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708613669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a Data Engineer. I worked as a freelancer for past 4 months. Before that I lost my job due to layoffs and was having overall experience of 2 yrs. Now I got an offer from a company for which I have said that I have overall experience of 2.4 yrs ( Including Freelancing work). How can I show that I really did freelancing work. I have no idea I am new to this. Please guide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ax8av0", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Zookeepergame256", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax8av0/how_to_show_my_freelancing_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax8av0/how_to_show_my_freelancing_experience/", "subreddit_subscribers": 162887, "created_utc": 1708613669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a debate within my team about when it is appropriate to create mini dimensions in a dimensional model. I always thought - based on experience and documentation I've read, that the reasons to create minidimensions are:\n\n* attributes that are frequently consumed together (and usually also separately from the main dimension attributes). For example, I might have a large CONTRACT dimension. I might want to isolate CONTRACT attributes such as RETIRED\\_CUSTOMER\\_FLAG and VULNERABLE\\_CUSTOMER\\_FLAG, if these two attributes are frequently used together in reports, especially if they are used in reports without any other CONTRACT attributes. Thus, I avoid joining the larger CONTRACT dimension for a report that only requires these flags.\n* attributes that change frequently. I might have a CLIENT\\_IN\\_DEBT flag, which I want to keep separate from the main CLIENT dimension, because I noticed that this flag changes frequently, and I do not want to have to insert a new line in the CLIENT dimension each time this happens. Instead the matrix aspect of the minidimension helps me here.\n\nOn the other hand, I have seen people who tend to overuse minidimensions: they create 2-3 minidimensions for each dimension. In these minidimensions, they put attributes with low cardinality. Even for attributes with low cardinality, the minidim can grow quite big if you have 6-7 attributes already. They claim it is more efficient this way, though I can't really see how.\n\nWhat's your reason for using minidimensions? How often do you use them? Would love some documentation if you can provide (other than Kimball).\n\nTHANK YOU!\n\n\\*If not clear what I mean by dimensions/minidimensions: [https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity](https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity)", "author_fullname": "t2_3dghdpvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you create minidimensions in a dimensional model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax719q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708610364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a debate within my team about when it is appropriate to create mini dimensions in a dimensional model. I always thought - based on experience and documentation I&amp;#39;ve read, that the reasons to create minidimensions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;attributes that are frequently consumed together (and usually also separately from the main dimension attributes). For example, I might have a large CONTRACT dimension. I might want to isolate CONTRACT attributes such as RETIRED_CUSTOMER_FLAG and VULNERABLE_CUSTOMER_FLAG, if these two attributes are frequently used together in reports, especially if they are used in reports without any other CONTRACT attributes. Thus, I avoid joining the larger CONTRACT dimension for a report that only requires these flags.&lt;/li&gt;\n&lt;li&gt;attributes that change frequently. I might have a CLIENT_IN_DEBT flag, which I want to keep separate from the main CLIENT dimension, because I noticed that this flag changes frequently, and I do not want to have to insert a new line in the CLIENT dimension each time this happens. Instead the matrix aspect of the minidimension helps me here.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On the other hand, I have seen people who tend to overuse minidimensions: they create 2-3 minidimensions for each dimension. In these minidimensions, they put attributes with low cardinality. Even for attributes with low cardinality, the minidim can grow quite big if you have 6-7 attributes already. They claim it is more efficient this way, though I can&amp;#39;t really see how.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your reason for using minidimensions? How often do you use them? Would love some documentation if you can provide (other than Kimball).&lt;/p&gt;\n\n&lt;p&gt;THANK YOU!&lt;/p&gt;\n\n&lt;p&gt;*If not clear what I mean by dimensions/minidimensions: &lt;a href=\"https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity\"&gt;https://www.ibm.com/docs/en/bfmdw/8.9.1?topic=elements-dimension-entity&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ax719q", "is_robot_indexable": true, "report_reasons": null, "author": "MaLinChao", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax719q/when_do_you_create_minidimensions_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax719q/when_do_you_create_minidimensions_in_a/", "subreddit_subscribers": 162887, "created_utc": 1708610364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Folks,\n\n&amp;#x200B;\n\nI just attended a data infrastructure conference, and more than half of the presentations mentioned ontologies.  \n\n\nI just want to survey this group on its thoughts on the topic.  \n\n\nI personally kinda hate them and think they are antiquated in the age of LLMs and AI generally.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_ddwwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "General Thoughts on Ontologies, Knowledge Graphs, SPARQL, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax69pr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708608106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I just attended a data infrastructure conference, and more than half of the presentations mentioned ontologies.  &lt;/p&gt;\n\n&lt;p&gt;I just want to survey this group on its thoughts on the topic.  &lt;/p&gt;\n\n&lt;p&gt;I personally kinda hate them and think they are antiquated in the age of LLMs and AI generally.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax69pr", "is_robot_indexable": true, "report_reasons": null, "author": "Jimmyfatz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax69pr/general_thoughts_on_ontologies_knowledge_graphs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax69pr/general_thoughts_on_ontologies_knowledge_graphs/", "subreddit_subscribers": 162887, "created_utc": 1708608106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is Oracle Data Integrator (ODI) a kind of orchestrator ? Or is it just a ETL/ELT tool?\n\nHow do you use ODI in your tasks?", "author_fullname": "t2_rvp7wzgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I classify the Oracle Data Integrator (ODI)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax4nas", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708602867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is Oracle Data Integrator (ODI) a kind of orchestrator ? Or is it just a ETL/ELT tool?&lt;/p&gt;\n\n&lt;p&gt;How do you use ODI in your tasks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ax4nas", "is_robot_indexable": true, "report_reasons": null, "author": "nivlek_miroma", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax4nas/how_can_i_classify_the_oracle_data_integrator_odi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ax4nas/how_can_i_classify_the_oracle_data_integrator_odi/", "subreddit_subscribers": 162887, "created_utc": 1708602867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Glue ETL jobs let's you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it's first time I hear about Ray what is it?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ray vs Spark for AWS Glue jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axfi1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708630793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Glue ETL jobs let&amp;#39;s you pick a distributed computing framework. Which one do you go for and when? I read Spark book, but it&amp;#39;s first time I hear about Ray what is it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1axfi1x", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axfi1x/ray_vs_spark_for_aws_glue_jobs/", "subreddit_subscribers": 162887, "created_utc": 1708630793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Checking many websites or newsletters every day? Feedly?  \nIs there something similar to TechCrunch for data industry?  \nHow do you keep up with new tools or database releases?\n\nI'm building a news aggregator/feed for me and my team, and hopefully others ([here the is link to it](https://recactus.net/eye/), it\u2019s still under development tho), and I\u2019m looking for news sources to improve the feed.", "author_fullname": "t2_125gsz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you stay up to date with industry tools and changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axdu88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708626811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Checking many websites or newsletters every day? Feedly?&lt;br/&gt;\nIs there something similar to TechCrunch for data industry?&lt;br/&gt;\nHow do you keep up with new tools or database releases?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a news aggregator/feed for me and my team, and hopefully others (&lt;a href=\"https://recactus.net/eye/\"&gt;here the is link to it&lt;/a&gt;, it\u2019s still under development tho), and I\u2019m looking for news sources to improve the feed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axdu88", "is_robot_indexable": true, "report_reasons": null, "author": "pebolax", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axdu88/how_do_you_stay_up_to_date_with_industry_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axdu88/how_do_you_stay_up_to_date_with_industry_tools/", "subreddit_subscribers": 162887, "created_utc": 1708626811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been in the data engineering world for several years. Lately I've been increasingly interested in the management side of things, especially data governance. Is it worthwhile to delve deeper into this field through a more formal path, such as a diploma or year-long course?\n\nabout \"as a data engineer\" I mean that I see a lot of people from other fields like law in this kind of teams and not many from engineering.\n\nThanks", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it worth studying data governance as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1axcz59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708624849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been in the data engineering world for several years. Lately I&amp;#39;ve been increasingly interested in the management side of things, especially data governance. Is it worthwhile to delve deeper into this field through a more formal path, such as a diploma or year-long course?&lt;/p&gt;\n\n&lt;p&gt;about &amp;quot;as a data engineer&amp;quot; I mean that I see a lot of people from other fields like law in this kind of teams and not many from engineering.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1axcz59", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axcz59/is_it_worth_studying_data_governance_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axcz59/is_it_worth_studying_data_governance_as_a_data/", "subreddit_subscribers": 162887, "created_utc": 1708624849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_yeda6sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Airflow Alternatives for Data Orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_1ax9d7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ErnUAlKnDZfL2lxevo_B6XR7MQUdazhvhL0UpeCeCuY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708616362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/5-airflow-alternatives-for-data-orchestration", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?auto=webp&amp;s=a6e09eb7a5c4bb184c26cb803c3fe586d04b874a", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fcbfc472cdaf5e6adbbdbcc35fa25a6775c80c9", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cda9d0873955365237c44f0044afb3e6bee9500e", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e1c769ad61c10817f3427b932369fd5d7dfa1a3", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cc6eb2e6aaf3be4a44add27a2b9388e819b38d1", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/hG98p0Pz4Ofm51odBO_mnbsSLJmN_LIVCj_C39a-FSw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6de7c6238f88f80bc1f980ef939843b7c6720e9", "width": 960, "height": 576}], "variants": {}, "id": "8-MIt8kQiGL-ZgGM0VyEPSS05YRLNzzK3emWNB1qFNY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ax9d7f", "is_robot_indexable": true, "report_reasons": null, "author": "kingabzpro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ax9d7f/5_airflow_alternatives_for_data_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/5-airflow-alternatives-for-data-orchestration", "subreddit_subscribers": 162887, "created_utc": 1708616362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were interested originally they were brought in as varchar(max). I've been talked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?", "author_fullname": "t2_bz1aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate data type selection for SQL tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1axit9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708638541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your best ways to automate the data type selection process when crafting tables. Currently I need to update some tables that have hundreds of columns coming from netsuite. When they were interested originally they were brought in as varchar(max). I&amp;#39;ve been talked with correcting this. Any suggestions of ways to make this easier? I did one manually and it was rough. I know I can load a sample of the table into pandas to get suggestions is that my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1axit9z", "is_robot_indexable": true, "report_reasons": null, "author": "Im_probably_naked", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1axit9z/automate_data_type_selection_for_sql_tables/", "subreddit_subscribers": 162887, "created_utc": 1708638541.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}