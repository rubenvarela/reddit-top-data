{"kind": "Listing", "data": {"after": "t3_1atcwjw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After careful consideration and listening to your feedback, we've decided to no longer allow interview-related posts because they take away focus from our community's main purpose.\n\nIn the past, although they usually weren't directly related to data engineering we've allowed interview posts like \"What are interviews like at XYZ company?\" or \"What should I prepare/study for XYZ position?\"\n\nThese questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.\n\nThank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update to interview posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at7mdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708191888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After careful consideration and listening to your feedback, we&amp;#39;ve decided to no longer allow interview-related posts because they take away focus from our community&amp;#39;s main purpose.&lt;/p&gt;\n\n&lt;p&gt;In the past, although they usually weren&amp;#39;t directly related to data engineering we&amp;#39;ve allowed interview posts like &amp;quot;What are interviews like at XYZ company?&amp;quot; or &amp;quot;What should I prepare/study for XYZ position?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;These questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.&lt;/p&gt;\n\n&lt;p&gt;Thank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1at7mdg", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "subreddit_subscribers": 161585, "created_utc": 1708191888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a BI developer looking to upgrade by solidifying my ETL/Pipelining skills. I read many times that a great start would be to read the book Fundamentals of Data Engineering, but I happen to absorb concepts better through video. Any course equivalent that you recommend?", "author_fullname": "t2_144emf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any video course equivalent of the fundamentals of data engineering out there?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atbt4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708202582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a BI developer looking to upgrade by solidifying my ETL/Pipelining skills. I read many times that a great start would be to read the book Fundamentals of Data Engineering, but I happen to absorb concepts better through video. Any course equivalent that you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atbt4x", "is_robot_indexable": true, "report_reasons": null, "author": "Alno1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atbt4x/any_video_course_equivalent_of_the_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atbt4x/any_video_course_equivalent_of_the_fundamentals/", "subreddit_subscribers": 161585, "created_utc": 1708202582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When do you use which one?\n\nWhich is better?\n\nUsing auto-generating PK:\n\n`CREATE TABLE fact_orderlines (`  \n`order_id INTEGER IDENTITY(1,1) PRIMARY KEY, -- SERIAL not supported use IDENTITY instead`  \n`product_id INTEGER,`  \n`category_id INTEGER,`  \n`date_id INTEGER,`  \n`market_id INTEGER,`  \n`customer_id INTEGER,`  \n`amount DECIMAL(10,2) \u00a0-- MONEY data type is not supported, use DECIMAL instead`  \n`);`\n\nOr using a composite key:\n\n`ALTER TABLE fact_orderlines ADD CONSTRAINT pk_fact_orderlines PRIMARY KEY (product_id, category_id, date_id, market_id, customer_id);`", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use surrogate key or composite key in fact tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at32dw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708179699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When do you use which one?&lt;/p&gt;\n\n&lt;p&gt;Which is better?&lt;/p&gt;\n\n&lt;p&gt;Using auto-generating PK:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE fact_orderlines (&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;order_id INTEGER IDENTITY(1,1) PRIMARY KEY, -- SERIAL not supported use IDENTITY instead&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;product_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;category_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;date_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;market_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;customer_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;amount DECIMAL(10,2) \u00a0-- MONEY data type is not supported, use DECIMAL instead&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;);&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Or using a composite key:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE fact_orderlines ADD CONSTRAINT pk_fact_orderlines PRIMARY KEY (product_id, category_id, date_id, market_id, customer_id);&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at32dw", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at32dw/do_you_use_surrogate_key_or_composite_key_in_fact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at32dw/do_you_use_surrogate_key_or_composite_key_in_fact/", "subreddit_subscribers": 161585, "created_utc": 1708179699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I updated Geoglify on GitHub. It transforms real-world geospatial data into patterns or shapes. Currently supports 40 patterns, but can load more.\n\nhttps://github.com/geoglify/geoglify", "author_fullname": "t2_7svy5qp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geoglify: now supports visualizing geo data through patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1at0gkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_96.mp4", "dash_url": "https://v.redd.it/3ryx4ffyy4jc1/DASHPlaylist.mpd?a=1710841086%2CMzdhYzQ3MDUxYWQ3MmNhODg3NGQyMTAxMTI5MGU2ZjY2YTYzY2Q4ZjAxOTRmOTBhZjRiOTQ3OWQ0ODBlMWExNw%3D%3D&amp;v=1&amp;f=sd", "duration": 31, "hls_url": "https://v.redd.it/3ryx4ffyy4jc1/HLSPlaylist.m3u8?a=1710841086%2CNmQxMzYzZjJlOTk4MjcwNzBkNWQxYTE4NWQ3YTYyMzdlMWNiNjBhNjkyY2ZhZGU3OWZjODYyZDJhMDIzYTI3NQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=040417f63f365cd148d663615fcfa758dfbd2f54", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708171529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I updated Geoglify on GitHub. It transforms real-world geospatial data into patterns or shapes. Currently supports 40 patterns, but can load more.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/geoglify/geoglify\"&gt;https://github.com/geoglify/geoglify&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/3ryx4ffyy4jc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?format=pjpg&amp;auto=webp&amp;s=8ca885121e34f4432f561f532d7d666af7aee7e5", "width": 720, "height": 405}, "resolutions": [{"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=50a29c0e6d76bfb3516ee960e4e685224515fd78", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ccd209a17902d39fb6ce537639316785ff9afe34", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=60b6a5aa98cc75dc95c8515d09598ac58d221165", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=379574c1041f2be8125269b1f0d09f50c14d673a", "width": 640, "height": 360}], "variants": {}, "id": "NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1at0gkz", "is_robot_indexable": true, "report_reasons": null, "author": "leoneljdias", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at0gkz/geoglify_now_supports_visualizing_geo_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/3ryx4ffyy4jc1", "subreddit_subscribers": 161585, "created_utc": 1708171529.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_96.mp4", "dash_url": "https://v.redd.it/3ryx4ffyy4jc1/DASHPlaylist.mpd?a=1710841086%2CMzdhYzQ3MDUxYWQ3MmNhODg3NGQyMTAxMTI5MGU2ZjY2YTYzY2Q4ZjAxOTRmOTBhZjRiOTQ3OWQ0ODBlMWExNw%3D%3D&amp;v=1&amp;f=sd", "duration": 31, "hls_url": "https://v.redd.it/3ryx4ffyy4jc1/HLSPlaylist.m3u8?a=1710841086%2CNmQxMzYzZjJlOTk4MjcwNzBkNWQxYTE4NWQ3YTYyMzdlMWNiNjBhNjkyY2ZhZGU3OWZjODYyZDJhMDIzYTI3NQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello \n\nI am confused about this technology. \n\nWe are a heavy snowflake and AWS shop. \n\nWhat are some use cases of Spark on such an org if any?  Are there things that could be done more efficiently or made us more productive?\n\nI understand it\u2019s hard to give advices without the specifics. Please ask me questions and I\u2019ll try to answer without getting in trouble at work. \n\nThank you. ", "author_fullname": "t2_lcc2eyxje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Question - Use cases of Spark if we are on Snowflake, AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at1rel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708228616.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708175834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;/p&gt;\n\n&lt;p&gt;I am confused about this technology. &lt;/p&gt;\n\n&lt;p&gt;We are a heavy snowflake and AWS shop. &lt;/p&gt;\n\n&lt;p&gt;What are some use cases of Spark on such an org if any?  Are there things that could be done more efficiently or made us more productive?&lt;/p&gt;\n\n&lt;p&gt;I understand it\u2019s hard to give advices without the specifics. Please ask me questions and I\u2019ll try to answer without getting in trouble at work. &lt;/p&gt;\n\n&lt;p&gt;Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at1rel", "is_robot_indexable": true, "report_reasons": null, "author": "20231027", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at1rel/beginner_question_use_cases_of_spark_if_we_are_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at1rel/beginner_question_use_cases_of_spark_if_we_are_on/", "subreddit_subscribers": 161585, "created_utc": 1708175834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will be focusing on SQL as the first tech stack because SQL is the core of DE and anything related to data. This blog will cover what you need to learn, the resources to learn as well as where you need to practice it.\n\n**Step 1**: If you are completely new to SQL and want to understand the basics you can [refer](https://www.w3schools.com/sql/). Next refer [this playlist](https://www.youtube.com/watch?v=7GVFYt6_ZFM&amp;list=PL08903FB7ACA1C2FB) to help you in understanding things like select, from, where, joins, indexes, window functions, stored procedures, functions, triggers.\n\n**Step 2**: Next, you need to understand how the execution order of SQL is, as long as you do not understand the flow of your data in your queries you will not be able to visualize how to write queries or at least efficient ones. [This video](https://youtu.be/uEmAvzuA7u8) should help you with that.\n\n**Step 3**: I feel that you should understand how queries are written, what should be the approach to solve a problem and for this you need to develop an intuition. I will recommend first going through the channel I mentioned in Step 2, [link](https://www.youtube.com/@ankitbansal6).\n\nHe has 4 playlists, which has really taken my query writing skills to next level. I will recommend watching them in below order:\n\n1. [SQL Tips and Tricks](https://www.youtube.com/watch?v=4xPxGX4mfb4&amp;list=PLBTZqjSKn0IcR6DhoLUibOG8frnWbZdSH)\n2. [SQL Medium Complex Interview Problems](https://www.youtube.com/watch?v=dOLBRfwzYcU&amp;list=PLBTZqjSKn0IfuIqbMIqzS-waofsPHMS0E)\n3. [Leetcode SQL Hard Problems](https://www.youtube.com/watch?v=tDfAo7uw-3w&amp;list=PLBTZqjSKn0IfULLRo9Tm4lESxYMAG7fUQ)\n4. [Complex SQL questions for Interview Preparation](https://www.youtube.com/watch?v=qyAgWL066Vo&amp;list=PLBTZqjSKn0IeKBQDjLmzisazhqQy4iGkb)\n\nI will recommend after watching the above playlists, to get your hands dirty. So far you have gained all the theoretical knowledge required now you need to start practicing it on a daily basis. The channel has the DDL and DML statements for almost all his videos in the description, if you do not have a local setup, you can use [this](https://sqliteonline.com/) and run the commands over there and start writing queries.\n\nNow that you are through his playlists twice the questions will feel repetitive and you have gained the querying experience needed to handle problems on your own which brings us to step 4.\n\n**Step 4**: Start Practicing.\n\nLearning by just watching will help clear your concepts but you will need to continuously apply them to be really comfortable with it and SQL query writing practice should be done daily even if it's just 1 or 2 problems everyday. After you are done practicing the Ankit Bansal playlists you can start with below:\n\n1. Hackerrank.\n2. Go to leetcode and solve all the free questions from easy, medium and then hard level.\n3. Stratascratch has good questions so you can solve the free questions over there too.\n\nBy now, you should be really good at writing complex queries. If you have followed properly then by step 3 itself you should be good at joins, windows functions, group by, cte, etc.\n\n**Step 5**: Understand some concepts mentioned below.\n\nYou can do this step in parallel with other steps. Below are some of the concepts you need to be aware about and I will be adding pointers and updating links if and as I find them.\n\n1. SQL Indexes, types, uses\n2. [SCD Types](https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types)\n3. OLAP vs OLTP\n4. Normalization vs Denormalization\n5. Normalization Forms\n6. Data Warehouse Concepts \\[[English](https://youtu.be/h0j0QN2b57M?list=PL_c9BZzLwBRK0Pc28IdvPQizD2mJlgoID), [Hindi](https://www.youtube.com/watch?v=UiTvqSd52ak&amp;list=PLTsNSGeIpGnGP8A74Ie1PgqHhewsqD3fv)\\]\n\n[Link to Table of Content Post](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nThank You..!! Please do let me know in comments if you liked the blog, if there is anything else you want to ask related to SQL or if there are any constructive criticism you would like to give.\n\nNote: This blog has focused on relational SQL, NoSQL is something I was aware of on a very very high level but I was honest with interviewers and told them I had no experience with NoSQL and they were okay with it (most did not bother asking about it anyways).", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog 2 - Learning SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asxqdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708175667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708160709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will be focusing on SQL as the first tech stack because SQL is the core of DE and anything related to data. This blog will cover what you need to learn, the resources to learn as well as where you need to practice it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: If you are completely new to SQL and want to understand the basics you can &lt;a href=\"https://www.w3schools.com/sql/\"&gt;refer&lt;/a&gt;. Next refer &lt;a href=\"https://www.youtube.com/watch?v=7GVFYt6_ZFM&amp;amp;list=PL08903FB7ACA1C2FB\"&gt;this playlist&lt;/a&gt; to help you in understanding things like select, from, where, joins, indexes, window functions, stored procedures, functions, triggers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Next, you need to understand how the execution order of SQL is, as long as you do not understand the flow of your data in your queries you will not be able to visualize how to write queries or at least efficient ones. &lt;a href=\"https://youtu.be/uEmAvzuA7u8\"&gt;This video&lt;/a&gt; should help you with that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: I feel that you should understand how queries are written, what should be the approach to solve a problem and for this you need to develop an intuition. I will recommend first going through the channel I mentioned in Step 2, &lt;a href=\"https://www.youtube.com/@ankitbansal6\"&gt;link&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;He has 4 playlists, which has really taken my query writing skills to next level. I will recommend watching them in below order:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=4xPxGX4mfb4&amp;amp;list=PLBTZqjSKn0IcR6DhoLUibOG8frnWbZdSH\"&gt;SQL Tips and Tricks&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=dOLBRfwzYcU&amp;amp;list=PLBTZqjSKn0IfuIqbMIqzS-waofsPHMS0E\"&gt;SQL Medium Complex Interview Problems&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=tDfAo7uw-3w&amp;amp;list=PLBTZqjSKn0IfULLRo9Tm4lESxYMAG7fUQ\"&gt;Leetcode SQL Hard Problems&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=qyAgWL066Vo&amp;amp;list=PLBTZqjSKn0IeKBQDjLmzisazhqQy4iGkb\"&gt;Complex SQL questions for Interview Preparation&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I will recommend after watching the above playlists, to get your hands dirty. So far you have gained all the theoretical knowledge required now you need to start practicing it on a daily basis. The channel has the DDL and DML statements for almost all his videos in the description, if you do not have a local setup, you can use &lt;a href=\"https://sqliteonline.com/\"&gt;this&lt;/a&gt; and run the commands over there and start writing queries.&lt;/p&gt;\n\n&lt;p&gt;Now that you are through his playlists twice the questions will feel repetitive and you have gained the querying experience needed to handle problems on your own which brings us to step 4.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Start Practicing.&lt;/p&gt;\n\n&lt;p&gt;Learning by just watching will help clear your concepts but you will need to continuously apply them to be really comfortable with it and SQL query writing practice should be done daily even if it&amp;#39;s just 1 or 2 problems everyday. After you are done practicing the Ankit Bansal playlists you can start with below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hackerrank.&lt;/li&gt;\n&lt;li&gt;Go to leetcode and solve all the free questions from easy, medium and then hard level.&lt;/li&gt;\n&lt;li&gt;Stratascratch has good questions so you can solve the free questions over there too.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;By now, you should be really good at writing complex queries. If you have followed properly then by step 3 itself you should be good at joins, windows functions, group by, cte, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Understand some concepts mentioned below.&lt;/p&gt;\n\n&lt;p&gt;You can do this step in parallel with other steps. Below are some of the concepts you need to be aware about and I will be adding pointers and updating links if and as I find them.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL Indexes, types, uses&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types\"&gt;SCD Types&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;OLAP vs OLTP&lt;/li&gt;\n&lt;li&gt;Normalization vs Denormalization&lt;/li&gt;\n&lt;li&gt;Normalization Forms&lt;/li&gt;\n&lt;li&gt;Data Warehouse Concepts [&lt;a href=\"https://youtu.be/h0j0QN2b57M?list=PL_c9BZzLwBRK0Pc28IdvPQizD2mJlgoID\"&gt;English&lt;/a&gt;, &lt;a href=\"https://www.youtube.com/watch?v=UiTvqSd52ak&amp;amp;list=PLTsNSGeIpGnGP8A74Ie1PgqHhewsqD3fv\"&gt;Hindi&lt;/a&gt;]&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Link to Table of Content Post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank You..!! Please do let me know in comments if you liked the blog, if there is anything else you want to ask related to SQL or if there are any constructive criticism you would like to give.&lt;/p&gt;\n\n&lt;p&gt;Note: This blog has focused on relational SQL, NoSQL is something I was aware of on a very very high level but I was honest with interviewers and told them I had no experience with NoSQL and they were okay with it (most did not bother asking about it anyways).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?auto=webp&amp;s=4fee18cfd89cec5ab0520d892b2ce5eb0b9be1f7", "width": 436, "height": 228}, "resolutions": [{"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=740ea9501022e21c257895ff17338b1b33a9a989", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e941fd0c01e8d91f712bca4241a2196e537fcad2", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=750005e89b1a398842805c884bbf0fe3131ebd71", "width": 320, "height": 167}], "variants": {}, "id": "UjnWFWLTELxtFI-XmIDZdeUNDhicy5GUBXJWF3AfpVU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1asxqdx", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/", "subreddit_subscribers": 161585, "created_utc": 1708160709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven't personally used Airbyte, on paper, it seems ideal.\n\nIt offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.\n\nAirbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.\n\nI'm eager to learn about others' experiences with Airbyte.\n\nAdditionally, I'm exploring other tools in the market. I'm in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:\n\n- Open-source and capable of being hosted independently (I don\u2019t want to pay for it)\n- The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi\n- The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.\n\nHistorically, I've set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I'm drawn to the concept of EL tools that offer a standardized approach to configuring these connections.", "author_fullname": "t2_tux1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte and similar EL tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at76os", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708190772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven&amp;#39;t personally used Airbyte, on paper, it seems ideal.&lt;/p&gt;\n\n&lt;p&gt;It offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.&lt;/p&gt;\n\n&lt;p&gt;Airbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m eager to learn about others&amp;#39; experiences with Airbyte.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;m exploring other tools in the market. I&amp;#39;m in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Open-source and capable of being hosted independently (I don\u2019t want to pay for it)&lt;/li&gt;\n&lt;li&gt;The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi&lt;/li&gt;\n&lt;li&gt;The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Historically, I&amp;#39;ve set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I&amp;#39;m drawn to the concept of EL tools that offer a standardized approach to configuring these connections.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at76os", "is_robot_indexable": true, "report_reasons": null, "author": "caksters", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "subreddit_subscribers": 161585, "created_utc": 1708190772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i'd manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.\n\nAs I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.\n\nCan iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?", "author_fullname": "t2_hgupt3800", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About iceberg tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at8bft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708193702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i&amp;#39;d manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.&lt;/p&gt;\n\n&lt;p&gt;As I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.&lt;/p&gt;\n\n&lt;p&gt;Can iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at8bft", "is_robot_indexable": true, "report_reasons": null, "author": "Annual_Scratch7181", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "subreddit_subscribers": 161585, "created_utc": 1708193702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).\n\nI figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a [GitHub repo](https://github.com/nydasco/data-pipeline-demo), and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).\n\nAs an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.\n\nIt was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.\n\nThe README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.\n\nWould be great to get some feedback.", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline Demo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ato76r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708240154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).&lt;/p&gt;\n\n&lt;p&gt;I figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a &lt;a href=\"https://github.com/nydasco/data-pipeline-demo\"&gt;GitHub repo&lt;/a&gt;, and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).&lt;/p&gt;\n\n&lt;p&gt;As an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.&lt;/p&gt;\n\n&lt;p&gt;It was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.&lt;/p&gt;\n\n&lt;p&gt;The README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.&lt;/p&gt;\n\n&lt;p&gt;Would be great to get some feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?auto=webp&amp;s=d5e1b5a1295da29bd31f8206981e0e1e5d53f95e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc90e29683a8ae5760a4f29956edb0c92c2ed9b5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6a82ed25bfb68cf1965fbb55857beb3c0b2a1f9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93f00fa6cfa189df538ba3130e6499ceeb3f9592", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad4617dfb8af85266c1374704a084aab234f688a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d32a149c1a598fe27e83d18750f21e368d583123", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=85d974e50a5b8eedcb924948a1317abbdc769480", "width": 1080, "height": 540}], "variants": {}, "id": "36ETUUJYxPv1lVclwgNZSM_U8UZshOnVtzPPj2-LQp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ato76r", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "subreddit_subscribers": 161585, "created_utc": 1708240154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy. Does anybody have any technical books related to DE that they particularly like?\n\nFor me, it\u2019s Data Pipelines with Apache Airflow; Docker Up and Running; Learning Spark; and Kafka: The Definitive Guide.", "author_fullname": "t2_elthwsuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favourite technical books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at2ir9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708178135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy. Does anybody have any technical books related to DE that they particularly like?&lt;/p&gt;\n\n&lt;p&gt;For me, it\u2019s Data Pipelines with Apache Airflow; Docker Up and Running; Learning Spark; and Kafka: The Definitive Guide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at2ir9", "is_robot_indexable": true, "report_reasons": null, "author": "Low-Sandwich-7607", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at2ir9/favourite_technical_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at2ir9/favourite_technical_books/", "subreddit_subscribers": 161585, "created_utc": 1708178135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nI am deploying clickhouse in production, total keeper 2 nodes, clickhouse 3 nodes. Read a lot here about problems scaling it on production, shards and configuration management.  \n\n\nWould love to hear about what to watch out for and any tips.  \n\n\nCurrent infra is aws and clickhouse will be deployed inside private vpc.  Using  \n1. Clickhouse Keeper (instead of zookeeper) (with docker) - 2 nodes - t2.medium  \n2. Clickhouse docker images with attached EBS volume (in case to scale or add extra storage if needed ) 3 nodes - t3.xlarge - 500GB volume each  \n3. Grafana for monitoring.  \n4. Looking at ELB for connecting to kafka for data pipeline  \n5. Openssl for certificates for hosts for clickhouse  \nThanks", "author_fullname": "t2_gyoai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Clickhouse in production - what to watch out for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at0xbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708173097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am deploying clickhouse in production, total keeper 2 nodes, clickhouse 3 nodes. Read a lot here about problems scaling it on production, shards and configuration management.  &lt;/p&gt;\n\n&lt;p&gt;Would love to hear about what to watch out for and any tips.  &lt;/p&gt;\n\n&lt;p&gt;Current infra is aws and clickhouse will be deployed inside private vpc.  Using&lt;br/&gt;\n1. Clickhouse Keeper (instead of zookeeper) (with docker) - 2 nodes - t2.medium&lt;br/&gt;\n2. Clickhouse docker images with attached EBS volume (in case to scale or add extra storage if needed ) 3 nodes - t3.xlarge - 500GB volume each&lt;br/&gt;\n3. Grafana for monitoring.&lt;br/&gt;\n4. Looking at ELB for connecting to kafka for data pipeline&lt;br/&gt;\n5. Openssl for certificates for hosts for clickhouse&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at0xbv", "is_robot_indexable": true, "report_reasons": null, "author": "abhishekgahlot", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at0xbv/deploying_clickhouse_in_production_what_to_watch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at0xbv/deploying_clickhouse_in_production_what_to_watch/", "subreddit_subscribers": 161585, "created_utc": 1708173097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am working as a consultant for an international company as part of the solution architecture team and we are struggling to find ways to improve our orchestration setup. We are working in a Microsoft landscape with 30+ external data sources ingested through Azure Data Factory pipelines that load data into a data lake and then into Azure Synapse. Each source can be expected to have a unique extraction method (custom APIs, relational databases, sftp, SAP etc). We then need to run approximately 100 procedures to create our dimensions and another 20 to create fact tables. Currently each dimension has an identity column that is passed into the facts. Everything is then loaded into a PowerBI model that has reached 50Gb of size. Currently all steps run in sequence which can take up to 7 hours. We want to move to a more granular and flexible orchestration or implement practices that makes data processing more agile (stable surrogate keys, schedule jobs individually). Our largest bottleneck is the refresh limitations of PowerBI. \n\nI would really appreciate some input on our architecture and suggestions/ideas/brainstorming how we could improve.\n\nIn the future we plan to move to dbt and a more lakehouse-like architecture and do some fabric stuff.\n\nThis is like my first reddit post ever so i hope some of you find interest in this post.", "author_fullname": "t2_7qas2xv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to orchestrate jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atdk8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708207191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working as a consultant for an international company as part of the solution architecture team and we are struggling to find ways to improve our orchestration setup. We are working in a Microsoft landscape with 30+ external data sources ingested through Azure Data Factory pipelines that load data into a data lake and then into Azure Synapse. Each source can be expected to have a unique extraction method (custom APIs, relational databases, sftp, SAP etc). We then need to run approximately 100 procedures to create our dimensions and another 20 to create fact tables. Currently each dimension has an identity column that is passed into the facts. Everything is then loaded into a PowerBI model that has reached 50Gb of size. Currently all steps run in sequence which can take up to 7 hours. We want to move to a more granular and flexible orchestration or implement practices that makes data processing more agile (stable surrogate keys, schedule jobs individually). Our largest bottleneck is the refresh limitations of PowerBI. &lt;/p&gt;\n\n&lt;p&gt;I would really appreciate some input on our architecture and suggestions/ideas/brainstorming how we could improve.&lt;/p&gt;\n\n&lt;p&gt;In the future we plan to move to dbt and a more lakehouse-like architecture and do some fabric stuff.&lt;/p&gt;\n\n&lt;p&gt;This is like my first reddit post ever so i hope some of you find interest in this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atdk8c", "is_robot_indexable": true, "report_reasons": null, "author": "SKll75", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atdk8c/how_to_orchestrate_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atdk8c/how_to_orchestrate_jobs/", "subreddit_subscribers": 161585, "created_utc": 1708207191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_8u4sc3aj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good sources to learn Scala and Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at5kpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708186647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1at5kpz", "is_robot_indexable": true, "report_reasons": null, "author": "Present-Yogurt-1998", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "subreddit_subscribers": 161585, "created_utc": 1708186647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. \n\nAfter listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. \n\nIs that right?\n\nhttps://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1", "author_fullname": "t2_a452rbie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache datafusion question - primary use cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atj40m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708222627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. &lt;/p&gt;\n\n&lt;p&gt;After listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. &lt;/p&gt;\n\n&lt;p&gt;Is that right?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1\"&gt;https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?auto=webp&amp;s=d6f1ac4360cf49229134f99c3d153108689c59c2", "width": 640, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9aa6a892393ee232f0eb45a7ea58d22c4c3e6146", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d003e541b5fea5344745c480962668e3b833264", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ae4191256e575310a08a80dfdc57ec7bf6197bc", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b53852171fad55deabc07ee33f2d1864f8b7dd8", "width": 640, "height": 480}], "variants": {}, "id": "01KqIsa7WqHwmoOZOln6aLV18Tj-fG_SKL1QeSNhoc8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atj40m", "is_robot_indexable": true, "report_reasons": null, "author": "Double-Code1902", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "subreddit_subscribers": 161585, "created_utc": 1708222627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been passively searching for a new job for the last year but in the last couple of weeks it's become a more active search. I'd like to find remote work (currently on-site and hate it), but I'm also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I've started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.\n\nI have a lot of experience with .NET, SQL Server, and the \"Microsoft stack\" in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!\n\nI think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested [Databricks Certified Associate Developer for Apache Spark](https://www.databricks.com/learn/certification/apache-spark-developer-associate), but I'm wondering if [Databricks Certified Data Engineer Associate](https://www.databricks.com/learn/certification/data-engineer-associate) would be more worth my time. I am leaning towards the \"Developer for Apache Spark\" one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?\n\nThanks so much!", "author_fullname": "t2_tu8mg1tj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Databricks certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atipxm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708221462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been passively searching for a new job for the last year but in the last couple of weeks it&amp;#39;s become a more active search. I&amp;#39;d like to find remote work (currently on-site and hate it), but I&amp;#39;m also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I&amp;#39;ve started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of experience with .NET, SQL Server, and the &amp;quot;Microsoft stack&amp;quot; in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!&lt;/p&gt;\n\n&lt;p&gt;I think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested &lt;a href=\"https://www.databricks.com/learn/certification/apache-spark-developer-associate\"&gt;Databricks Certified Associate Developer for Apache Spark&lt;/a&gt;, but I&amp;#39;m wondering if &lt;a href=\"https://www.databricks.com/learn/certification/data-engineer-associate\"&gt;Databricks Certified Data Engineer Associate&lt;/a&gt; would be more worth my time. I am leaning towards the &amp;quot;Developer for Apache Spark&amp;quot; one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?auto=webp&amp;s=757df626a63f83d9b1b9f06f8d8ba2d8237cc58f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a16398fc492032970830de9a00c08df10b34b4f6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a41a848b62534920ef03a38d809533173e246992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40a54960ac2c4744e4b273083930feb825efabd6", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=014f43b3314169ca926f88efd3b0ee45aa5e04c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=35758df6b7e49498041bec685a142e08302fa089", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6db5a3a3031ae960d05d7a0590693b1808444daf", "width": 1080, "height": 565}], "variants": {}, "id": "HwfMz-OF8GV89jc_qQVRVxc8W8oTe2mVUnnrYLKLhX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atipxm", "is_robot_indexable": true, "report_reasons": null, "author": "Le-Melancolique", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "subreddit_subscribers": 161585, "created_utc": 1708221462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello ,\n\nWe are having a customer application, which moves data from multiple source systems to target database. It collects real time customer transactions(\\~15K transactions/sec) and send/stream those in files to a target database(which is an OLTP database i.e. postgres). And its using kafka for streaming those input files to the target database. The OLTP database will be used for real time reporting and some batch reporting usecase of customer transactions.  \n\nNow the issue we are facing is ,\n\nThe database holds normalized table structure. And when we say , one complete transaction means it insert to multiple tables say one record in table1, two related records into table2 and one record into table3. And when the kafka events streams the data from the files, the records are not coming in proper order, means the correct scenario will be that , the transactions should first persists in TABLE1 and TABLE2 and then TABLE3, we are seeing some times the transaction has been present/reached/committed in the TABLE2 but not yet populated in TABLE1 and reports showing wrong value.\n\nSometimes when we try to backtrack those missing transactions , team says that events can go missing due to various reasons and its expected behavior, so you will have to replay the transactions. Sometimes it results into duplicate transactions etc.\n\nIn an ideal scenario , either the transaction should be persisted in full or not at all in the database as its atomic and should obey the ACID property as these are banking/financial transactions, but because of the event based model it seems we are not able to get it correct in the OLTP data store. And to achieve the performance/speed reference keys constraints has not been maintained in the target tables.\n\nSo my question is , how we normally design such systems. I understand Kafka is heavily used for such data streaming, so is there something which will ensure the transaction either persists in full or not at all in the target databases and will not have the missing events scenarios considering this will be a highly active system processing \\~15K transaction per second during peak time? And how to ensure the completeness of a transaction in such scenario.\n\n&amp;#x200B;", "author_fullname": "t2_cxqpzxgel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on data pipeline and Kafka events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atccw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708204022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello ,&lt;/p&gt;\n\n&lt;p&gt;We are having a customer application, which moves data from multiple source systems to target database. It collects real time customer transactions(~15K transactions/sec) and send/stream those in files to a target database(which is an OLTP database i.e. postgres). And its using kafka for streaming those input files to the target database. The OLTP database will be used for real time reporting and some batch reporting usecase of customer transactions.  &lt;/p&gt;\n\n&lt;p&gt;Now the issue we are facing is ,&lt;/p&gt;\n\n&lt;p&gt;The database holds normalized table structure. And when we say , one complete transaction means it insert to multiple tables say one record in table1, two related records into table2 and one record into table3. And when the kafka events streams the data from the files, the records are not coming in proper order, means the correct scenario will be that , the transactions should first persists in TABLE1 and TABLE2 and then TABLE3, we are seeing some times the transaction has been present/reached/committed in the TABLE2 but not yet populated in TABLE1 and reports showing wrong value.&lt;/p&gt;\n\n&lt;p&gt;Sometimes when we try to backtrack those missing transactions , team says that events can go missing due to various reasons and its expected behavior, so you will have to replay the transactions. Sometimes it results into duplicate transactions etc.&lt;/p&gt;\n\n&lt;p&gt;In an ideal scenario , either the transaction should be persisted in full or not at all in the database as its atomic and should obey the ACID property as these are banking/financial transactions, but because of the event based model it seems we are not able to get it correct in the OLTP data store. And to achieve the performance/speed reference keys constraints has not been maintained in the target tables.&lt;/p&gt;\n\n&lt;p&gt;So my question is , how we normally design such systems. I understand Kafka is heavily used for such data streaming, so is there something which will ensure the transaction either persists in full or not at all in the target databases and will not have the missing events scenarios considering this will be a highly active system processing ~15K transaction per second during peak time? And how to ensure the completeness of a transaction in such scenario.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atccw5", "is_robot_indexable": true, "report_reasons": null, "author": "Upper-Lifeguard-8478", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atccw5/question_on_data_pipeline_and_kafka_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atccw5/question_on_data_pipeline_and_kafka_events/", "subreddit_subscribers": 161585, "created_utc": 1708204022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently been hired to be the sole data engineer for a public database that has built up a lot of tech debt. Everything is stored in Postgres with foreign data wrappers, with custom scrapers built in Python. We\u2019re looking to ingest some new data into a table, but want to include columns that are based on what is already in the database (if IDs match, comparing timestamps, etc). What\u2019s the best tool for this in Python? Is this just querying the database in Python and using that return to augment our data? Is this a duckdb use case? Trying to figure it out, any help would be appreciated!\n", "author_fullname": "t2_4xul99ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to ingest data that is based on data already in the database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at262v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708177079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently been hired to be the sole data engineer for a public database that has built up a lot of tech debt. Everything is stored in Postgres with foreign data wrappers, with custom scrapers built in Python. We\u2019re looking to ingest some new data into a table, but want to include columns that are based on what is already in the database (if IDs match, comparing timestamps, etc). What\u2019s the best tool for this in Python? Is this just querying the database in Python and using that return to augment our data? Is this a duckdb use case? Trying to figure it out, any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at262v", "is_robot_indexable": true, "report_reasons": null, "author": "RichOkra", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at262v/best_way_to_ingest_data_that_is_based_on_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at262v/best_way_to_ingest_data_that_is_based_on_data/", "subreddit_subscribers": 161585, "created_utc": 1708177079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone in this sub work with healthcare provider data? \n\nI started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.\n\nCurrently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.\n\nI\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. \n\nFeel free to DM me as well.", "author_fullname": "t2_n2poi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage healthcare provider data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atkfkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708226802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone in this sub work with healthcare provider data? &lt;/p&gt;\n\n&lt;p&gt;I started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.&lt;/p&gt;\n\n&lt;p&gt;Currently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. &lt;/p&gt;\n\n&lt;p&gt;Feel free to DM me as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atkfkl", "is_robot_indexable": true, "report_reasons": null, "author": "Elmopo74", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "subreddit_subscribers": 161585, "created_utc": 1708226802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if anyone is using load balancers for your pipelines. Right now the data we are ingesting is about to explode from adding another new platform double our load. The situation is our singular pipeline is able to ingest data from different platforms but the amount of ingestion data is about to outpace the processing speed on a single EC2. What we plan to do is to port our pipeline to a cluster and use a load balancer to handle ingestions concurrently for different platforms. \n\nAnd we dont want to split the ingestion up across several EC2 because it would lead to a complete mess. I just want to know what is your opinion on this.", "author_fullname": "t2_1032tl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load balancer for pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ataxyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708200356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if anyone is using load balancers for your pipelines. Right now the data we are ingesting is about to explode from adding another new platform double our load. The situation is our singular pipeline is able to ingest data from different platforms but the amount of ingestion data is about to outpace the processing speed on a single EC2. What we plan to do is to port our pipeline to a cluster and use a load balancer to handle ingestions concurrently for different platforms. &lt;/p&gt;\n\n&lt;p&gt;And we dont want to split the ingestion up across several EC2 because it would lead to a complete mess. I just want to know what is your opinion on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ataxyd", "is_robot_indexable": true, "report_reasons": null, "author": "Amrita_Kai", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ataxyd/load_balancer_for_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ataxyd/load_balancer_for_pipelines/", "subreddit_subscribers": 161585, "created_utc": 1708200356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI'm really a newbie and trying to get into a Data Engineering field. I'm trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.\n\nThank so much in advance ", "author_fullname": "t2_i6yfsp2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Labs for learning Spark or Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1atpbay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708244596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really a newbie and trying to get into a Data Engineering field. I&amp;#39;m trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.&lt;/p&gt;\n\n&lt;p&gt;Thank so much in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atpbay", "is_robot_indexable": true, "report_reasons": null, "author": "BarberCultural4665", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "subreddit_subscribers": 161585, "created_utc": 1708244596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,\n\n@dlt.view\ndef taxi_raw():\n  return spark.read.format(\"json\").load(\"/databricks-datasets/nyctaxi/sample/json/\")\n\nThe request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.\n", "author_fullname": "t2_zb0y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For loop in Delta live table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atjppu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708224484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,&lt;/p&gt;\n\n&lt;p&gt;@dlt.view\ndef taxi_raw():\n  return spark.read.format(&amp;quot;json&amp;quot;).load(&amp;quot;/databricks-datasets/nyctaxi/sample/json/&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;The request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atjppu", "is_robot_indexable": true, "report_reasons": null, "author": "stock_daddy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "subreddit_subscribers": 161585, "created_utc": 1708224484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm currently going to school for rangeland sciences, and I am interested in learning more about GIS, specifically to utilize the program. I honestly can't wrap my head around the theories and concepts my class is currently discussing XD. But I can through repetition use gis to create the things to bring environmental data to live on a map(at least that is how I see myself utilizing it). But my question is what is the best way to (I don't even really know how to say this) create the data used in the program. Excell, Acess, R-programing? \n\nI was never really much good at programming, and I utilized excell while in the military and found out it can be an extremely powerful tool. So I thought to ask you all for some advice since data never really was my strong suit, but I need to be able to navigate to an extent. \n\n&amp;#x200B;\n\nBest,\n\nJacob", "author_fullname": "t2_4djf8im9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "some advice about data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1athjdm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708217917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently going to school for rangeland sciences, and I am interested in learning more about GIS, specifically to utilize the program. I honestly can&amp;#39;t wrap my head around the theories and concepts my class is currently discussing XD. But I can through repetition use gis to create the things to bring environmental data to live on a map(at least that is how I see myself utilizing it). But my question is what is the best way to (I don&amp;#39;t even really know how to say this) create the data used in the program. Excell, Acess, R-programing? &lt;/p&gt;\n\n&lt;p&gt;I was never really much good at programming, and I utilized excell while in the military and found out it can be an extremely powerful tool. So I thought to ask you all for some advice since data never really was my strong suit, but I need to be able to navigate to an extent. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Best,&lt;/p&gt;\n\n&lt;p&gt;Jacob&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1athjdm", "is_robot_indexable": true, "report_reasons": null, "author": "jmcdougal117", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1athjdm/some_advice_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1athjdm/some_advice_about_data/", "subreddit_subscribers": 161585, "created_utc": 1708217917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.\n\nI may have missed some important pieces and some of your favorite ones, please remind me in the comments.\n\nAdded some comments and thoughts that will be helpful: https://www.junaideffendi.com/p/end-to-end-data-engineering", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to End Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1at8a9q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfEs7-UDmzuPpDmkUrSJTdayf3C3Cnz_rbuhCsgoHAI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708193612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "junaideffendi.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.&lt;/p&gt;\n\n&lt;p&gt;I may have missed some important pieces and some of your favorite ones, please remind me in the comments.&lt;/p&gt;\n\n&lt;p&gt;Added some comments and thoughts that will be helpful: &lt;a href=\"https://www.junaideffendi.com/p/end-to-end-data-engineering\"&gt;https://www.junaideffendi.com/p/end-to-end-data-engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?auto=webp&amp;s=1ed846a57b8f80325a7a2e15e0097e32eaa423f3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27190432bd95df1e73b8119755399dab8c18f679", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e1528d7b338d331fbbfb64f4d3b6c0607a304", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18abef8c17aa632e8d49b11b8f77045c6cd0a811", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cff001fc4237ad505221528f4994bf6427b1e37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebb983bf58d7cf9d528b7d7efd422a2dc7203535", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5b18e9de6272ac01cd6f0945539d2043effe601", "width": 1080, "height": 540}], "variants": {}, "id": "bokfuc0QawGL9Kh2VpDq4Cqd6CsLSESH7Y0mqj5BFaY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1at8a9q", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8a9q/end_to_end_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "subreddit_subscribers": 161585, "created_utc": 1708193612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have a bit of a conundrum:\n\nI'm a data scientist and I've got the opportunity to turn a monster of an Excel workbook that's crucial for operations and KPI tracking into a database - or at least into a more efficient solution. There's a ton of business processes intertwined with this workbook and no one actually knows how many processes rely on it - or to what level detail. I'm interested in doing this as I think this is what the business needs, because I want to learn more data engineering tools and I want to learn more about that aspect of our operations.\n\nMy manager is not too technical so I have to deliver judgement calls on the more technical aspects but I have his support to delve into the topic for a couple of months and map out what I can and what I cannot do, and whether it's feasible for our small team to develop a full solution.\n\nIf the result is a \"no\" then that would be acceptable for him, too (obviously I'd work out the next steps and use the lessons learned to \"outsource\" and continue the project).\n\nSo far, so good. I talked with the colleague who developed the excel workbook over the past 5-10 years and he was of the very strong opinion of not developing a database solution ourselves but to get a professional solution for three reasons:\n\n* we'd be 'stuck' with maintenance down the road and we're actually too small a team for that\n* he thinks that we should look at the much, much bigger picture (i.e. involving more sites and departments and looking at the business processes way more holistically), and consequently buy the \"infrastructure\" to drive home the point of how crucial this technology is. Also it would make access to data silos potentially easier.\n* he also fears that higher management (who's aware that this is a crucial bit of infrastructure) will then be happy with whatever \"small\" solution we come up with, and then won't be willing to pay for the \"big\" solution that would actually be necessary. The latter solution would take a couple of years to get done.\n\nMy manager is playing with the idea that we can entertain two scenarios, a short-to- midterm solution that we maybe develop ourselves and a mid-to-longterm scenario with a \"professional\" (maybe internal/central or maybe external) solution. So, I asked for another colleague and I to get a couple of months time to map the data flows and built a pilot/prototype data solution because I believe that an approach in small, reversible steps might be very valuable here - and no one has considered that yet over the last couple of years. (A couple of months because I want some time to ideate and I can't focus on this topic full-time).\n\nThe benefit of delivering a \"small\" solution would be that we'd be making a name for ourselves across the entire hierarchy. Disadvantage will be that accessing existing silos will be a slog and likely not easily available, so we'd still need some manual/workaround type of data handover to our new, independent system (at least initially).\n\nLastly, I talked with a senior data scientist from the central data team and he said that we should really move away from those \"small\" solutions and look at the bigger picture because they think that we should by default look at the bigger picture to move the company ahead more sustainably. After talking a bit he have me the advice of at least interviewing most of the stakeholders before even starting any coding.\n\nSo, I'm a bit confused. I just want to go ahead, get my hands dirty and get started doing things, rather than having these endless discussions. But I also see the benefits of looking at the bigger picture and I don't want to outright dismiss my (very experienced) colleagues' take on the matter.\n\nDo you have some advice for me? Have you been in a comparable situation - and if so, how did/would you approach it?\n\nTL;DR:\n\nI have the opportunity to develop an in-house database solution but I'm not sure if I'm thinking too small.\n\nMany thanks!", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a database project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at7mp7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708191909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have a bit of a conundrum:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data scientist and I&amp;#39;ve got the opportunity to turn a monster of an Excel workbook that&amp;#39;s crucial for operations and KPI tracking into a database - or at least into a more efficient solution. There&amp;#39;s a ton of business processes intertwined with this workbook and no one actually knows how many processes rely on it - or to what level detail. I&amp;#39;m interested in doing this as I think this is what the business needs, because I want to learn more data engineering tools and I want to learn more about that aspect of our operations.&lt;/p&gt;\n\n&lt;p&gt;My manager is not too technical so I have to deliver judgement calls on the more technical aspects but I have his support to delve into the topic for a couple of months and map out what I can and what I cannot do, and whether it&amp;#39;s feasible for our small team to develop a full solution.&lt;/p&gt;\n\n&lt;p&gt;If the result is a &amp;quot;no&amp;quot; then that would be acceptable for him, too (obviously I&amp;#39;d work out the next steps and use the lessons learned to &amp;quot;outsource&amp;quot; and continue the project).&lt;/p&gt;\n\n&lt;p&gt;So far, so good. I talked with the colleague who developed the excel workbook over the past 5-10 years and he was of the very strong opinion of not developing a database solution ourselves but to get a professional solution for three reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;we&amp;#39;d be &amp;#39;stuck&amp;#39; with maintenance down the road and we&amp;#39;re actually too small a team for that&lt;/li&gt;\n&lt;li&gt;he thinks that we should look at the much, much bigger picture (i.e. involving more sites and departments and looking at the business processes way more holistically), and consequently buy the &amp;quot;infrastructure&amp;quot; to drive home the point of how crucial this technology is. Also it would make access to data silos potentially easier.&lt;/li&gt;\n&lt;li&gt;he also fears that higher management (who&amp;#39;s aware that this is a crucial bit of infrastructure) will then be happy with whatever &amp;quot;small&amp;quot; solution we come up with, and then won&amp;#39;t be willing to pay for the &amp;quot;big&amp;quot; solution that would actually be necessary. The latter solution would take a couple of years to get done.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My manager is playing with the idea that we can entertain two scenarios, a short-to- midterm solution that we maybe develop ourselves and a mid-to-longterm scenario with a &amp;quot;professional&amp;quot; (maybe internal/central or maybe external) solution. So, I asked for another colleague and I to get a couple of months time to map the data flows and built a pilot/prototype data solution because I believe that an approach in small, reversible steps might be very valuable here - and no one has considered that yet over the last couple of years. (A couple of months because I want some time to ideate and I can&amp;#39;t focus on this topic full-time).&lt;/p&gt;\n\n&lt;p&gt;The benefit of delivering a &amp;quot;small&amp;quot; solution would be that we&amp;#39;d be making a name for ourselves across the entire hierarchy. Disadvantage will be that accessing existing silos will be a slog and likely not easily available, so we&amp;#39;d still need some manual/workaround type of data handover to our new, independent system (at least initially).&lt;/p&gt;\n\n&lt;p&gt;Lastly, I talked with a senior data scientist from the central data team and he said that we should really move away from those &amp;quot;small&amp;quot; solutions and look at the bigger picture because they think that we should by default look at the bigger picture to move the company ahead more sustainably. After talking a bit he have me the advice of at least interviewing most of the stakeholders before even starting any coding.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m a bit confused. I just want to go ahead, get my hands dirty and get started doing things, rather than having these endless discussions. But I also see the benefits of looking at the bigger picture and I don&amp;#39;t want to outright dismiss my (very experienced) colleagues&amp;#39; take on the matter.&lt;/p&gt;\n\n&lt;p&gt;Do you have some advice for me? Have you been in a comparable situation - and if so, how did/would you approach it?&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;p&gt;I have the opportunity to develop an in-house database solution but I&amp;#39;m not sure if I&amp;#39;m thinking too small.&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at7mp7", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at7mp7/need_help_with_a_database_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at7mp7/need_help_with_a_database_project/", "subreddit_subscribers": 161585, "created_utc": 1708191909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI recently graduated with a degree in Software Engineering from a top university in Mexico, and I've been working as a data engineer for three years now at a transitional company in my city. My ultimate career goal is to break into the US job market, either by relocating to a city in the US or by working remotely for a US-based company from Mexico.\n\nI'm eager to enhance my career prospects, and I'm considering pursuing a master's degree in either Data Science or Statistics in the US. However, I'm torn between whether this would be a worthwhile investment of both time and money, or if I could achieve my goals through self-study and professional development.\n\nWhat are your thoughts on this? Do you believe pursuing a master's degree in the US would significantly improve my chances of entering the US job market, or would it be more beneficial to pursue other avenues of skill development independently?\n\nI'd greatly appreciate any insights or advice you can offer. Thank you in advance for your help!", "author_fullname": "t2_n2h013y2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Data Engineering in Mexico to the US Job Market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atcwjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708205477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently graduated with a degree in Software Engineering from a top university in Mexico, and I&amp;#39;ve been working as a data engineer for three years now at a transitional company in my city. My ultimate career goal is to break into the US job market, either by relocating to a city in the US or by working remotely for a US-based company from Mexico.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m eager to enhance my career prospects, and I&amp;#39;m considering pursuing a master&amp;#39;s degree in either Data Science or Statistics in the US. However, I&amp;#39;m torn between whether this would be a worthwhile investment of both time and money, or if I could achieve my goals through self-study and professional development.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this? Do you believe pursuing a master&amp;#39;s degree in the US would significantly improve my chances of entering the US job market, or would it be more beneficial to pursue other avenues of skill development independently?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d greatly appreciate any insights or advice you can offer. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atcwjw", "is_robot_indexable": true, "report_reasons": null, "author": "josegzza", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atcwjw/transitioning_from_data_engineering_in_mexico_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atcwjw/transitioning_from_data_engineering_in_mexico_to/", "subreddit_subscribers": 161585, "created_utc": 1708205477.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}