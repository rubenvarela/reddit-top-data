{"kind": "Listing", "data": {"after": "t3_1atvls5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After careful consideration and listening to your feedback, we've decided to no longer allow interview-related posts because they take away focus from our community's main purpose.\n\nIn the past, although they usually weren't directly related to data engineering we've allowed interview posts like \"What are interviews like at XYZ company?\" or \"What should I prepare/study for XYZ position?\"\n\nThese questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.\n\nThank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update to interview posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at7mdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708191888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After careful consideration and listening to your feedback, we&amp;#39;ve decided to no longer allow interview-related posts because they take away focus from our community&amp;#39;s main purpose.&lt;/p&gt;\n\n&lt;p&gt;In the past, although they usually weren&amp;#39;t directly related to data engineering we&amp;#39;ve allowed interview posts like &amp;quot;What are interviews like at XYZ company?&amp;quot; or &amp;quot;What should I prepare/study for XYZ position?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;These questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.&lt;/p&gt;\n\n&lt;p&gt;Thank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1at7mdg", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "subreddit_subscribers": 161641, "created_utc": 1708191888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a BI developer looking to upgrade by solidifying my ETL/Pipelining skills. I read many times that a great start would be to read the book Fundamentals of Data Engineering, but I happen to absorb concepts better through video. Any course equivalent that you recommend?", "author_fullname": "t2_144emf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any video course equivalent of the fundamentals of data engineering out there?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atbt4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708202582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a BI developer looking to upgrade by solidifying my ETL/Pipelining skills. I read many times that a great start would be to read the book Fundamentals of Data Engineering, but I happen to absorb concepts better through video. Any course equivalent that you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atbt4x", "is_robot_indexable": true, "report_reasons": null, "author": "Alno1", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atbt4x/any_video_course_equivalent_of_the_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atbt4x/any_video_course_equivalent_of_the_fundamentals/", "subreddit_subscribers": 161641, "created_utc": 1708202582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nOur team is currently in the process of evaluating various ETL/ELT platforms to enhance our data integration and transformation capabilities with Google BigQuery. We've been using Skyvia but are looking for something more scalable and robust.\nWe\u2019ve compiled a comparison chart of several platforms (Informatica, Microsoft, Oracle, Qlik, SAP, and Talend) with various features such as ease of use, scalability, cost, performance, security, resources, strengths, and weaknesses.\nBased on your experience, which of these platforms would you recommend for use with BigQuery? I\u2019m particularly interested in scalability and performance. If you've used any of these platforms, I\u2019d love to hear your thoughts and experiences and integration with BigQuery.\nYour insights and experiences would be invaluable in helping us make an informed decision. Thank you in advance!", "author_fullname": "t2_di68hd87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on ETL/ELT Platforms \u2013 Your Experiences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "name": "t3_1atq6yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AGH1_W7nb4WG1VplZQtXkKI3aq_fNdejtmUvMRerJzk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708248263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nOur team is currently in the process of evaluating various ETL/ELT platforms to enhance our data integration and transformation capabilities with Google BigQuery. We&amp;#39;ve been using Skyvia but are looking for something more scalable and robust.\nWe\u2019ve compiled a comparison chart of several platforms (Informatica, Microsoft, Oracle, Qlik, SAP, and Talend) with various features such as ease of use, scalability, cost, performance, security, resources, strengths, and weaknesses.\nBased on your experience, which of these platforms would you recommend for use with BigQuery? I\u2019m particularly interested in scalability and performance. If you&amp;#39;ve used any of these platforms, I\u2019d love to hear your thoughts and experiences and integration with BigQuery.\nYour insights and experiences would be invaluable in helping us make an informed decision. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jssiglo4bbjc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?auto=webp&amp;s=cea186fe4588d0b54ee2648e9131da305df17054", "width": 932, "height": 684}, "resolutions": [{"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4ba58a7661c8f58795cbd66bf974cb922e3087b", "width": 108, "height": 79}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=85ef5e17c7ba7ecee686d107943d63dc52538fcc", "width": 216, "height": 158}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75e94ecb0f9def2a9d5330eaed95a6fce225a2d9", "width": 320, "height": 234}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6123534c8f06365f7026cc086fe59b285afe8e42", "width": 640, "height": 469}], "variants": {}, "id": "geddnMcoZyxKt3HLjtyxX9qwSlhp15jujsgWhBXLheo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atq6yq", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Okra222", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atq6yq/seeking_advice_on_etlelt_platforms_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jssiglo4bbjc1.jpeg", "subreddit_subscribers": 161641, "created_utc": 1708248263.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).\n\nI figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a [GitHub repo](https://github.com/nydasco/data-pipeline-demo), and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).\n\nAs an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.\n\nIt was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.\n\nThe README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.\n\nWould be great to get some feedback.", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline Demo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ato76r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708240154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).&lt;/p&gt;\n\n&lt;p&gt;I figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a &lt;a href=\"https://github.com/nydasco/data-pipeline-demo\"&gt;GitHub repo&lt;/a&gt;, and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).&lt;/p&gt;\n\n&lt;p&gt;As an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.&lt;/p&gt;\n\n&lt;p&gt;It was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.&lt;/p&gt;\n\n&lt;p&gt;The README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.&lt;/p&gt;\n\n&lt;p&gt;Would be great to get some feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?auto=webp&amp;s=d5e1b5a1295da29bd31f8206981e0e1e5d53f95e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc90e29683a8ae5760a4f29956edb0c92c2ed9b5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6a82ed25bfb68cf1965fbb55857beb3c0b2a1f9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93f00fa6cfa189df538ba3130e6499ceeb3f9592", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad4617dfb8af85266c1374704a084aab234f688a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d32a149c1a598fe27e83d18750f21e368d583123", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iTR8gkFC1GJTDpmjeA5DjStN4F6oVwNT9RfsuzwhWr8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=85d974e50a5b8eedcb924948a1317abbdc769480", "width": 1080, "height": 540}], "variants": {}, "id": "36ETUUJYxPv1lVclwgNZSM_U8UZshOnVtzPPj2-LQp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ato76r", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "subreddit_subscribers": 161641, "created_utc": 1708240154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven't personally used Airbyte, on paper, it seems ideal.\n\nIt offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.\n\nAirbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.\n\nI'm eager to learn about others' experiences with Airbyte.\n\nAdditionally, I'm exploring other tools in the market. I'm in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:\n\n- Open-source and capable of being hosted independently (I don\u2019t want to pay for it)\n- The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi\n- The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.\n\nHistorically, I've set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I'm drawn to the concept of EL tools that offer a standardized approach to configuring these connections.", "author_fullname": "t2_tux1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte and similar EL tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at76os", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708190772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven&amp;#39;t personally used Airbyte, on paper, it seems ideal.&lt;/p&gt;\n\n&lt;p&gt;It offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.&lt;/p&gt;\n\n&lt;p&gt;Airbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m eager to learn about others&amp;#39; experiences with Airbyte.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;m exploring other tools in the market. I&amp;#39;m in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Open-source and capable of being hosted independently (I don\u2019t want to pay for it)&lt;/li&gt;\n&lt;li&gt;The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi&lt;/li&gt;\n&lt;li&gt;The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Historically, I&amp;#39;ve set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I&amp;#39;m drawn to the concept of EL tools that offer a standardized approach to configuring these connections.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at76os", "is_robot_indexable": true, "report_reasons": null, "author": "caksters", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "subreddit_subscribers": 161641, "created_utc": 1708190772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i'd manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.\n\nAs I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.\n\nCan iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?", "author_fullname": "t2_hgupt3800", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About iceberg tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at8bft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708193702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i&amp;#39;d manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.&lt;/p&gt;\n\n&lt;p&gt;As I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.&lt;/p&gt;\n\n&lt;p&gt;Can iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at8bft", "is_robot_indexable": true, "report_reasons": null, "author": "Annual_Scratch7181", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "subreddit_subscribers": 161641, "created_utc": 1708193702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been toying around with all of the mentioned products in the last weeks as we are looking to rebuild our data stack from the existing mix of shell scripts, python and talend jobs. We are running on AWS. One thing I noticed is that none of these tools support configurations that are pretty common in enterprise settings like ours. For example, we enforce KMS encryption on S3 buckets via service control policies. None of the tools seem to support this, which also prevents loading of data into Redshift. While I am thinking \"yes, this is open source, I could add this myself\" I am wondering what else is in store, if no one in a larger org seems to have run into this. Same goes for things like support for dynamic AWS credentials (we don't allow IAM users) and probably other surprises. I'm not necessarily blaming the tools, as there are a bunch of AWS' own services that don't support this (looking at you Datasync) either.", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are the \"cool\" tools (Meltano, dlt, sling, Airbyte, etc) really production ready", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atqm34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708249949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been toying around with all of the mentioned products in the last weeks as we are looking to rebuild our data stack from the existing mix of shell scripts, python and talend jobs. We are running on AWS. One thing I noticed is that none of these tools support configurations that are pretty common in enterprise settings like ours. For example, we enforce KMS encryption on S3 buckets via service control policies. None of the tools seem to support this, which also prevents loading of data into Redshift. While I am thinking &amp;quot;yes, this is open source, I could add this myself&amp;quot; I am wondering what else is in store, if no one in a larger org seems to have run into this. Same goes for things like support for dynamic AWS credentials (we don&amp;#39;t allow IAM users) and probably other surprises. I&amp;#39;m not necessarily blaming the tools, as there are a bunch of AWS&amp;#39; own services that don&amp;#39;t support this (looking at you Datasync) either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atqm34", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atqm34/are_the_cool_tools_meltano_dlt_sling_airbyte_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atqm34/are_the_cool_tools_meltano_dlt_sling_airbyte_etc/", "subreddit_subscribers": 161641, "created_utc": 1708249949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI'm really a newbie and trying to get into a Data Engineering field. I'm trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.\n\nThank so much in advance ", "author_fullname": "t2_i6yfsp2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Labs for learning Spark or Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atpbay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708244596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really a newbie and trying to get into a Data Engineering field. I&amp;#39;m trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.&lt;/p&gt;\n\n&lt;p&gt;Thank so much in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atpbay", "is_robot_indexable": true, "report_reasons": null, "author": "BarberCultural4665", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "subreddit_subscribers": 161641, "created_utc": 1708244596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In today's blog we will talk about Azure Data Factory (ADF).\n\n**Step 1**: Short introduction about how ADF is used.\n\nADF can be used in two of the following ways:\n\n1. As an **orchestrator**.\n2. As an **ETL tool**.\n\nThe choice of how it is being used will differ with each organization. In my preparation I practiced it as an orchestrator, the reason behind it being two fold:\n\n1. Data Flows go out of the picture, thereby reducing the learning curve.\n2. PySpark does exactly that with us having a lot more control and I personally loved the idea of using my own codes for transformations than using a almost code-less GUI.\n\nHow you perceive ADF amongst the two choices, I will leave it up to you guys. \n\n**Step 2**: How to go about preparing for ADF. \n\n1. You should first understand what ADF is.\n2. You need to understand top level concepts like Linked Services, Datasets, Activities, Pipelines, and Triggers. By this I mean you should know that such things exist and what they do.\n3. Go through the different kinds of activities available, through YouTube tutorials and Microsoft documentations. At this point you should be aware about the different activities available to us and what they do.\n4. Go through the different triggers available and understand when to use what.\n5. Learn to make your pipelines dynamic by avoiding hard-coding values in your pipelines and by using variables and parameters. This will also introduce you to a service called Key Vault.\n6. Learn about error-handling in your pipelines (different methods of error handling) and various ways to send notifications about failures (web/webhook activities using logic apps, using alerts from data studio).\n7. How to troubleshoot your pipelines, how to retain logs for different time frames, how to restart from a certain point if your pipeline fails, how to debug.\n8. What is CI/CD. How to implement CI/CD in your data factories and how to work using it, by this I mean to say you should be comfortable with: creating feature branches, publishing from main branch, creating artifacts and builds.\n9. How to integrate ADF with Databricks.\n\n**Step 3**: Resources I used to prepare\n\n1. Go through the ADF videos in [this channel](https://www.youtube.com/@TybulOnAzure), I have already shared it in my first blog, he has taught really well. At least watch his error handling video, you can ignore the CI/CD videos as I found other video more easy than that approach.\n2. Go through [this playlist](https://www.youtube.com/watch?v=Mc9JAra8WZU&amp;list=PLMWaZteqtEaLTJffbbBzVOv9C0otal1FO). You can ignore the data flow videos if you plan to use ADF as an orchestrator. This will give a very good idea about all the points in step 2 except point 6 and 8.   \n**Tip:** He does spend a lot of time creating linked services and datasets in every video, so once you are comfortable you can just skip those parts and watch at 1.5x speed to save a lot of time.\n3. Now go through the [Microsoft documentations](https://learn.microsoft.com/en-us/azure/data-factory/) to really get in what you have learned so far. You have to skim through it, don't spend a lot of time on that.\n\n**Step 4**: Practical\n\nYou can open a Azure account and practice side by side along with the tutorials. This will get a lot of hate in comments but I personally would recommend to wait a bit on this part, first understand pyspark, Azure Databricks, how to integrate with ADF, etc and then start practicing as Azure is free for only 1 month and services like Databricks will cost a bit. \n\nI would recommend first understanding the stack, creating a rough idea about real life data flow, then opening your account and creating different projects for your learnings. \n\n[Link To Table Of Content](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nPlease do let me know in comments if you have any feedback on this blog or feel I should add anything, also interactive comments helps me in understanding that people are going through this and engaging with it so it motivates me to spend the time to bring that content to you.\n\nThank You..!!", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog 3 - Let's talk ADF!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1atuvav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708267450.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708265245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In today&amp;#39;s blog we will talk about Azure Data Factory (ADF).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Short introduction about how ADF is used.&lt;/p&gt;\n\n&lt;p&gt;ADF can be used in two of the following ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As an &lt;strong&gt;orchestrator&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;As an &lt;strong&gt;ETL tool&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The choice of how it is being used will differ with each organization. In my preparation I practiced it as an orchestrator, the reason behind it being two fold:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Flows go out of the picture, thereby reducing the learning curve.&lt;/li&gt;\n&lt;li&gt;PySpark does exactly that with us having a lot more control and I personally loved the idea of using my own codes for transformations than using a almost code-less GUI.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How you perceive ADF amongst the two choices, I will leave it up to you guys. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: How to go about preparing for ADF. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You should first understand what ADF is.&lt;/li&gt;\n&lt;li&gt;You need to understand top level concepts like Linked Services, Datasets, Activities, Pipelines, and Triggers. By this I mean you should know that such things exist and what they do.&lt;/li&gt;\n&lt;li&gt;Go through the different kinds of activities available, through YouTube tutorials and Microsoft documentations. At this point you should be aware about the different activities available to us and what they do.&lt;/li&gt;\n&lt;li&gt;Go through the different triggers available and understand when to use what.&lt;/li&gt;\n&lt;li&gt;Learn to make your pipelines dynamic by avoiding hard-coding values in your pipelines and by using variables and parameters. This will also introduce you to a service called Key Vault.&lt;/li&gt;\n&lt;li&gt;Learn about error-handling in your pipelines (different methods of error handling) and various ways to send notifications about failures (web/webhook activities using logic apps, using alerts from data studio).&lt;/li&gt;\n&lt;li&gt;How to troubleshoot your pipelines, how to retain logs for different time frames, how to restart from a certain point if your pipeline fails, how to debug.&lt;/li&gt;\n&lt;li&gt;What is CI/CD. How to implement CI/CD in your data factories and how to work using it, by this I mean to say you should be comfortable with: creating feature branches, publishing from main branch, creating artifacts and builds.&lt;/li&gt;\n&lt;li&gt;How to integrate ADF with Databricks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Resources I used to prepare&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go through the ADF videos in &lt;a href=\"https://www.youtube.com/@TybulOnAzure\"&gt;this channel&lt;/a&gt;, I have already shared it in my first blog, he has taught really well. At least watch his error handling video, you can ignore the CI/CD videos as I found other video more easy than that approach.&lt;/li&gt;\n&lt;li&gt;Go through &lt;a href=\"https://www.youtube.com/watch?v=Mc9JAra8WZU&amp;amp;list=PLMWaZteqtEaLTJffbbBzVOv9C0otal1FO\"&gt;this playlist&lt;/a&gt;. You can ignore the data flow videos if you plan to use ADF as an orchestrator. This will give a very good idea about all the points in step 2 except point 6 and 8.&lt;br/&gt;\n&lt;strong&gt;Tip:&lt;/strong&gt; He does spend a lot of time creating linked services and datasets in every video, so once you are comfortable you can just skip those parts and watch at 1.5x speed to save a lot of time.&lt;/li&gt;\n&lt;li&gt;Now go through the &lt;a href=\"https://learn.microsoft.com/en-us/azure/data-factory/\"&gt;Microsoft documentations&lt;/a&gt; to really get in what you have learned so far. You have to skim through it, don&amp;#39;t spend a lot of time on that.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Practical&lt;/p&gt;\n\n&lt;p&gt;You can open a Azure account and practice side by side along with the tutorials. This will get a lot of hate in comments but I personally would recommend to wait a bit on this part, first understand pyspark, Azure Databricks, how to integrate with ADF, etc and then start practicing as Azure is free for only 1 month and services like Databricks will cost a bit. &lt;/p&gt;\n\n&lt;p&gt;I would recommend first understanding the stack, creating a rough idea about real life data flow, then opening your account and creating different projects for your learnings. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Link To Table Of Content&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please do let me know in comments if you have any feedback on this blog or feel I should add anything, also interactive comments helps me in understanding that people are going through this and engaging with it so it motivates me to spend the time to bring that content to you.&lt;/p&gt;\n\n&lt;p&gt;Thank You..!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?auto=webp&amp;s=b89998b8009f6ed380e4ddb97e465419e3896c64", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca01c758d5fa5b44264cfb0498b961588cdb4208", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=16334c3bf943dfe4ace7ad968d2dc11b89a61eae", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da069911f236610bd9b661cde061bbdda31797a8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f6bc143bf4207f73245b25a20e679cacc056cae", "width": 640, "height": 640}], "variants": {}, "id": "Y_tM-phmGdqUx-mShSmQnJRgDvBVdeU-A9Y4N-Jk4yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atuvav", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/", "subreddit_subscribers": 161641, "created_utc": 1708265245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAs I prepare to depart from my current company in a month, I'm encountering a common data engineering challenge:\n\n- I had two months to build pipelines for staging, as well as fact and model tables for a project, which I drafted on paper (not submitted yet).\n- The team spent a month configuring dbt and related tools using terraform, and the source tables were only provided to me afterward, with minimal documentation available, except for an Excel sheet with some columns.\n- The data engineer responsible for setting up the source tables took a three-week vacation, the day after he pushed the source tables, leaving behind minimal documentation.\n\nNow, with just one month left, scarce documentation, and no stakeholder support, what would you do?", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From a Data Engineer to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atsmdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708257865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;As I prepare to depart from my current company in a month, I&amp;#39;m encountering a common data engineering challenge:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I had two months to build pipelines for staging, as well as fact and model tables for a project, which I drafted on paper (not submitted yet).&lt;/li&gt;\n&lt;li&gt;The team spent a month configuring dbt and related tools using terraform, and the source tables were only provided to me afterward, with minimal documentation available, except for an Excel sheet with some columns.&lt;/li&gt;\n&lt;li&gt;The data engineer responsible for setting up the source tables took a three-week vacation, the day after he pushed the source tables, leaving behind minimal documentation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now, with just one month left, scarce documentation, and no stakeholder support, what would you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atsmdv", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atsmdv/from_a_data_engineer_to_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atsmdv/from_a_data_engineer_to_another/", "subreddit_subscribers": 161641, "created_utc": 1708257865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone in this sub work with healthcare provider data? \n\nI started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.\n\nCurrently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.\n\nI\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. \n\nFeel free to DM me as well.", "author_fullname": "t2_n2poi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage healthcare provider data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atkfkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708226802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone in this sub work with healthcare provider data? &lt;/p&gt;\n\n&lt;p&gt;I started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.&lt;/p&gt;\n\n&lt;p&gt;Currently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. &lt;/p&gt;\n\n&lt;p&gt;Feel free to DM me as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atkfkl", "is_robot_indexable": true, "report_reasons": null, "author": "Elmopo74", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "subreddit_subscribers": 161641, "created_utc": 1708226802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been passively searching for a new job for the last year but in the last couple of weeks it's become a more active search. I'd like to find remote work (currently on-site and hate it), but I'm also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I've started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.\n\nI have a lot of experience with .NET, SQL Server, and the \"Microsoft stack\" in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!\n\nI think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested [Databricks Certified Associate Developer for Apache Spark](https://www.databricks.com/learn/certification/apache-spark-developer-associate), but I'm wondering if [Databricks Certified Data Engineer Associate](https://www.databricks.com/learn/certification/data-engineer-associate) would be more worth my time. I am leaning towards the \"Developer for Apache Spark\" one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?\n\nThanks so much!", "author_fullname": "t2_tu8mg1tj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Databricks certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atipxm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708221462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been passively searching for a new job for the last year but in the last couple of weeks it&amp;#39;s become a more active search. I&amp;#39;d like to find remote work (currently on-site and hate it), but I&amp;#39;m also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I&amp;#39;ve started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of experience with .NET, SQL Server, and the &amp;quot;Microsoft stack&amp;quot; in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!&lt;/p&gt;\n\n&lt;p&gt;I think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested &lt;a href=\"https://www.databricks.com/learn/certification/apache-spark-developer-associate\"&gt;Databricks Certified Associate Developer for Apache Spark&lt;/a&gt;, but I&amp;#39;m wondering if &lt;a href=\"https://www.databricks.com/learn/certification/data-engineer-associate\"&gt;Databricks Certified Data Engineer Associate&lt;/a&gt; would be more worth my time. I am leaning towards the &amp;quot;Developer for Apache Spark&amp;quot; one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?auto=webp&amp;s=757df626a63f83d9b1b9f06f8d8ba2d8237cc58f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a16398fc492032970830de9a00c08df10b34b4f6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a41a848b62534920ef03a38d809533173e246992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40a54960ac2c4744e4b273083930feb825efabd6", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=014f43b3314169ca926f88efd3b0ee45aa5e04c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=35758df6b7e49498041bec685a142e08302fa089", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6db5a3a3031ae960d05d7a0590693b1808444daf", "width": 1080, "height": 565}], "variants": {}, "id": "HwfMz-OF8GV89jc_qQVRVxc8W8oTe2mVUnnrYLKLhX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atipxm", "is_robot_indexable": true, "report_reasons": null, "author": "Le-Melancolique", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "subreddit_subscribers": 161641, "created_utc": 1708221462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am working as a consultant for an international company as part of the solution architecture team and we are struggling to find ways to improve our orchestration setup. We are working in a Microsoft landscape with 30+ external data sources ingested through Azure Data Factory pipelines that load data into a data lake and then into Azure Synapse. Each source can be expected to have a unique extraction method (custom APIs, relational databases, sftp, SAP etc). We then need to run approximately 100 procedures to create our dimensions and another 20 to create fact tables. Currently each dimension has an identity column that is passed into the facts. Everything is then loaded into a PowerBI model that has reached 50Gb of size. Currently all steps run in sequence which can take up to 7 hours. We want to move to a more granular and flexible orchestration or implement practices that makes data processing more agile (stable surrogate keys, schedule jobs individually). Our largest bottleneck is the refresh limitations of PowerBI. \n\nI would really appreciate some input on our architecture and suggestions/ideas/brainstorming how we could improve.\n\nIn the future we plan to move to dbt and a more lakehouse-like architecture and do some fabric stuff.\n\nThis is like my first reddit post ever so i hope some of you find interest in this post.", "author_fullname": "t2_7qas2xv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to orchestrate jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atdk8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708207191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working as a consultant for an international company as part of the solution architecture team and we are struggling to find ways to improve our orchestration setup. We are working in a Microsoft landscape with 30+ external data sources ingested through Azure Data Factory pipelines that load data into a data lake and then into Azure Synapse. Each source can be expected to have a unique extraction method (custom APIs, relational databases, sftp, SAP etc). We then need to run approximately 100 procedures to create our dimensions and another 20 to create fact tables. Currently each dimension has an identity column that is passed into the facts. Everything is then loaded into a PowerBI model that has reached 50Gb of size. Currently all steps run in sequence which can take up to 7 hours. We want to move to a more granular and flexible orchestration or implement practices that makes data processing more agile (stable surrogate keys, schedule jobs individually). Our largest bottleneck is the refresh limitations of PowerBI. &lt;/p&gt;\n\n&lt;p&gt;I would really appreciate some input on our architecture and suggestions/ideas/brainstorming how we could improve.&lt;/p&gt;\n\n&lt;p&gt;In the future we plan to move to dbt and a more lakehouse-like architecture and do some fabric stuff.&lt;/p&gt;\n\n&lt;p&gt;This is like my first reddit post ever so i hope some of you find interest in this post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atdk8c", "is_robot_indexable": true, "report_reasons": null, "author": "SKll75", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atdk8c/how_to_orchestrate_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atdk8c/how_to_orchestrate_jobs/", "subreddit_subscribers": 161641, "created_utc": 1708207191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello ,\n\nWe are having a customer application, which moves data from multiple source systems to target database. It collects real time customer transactions(\\~15K transactions/sec) and send/stream those in files to a target database(which is an OLTP database i.e. postgres). And its using kafka for streaming those input files to the target database. The OLTP database will be used for real time reporting and some batch reporting usecase of customer transactions.  \n\nNow the issue we are facing is ,\n\nThe database holds normalized table structure. And when we say , one complete transaction means it insert to multiple tables say one record in table1, two related records into table2 and one record into table3. And when the kafka events streams the data from the files, the records are not coming in proper order, means the correct scenario will be that , the transactions should first persists in TABLE1 and TABLE2 and then TABLE3, we are seeing some times the transaction has been present/reached/committed in the TABLE2 but not yet populated in TABLE1 and reports showing wrong value.\n\nSometimes when we try to backtrack those missing transactions , team says that events can go missing due to various reasons and its expected behavior, so you will have to replay the transactions. Sometimes it results into duplicate transactions etc.\n\nIn an ideal scenario , either the transaction should be persisted in full or not at all in the database as its atomic and should obey the ACID property as these are banking/financial transactions, but because of the event based model it seems we are not able to get it correct in the OLTP data store. And to achieve the performance/speed reference keys constraints has not been maintained in the target tables.\n\nSo my question is , how we normally design such systems. I understand Kafka is heavily used for such data streaming, so is there something which will ensure the transaction either persists in full or not at all in the target databases and will not have the missing events scenarios considering this will be a highly active system processing \\~15K transaction per second during peak time? And how to ensure the completeness of a transaction in such scenario.\n\n&amp;#x200B;", "author_fullname": "t2_cxqpzxgel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on data pipeline and Kafka events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atccw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708204022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello ,&lt;/p&gt;\n\n&lt;p&gt;We are having a customer application, which moves data from multiple source systems to target database. It collects real time customer transactions(~15K transactions/sec) and send/stream those in files to a target database(which is an OLTP database i.e. postgres). And its using kafka for streaming those input files to the target database. The OLTP database will be used for real time reporting and some batch reporting usecase of customer transactions.  &lt;/p&gt;\n\n&lt;p&gt;Now the issue we are facing is ,&lt;/p&gt;\n\n&lt;p&gt;The database holds normalized table structure. And when we say , one complete transaction means it insert to multiple tables say one record in table1, two related records into table2 and one record into table3. And when the kafka events streams the data from the files, the records are not coming in proper order, means the correct scenario will be that , the transactions should first persists in TABLE1 and TABLE2 and then TABLE3, we are seeing some times the transaction has been present/reached/committed in the TABLE2 but not yet populated in TABLE1 and reports showing wrong value.&lt;/p&gt;\n\n&lt;p&gt;Sometimes when we try to backtrack those missing transactions , team says that events can go missing due to various reasons and its expected behavior, so you will have to replay the transactions. Sometimes it results into duplicate transactions etc.&lt;/p&gt;\n\n&lt;p&gt;In an ideal scenario , either the transaction should be persisted in full or not at all in the database as its atomic and should obey the ACID property as these are banking/financial transactions, but because of the event based model it seems we are not able to get it correct in the OLTP data store. And to achieve the performance/speed reference keys constraints has not been maintained in the target tables.&lt;/p&gt;\n\n&lt;p&gt;So my question is , how we normally design such systems. I understand Kafka is heavily used for such data streaming, so is there something which will ensure the transaction either persists in full or not at all in the target databases and will not have the missing events scenarios considering this will be a highly active system processing ~15K transaction per second during peak time? And how to ensure the completeness of a transaction in such scenario.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atccw5", "is_robot_indexable": true, "report_reasons": null, "author": "Upper-Lifeguard-8478", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atccw5/question_on_data_pipeline_and_kafka_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atccw5/question_on_data_pipeline_and_kafka_events/", "subreddit_subscribers": 161641, "created_utc": 1708204022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_8u4sc3aj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good sources to learn Scala and Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at5kpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708186647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1at5kpz", "is_robot_indexable": true, "report_reasons": null, "author": "Present-Yogurt-1998", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "subreddit_subscribers": 161641, "created_utc": 1708186647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_e2n97hil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Fundamentals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atqdps", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708249023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tontinton.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tontinton.com/posts/database-fundementals", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atqdps", "is_robot_indexable": true, "report_reasons": null, "author": "youmarye", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atqdps/database_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tontinton.com/posts/database-fundementals", "subreddit_subscribers": 161641, "created_utc": 1708249023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. \n\nAfter listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. \n\nIs that right?\n\nhttps://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1", "author_fullname": "t2_a452rbie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache datafusion question - primary use cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atj40m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708222627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. &lt;/p&gt;\n\n&lt;p&gt;After listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. &lt;/p&gt;\n\n&lt;p&gt;Is that right?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1\"&gt;https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?auto=webp&amp;s=d6f1ac4360cf49229134f99c3d153108689c59c2", "width": 640, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9aa6a892393ee232f0eb45a7ea58d22c4c3e6146", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d003e541b5fea5344745c480962668e3b833264", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ae4191256e575310a08a80dfdc57ec7bf6197bc", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b53852171fad55deabc07ee33f2d1864f8b7dd8", "width": 640, "height": 480}], "variants": {}, "id": "01KqIsa7WqHwmoOZOln6aLV18Tj-fG_SKL1QeSNhoc8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atj40m", "is_robot_indexable": true, "report_reasons": null, "author": "Double-Code1902", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "subreddit_subscribers": 161641, "created_utc": 1708222627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.\n\nI may have missed some important pieces and some of your favorite ones, please remind me in the comments.\n\nAdded some comments and thoughts that will be helpful: https://www.junaideffendi.com/p/end-to-end-data-engineering", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to End Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1at8a9q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfEs7-UDmzuPpDmkUrSJTdayf3C3Cnz_rbuhCsgoHAI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708193612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "junaideffendi.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.&lt;/p&gt;\n\n&lt;p&gt;I may have missed some important pieces and some of your favorite ones, please remind me in the comments.&lt;/p&gt;\n\n&lt;p&gt;Added some comments and thoughts that will be helpful: &lt;a href=\"https://www.junaideffendi.com/p/end-to-end-data-engineering\"&gt;https://www.junaideffendi.com/p/end-to-end-data-engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?auto=webp&amp;s=1ed846a57b8f80325a7a2e15e0097e32eaa423f3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27190432bd95df1e73b8119755399dab8c18f679", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e1528d7b338d331fbbfb64f4d3b6c0607a304", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18abef8c17aa632e8d49b11b8f77045c6cd0a811", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cff001fc4237ad505221528f4994bf6427b1e37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebb983bf58d7cf9d528b7d7efd422a2dc7203535", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5b18e9de6272ac01cd6f0945539d2043effe601", "width": 1080, "height": 540}], "variants": {}, "id": "bokfuc0QawGL9Kh2VpDq4Cqd6CsLSESH7Y0mqj5BFaY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1at8a9q", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8a9q/end_to_end_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "subreddit_subscribers": 161641, "created_utc": 1708193612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nMy team is currently in the migration our data platform from on-prem to azure cloud. Our current setup is dbt core orchestrated with airflow and we asked to move to databricks/ADF. Which of the adoption is more feasible dbt core or dbt cloud  with databricks in your opinion/experience or would you consider any other options...", "author_fullname": "t2_je149dlo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "databricks + dbt core or dbt cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1atvon9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708267630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My team is currently in the migration our data platform from on-prem to azure cloud. Our current setup is dbt core orchestrated with airflow and we asked to move to databricks/ADF. Which of the adoption is more feasible dbt core or dbt cloud  with databricks in your opinion/experience or would you consider any other options...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atvon9", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Bad1906", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvon9/databricks_dbt_core_or_dbt_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvon9/databricks_dbt_core_or_dbt_cloud/", "subreddit_subscribers": 161641, "created_utc": 1708267630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI learn best when I follow along and implement projects. I have been successful learning a lot of data science through it and would\u2019ve loved to do so at the engineering aspects. Here is an example https://sagemaker-examples.readthedocs.io/en/latest/ \n\n\nMost of the resources I have been recommended are very theoretical. And I don\u2019t have the luxury to practice at work. \n\nI looked at Acloudguru and databricks academy. Both look promising. I am trying to widen the net with respect to technology so that I can pick up best practices and intuitions. \n\nAre there other hands on guides you recommend?\n\nThank you all. \n", "author_fullname": "t2_lcc2eyxje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner here. How do I learn by practicing? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1attvja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708262227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I learn best when I follow along and implement projects. I have been successful learning a lot of data science through it and would\u2019ve loved to do so at the engineering aspects. Here is an example &lt;a href=\"https://sagemaker-examples.readthedocs.io/en/latest/\"&gt;https://sagemaker-examples.readthedocs.io/en/latest/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Most of the resources I have been recommended are very theoretical. And I don\u2019t have the luxury to practice at work. &lt;/p&gt;\n\n&lt;p&gt;I looked at Acloudguru and databricks academy. Both look promising. I am trying to widen the net with respect to technology so that I can pick up best practices and intuitions. &lt;/p&gt;\n\n&lt;p&gt;Are there other hands on guides you recommend?&lt;/p&gt;\n\n&lt;p&gt;Thank you all. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1attvja", "is_robot_indexable": true, "report_reasons": null, "author": "20231027", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1attvja/beginner_here_how_do_i_learn_by_practicing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1attvja/beginner_here_how_do_i_learn_by_practicing/", "subreddit_subscribers": 161641, "created_utc": 1708262227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dgovt4x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The DBT of AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atrusw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708254998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "matsmoll.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://matsmoll.github.io/posts/the-dbt-of-ai", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atrusw", "is_robot_indexable": true, "report_reasons": null, "author": "FewComfort75", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atrusw/the_dbt_of_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://matsmoll.github.io/posts/the-dbt-of-ai", "subreddit_subscribers": 161641, "created_utc": 1708254998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,\n\n@dlt.view\ndef taxi_raw():\n  return spark.read.format(\"json\").load(\"/databricks-datasets/nyctaxi/sample/json/\")\n\nThe request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.\n", "author_fullname": "t2_zb0y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For loop in Delta live table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atjppu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708224484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,&lt;/p&gt;\n\n&lt;p&gt;@dlt.view\ndef taxi_raw():\n  return spark.read.format(&amp;quot;json&amp;quot;).load(&amp;quot;/databricks-datasets/nyctaxi/sample/json/&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;The request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atjppu", "is_robot_indexable": true, "report_reasons": null, "author": "stock_daddy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "subreddit_subscribers": 161641, "created_utc": 1708224484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if anyone is using load balancers for your pipelines. Right now the data we are ingesting is about to explode from adding another new platform double our load. The situation is our singular pipeline is able to ingest data from different platforms but the amount of ingestion data is about to outpace the processing speed on a single EC2. What we plan to do is to port our pipeline to a cluster and use a load balancer to handle ingestions concurrently for different platforms. \n\nAnd we dont want to split the ingestion up across several EC2 because it would lead to a complete mess. I just want to know what is your opinion on this.", "author_fullname": "t2_1032tl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load balancer for pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ataxyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708200356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if anyone is using load balancers for your pipelines. Right now the data we are ingesting is about to explode from adding another new platform double our load. The situation is our singular pipeline is able to ingest data from different platforms but the amount of ingestion data is about to outpace the processing speed on a single EC2. What we plan to do is to port our pipeline to a cluster and use a load balancer to handle ingestions concurrently for different platforms. &lt;/p&gt;\n\n&lt;p&gt;And we dont want to split the ingestion up across several EC2 because it would lead to a complete mess. I just want to know what is your opinion on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ataxyd", "is_robot_indexable": true, "report_reasons": null, "author": "Amrita_Kai", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ataxyd/load_balancer_for_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ataxyd/load_balancer_for_pipelines/", "subreddit_subscribers": 161641, "created_utc": 1708200356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I want to create my own data architecture with it revolving Azure tools. in my mind it will be like source &gt; ingestion &gt; storage &gt; integration &gt; visualization.. what kind of Azure tools should i use for each component? and my data processing pipeline is source &gt; rest api &gt; Azure function &gt; sql server. am i in the right direction?", "author_fullname": "t2_gengojk0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1atvwhu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708268231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to create my own data architecture with it revolving Azure tools. in my mind it will be like source &amp;gt; ingestion &amp;gt; storage &amp;gt; integration &amp;gt; visualization.. what kind of Azure tools should i use for each component? and my data processing pipeline is source &amp;gt; rest api &amp;gt; Azure function &amp;gt; sql server. am i in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atvwhu", "is_robot_indexable": true, "report_reasons": null, "author": "shinon_7652", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvwhu/data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvwhu/data_architecture/", "subreddit_subscribers": 161641, "created_utc": 1708268231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a complete newbie in Data Engineering, but I have some knowledge about programming so I decided to jump into big project and I think it destroyed me at in the beggining. I have an API that takes data from steam, but I want to automate it so I decided to run this on Apache Airflow on EC2 machine. I created an instance, connected, but the first problem appeared when I wanted to initialize airflow. I have read bunch of tutorials, watched youtube videos, but every person does it different way. some of them uses airflow standalone, some airflow db init.. etc. According to Airflow Documentation I should use the version with constraints:  \n[https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html)  \nBut some people use the python venv, some of them no. Can anyone provide not outdated source of information where I can learn it? Thank you very much &lt;3", "author_fullname": "t2_81vhqati", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow + AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1atvls5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708267403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a complete newbie in Data Engineering, but I have some knowledge about programming so I decided to jump into big project and I think it destroyed me at in the beggining. I have an API that takes data from steam, but I want to automate it so I decided to run this on Apache Airflow on EC2 machine. I created an instance, connected, but the first problem appeared when I wanted to initialize airflow. I have read bunch of tutorials, watched youtube videos, but every person does it different way. some of them uses airflow standalone, some airflow db init.. etc. According to Airflow Documentation I should use the version with constraints:&lt;br/&gt;\n&lt;a href=\"https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html\"&gt;https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html&lt;/a&gt;&lt;br/&gt;\nBut some people use the python venv, some of them no. Can anyone provide not outdated source of information where I can learn it? Thank you very much &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atvls5", "is_robot_indexable": true, "report_reasons": null, "author": "GlZM0O", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvls5/apache_airflow_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvls5/apache_airflow_aws/", "subreddit_subscribers": 161641, "created_utc": 1708267403.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}