{"kind": "Listing", "data": {"after": "t3_1arvrrn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "7 if I count the person who did phone screen. Had a positive experience with majority of the interviewers but hiring manager and another interviewer appeared very uninterested and seems didn\u2019t even read my resume. Almost 0 coding and majority was behavioral questions despite the fact that this is mid level data eng position. \nWith this much skewed perceived diversity, I can\u2019t help thinking they\u2019re looking for another person from their own culture. \n\n\nEdit:\nSeems like many other also witness this trend: https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X \n\n", "author_fullname": "t2_1g1puj9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Had an onsite interview with one of FAANG, all 6 interviewers were Indian ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arwf4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 353, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 353, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708048742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708047303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;7 if I count the person who did phone screen. Had a positive experience with majority of the interviewers but hiring manager and another interviewer appeared very uninterested and seems didn\u2019t even read my resume. Almost 0 coding and majority was behavioral questions despite the fact that this is mid level data eng position. \nWith this much skewed perceived diversity, I can\u2019t help thinking they\u2019re looking for another person from their own culture. &lt;/p&gt;\n\n&lt;p&gt;Edit:\nSeems like many other also witness this trend: &lt;a href=\"https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X\"&gt;https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1arwf4u", "is_robot_indexable": true, "report_reasons": null, "author": "HiroKifa", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arwf4u/had_an_onsite_interview_with_one_of_faang_all_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arwf4u/had_an_onsite_interview_with_one_of_faang_all_6/", "subreddit_subscribers": 161031, "created_utc": 1708047303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yah. \n\nI switched from working for the department of defense to Healthcare for job security and to expand my resume. \n\n\nI really enjoyed having my own office. The project was extremely interesting to me. \n\n\nBut uh, the entire hospital group. So like 67 clinics or something went under and I stopped have a job Monday with a 2 week severance. \n\n\nSo thats new. \n\n\nAnyways. I did a couple interviews since then. Still applying to more and more roles. Maybe i'll get a remote one. I'm taking time to push and finish my bachelor's. \n\n\nBut yah. I'll admit. Been at this for almost a decade and wasn't expecting that. Maybe it's time to go back ot defense. \n\n\nHope everyone in our field is doing well.", "author_fullname": "t2_gzuzr62b1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got Laid Off", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1artr7q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708039915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yah. &lt;/p&gt;\n\n&lt;p&gt;I switched from working for the department of defense to Healthcare for job security and to expand my resume. &lt;/p&gt;\n\n&lt;p&gt;I really enjoyed having my own office. The project was extremely interesting to me. &lt;/p&gt;\n\n&lt;p&gt;But uh, the entire hospital group. So like 67 clinics or something went under and I stopped have a job Monday with a 2 week severance. &lt;/p&gt;\n\n&lt;p&gt;So thats new. &lt;/p&gt;\n\n&lt;p&gt;Anyways. I did a couple interviews since then. Still applying to more and more roles. Maybe i&amp;#39;ll get a remote one. I&amp;#39;m taking time to push and finish my bachelor&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;But yah. I&amp;#39;ll admit. Been at this for almost a decade and wasn&amp;#39;t expecting that. Maybe it&amp;#39;s time to go back ot defense. &lt;/p&gt;\n\n&lt;p&gt;Hope everyone in our field is doing well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1artr7q", "is_robot_indexable": true, "report_reasons": null, "author": "Not_Another_Cookbook", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1artr7q/got_laid_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1artr7q/got_laid_off/", "subreddit_subscribers": 161031, "created_utc": 1708039915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI was a DA who wanted to transition into Azure DE role and found the guidance and resources all over the place and no one to really guide in a structured way. Well, after 3-4 months of studying I have been able to crack interviews on regular basis now. I know there are a lot of people in the same boat and the journey is overwhelming, so please let me know if you guys want me to post a series of blogs about what to do study, resources, interviewer expectations, etc. If anyone needs just a quick guidance you can comment here or reach out to me in DMs. \n\nI am doing this as a way of giving something back to the community so my guidance will be free and so will be the resources I'll recommend. All you need is practice and 3-4 months of dedication.", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guiding others to transition into Azure DE Role.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arpamc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708028942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I was a DA who wanted to transition into Azure DE role and found the guidance and resources all over the place and no one to really guide in a structured way. Well, after 3-4 months of studying I have been able to crack interviews on regular basis now. I know there are a lot of people in the same boat and the journey is overwhelming, so please let me know if you guys want me to post a series of blogs about what to do study, resources, interviewer expectations, etc. If anyone needs just a quick guidance you can comment here or reach out to me in DMs. &lt;/p&gt;\n\n&lt;p&gt;I am doing this as a way of giving something back to the community so my guidance will be free and so will be the resources I&amp;#39;ll recommend. All you need is practice and 3-4 months of dedication.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1arpamc", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/", "subreddit_subscribers": 161031, "created_utc": 1708028942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019m looking to curate a list of the most valuable and highly sought after data engineering technical/hard skills.\n\nSo far I have the following:\n\nSQL\nPython\nScala\nR\nApache Spark\nApache Kafka\nApache Hadoop\nTerraform\nGolang\nKubernetes\nPandas\nScikit-learn\nCloud (AWS, Azure, GCP)\n\nHow do these flow together? Is there anything you would add?\n\nThank you!", "author_fullname": "t2_rsi4ujs6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Valuable Data Engineering Skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arfovi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708004543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to curate a list of the most valuable and highly sought after data engineering technical/hard skills.&lt;/p&gt;\n\n&lt;p&gt;So far I have the following:&lt;/p&gt;\n\n&lt;p&gt;SQL\nPython\nScala\nR\nApache Spark\nApache Kafka\nApache Hadoop\nTerraform\nGolang\nKubernetes\nPandas\nScikit-learn\nCloud (AWS, Azure, GCP)&lt;/p&gt;\n\n&lt;p&gt;How do these flow together? Is there anything you would add?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arfovi", "is_robot_indexable": true, "report_reasons": null, "author": "HotAcanthocephala854", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arfovi/most_valuable_data_engineering_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arfovi/most_valuable_data_engineering_skills/", "subreddit_subscribers": 161031, "created_utc": 1708004543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are a small but data heavy startup, on our second year of operations. \n\nOur data stack consists of\n\n* BigQuery as warehouse / datalake (with GCS)\n* dbt Cloud for modelling\n* Prefect Cloud (backed by Google Cloud Run) for fetching raw data, and various tasks\n* Google Vertex for ML model building\n* A plethora of Cloud Functions for various tasks\n* Github for ...you know, code. \n* Started out using Airbyte Cloud early on for raw data ingestion stuff, but fell out of love with it quickly. Really wanted to like it, but it felt half baked as a product.\n\nWe are quite happy with the setup, but I would still love to know if there is any hot new tools that all the cool kids are using, or any other feedback?", "author_fullname": "t2_82s0a64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats hot in 2024?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ariy7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708013307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are a small but data heavy startup, on our second year of operations. &lt;/p&gt;\n\n&lt;p&gt;Our data stack consists of&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BigQuery as warehouse / datalake (with GCS)&lt;/li&gt;\n&lt;li&gt;dbt Cloud for modelling&lt;/li&gt;\n&lt;li&gt;Prefect Cloud (backed by Google Cloud Run) for fetching raw data, and various tasks&lt;/li&gt;\n&lt;li&gt;Google Vertex for ML model building&lt;/li&gt;\n&lt;li&gt;A plethora of Cloud Functions for various tasks&lt;/li&gt;\n&lt;li&gt;Github for ...you know, code. &lt;/li&gt;\n&lt;li&gt;Started out using Airbyte Cloud early on for raw data ingestion stuff, but fell out of love with it quickly. Really wanted to like it, but it felt half baked as a product.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We are quite happy with the setup, but I would still love to know if there is any hot new tools that all the cool kids are using, or any other feedback?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ariy7h", "is_robot_indexable": true, "report_reasons": null, "author": "Ootoootooo", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ariy7h/whats_hot_in_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ariy7h/whats_hot_in_2024/", "subreddit_subscribers": 161031, "created_utc": 1708013307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Additionally I would like to know what the stack looks like, I guess lots of Kafka, Flink, Java ...", "author_fullname": "t2_4bchq4zo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are streaming focused jobs betted paid than others ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arfphw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708004603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Additionally I would like to know what the stack looks like, I guess lots of Kafka, Flink, Java ...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1arfphw", "is_robot_indexable": true, "report_reasons": null, "author": "Grand-Theory", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arfphw/are_streaming_focused_jobs_betted_paid_than_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arfphw/are_streaming_focused_jobs_betted_paid_than_others/", "subreddit_subscribers": 161031, "created_utc": 1708004603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any good intellisense for writing sql queries in python? The problem is you write sql queries as a  string and get lots of syntactical errors. \nIs there better alternative?", "author_fullname": "t2_aenh4mw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intellisense for SQL in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as1jy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708063459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any good intellisense for writing sql queries in python? The problem is you write sql queries as a  string and get lots of syntactical errors. \nIs there better alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1as1jy2", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_Peanut282", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as1jy2/intellisense_for_sql_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as1jy2/intellisense_for_sql_in_python/", "subreddit_subscribers": 161031, "created_utc": 1708063459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on evaluating Kafka as a central data broker for my org. Looking at msk, confluent, and redpanda. Glanced at pulsar but not sure it\u2019s the right fit. Curious what folks experience has been or if we\u2019re missing a better vendor. Could do aws or gcp byoc too.", "author_fullname": "t2_fv8iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions on Hosted Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as09mo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708059138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on evaluating Kafka as a central data broker for my org. Looking at msk, confluent, and redpanda. Glanced at pulsar but not sure it\u2019s the right fit. Curious what folks experience has been or if we\u2019re missing a better vendor. Could do aws or gcp byoc too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1as09mo", "is_robot_indexable": true, "report_reasons": null, "author": "Sp00ky_6", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as09mo/opinions_on_hosted_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as09mo/opinions_on_hosted_kafka/", "subreddit_subscribers": 161031, "created_utc": 1708059138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI recently transitioned from a Data Scientist to a Data Engineer within our company and am still familiarizing myself with the role. I'm currently facing a challenge related to processing a large dataset of sports matches stored on an on-premise SQL server. Each match is uniquely identified by a match ID.\n\nOur objective is to analyze each match and generate a comprehensive table containing various statistics for each player. We rely on an self-written package that inputs a match as a Pandas DataFrame and outputs another DataFrame with 400 columns of aggregated player statistics. Short-term, modifying this package is not feasible.\n\nThe current workflow involves downloading the data to an on-premise Linux server, processing it, and then uploading the results to a statistics table. The breakdown of processing time is approximately 10% for downloading data, 70% for computing statistics, and 20% for uploading to the new table. This timeline is manageable for daily updates, but the initial loading process can exceed a week. (ca. 40 000 matches 3500 rows x 58 columns). Although I have developed a threaded solution that reduces processing time, it leads to deadlocks during the upload phase after reaching a certain threshold.\n\nAs we are planning to migrate to the cloud, I have the opportunity to propose new technological solutions. However, I have limited experience with cloud technology, as our data volumes previously did not warrant their use. Could you recommend a data architecture or alternative methods to enhance our processing efficiency?\n\nHelp a brother out, will you?", "author_fullname": "t2_8r13fthn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on data processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ariadu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708011656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently transitioned from a Data Scientist to a Data Engineer within our company and am still familiarizing myself with the role. I&amp;#39;m currently facing a challenge related to processing a large dataset of sports matches stored on an on-premise SQL server. Each match is uniquely identified by a match ID.&lt;/p&gt;\n\n&lt;p&gt;Our objective is to analyze each match and generate a comprehensive table containing various statistics for each player. We rely on an self-written package that inputs a match as a Pandas DataFrame and outputs another DataFrame with 400 columns of aggregated player statistics. Short-term, modifying this package is not feasible.&lt;/p&gt;\n\n&lt;p&gt;The current workflow involves downloading the data to an on-premise Linux server, processing it, and then uploading the results to a statistics table. The breakdown of processing time is approximately 10% for downloading data, 70% for computing statistics, and 20% for uploading to the new table. This timeline is manageable for daily updates, but the initial loading process can exceed a week. (ca. 40 000 matches 3500 rows x 58 columns). Although I have developed a threaded solution that reduces processing time, it leads to deadlocks during the upload phase after reaching a certain threshold.&lt;/p&gt;\n\n&lt;p&gt;As we are planning to migrate to the cloud, I have the opportunity to propose new technological solutions. However, I have limited experience with cloud technology, as our data volumes previously did not warrant their use. Could you recommend a data architecture or alternative methods to enhance our processing efficiency?&lt;/p&gt;\n\n&lt;p&gt;Help a brother out, will you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ariadu", "is_robot_indexable": true, "report_reasons": null, "author": "FunBrilliant5712", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ariadu/advice_on_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ariadu/advice_on_data_processing/", "subreddit_subscribers": 161031, "created_utc": 1708011656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am a mechanical engineer with few years of work experience. I have been into programming since college and I have heard some really good things about DE. But I really need your help in knowing what the first steps are, resources, practice steps and so on. Anything you can provide is greatly appreciated.\n\n&amp;#x200B;\n\nThank you for your time.", "author_fullname": "t2_an11k37c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I become a DE from a non-IT background and which between DA and DE should be the first step?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arx0zy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708049030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am a mechanical engineer with few years of work experience. I have been into programming since college and I have heard some really good things about DE. But I really need your help in knowing what the first steps are, resources, practice steps and so on. Anything you can provide is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1arx0zy", "is_robot_indexable": true, "report_reasons": null, "author": "Spiritual_Yak5933", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arx0zy/how_do_i_become_a_de_from_a_nonit_background_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arx0zy/how_do_i_become_a_de_from_a_nonit_background_and/", "subreddit_subscribers": 161031, "created_utc": 1708049030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have about 10 DBs and each has at least 10 tables. \n\nWe ingest this data (in batch) into a Redshift cluster using AWS Glue (Spark). We have designed it such that 1 Glue Job will handle 1 table. This gives us fine-grained control of the \"bookmark\" (the last timestamp that was processed).\n\nProblem: Now we have 100 connections to the databases going! And so many Glue Jobs!\n\nWe want to move toward 1 Glue Job per DB, but we wonder...\n\n...What is the best practice for coding this Spark job? \n\nWe imagine we'll:\n\n1. Make 1 Spark job per db\n2. In that job, make 10 DataFrames via jdbc connection (1 for each table in the DB)\n3. Write each DF to Redshift via jdbc connection info.\n\nIs that a misuse of Spark cluster resources?\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reduce DB Connections in Spark Job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arn1as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708023364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have about 10 DBs and each has at least 10 tables. &lt;/p&gt;\n\n&lt;p&gt;We ingest this data (in batch) into a Redshift cluster using AWS Glue (Spark). We have designed it such that 1 Glue Job will handle 1 table. This gives us fine-grained control of the &amp;quot;bookmark&amp;quot; (the last timestamp that was processed).&lt;/p&gt;\n\n&lt;p&gt;Problem: Now we have 100 connections to the databases going! And so many Glue Jobs!&lt;/p&gt;\n\n&lt;p&gt;We want to move toward 1 Glue Job per DB, but we wonder...&lt;/p&gt;\n\n&lt;p&gt;...What is the best practice for coding this Spark job? &lt;/p&gt;\n\n&lt;p&gt;We imagine we&amp;#39;ll:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Make 1 Spark job per db&lt;/li&gt;\n&lt;li&gt;In that job, make 10 DataFrames via jdbc connection (1 for each table in the DB)&lt;/li&gt;\n&lt;li&gt;Write each DF to Redshift via jdbc connection info.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is that a misuse of Spark cluster resources?&lt;/p&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1arn1as", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arn1as/reduce_db_connections_in_spark_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arn1as/reduce_db_connections_in_spark_job/", "subreddit_subscribers": 161031, "created_utc": 1708023364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Refter](https://refter.io) is a **hosted** dbt documentation tool with a heavy focus on data consumers and end users. Think analysts, product managers, data scientists, sql nerds, etc. In short, people that are directly accessing your modeled and integrated data. \n\nIt\u2019s simple, to the point, and most of all, effective for the audience it serves. It\u2019ll empower your data customers and stakeholders to self-serve, use your data correctly and overall help your teams to be more data-driven.  \n\nOn top of **everything** the standard **dbt docs** offer, refter includes:  \n\n\ud83c\udf33 **Entity relationship diagrams** \\- increase data understanding - showing relationship and ERD diagrams to end-users will make a huge difference in how they use your data.  \n\n\u2764\ufe0f\u200d\ud83e\ude79 **Lifecycle management** \\- show what, when and why your data model is changing - it\u2019ll help your end-users use the correct data and increase trust in your data.  \n\n\ud83e\udde0 **Knowledge sharing** \\- empower end-users to create and share their own knowledge base; or seed it with best practice queries and more.", "author_fullname": "t2_4he3cey3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt documentation tool for data consumers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arxqn9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708051125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://refter.io\"&gt;Refter&lt;/a&gt; is a &lt;strong&gt;hosted&lt;/strong&gt; dbt documentation tool with a heavy focus on data consumers and end users. Think analysts, product managers, data scientists, sql nerds, etc. In short, people that are directly accessing your modeled and integrated data. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s simple, to the point, and most of all, effective for the audience it serves. It\u2019ll empower your data customers and stakeholders to self-serve, use your data correctly and overall help your teams to be more data-driven.  &lt;/p&gt;\n\n&lt;p&gt;On top of &lt;strong&gt;everything&lt;/strong&gt; the standard &lt;strong&gt;dbt docs&lt;/strong&gt; offer, refter includes:  &lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf33 &lt;strong&gt;Entity relationship diagrams&lt;/strong&gt; - increase data understanding - showing relationship and ERD diagrams to end-users will make a huge difference in how they use your data.  &lt;/p&gt;\n\n&lt;p&gt;\u2764\ufe0f\u200d\ud83e\ude79 &lt;strong&gt;Lifecycle management&lt;/strong&gt; - show what, when and why your data model is changing - it\u2019ll help your end-users use the correct data and increase trust in your data.  &lt;/p&gt;\n\n&lt;p&gt;\ud83e\udde0 &lt;strong&gt;Knowledge sharing&lt;/strong&gt; - empower end-users to create and share their own knowledge base; or seed it with best practice queries and more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1arxqn9", "is_robot_indexable": true, "report_reasons": null, "author": "icebergh-io", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arxqn9/dbt_documentation_tool_for_data_consumers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arxqn9/dbt_documentation_tool_for_data_consumers/", "subreddit_subscribers": 161031, "created_utc": 1708051125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I am curious what you think  the key differences are between a senior DE and a lead DE?  What are the responsibilities of a lead  and what would make them great in the role? \n\nAsking as a current senior DE looking to become a lead", "author_fullname": "t2_8qi1s4iv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between a Senior &amp; Lead data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arwlba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708047806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am curious what you think  the key differences are between a senior DE and a lead DE?  What are the responsibilities of a lead  and what would make them great in the role? &lt;/p&gt;\n\n&lt;p&gt;Asking as a current senior DE looking to become a lead&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1arwlba", "is_robot_indexable": true, "report_reasons": null, "author": "fancyfanch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arwlba/difference_between_a_senior_lead_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arwlba/difference_between_a_senior_lead_data_engineer/", "subreddit_subscribers": 161031, "created_utc": 1708047806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am responsible for managing a large SQL Server database. Currently, we ingest data from a NoSQL database into SQL Server using an ETL tool. The data, such as customer information, is queried through endlink/customerid and returns unstructured JSON data that can change dynamically.\n\nIn our ETL process, we flatten the data and load most attributes separately, with some being placed in separate tables for nested objects. The full load is slow because querying endlink/customeridretrieves all data. To address this, we perform incremental loads, acknowledging changes have been received. However, occasional full loads are necessary, taking up to 10 hours and prone to failures.\n\nI am seeking advice on a more efficient architecture. I am considering implementing slowly changing dimensions (SCD) as well to provide more utility to end users (I understand the gist of SCDs but have not ever implemented myself on such a large scale) but unsure how to manage multiple queries for each customer attribute. Another consideration is loading all data into an interim data lake, like Azure Data Lake, before loading into SQL Server.\n\nIs there a clearer way to structure this process? I'm open to all ideas as well as live conversations/DMs/etc.", "author_fullname": "t2_bd0voyth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Soliciting Advice on NoSQL ingestion to SQL Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arvhju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708044615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am responsible for managing a large SQL Server database. Currently, we ingest data from a NoSQL database into SQL Server using an ETL tool. The data, such as customer information, is queried through endlink/customerid and returns unstructured JSON data that can change dynamically.&lt;/p&gt;\n\n&lt;p&gt;In our ETL process, we flatten the data and load most attributes separately, with some being placed in separate tables for nested objects. The full load is slow because querying endlink/customeridretrieves all data. To address this, we perform incremental loads, acknowledging changes have been received. However, occasional full loads are necessary, taking up to 10 hours and prone to failures.&lt;/p&gt;\n\n&lt;p&gt;I am seeking advice on a more efficient architecture. I am considering implementing slowly changing dimensions (SCD) as well to provide more utility to end users (I understand the gist of SCDs but have not ever implemented myself on such a large scale) but unsure how to manage multiple queries for each customer attribute. Another consideration is loading all data into an interim data lake, like Azure Data Lake, before loading into SQL Server.&lt;/p&gt;\n\n&lt;p&gt;Is there a clearer way to structure this process? I&amp;#39;m open to all ideas as well as live conversations/DMs/etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arvhju", "is_robot_indexable": true, "report_reasons": null, "author": "bananahramah", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arvhju/soliciting_advice_on_nosql_ingestion_to_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arvhju/soliciting_advice_on_nosql_ingestion_to_sql_server/", "subreddit_subscribers": 161031, "created_utc": 1708044615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to configure dbt Core to work without an actual connection? Basically my company uses a source that isn't supported by dbt, and I already have a bunch of the sql files written out with the various tables. I really just want to take advantage of the generate docs feature of dbt if possible.", "author_fullname": "t2_1gyiwuuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Core without a Database Connection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arsmxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708037091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to configure dbt Core to work without an actual connection? Basically my company uses a source that isn&amp;#39;t supported by dbt, and I already have a bunch of the sql files written out with the various tables. I really just want to take advantage of the generate docs feature of dbt if possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arsmxt", "is_robot_indexable": true, "report_reasons": null, "author": "trianglesteve", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arsmxt/dbt_core_without_a_database_connection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arsmxt/dbt_core_without_a_database_connection/", "subreddit_subscribers": 161031, "created_utc": 1708037091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seeking feedback on one of these conferences... I'm looking to request for approval to attend one. We're in the early stages of developing a data lake solution. Just trying to figure which one is most valuable to request. Any input is appreciated.\n\n&amp;#x200B;", "author_fullname": "t2_3rwsh2j1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conferences: Databrick vs Google Next vs. Amazon Re:invent", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arryct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708035397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking feedback on one of these conferences... I&amp;#39;m looking to request for approval to attend one. We&amp;#39;re in the early stages of developing a data lake solution. Just trying to figure which one is most valuable to request. Any input is appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1arryct", "is_robot_indexable": true, "report_reasons": null, "author": "Aware-Expression4004", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arryct/conferences_databrick_vs_google_next_vs_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arryct/conferences_databrick_vs_google_next_vs_amazon/", "subreddit_subscribers": 161031, "created_utc": 1708035397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm currently working on a project for school that requires me to utilize the Hadoop ecosystem (HDFS, Hive, etc.). I've set up a Cloudera VM on my Mac using Docker, but I'm wondering if this is the most efficient way to work with Hadoop locally. Specifically, I'm curious if I can access data stored on HDFS in Cloudera using my local machine, for instance, running a Python code.\n\nAny insights or recommendations would be greatly appreciated! Thanks in advance!", "author_fullname": "t2_e1i3duqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Way to Use Hadoop Ecosystem on Local Machine for School Project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arpxne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708030488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a project for school that requires me to utilize the Hadoop ecosystem (HDFS, Hive, etc.). I&amp;#39;ve set up a Cloudera VM on my Mac using Docker, but I&amp;#39;m wondering if this is the most efficient way to work with Hadoop locally. Specifically, I&amp;#39;m curious if I can access data stored on HDFS in Cloudera using my local machine, for instance, running a Python code.&lt;/p&gt;\n\n&lt;p&gt;Any insights or recommendations would be greatly appreciated! Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arpxne", "is_robot_indexable": true, "report_reasons": null, "author": "ryan7ait", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arpxne/best_way_to_use_hadoop_ecosystem_on_local_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arpxne/best_way_to_use_hadoop_ecosystem_on_local_machine/", "subreddit_subscribers": 161031, "created_utc": 1708030488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What have been some of the trickiest partitioning experiences you had, how did you overcome?", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Biggest Partitioning Pains?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1armg0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708021879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What have been some of the trickiest partitioning experiences you had, how did you overcome?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1armg0e", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1armg0e/biggest_partitioning_pains/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1armg0e/biggest_partitioning_pains/", "subreddit_subscribers": 161031, "created_utc": 1708021879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks. I've put together my first end-to-end data engineering project which is building a batch ELT pipeline to gather tennis match-level data and transform it for analytics and visualization.\n\nYou can see the [project repo here](https://github.com/sethltaylor/Tennis-Analytics-Pipeline). I also gave a [talk on the project](https://www.youtube.com/watch?v=4dIt-vobJN8) to a local data engineering meetup group if you want to hear me go more in depth on the pipeline and my thought process.\n\nThe core elements of the pipeline are:\n\n* Terraform\n   * Creating and managing the cloud infrastructure (Cloud Storage and BigQuery) as code.\n* Python + Prefect\n   * Extraction and loading of the raw data into Cloud Storage and BigQuery. Prefect is used as the orchestrator to schedule the batch runs and to parameterize the scripts and manage credentials/connections.\n* dbt\n   * dbt is used to transform the data within the BigQuery data warehouse. Data lands in raw tables and then is transformed and combined with other data sources in staging models before final analytical models are published into production.\n   * dbt tests are also used to check for things like referential integrity,  uniqueness and completeness of unique identifiers, and acceptable value constraints on numeric data.\n   * The modeling is more of a one big table approach instead of dimensional modelling.\n* Looker Studio is used to produce the [final dashboard.](https://lookerstudio.google.com/u/0/reporting/faf90ed4-8bda-40dd-9a1b-cdd4466e6d49)\n   * Dashboarding wasn't really my core goal here and I'm not the best dashboarder in the world, so this just addresses a couple core questions like:\n      * Player performance over time and by country\n      * Number of bagels by player over time\n\nSince this was my first DE project I'm sure there's a lot of things I could add like CI/CD for the pipeline, but interested to hear people's thoughts.", "author_fullname": "t2_u7dxdkls1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing an Analytics Pipeline on GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arlw5t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708020540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks. I&amp;#39;ve put together my first end-to-end data engineering project which is building a batch ELT pipeline to gather tennis match-level data and transform it for analytics and visualization.&lt;/p&gt;\n\n&lt;p&gt;You can see the &lt;a href=\"https://github.com/sethltaylor/Tennis-Analytics-Pipeline\"&gt;project repo here&lt;/a&gt;. I also gave a &lt;a href=\"https://www.youtube.com/watch?v=4dIt-vobJN8\"&gt;talk on the project&lt;/a&gt; to a local data engineering meetup group if you want to hear me go more in depth on the pipeline and my thought process.&lt;/p&gt;\n\n&lt;p&gt;The core elements of the pipeline are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Terraform\n\n&lt;ul&gt;\n&lt;li&gt;Creating and managing the cloud infrastructure (Cloud Storage and BigQuery) as code.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Python + Prefect\n\n&lt;ul&gt;\n&lt;li&gt;Extraction and loading of the raw data into Cloud Storage and BigQuery. Prefect is used as the orchestrator to schedule the batch runs and to parameterize the scripts and manage credentials/connections.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;dbt\n\n&lt;ul&gt;\n&lt;li&gt;dbt is used to transform the data within the BigQuery data warehouse. Data lands in raw tables and then is transformed and combined with other data sources in staging models before final analytical models are published into production.&lt;/li&gt;\n&lt;li&gt;dbt tests are also used to check for things like referential integrity,  uniqueness and completeness of unique identifiers, and acceptable value constraints on numeric data.&lt;/li&gt;\n&lt;li&gt;The modeling is more of a one big table approach instead of dimensional modelling.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Looker Studio is used to produce the &lt;a href=\"https://lookerstudio.google.com/u/0/reporting/faf90ed4-8bda-40dd-9a1b-cdd4466e6d49\"&gt;final dashboard.&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dashboarding wasn&amp;#39;t really my core goal here and I&amp;#39;m not the best dashboarder in the world, so this just addresses a couple core questions like:\n\n&lt;ul&gt;\n&lt;li&gt;Player performance over time and by country&lt;/li&gt;\n&lt;li&gt;Number of bagels by player over time&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Since this was my first DE project I&amp;#39;m sure there&amp;#39;s a lot of things I could add like CI/CD for the pipeline, but interested to hear people&amp;#39;s thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?auto=webp&amp;s=6e067fcda8568abbf80c93da0f9dfe2ca608bc52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6af1f18c6b883049db240a0f23db125bf8d6a21", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f91a9649070e12a3515498c274fd4d9d0023d7e0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22e62d9367a4e7d4b885190fbb5a0431e3cbc186", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f95973dd7150fc93f1ba77fe53d5fe725fe41978", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bfb52372a563acec7c736351c39828fb06bc5c51", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/b0WkdQGdVpbDyRwhk1lFI5qY1mI0oI91irghkrU4PoM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f82ed50f2db5bcf5876dc024184789736896c23", "width": 1080, "height": 540}], "variants": {}, "id": "eI3fN87Z59tk6zpsw0oLqvrV56XSkgp1YY0GBkGqY4c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1arlw5t", "is_robot_indexable": true, "report_reasons": null, "author": "SchemaScorcher", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arlw5t/designing_an_analytics_pipeline_on_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arlw5t/designing_an_analytics_pipeline_on_gcp/", "subreddit_subscribers": 161031, "created_utc": 1708020540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys ! I was just wondering is there a way to connect to azure sql database from azure databricks notebook using python. I am new to azure but any kind of help would be great! I have already created a cluster etc have a well organised database setup as well. Just want to establish a connection to do some analysis using the tables in azure sql database. ", "author_fullname": "t2_9sfh3rih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting To Azure sql data base using python from azure data ricks notebook.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arj04h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708013440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys ! I was just wondering is there a way to connect to azure sql database from azure databricks notebook using python. I am new to azure but any kind of help would be great! I have already created a cluster etc have a well organised database setup as well. Just want to establish a connection to do some analysis using the tables in azure sql database. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arj04h", "is_robot_indexable": true, "report_reasons": null, "author": "Automatic_Will_5137", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arj04h/connecting_to_azure_sql_data_base_using_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arj04h/connecting_to_azure_sql_data_base_using_python/", "subreddit_subscribers": 161031, "created_utc": 1708013440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am new to DE and I am trying to build a pipeline using Airflow for handling the task , Spark for transforming  (processing) and I am still kind of lost when it comes to the data lake, since Xcom cannot transmit huge amount of data between tasks I need a data lake to read from and then after transforming I'll be loading it in the data warehouse.  \nWhat I am confused about right now is what kind of data lake should I use, by that  I mean that should I use apache hive or Hbase or anything you suggest for the 2 I am using up above?  \nThank you so much in advance for the help!!", "author_fullname": "t2_jebuisef", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with data lakes in a pipeline!~", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arfu1y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708005002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am new to DE and I am trying to build a pipeline using Airflow for handling the task , Spark for transforming  (processing) and I am still kind of lost when it comes to the data lake, since Xcom cannot transmit huge amount of data between tasks I need a data lake to read from and then after transforming I&amp;#39;ll be loading it in the data warehouse.&lt;br/&gt;\nWhat I am confused about right now is what kind of data lake should I use, by that  I mean that should I use apache hive or Hbase or anything you suggest for the 2 I am using up above?&lt;br/&gt;\nThank you so much in advance for the help!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arfu1y", "is_robot_indexable": true, "report_reasons": null, "author": "panda-drinking-boba", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arfu1y/help_with_data_lakes_in_a_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arfu1y/help_with_data_lakes_in_a_pipeline/", "subreddit_subscribers": 161031, "created_utc": 1708005002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m so annoyed at this crappy company and wanted to know if others experienced the same.\n\nI was enrolled to write the databricks data engineer associate exam which is proctored by Kryterion. From the very start I had a hard time getting setup. Luckily I started my setup 30 minutes before the exam but because I had some glitches I was just a few minutes late. This just gives you an idea of how bad their process is. \n\nAnyway moving along it gets worse, mid way through my test they messaged me and told me they can\u2019t see my on screen because they are experiencing technical difficulties and asked me to reboot and come back on again. After rebooting, I wasn\u2019t even able to sign up and they sent me an incorrect link so I had to quickly message someone on Kryterion to help me and they sent me a working link. Finally, that link worked and I was able to resume my test.\n\nDuring my exam they kept messaging me saying that my eyes are not on the screen even though it was. Then they ping me again and asked to see my desk and all the walls in my room which I showed them and there was nothing. They told me to wait and low and behold they suspended my exam. They gave a generic response of environmental and behavioural factors which led to them suspending my exam.  I am beyond infuriated because I spent WEEKS preparing for this. \n\nDid anyone have this happen to them with Kryterion? I had such a terrible from start to finish. I wrote exams before for azure via Pearson Vue and never had any issues.", "author_fullname": "t2_oh3dal99d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kryterion suspended my exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as34ck", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708069494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m so annoyed at this crappy company and wanted to know if others experienced the same.&lt;/p&gt;\n\n&lt;p&gt;I was enrolled to write the databricks data engineer associate exam which is proctored by Kryterion. From the very start I had a hard time getting setup. Luckily I started my setup 30 minutes before the exam but because I had some glitches I was just a few minutes late. This just gives you an idea of how bad their process is. &lt;/p&gt;\n\n&lt;p&gt;Anyway moving along it gets worse, mid way through my test they messaged me and told me they can\u2019t see my on screen because they are experiencing technical difficulties and asked me to reboot and come back on again. After rebooting, I wasn\u2019t even able to sign up and they sent me an incorrect link so I had to quickly message someone on Kryterion to help me and they sent me a working link. Finally, that link worked and I was able to resume my test.&lt;/p&gt;\n\n&lt;p&gt;During my exam they kept messaging me saying that my eyes are not on the screen even though it was. Then they ping me again and asked to see my desk and all the walls in my room which I showed them and there was nothing. They told me to wait and low and behold they suspended my exam. They gave a generic response of environmental and behavioural factors which led to them suspending my exam.  I am beyond infuriated because I spent WEEKS preparing for this. &lt;/p&gt;\n\n&lt;p&gt;Did anyone have this happen to them with Kryterion? I had such a terrible from start to finish. I wrote exams before for azure via Pearson Vue and never had any issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1as34ck", "is_robot_indexable": true, "report_reasons": null, "author": "Worldly_Horror_2754", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as34ck/kryterion_suspended_my_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as34ck/kryterion_suspended_my_exam/", "subreddit_subscribers": 161031, "created_utc": 1708069494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, just wanted to make sure this approach is reasonable. On the OLAP end of the business we have an analyst team comfortable in SQL and want to empower them to create summary views on some raw data sources (very wide sparse parquet tables with some nested structs) to make it more ergonomic/useful for reading into BI tools.\n\nIt seems to me like Athena views are a good way to let them do this, but for some of the more complicated processes I'd like to materialise the output views. It seems like implementing these looks like (roughly in order of complexity):\n\n1. No need to materialise the view: just use Athena `CREATE VIEW &lt;view&gt; AS SELECT &lt;query&gt;`\n2. Whole-table materialisations: periodically trigger an Athena `CREATE TABLE &lt;tbl&gt; AS SELECT &lt;view&gt;`\n3. Incremental materialisations (append-only): trigger the drop/CTAS on schema change but periodically filter the virtual table on a key for an Athena `INSERT INTO`\n4. Incremental materialisations with mutable rows: as above but with an Iceberg table and `MERGE INTOs`\n\nFor simplicity maybe just opt for Iceberg wherever. Rough implementation would be letting the analyst deploy the view query SQL to a bucket, which would then be executed first as a `CREATE VIEW` which would be then used for the CTAS/INSERTs/MERGEs (handled by a lambda or a barebones python glue job). This would roughly support forking summary table definitions and blue/green iterations. \n\nSound reasonable? The ideal scenario would be having a repo with all the current view definitions and executions transparently available. Something like Trino's [`CREATE MATERIALIZED VIEW`](https://trino.io/docs/current/sql/create-materialized-view.html) would better of course but I don't think its supported by Athena yet and I don't think we'll be moving off that any time soon.", "author_fullname": "t2_i0a3z0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental Materialisation for OLAP in Athena + Iceberg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as0q8u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708060615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, just wanted to make sure this approach is reasonable. On the OLAP end of the business we have an analyst team comfortable in SQL and want to empower them to create summary views on some raw data sources (very wide sparse parquet tables with some nested structs) to make it more ergonomic/useful for reading into BI tools.&lt;/p&gt;\n\n&lt;p&gt;It seems to me like Athena views are a good way to let them do this, but for some of the more complicated processes I&amp;#39;d like to materialise the output views. It seems like implementing these looks like (roughly in order of complexity):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;No need to materialise the view: just use Athena &lt;code&gt;CREATE VIEW &amp;lt;view&amp;gt; AS SELECT &amp;lt;query&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Whole-table materialisations: periodically trigger an Athena &lt;code&gt;CREATE TABLE &amp;lt;tbl&amp;gt; AS SELECT &amp;lt;view&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Incremental materialisations (append-only): trigger the drop/CTAS on schema change but periodically filter the virtual table on a key for an Athena &lt;code&gt;INSERT INTO&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Incremental materialisations with mutable rows: as above but with an Iceberg table and &lt;code&gt;MERGE INTOs&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For simplicity maybe just opt for Iceberg wherever. Rough implementation would be letting the analyst deploy the view query SQL to a bucket, which would then be executed first as a &lt;code&gt;CREATE VIEW&lt;/code&gt; which would be then used for the CTAS/INSERTs/MERGEs (handled by a lambda or a barebones python glue job). This would roughly support forking summary table definitions and blue/green iterations. &lt;/p&gt;\n\n&lt;p&gt;Sound reasonable? The ideal scenario would be having a repo with all the current view definitions and executions transparently available. Something like Trino&amp;#39;s &lt;a href=\"https://trino.io/docs/current/sql/create-materialized-view.html\"&gt;&lt;code&gt;CREATE MATERIALIZED VIEW&lt;/code&gt;&lt;/a&gt; would better of course but I don&amp;#39;t think its supported by Athena yet and I don&amp;#39;t think we&amp;#39;ll be moving off that any time soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1as0q8u", "is_robot_indexable": true, "report_reasons": null, "author": "sansampersamp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as0q8u/incremental_materialisation_for_olap_in_athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as0q8u/incremental_materialisation_for_olap_in_athena/", "subreddit_subscribers": 161031, "created_utc": 1708060615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nNewbie here. I'm wondering if community knows about any sort of base/classic scenarios one can use to exercise the process of spin up pipelines. Something like a \"todo MVC\" for programming.\n\nQuite sure I can have that buying online courses on a specialized book (do not hesitate on recommend good ones) but I was wondering if the community had something more universal per say.\n\nThanks.", "author_fullname": "t2_tbr7l72a2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Base Scenarios for Exercise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arxw6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708051589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Newbie here. I&amp;#39;m wondering if community knows about any sort of base/classic scenarios one can use to exercise the process of spin up pipelines. Something like a &amp;quot;todo MVC&amp;quot; for programming.&lt;/p&gt;\n\n&lt;p&gt;Quite sure I can have that buying online courses on a specialized book (do not hesitate on recommend good ones) but I was wondering if the community had something more universal per say.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1arxw6g", "is_robot_indexable": true, "report_reasons": null, "author": "RustyTheCynical", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1arxw6g/base_scenarios_for_exercise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arxw6g/base_scenarios_for_exercise/", "subreddit_subscribers": 161031, "created_utc": 1708051589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have joined this company for the Data Warehouse Team and I was looking at the mapping document for Source to Target.\n\n I noticed that same source database, tables &amp; columns gets loaded into the target database even after the transformation, I would like to know what could be the possible reason behind it? What concepts should I look into to understand it?\n\nI am novice to the data engineer field so my question might sound silly so bear with me. Any help or advice will be greatly appreciated. Thanks in advance.", "author_fullname": "t2_d8bo9rvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with the logic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1arvrrn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708045435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have joined this company for the Data Warehouse Team and I was looking at the mapping document for Source to Target.&lt;/p&gt;\n\n&lt;p&gt;I noticed that same source database, tables &amp;amp; columns gets loaded into the target database even after the transformation, I would like to know what could be the possible reason behind it? What concepts should I look into to understand it?&lt;/p&gt;\n\n&lt;p&gt;I am novice to the data engineer field so my question might sound silly so bear with me. Any help or advice will be greatly appreciated. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1arvrrn", "is_robot_indexable": true, "report_reasons": null, "author": "Mammoth_Currency404", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1arvrrn/need_help_with_the_logic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1arvrrn/need_help_with_the_logic/", "subreddit_subscribers": 161031, "created_utc": 1708045435.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}