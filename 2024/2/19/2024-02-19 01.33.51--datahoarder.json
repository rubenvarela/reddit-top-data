{"kind": "Listing", "data": {"after": "t3_1athqo7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello! my beloved great grandmother's service was livestreamed on this [website](https://view.oneroomstreaming.com/index.php?data=MTcwMDM0OTIxMzI4MTczMyZvbmVyb29tLWFkbWluJmNvcHlfbGluaw==), and while they offer the option of downloading it, they want you to pay $40(or something like that) for it!\n\nheck no, screw that!!\n\ncould you guys please assist me in this endeavor? I've dabbled with developer tools and the like, but goddamn I am really out of touch, I haven't had any success.\n\nthank you guys! \ud83d\udc97", "author_fullname": "t2_rdye144", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download my loved one's funeral service from this website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atm2xr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 253, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 253, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708232429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "view.oneroomstreaming.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello! my beloved great grandmother&amp;#39;s service was livestreamed on this &lt;a href=\"https://view.oneroomstreaming.com/index.php?data=MTcwMDM0OTIxMzI4MTczMyZvbmVyb29tLWFkbWluJmNvcHlfbGluaw==\"&gt;website&lt;/a&gt;, and while they offer the option of downloading it, they want you to pay $40(or something like that) for it!&lt;/p&gt;\n\n&lt;p&gt;heck no, screw that!!&lt;/p&gt;\n\n&lt;p&gt;could you guys please assist me in this endeavor? I&amp;#39;ve dabbled with developer tools and the like, but goddamn I am really out of touch, I haven&amp;#39;t had any success.&lt;/p&gt;\n\n&lt;p&gt;thank you guys! \ud83d\udc97&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://view.oneroomstreaming.com/index.php?data=MTcwMDM0OTIxMzI4MTczMyZvbmVyb29tLWFkbWluJmNvcHlfbGluaw==", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atm2xr", "is_robot_indexable": true, "report_reasons": null, "author": "lovenet99", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atm2xr/how_can_i_download_my_loved_ones_funeral_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://view.oneroomstreaming.com/index.php?data=MTcwMDM0OTIxMzI4MTczMyZvbmVyb29tLWFkbWluJmNvcHlfbGluaw==", "subreddit_subscribers": 732982, "created_utc": 1708232429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Old tech question someone here with more experience may be able to help with:\n\nI'm gearing up to archive a large volume of old DVCAM/Video8 tapes. The recorders I have for this can output as a video or sdi. \n\nAm I better off buying an HDMI capture card and  converter boxes from one of these formats, or just a card that takes multiple SDI streams? \n\nPhoto is just for attention, I have another half dozen various Sony pro level tape recording decks.\n\nHelp appreciated!\n\n", "author_fullname": "t2_mzwj8p0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to capture multiple analog video streams from one computer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1atyk2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ybYb4iD5EjMWtRcOAsIxwLQA8G13qiYq9vK5YDdQLeE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708275104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Old tech question someone here with more experience may be able to help with:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m gearing up to archive a large volume of old DVCAM/Video8 tapes. The recorders I have for this can output as a video or sdi. &lt;/p&gt;\n\n&lt;p&gt;Am I better off buying an HDMI capture card and  converter boxes from one of these formats, or just a card that takes multiple SDI streams? &lt;/p&gt;\n\n&lt;p&gt;Photo is just for attention, I have another half dozen various Sony pro level tape recording decks.&lt;/p&gt;\n\n&lt;p&gt;Help appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0afzavvxidjc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?auto=webp&amp;s=bab7f84c79390cedc297035b2b92f81491ef6497", "width": 4080, "height": 3072}, "resolutions": [{"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3376170c8c9d2325bf25cdb02f4da0a239d7964", "width": 108, "height": 81}, {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83978f6c0adc145eea91fa3ae2325226390e7be6", "width": 216, "height": 162}, {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3ebb540c3eafdc8dbe5c646b795f9ddb183c291", "width": 320, "height": 240}, {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e897f1c621629b2708ac44c5b4af0f041e6048f", "width": 640, "height": 481}, {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f7a5e13fc1613e450acd858ea89c861aad4dbd3", "width": 960, "height": 722}, {"url": "https://preview.redd.it/0afzavvxidjc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cca267cf6a3426917f2d2e09be2dc4660e64aed", "width": 1080, "height": 813}], "variants": {}, "id": "bZ6WWdmJ-D7vNxxnDdi1cWXOGkBgOVZ2bVBInLzccWs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atyk2m", "is_robot_indexable": true, "report_reasons": null, "author": "Coastalfilmlab", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atyk2m/best_way_to_capture_multiple_analog_video_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0afzavvxidjc1.jpeg", "subreddit_subscribers": 732982, "created_utc": 1708275104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m looking for a cloud backup solution and was wondering if installing ownCloud on my (unlimited) shared hosting space and using that instead, is something to consider? There are many shared hosting providers that provide unlimited storage space. What would be the drawbacks to using this, as compared to a sync.com (for example)? I\u2019m looking to backup ~10TB (or more) of data and AWS\u2019 glacier is beyond my budget. I\u2019ve no experience with ownCloud either, so I\u2019ve yet to explore whether it\u2019ll allow me to sync my external drives and/or folders from my NAS.", "author_fullname": "t2_ayrsv8ji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ownCloud on Shared Hosting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au7b0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708296305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for a cloud backup solution and was wondering if installing ownCloud on my (unlimited) shared hosting space and using that instead, is something to consider? There are many shared hosting providers that provide unlimited storage space. What would be the drawbacks to using this, as compared to a sync.com (for example)? I\u2019m looking to backup ~10TB (or more) of data and AWS\u2019 glacier is beyond my budget. I\u2019ve no experience with ownCloud either, so I\u2019ve yet to explore whether it\u2019ll allow me to sync my external drives and/or folders from my NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au7b0w", "is_robot_indexable": true, "report_reasons": null, "author": "redt-aa", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au7b0w/owncloud_on_shared_hosting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au7b0w/owncloud_on_shared_hosting/", "subreddit_subscribers": 732982, "created_utc": 1708296305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/1att9cr)", "author_fullname": "t2_umytgvvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best SSD Brand and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1att9cr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708260151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1att9cr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "19.7TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1att9cr", "is_robot_indexable": true, "report_reasons": null, "author": "ElonTastical", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1708432951043, "options": [{"text": "Samsung", "id": "27132488"}, {"text": "Kingston", "id": "27132489"}, {"text": "Sandisk", "id": "27132490"}, {"text": "WD", "id": "27132491"}, {"text": "Other", "id": "27132492"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 666, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1att9cr/best_ssd_brand_and_why/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/1att9cr/best_ssd_brand_and_why/", "subreddit_subscribers": 732982, "created_utc": 1708260151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I like to archive YouTube videos (specifically those that are attached to controversy) as a hobby, and back in October, I decided to archive the entirety of the YouTuber SSSniperWolf following the Jacksfilms doxing incident. One thing I wanted to try was to upload it all in one compressed file, purely to see if it could be done. I am curious if anyone here has any experience with Archive.org, and if this is truly the largest single file to have ever been archived on the site. (r/internetarchive appears to be fairly dead.) It was such a pain to even download all the videos, as SSSniperWolf names many of her videos the same thing, and the sheer number of them made JDownloader2 a nightmare to work with. Compression took about a week on my old cpu, and the upload took a week as well. But, it's all done now, and the file is up [here](https://archive.org/details/the-sssniper-wolf-collection). Again, I'm curious to see if anyone else knows of a larger (single) file, or has any similar experiences with uploading large files to the Internet Archive. ", "author_fullname": "t2_m9358ab7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I just uploaded a 621gb file onto Archive.org.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1au8xg7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708300395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I like to archive YouTube videos (specifically those that are attached to controversy) as a hobby, and back in October, I decided to archive the entirety of the YouTuber SSSniperWolf following the Jacksfilms doxing incident. One thing I wanted to try was to upload it all in one compressed file, purely to see if it could be done. I am curious if anyone here has any experience with Archive.org, and if this is truly the largest single file to have ever been archived on the site. (&lt;a href=\"/r/internetarchive\"&gt;r/internetarchive&lt;/a&gt; appears to be fairly dead.) It was such a pain to even download all the videos, as SSSniperWolf names many of her videos the same thing, and the sheer number of them made JDownloader2 a nightmare to work with. Compression took about a week on my old cpu, and the upload took a week as well. But, it&amp;#39;s all done now, and the file is up &lt;a href=\"https://archive.org/details/the-sssniper-wolf-collection\"&gt;here&lt;/a&gt;. Again, I&amp;#39;m curious to see if anyone else knows of a larger (single) file, or has any similar experiences with uploading large files to the Internet Archive. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JNelOFzn_6ryDE1DqvINg6U-paRZwpJKT2i3BCWsN5c.jpg?auto=webp&amp;s=8c8391229b0031768e8d3a03539efa97a29eb124", "width": 180, "height": 113}, "resolutions": [{"url": "https://external-preview.redd.it/JNelOFzn_6ryDE1DqvINg6U-paRZwpJKT2i3BCWsN5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72c14fcb875a943d770599598d7d1f51f9d45998", "width": 108, "height": 67}], "variants": {}, "id": "ubPq5BTO47KCSQh4-xjULOnCmSEQ6xcwdHeKPNZGXiA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1au8xg7", "is_robot_indexable": true, "report_reasons": null, "author": "Questtrek", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1au8xg7/i_just_uploaded_a_621gb_file_onto_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au8xg7/i_just_uploaded_a_621gb_file_onto_archiveorg/", "subreddit_subscribers": 732982, "created_utc": 1708300395.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently made post like an hour ago which SSD brand do you guys prefer and noticed many of you mentioned the Samsung is tricky they had hardware failures last year ago. I'm planning to buy Samsung Evo 870 (MZ-77E4T0BW) and 990 Pro (V9P4T0) Are they good? How do I know the models are prone to failure?", "author_fullname": "t2_umytgvvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung SSDs to avoid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atv6g7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708266177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently made post like an hour ago which SSD brand do you guys prefer and noticed many of you mentioned the Samsung is tricky they had hardware failures last year ago. I&amp;#39;m planning to buy Samsung Evo 870 (MZ-77E4T0BW) and 990 Pro (V9P4T0) Are they good? How do I know the models are prone to failure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "19.7TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atv6g7", "is_robot_indexable": true, "report_reasons": null, "author": "ElonTastical", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1atv6g7/samsung_ssds_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atv6g7/samsung_ssds_to_avoid/", "subreddit_subscribers": 732982, "created_utc": 1708266177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a used SAS controller, needs just 4 ports, looking for the absolute lowest TDP\n\nAny suggestions? \n\nLowest cost is best! as this will be a very temporary install", "author_fullname": "t2_6xkyp933", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lowest TDP PCI-E SAS controller? (Performance not an issue)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au1b5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708281803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a used SAS controller, needs just 4 ports, looking for the absolute lowest TDP&lt;/p&gt;\n\n&lt;p&gt;Any suggestions? &lt;/p&gt;\n\n&lt;p&gt;Lowest cost is best! as this will be a very temporary install&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au1b5f", "is_robot_indexable": true, "report_reasons": null, "author": "VviFMCgY", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au1b5f/lowest_tdp_pcie_sas_controller_performance_not_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au1b5f/lowest_tdp_pcie_sas_controller_performance_not_an/", "subreddit_subscribers": 732982, "created_utc": 1708281803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I apologize, I'm not too well versed in how this works.  \nI'm in the process of transcoding a good portion of my media library and also preparing to migrate the hoard to something a little more legit.\n\nRight now, I have three drives connected to an old windows gaming laptop, each set as a samba share. As I have time, and as I think of how I want things I've been accessing these remotely either on my phone and other computers during the day to copy/move/delete.  \nWhen I copy/move a file from one share/drive to another, it transfers over the network using my remote device as a relay which is definitely not ideal.\n\nUntil now I've been starting a remote desktop session to move files directly with Explorer but that's pretty cumbersome for just moving a few files, especially on a 6 inch phone screen.\n\nHow do I \"tell\" my system to copy things locally?", "author_fullname": "t2_7vm71g4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying between shared drives on the same system uses my remote client as a relay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atyvo2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708275886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize, I&amp;#39;m not too well versed in how this works.&lt;br/&gt;\nI&amp;#39;m in the process of transcoding a good portion of my media library and also preparing to migrate the hoard to something a little more legit.&lt;/p&gt;\n\n&lt;p&gt;Right now, I have three drives connected to an old windows gaming laptop, each set as a samba share. As I have time, and as I think of how I want things I&amp;#39;ve been accessing these remotely either on my phone and other computers during the day to copy/move/delete.&lt;br/&gt;\nWhen I copy/move a file from one share/drive to another, it transfers over the network using my remote device as a relay which is definitely not ideal.&lt;/p&gt;\n\n&lt;p&gt;Until now I&amp;#39;ve been starting a remote desktop session to move files directly with Explorer but that&amp;#39;s pretty cumbersome for just moving a few files, especially on a 6 inch phone screen.&lt;/p&gt;\n\n&lt;p&gt;How do I &amp;quot;tell&amp;quot; my system to copy things locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atyvo2", "is_robot_indexable": true, "report_reasons": null, "author": "c4pt1n54n0", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atyvo2/copying_between_shared_drives_on_the_same_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atyvo2/copying_between_shared_drives_on_the_same_system/", "subreddit_subscribers": 732982, "created_utc": 1708275886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This one's kind of complicated so I'll do my best to explain. I like to download comics from Twitter from specific artists.\n\nSometimes they post random things as well, so I usually just manually download but it's getting tedious.\n\nHere's what I need:\n- download automatically from specific account twitter\n- don't redownload deleted files (so I can manually remove their junk posts/not comics)\n\nIt sounds simple enough but I can't figure out a way to do this. So far I just use a thing called wfdownloader every while and filter with Twitter search query excluding time ranges of previous downloads.\n\nAnybody know how to go about this?", "author_fullname": "t2_6wxamqg4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to auto download from Twitter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atpw9l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708247043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This one&amp;#39;s kind of complicated so I&amp;#39;ll do my best to explain. I like to download comics from Twitter from specific artists.&lt;/p&gt;\n\n&lt;p&gt;Sometimes they post random things as well, so I usually just manually download but it&amp;#39;s getting tedious.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I need:\n- download automatically from specific account twitter\n- don&amp;#39;t redownload deleted files (so I can manually remove their junk posts/not comics)&lt;/p&gt;\n\n&lt;p&gt;It sounds simple enough but I can&amp;#39;t figure out a way to do this. So far I just use a thing called wfdownloader every while and filter with Twitter search query excluding time ranges of previous downloads.&lt;/p&gt;\n\n&lt;p&gt;Anybody know how to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atpw9l", "is_robot_indexable": true, "report_reasons": null, "author": "alfredospsta295720", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atpw9l/how_to_auto_download_from_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atpw9l/how_to_auto_download_from_twitter/", "subreddit_subscribers": 732982, "created_utc": 1708247043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've googled before asking and I've tried HHtrack and WebCopy, but both give me only one page (even thought at least webcopy should've been able to copy much more, but maybe I messed up with settings somehow?) so I decided to ask more knowledgeable people since data hording is somewhat new to me. The site I am interested in is tvtropes and a couple of sites of similar nature but smaller size. Also it would be nice to know if there is 1) a way to measure approximate size of the entire site like that and 2) a way to basically make a tool to copy only links that starts with tvtropes org and not go to actual wikipedia or something else even if thouse links are on one of the pages. Thanks in advance!", "author_fullname": "t2_blt28u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I download an entire website (which is close to a wiki in a structure) and not just one page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1au9nds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708302292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve googled before asking and I&amp;#39;ve tried HHtrack and WebCopy, but both give me only one page (even thought at least webcopy should&amp;#39;ve been able to copy much more, but maybe I messed up with settings somehow?) so I decided to ask more knowledgeable people since data hording is somewhat new to me. The site I am interested in is tvtropes and a couple of sites of similar nature but smaller size. Also it would be nice to know if there is 1) a way to measure approximate size of the entire site like that and 2) a way to basically make a tool to copy only links that starts with tvtropes org and not go to actual wikipedia or something else even if thouse links are on one of the pages. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au9nds", "is_robot_indexable": true, "report_reasons": null, "author": "Tempest_MoFFy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au9nds/how_do_i_download_an_entire_website_which_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au9nds/how_do_i_download_an_entire_website_which_is/", "subreddit_subscribers": 732982, "created_utc": 1708302292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to swap my 256 GB SSD to a 512 GB SSD for my laptop by replacing the stick and I'm using Macrium Reflect to clone everything to save my data. The 512 GB SSD was originally in my friend's laptop.\n\nHopefully this is the right subreddit to ask about this, but I'm having trouble trying to clone my data. Everything worked fine until the 3rd partition which is my C drive. It's been stuck on 98% current progress for C drive, but 1. the bar isn't green, 2. overall progress still says 0 and 3. it's been stuck like that for almost fifteen minutes now. [This is what it looks like](https://imgur.com/a/Cu5VJea)\n\nI'm not sure if I should just wait it out? Is this normal or is there something wrong? Should I just retry the cloning process? I'm not that tech competent so sorry if this sounds ignorant but I don't know why it's stuck and taking so long\n\nI'm following [this video](https://www.youtube.com/watch?v=uE9xMNidGEo) and using an adapter [here](https://imgur.com/a/nnS4aHI) instead of an enclosure. Friend said it was the same thing(??). So far everything is fine, I copied the tutorial to the dot but his only took eight minutes while mine is been more than half an hour at the same glitched percentage..", "author_fullname": "t2_aw273xef", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Reflect stuck during cloning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au86x5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708299641.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708298492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to swap my 256 GB SSD to a 512 GB SSD for my laptop by replacing the stick and I&amp;#39;m using Macrium Reflect to clone everything to save my data. The 512 GB SSD was originally in my friend&amp;#39;s laptop.&lt;/p&gt;\n\n&lt;p&gt;Hopefully this is the right subreddit to ask about this, but I&amp;#39;m having trouble trying to clone my data. Everything worked fine until the 3rd partition which is my C drive. It&amp;#39;s been stuck on 98% current progress for C drive, but 1. the bar isn&amp;#39;t green, 2. overall progress still says 0 and 3. it&amp;#39;s been stuck like that for almost fifteen minutes now. &lt;a href=\"https://imgur.com/a/Cu5VJea\"&gt;This is what it looks like&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if I should just wait it out? Is this normal or is there something wrong? Should I just retry the cloning process? I&amp;#39;m not that tech competent so sorry if this sounds ignorant but I don&amp;#39;t know why it&amp;#39;s stuck and taking so long&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m following &lt;a href=\"https://www.youtube.com/watch?v=uE9xMNidGEo\"&gt;this video&lt;/a&gt; and using an adapter &lt;a href=\"https://imgur.com/a/nnS4aHI\"&gt;here&lt;/a&gt; instead of an enclosure. Friend said it was the same thing(??). So far everything is fine, I copied the tutorial to the dot but his only took eight minutes while mine is been more than half an hour at the same glitched percentage..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nEju-6Vzt6S71Kf9GJrFWCKZf0eQ-2KoxSArZDYq-gQ.jpg?auto=webp&amp;s=405afbe52ca16a2580717d49a62115b01e7481e5", "width": 934, "height": 553}, "resolutions": [{"url": "https://external-preview.redd.it/nEju-6Vzt6S71Kf9GJrFWCKZf0eQ-2KoxSArZDYq-gQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af405080bb61f78046f474a786513492b4789181", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/nEju-6Vzt6S71Kf9GJrFWCKZf0eQ-2KoxSArZDYq-gQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=204ff25fecb2a1d3546244d83af01a0452b07abb", "width": 216, "height": 127}, {"url": "https://external-preview.redd.it/nEju-6Vzt6S71Kf9GJrFWCKZf0eQ-2KoxSArZDYq-gQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a8fb7e175033b62e157cc7f49ac6e1eb38073f6", "width": 320, "height": 189}, {"url": "https://external-preview.redd.it/nEju-6Vzt6S71Kf9GJrFWCKZf0eQ-2KoxSArZDYq-gQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0404ef8e4e4d53f436e769dc7863b102376a4d1d", "width": 640, "height": 378}], "variants": {}, "id": "axuz9mCKnArY7biGcDBKx7r1smfWDkivL-saS-qexcQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au86x5", "is_robot_indexable": true, "report_reasons": null, "author": "adjaplx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au86x5/macrium_reflect_stuck_during_cloning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au86x5/macrium_reflect_stuck_during_cloning/", "subreddit_subscribers": 732982, "created_utc": 1708298492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, \n\nI have a drive that is failing but I was curious of there are certain hdd stats that can be checked and never fixed? Ie bad sectors, etc. \n\nRight now I am testing a drive with hdsentinal and so far I think it's a drive that I need to just say it's dead/not trusted.", "author_fullname": "t2_o4md9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to tell hdd is dead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au7lqe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708297029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have a drive that is failing but I was curious of there are certain hdd stats that can be checked and never fixed? Ie bad sectors, etc. &lt;/p&gt;\n\n&lt;p&gt;Right now I am testing a drive with hdsentinal and so far I think it&amp;#39;s a drive that I need to just say it&amp;#39;s dead/not trusted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au7lqe", "is_robot_indexable": true, "report_reasons": null, "author": "crazydrve", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au7lqe/how_to_tell_hdd_is_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au7lqe/how_to_tell_hdd_is_dead/", "subreddit_subscribers": 732982, "created_utc": 1708297029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, \n\nI'd like to tidy up my external HDDs that I use to store my RAW files.\n\nMy dream would be to have them all sorted in the following folder structure:\nYYYY/YYYYMMDD\n(Where YYYYMMDD is actually from 0600 AM on YYYYMMDD to 0559 AM on YYYYMMDD+1, that way I don't break up events past midnight).\n\nWould there be an easy software or script to do that? Ideally on MacOS. \n\nBonus question: What's the best way to check for duplicate files that is not just based on filename?", "author_fullname": "t2_1z5z6i92", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software to organize RAW files by date?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au7fdk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708296597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to tidy up my external HDDs that I use to store my RAW files.&lt;/p&gt;\n\n&lt;p&gt;My dream would be to have them all sorted in the following folder structure:\nYYYY/YYYYMMDD\n(Where YYYYMMDD is actually from 0600 AM on YYYYMMDD to 0559 AM on YYYYMMDD+1, that way I don&amp;#39;t break up events past midnight).&lt;/p&gt;\n\n&lt;p&gt;Would there be an easy software or script to do that? Ideally on MacOS. &lt;/p&gt;\n\n&lt;p&gt;Bonus question: What&amp;#39;s the best way to check for duplicate files that is not just based on filename?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au7fdk", "is_robot_indexable": true, "report_reasons": null, "author": "PhilippeTk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au7fdk/software_to_organize_raw_files_by_date/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au7fdk/software_to_organize_raw_files_by_date/", "subreddit_subscribers": 732982, "created_utc": 1708296597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,  \n\n\nI have a Norco RPC-4220 that is not detecting my HGST  10TB SATA drives.  If I take the drives out of the case and connect them directly to my SAS controller (LIS SAS2008), they show up.  Other drives connected to the same backplane detect just fine, just not the HGST 10TB drives.  \n\n\nI am wondering what the issue could be.  Given the depth of experience in this group, I was wondering if anyone had any ideas about this?  I don't think it is a power issue since I have tried taking all of the drives connected to that specific backplane except for the HGST drive.  It just seems so odd.  \n\n\nThanks in advance for any ideas!", "author_fullname": "t2_3gxxcxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Norco backplane not detecting HGST 10TB drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au6n0o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708294673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,  &lt;/p&gt;\n\n&lt;p&gt;I have a Norco RPC-4220 that is not detecting my HGST  10TB SATA drives.  If I take the drives out of the case and connect them directly to my SAS controller (LIS SAS2008), they show up.  Other drives connected to the same backplane detect just fine, just not the HGST 10TB drives.  &lt;/p&gt;\n\n&lt;p&gt;I am wondering what the issue could be.  Given the depth of experience in this group, I was wondering if anyone had any ideas about this?  I don&amp;#39;t think it is a power issue since I have tried taking all of the drives connected to that specific backplane except for the HGST drive.  It just seems so odd.  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au6n0o", "is_robot_indexable": true, "report_reasons": null, "author": "mwomrbash", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au6n0o/norco_backplane_not_detecting_hgst_10tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au6n0o/norco_backplane_not_detecting_hgst_10tb_drives/", "subreddit_subscribers": 732982, "created_utc": 1708294673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is everyone as blown away and excited by AI\u2019s use cases in data hoarding of all types as I am?  Specifically what they\u2019re currently calling RAG (retrieval-augmented generation)?\n\nI\u2019m sure both LLMs and RAG will be different in months, possibly completely replaced by other techniques and approaches, but the use cases for whatever else are going to be the same: querying all your unstructured data in natural language and getting natural language responses. \n\nThat means, yes, keep organizing your stuff - but there\u2019s about to be a total revolution in how one organizes, in order to optimize for AI access.\n\nAbsolute game-changer in cataloguing and the *philosophy* of cataloguing. I\u2019m already trying to figure out how to optimize all sorts of data for AI retrieval that\u2019s not *quite* developed yet.  I find myself wanting to wait just a couple of months before taking on new projects, just because I\u2019m sure the tools to do the projects in a totally different way will exist by then.\n\nIMO yeah the majority of this data stuff is going to be all about the big companies, but local LLMs alone are developing rapidly enough to become more than sufficient to handle unstructured data at an individual level.\n\nEdit: I\u2019m strictly not talking about corporate LLMs for personal data - I am only talking about localLLMs for that. I made a post in another forum talking about how I freaked myself out with Gemini today.  I don\u2019t know about you guys, but my hoard is *my* hoard. Technology finally found the limit on how much privacy and control I\u2019ll give up, and I think I\u2019ll die on this OCD hill.", "author_fullname": "t2_4tob3sk2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI and data hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au5ixy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708302672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708291993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is everyone as blown away and excited by AI\u2019s use cases in data hoarding of all types as I am?  Specifically what they\u2019re currently calling RAG (retrieval-augmented generation)?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure both LLMs and RAG will be different in months, possibly completely replaced by other techniques and approaches, but the use cases for whatever else are going to be the same: querying all your unstructured data in natural language and getting natural language responses. &lt;/p&gt;\n\n&lt;p&gt;That means, yes, keep organizing your stuff - but there\u2019s about to be a total revolution in how one organizes, in order to optimize for AI access.&lt;/p&gt;\n\n&lt;p&gt;Absolute game-changer in cataloguing and the &lt;em&gt;philosophy&lt;/em&gt; of cataloguing. I\u2019m already trying to figure out how to optimize all sorts of data for AI retrieval that\u2019s not &lt;em&gt;quite&lt;/em&gt; developed yet.  I find myself wanting to wait just a couple of months before taking on new projects, just because I\u2019m sure the tools to do the projects in a totally different way will exist by then.&lt;/p&gt;\n\n&lt;p&gt;IMO yeah the majority of this data stuff is going to be all about the big companies, but local LLMs alone are developing rapidly enough to become more than sufficient to handle unstructured data at an individual level.&lt;/p&gt;\n\n&lt;p&gt;Edit: I\u2019m strictly not talking about corporate LLMs for personal data - I am only talking about localLLMs for that. I made a post in another forum talking about how I freaked myself out with Gemini today.  I don\u2019t know about you guys, but my hoard is &lt;em&gt;my&lt;/em&gt; hoard. Technology finally found the limit on how much privacy and control I\u2019ll give up, and I think I\u2019ll die on this OCD hill.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au5ixy", "is_robot_indexable": true, "report_reasons": null, "author": "hmmqzaz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au5ixy/ai_and_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au5ixy/ai_and_data_hoarding/", "subreddit_subscribers": 732982, "created_utc": 1708291993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " As the title said, does anyone know what is bitrate cap for 1080p videos on youtube and is the cap different for 24,25,30fps and 50 and 60 fps vids\n\n**sorry if this is not the right subreddit to post this question on (pls tell me where should i post this if this is not the right place)**", "author_fullname": "t2_tfewszxgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know what is the max kbps the youtube gives to 1080p videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au4s4a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708290200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title said, does anyone know what is bitrate cap for 1080p videos on youtube and is the cap different for 24,25,30fps and 50 and 60 fps vids&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;sorry if this is not the right subreddit to post this question on (pls tell me where should i post this if this is not the right place)&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au4s4a", "is_robot_indexable": true, "report_reasons": null, "author": "vvgameranx", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au4s4a/does_anyone_know_what_is_the_max_kbps_the_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au4s4a/does_anyone_know_what_is_the_max_kbps_the_youtube/", "subreddit_subscribers": 732982, "created_utc": 1708290200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been maintaining a plex/emby setup for a few years now and so far I've done literally everything by hand when it comes to sorting and organizing the files. This has led to a few mistakes over the years and I couldn't be asked to browse 400+ folders just to still miss a few of them. So I sat down and hammered together a PowerShell script to find the mistakes for me.\n\nNow I'm just, a hobby programmer and there are certainly better ways of doing this, but it's fast and it works.\n\n[Check the image for one of my first report files](https://imgur.com/a/BhNelyC)\n\n[Github link to the TV Series Report Script](https://github.com/AeonPrime/seriesreport/blob/main/TV%20Series%20Report.ps1)\n\n# Important notes!\n\nThis script does **NOT** modify or attempt to correct anything, it just generates the output file with the issues! This is just a ToDo list or report.\n\n**You need to set your file paths!** This isn't perfect and requires some setup, but you can set all parameters at the top of the script.\n\nThis script just checks **FOLDER Structures** and file types, it does **NOT** care about file naming conventions!\n\n# Main Features\n\n* Season 1 Check: Verifies the presence of a \"Season 1\" folder for each TV series. (for those new series where I just create the series folder and dumped the season 1 content directly into the main series folder...) (**Side note**, I use \"Season **1**\" instead of \"Season **01**\" so the script looks for this.)\n* Root Files Check: Detects any files located directly in the series' root folder. (forgotten zip files, etc)\n* Season Gaps Check: Identifies missing seasons by checking the sequence of season numbers.\n* Empty Folders Check: Finds any empty series folders or season subfolders.\n* Incorrect File Types Check: Flags any files that don't match common video formats (.mkv, .mp4, .avi, .mov).\n* Loose Subtitles Check: Lists season folders containing loose .srt subtitle files. (I added this since I am currently embedding all subtitles into their respective episodes into MKV files)\n* Exclusion List: Allows specifying series folders to be excluded from checks via an \"exclusion.txt\" file. Each line in the file specifies a Series name (**needs to be precisely the series folder name, not the full path!**)\n\n# How do I make it spin?\n\nTo set up the script download it from GitHub  and adjust the file paths at the top of the script. The script can be placed anywhere and the report/exclusion files can be set separately. Provide full paths for everything!\n\nThe script follows Plex's/Emby's folder naming/sorting conventions\n\nSeries -&gt; Adventure Time -&gt; Season 1\n\nSeries -&gt; Rick and Morty -&gt; Season 1\n\n**So if your structure is different than this, this will not work and probably just list your entire folder structure as one big mistake. Sit down and think about your choices.**\n\n# Flaws\n\nSo far I've ran into issues with series that don't follow the conventional naming scheme/release cycle of seasons. For example Popeye or Tom and Jerry (the original series) are grouped by release decade , 1940, 1950 for example. Since those are the outliers I chose to implement the exclusion file. I also haven't tried to see what happens if you try to run this for a UNC path on the network, since I run it locally on my machine. I would assume it works the same but no promises.", "author_fullname": "t2_5hcxz8tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Folder/File Maintenance Script for \"TV Series\"-like content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au4je7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708300098.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708289617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been maintaining a plex/emby setup for a few years now and so far I&amp;#39;ve done literally everything by hand when it comes to sorting and organizing the files. This has led to a few mistakes over the years and I couldn&amp;#39;t be asked to browse 400+ folders just to still miss a few of them. So I sat down and hammered together a PowerShell script to find the mistakes for me.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m just, a hobby programmer and there are certainly better ways of doing this, but it&amp;#39;s fast and it works.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/BhNelyC\"&gt;Check the image for one of my first report files&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/AeonPrime/seriesreport/blob/main/TV%20Series%20Report.ps1\"&gt;Github link to the TV Series Report Script&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Important notes!&lt;/h1&gt;\n\n&lt;p&gt;This script does &lt;strong&gt;NOT&lt;/strong&gt; modify or attempt to correct anything, it just generates the output file with the issues! This is just a ToDo list or report.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;You need to set your file paths!&lt;/strong&gt; This isn&amp;#39;t perfect and requires some setup, but you can set all parameters at the top of the script.&lt;/p&gt;\n\n&lt;p&gt;This script just checks &lt;strong&gt;FOLDER Structures&lt;/strong&gt; and file types, it does &lt;strong&gt;NOT&lt;/strong&gt; care about file naming conventions!&lt;/p&gt;\n\n&lt;h1&gt;Main Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Season 1 Check: Verifies the presence of a &amp;quot;Season 1&amp;quot; folder for each TV series. (for those new series where I just create the series folder and dumped the season 1 content directly into the main series folder...) (&lt;strong&gt;Side note&lt;/strong&gt;, I use &amp;quot;Season &lt;strong&gt;1&lt;/strong&gt;&amp;quot; instead of &amp;quot;Season &lt;strong&gt;01&lt;/strong&gt;&amp;quot; so the script looks for this.)&lt;/li&gt;\n&lt;li&gt;Root Files Check: Detects any files located directly in the series&amp;#39; root folder. (forgotten zip files, etc)&lt;/li&gt;\n&lt;li&gt;Season Gaps Check: Identifies missing seasons by checking the sequence of season numbers.&lt;/li&gt;\n&lt;li&gt;Empty Folders Check: Finds any empty series folders or season subfolders.&lt;/li&gt;\n&lt;li&gt;Incorrect File Types Check: Flags any files that don&amp;#39;t match common video formats (.mkv, .mp4, .avi, .mov).&lt;/li&gt;\n&lt;li&gt;Loose Subtitles Check: Lists season folders containing loose .srt subtitle files. (I added this since I am currently embedding all subtitles into their respective episodes into MKV files)&lt;/li&gt;\n&lt;li&gt;Exclusion List: Allows specifying series folders to be excluded from checks via an &amp;quot;exclusion.txt&amp;quot; file. Each line in the file specifies a Series name (&lt;strong&gt;needs to be precisely the series folder name, not the full path!&lt;/strong&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;How do I make it spin?&lt;/h1&gt;\n\n&lt;p&gt;To set up the script download it from GitHub  and adjust the file paths at the top of the script. The script can be placed anywhere and the report/exclusion files can be set separately. Provide full paths for everything!&lt;/p&gt;\n\n&lt;p&gt;The script follows Plex&amp;#39;s/Emby&amp;#39;s folder naming/sorting conventions&lt;/p&gt;\n\n&lt;p&gt;Series -&amp;gt; Adventure Time -&amp;gt; Season 1&lt;/p&gt;\n\n&lt;p&gt;Series -&amp;gt; Rick and Morty -&amp;gt; Season 1&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So if your structure is different than this, this will not work and probably just list your entire folder structure as one big mistake. Sit down and think about your choices.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Flaws&lt;/h1&gt;\n\n&lt;p&gt;So far I&amp;#39;ve ran into issues with series that don&amp;#39;t follow the conventional naming scheme/release cycle of seasons. For example Popeye or Tom and Jerry (the original series) are grouped by release decade , 1940, 1950 for example. Since those are the outliers I chose to implement the exclusion file. I also haven&amp;#39;t tried to see what happens if you try to run this for a UNC path on the network, since I run it locally on my machine. I would assume it works the same but no promises.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?auto=webp&amp;s=4cbae464784f143de377a4eb06b12906bbb535ec", "width": 1098, "height": 756}, "resolutions": [{"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da9c74190be558e690750096b827bc6d0d041634", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c49d307076559d6813fa83ffd13279905c54306", "width": 216, "height": 148}, {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3afaeab0872b807c38934720234da3eb936c0efc", "width": 320, "height": 220}, {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=495c690e92e19d338848f925ce4d55a73c799823", "width": 640, "height": 440}, {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6e31bb8a72465e534cafd1882aed53a38656628", "width": 960, "height": 660}, {"url": "https://external-preview.redd.it/bmkOiyYs5RmL7JFaZfFXSHPw4Og3HXNRBw60g--i4V0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=baa58cf89330b2ef67c2abbd6406f048a09f5e8b", "width": 1080, "height": 743}], "variants": {}, "id": "o4FEtG0Fi1q2fS91i9G1v5oMvQQd-Ab9h10gWgT8_u0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au4je7", "is_robot_indexable": true, "report_reasons": null, "author": "AeonPrime92", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au4je7/folderfile_maintenance_script_for_tv_serieslike/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au4je7/folderfile_maintenance_script_for_tv_serieslike/", "subreddit_subscribers": 732982, "created_utc": 1708289617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "New WD Red Pro 20TB random buzzing sound\n\nHello! I hope you\u2019re all having a good one!\n\nSo I have been running my little server with 2 x WD red Pro 10TB drives for around a year now, and decided to snag a 20TB model this past week to up my capacity and back up some things. It may bear mentioning that I have a Fractal Define R5 case, since they have a reputation for amplifying HDD noise, but Here\u2019s the deal:\n\nI turned it on for the first time Friday, and it immediately made a sort of buzzing sound, almost like a very exaggerated PWL sound but for like 4 seconds, then it stopped. While formatting it the past 24-ish hours, it\u2019s randomly make the same noise for like 10 seconds and stop, but the format came out \u201chealthy\u201d, CrystalDisk says it\u2019s good, and I\u2019m currently at about 60% on the WD SMART extended diagnostic test with it making a cluster of the noise once every hour or two. Should I just return it or am I being overly anxious about a normal sound for these bigger drives?\n\nThanks in advance!", "author_fullname": "t2_b8ctxlws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New WD Red Pro 20TB making periodical buzzing sound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au1rfz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708282895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New WD Red Pro 20TB random buzzing sound&lt;/p&gt;\n\n&lt;p&gt;Hello! I hope you\u2019re all having a good one!&lt;/p&gt;\n\n&lt;p&gt;So I have been running my little server with 2 x WD red Pro 10TB drives for around a year now, and decided to snag a 20TB model this past week to up my capacity and back up some things. It may bear mentioning that I have a Fractal Define R5 case, since they have a reputation for amplifying HDD noise, but Here\u2019s the deal:&lt;/p&gt;\n\n&lt;p&gt;I turned it on for the first time Friday, and it immediately made a sort of buzzing sound, almost like a very exaggerated PWL sound but for like 4 seconds, then it stopped. While formatting it the past 24-ish hours, it\u2019s randomly make the same noise for like 10 seconds and stop, but the format came out \u201chealthy\u201d, CrystalDisk says it\u2019s good, and I\u2019m currently at about 60% on the WD SMART extended diagnostic test with it making a cluster of the noise once every hour or two. Should I just return it or am I being overly anxious about a normal sound for these bigger drives?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1au1rfz", "is_robot_indexable": true, "report_reasons": null, "author": "Few-Reflection5671", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1au1rfz/new_wd_red_pro_20tb_making_periodical_buzzing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1au1rfz/new_wd_red_pro_20tb_making_periodical_buzzing/", "subreddit_subscribers": 732982, "created_utc": 1708282895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! I have 2x 4TB HDDs in \"RAID 1\" mirroring (quotes because I'm using the crappy embedded motherboard raid chipset) on NTFS.\n\nI'd like to swap on something more consistent to store all my personal stuff (mainly photos and videos), like a dedicated NAS.\n\nSince this is a huge topic for me, I'd like to hear what you'd do in my situation.\n\nI was thinking about buying a NAS with raid 1/5 support and maybe add more similar HDDs so I won't compromise my actual drives during the migration.\n\nI don't know if there's a home NAS solution that provides some of those features:\n\n* 2.5GBit ethernet\n* ZFS or BTRFS\n* no vendor or device lock-in (if something breaks, I don't wont to loose all data or stick with the same product again)\n* simple user/psw lock for specific folders (don't need encryption, just a simple lock while reading from NAS)\n* RAID 1/5 (don't know what's better for my needs considering that I won't put 6+ drives inside)\n* *~~Nice to have~~*~~: backup folders to or from Amazon Photos~~\n\nLet me know if you need more details, thanks!\n\n  \nEDIT: I removed the Amazon Photos nice-to-have coz on 29/02/2024 they turn off sync from desktop apps, so it's clearly a dying service, I'll stick with local only backups", "author_fullname": "t2_iye6tzk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions for good home storage solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atsmke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708274522.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708257884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I have 2x 4TB HDDs in &amp;quot;RAID 1&amp;quot; mirroring (quotes because I&amp;#39;m using the crappy embedded motherboard raid chipset) on NTFS.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to swap on something more consistent to store all my personal stuff (mainly photos and videos), like a dedicated NAS.&lt;/p&gt;\n\n&lt;p&gt;Since this is a huge topic for me, I&amp;#39;d like to hear what you&amp;#39;d do in my situation.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about buying a NAS with raid 1/5 support and maybe add more similar HDDs so I won&amp;#39;t compromise my actual drives during the migration.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if there&amp;#39;s a home NAS solution that provides some of those features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2.5GBit ethernet&lt;/li&gt;\n&lt;li&gt;ZFS or BTRFS&lt;/li&gt;\n&lt;li&gt;no vendor or device lock-in (if something breaks, I don&amp;#39;t wont to loose all data or stick with the same product again)&lt;/li&gt;\n&lt;li&gt;simple user/psw lock for specific folders (don&amp;#39;t need encryption, just a simple lock while reading from NAS)&lt;/li&gt;\n&lt;li&gt;RAID 1/5 (don&amp;#39;t know what&amp;#39;s better for my needs considering that I won&amp;#39;t put 6+ drives inside)&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;&lt;del&gt;Nice to have&lt;/del&gt;&lt;/em&gt;&lt;del&gt;: backup folders to or from Amazon Photos&lt;/del&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let me know if you need more details, thanks!&lt;/p&gt;\n\n&lt;p&gt;EDIT: I removed the Amazon Photos nice-to-have coz on 29/02/2024 they turn off sync from desktop apps, so it&amp;#39;s clearly a dying service, I&amp;#39;ll stick with local only backups&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atsmke", "is_robot_indexable": true, "report_reasons": null, "author": "Kevgretor", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atsmke/opinions_for_good_home_storage_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atsmke/opinions_for_good_home_storage_solution/", "subreddit_subscribers": 732982, "created_utc": 1708257884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Question: do you run 2 backup solutions in parallel to prevents issues with the backup software impacting your backups?\n\nIf cronjobs are set right and backup space is not the issue, does it have a downside?\n\nContext:\n- main server with photos, video and Docker to backup\n- rpi4 with 2 usb hdds\n- 3 OneDrive accounts with 1tb max\n\nDraft Backup strategy:\n- BTRFS send from main server to rpi (+snapshots for versioning)\n- Borg with multiple repositories to backup to rpi4 \n- kopia to backup to 3 OneDrive accounts (different data)\n- todo: offline rotation\nComments are welcome :)\n\n", "author_fullname": "t2_s591kfke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup strategy: both kopia and borg?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atple1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708245768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question: do you run 2 backup solutions in parallel to prevents issues with the backup software impacting your backups?&lt;/p&gt;\n\n&lt;p&gt;If cronjobs are set right and backup space is not the issue, does it have a downside?&lt;/p&gt;\n\n&lt;p&gt;Context:\n- main server with photos, video and Docker to backup\n- rpi4 with 2 usb hdds\n- 3 OneDrive accounts with 1tb max&lt;/p&gt;\n\n&lt;p&gt;Draft Backup strategy:\n- BTRFS send from main server to rpi (+snapshots for versioning)\n- Borg with multiple repositories to backup to rpi4 \n- kopia to backup to 3 OneDrive accounts (different data)\n- todo: offline rotation\nComments are welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atple1", "is_robot_indexable": true, "report_reasons": null, "author": "wokkieman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atple1/backup_strategy_both_kopia_and_borg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atple1/backup_strategy_both_kopia_and_borg/", "subreddit_subscribers": 732982, "created_utc": 1708245768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anybody gotten a WD white label to work with a thirdparty enclosure?\n\nI used to use this white label with my desktop using a tape method but that same method isn't working with an enclosure I bought. Was wondering if anybody here has gotten theirs to work with one.\n\n&amp;#x200B;", "author_fullname": "t2_wf5ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD white label w/ third party enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atncet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708236881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anybody gotten a WD white label to work with a thirdparty enclosure?&lt;/p&gt;\n\n&lt;p&gt;I used to use this white label with my desktop using a tape method but that same method isn&amp;#39;t working with an enclosure I bought. Was wondering if anybody here has gotten theirs to work with one.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atncet", "is_robot_indexable": true, "report_reasons": null, "author": "_xiaohei", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atncet/wd_white_label_w_third_party_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atncet/wd_white_label_w_third_party_enclosure/", "subreddit_subscribers": 732982, "created_utc": 1708236881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm on the fence about this topic as i'm sure many of you are since the hoarder side of me doesnt ever want to throw anything away for archival purposes. \n\nRecently I lost a bunch of data in a cross country move which was devastating so I am in the process of patching together frm a physical and logical perspective what documents, photos, and images I used to have that I can no longer find. \n\nI'm mentioning this to say that even if you have the original and one or two backups it never quite seems to be enough especially if you're the kind of person to encrypt data and those volumes can become corrupt something that's happened to me several times as well. \n\nOn the other hand I understand that these companies are running a business and working in tech myself I can understand equipment is expensive and painstaking to maintain. \n\nSo I'm wondering what companies you guys are using currently that hasn't seemingly moved to this new trend every company seems to be adopting of a 1 year inactivity and account deletion policy. ", "author_fullname": "t2_ndukv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seems like every cloud service provider is switching over to the 1 year inactivity policy as an industry standard.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atktw0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708228146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m on the fence about this topic as i&amp;#39;m sure many of you are since the hoarder side of me doesnt ever want to throw anything away for archival purposes. &lt;/p&gt;\n\n&lt;p&gt;Recently I lost a bunch of data in a cross country move which was devastating so I am in the process of patching together frm a physical and logical perspective what documents, photos, and images I used to have that I can no longer find. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m mentioning this to say that even if you have the original and one or two backups it never quite seems to be enough especially if you&amp;#39;re the kind of person to encrypt data and those volumes can become corrupt something that&amp;#39;s happened to me several times as well. &lt;/p&gt;\n\n&lt;p&gt;On the other hand I understand that these companies are running a business and working in tech myself I can understand equipment is expensive and painstaking to maintain. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering what companies you guys are using currently that hasn&amp;#39;t seemingly moved to this new trend every company seems to be adopting of a 1 year inactivity and account deletion policy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atktw0", "is_robot_indexable": true, "report_reasons": null, "author": "TR330", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atktw0/seems_like_every_cloud_service_provider_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atktw0/seems_like_every_cloud_service_provider_is/", "subreddit_subscribers": 732982, "created_utc": 1708228146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any one run theses in bulk or have experience with them? I bought some \"refurbished\" at about $10/TB. 80% are barley old with only 200hours on them with less than 100 load cycles. The other 20% I am worried about. I ran a SMART long test before I put any drive in service and all passed. But the 20% are 5k hours+(Not really an issue seeing the specs are 1.5mil or something, memory fails to recall) \\~300 load cycles, and 1500+ TB/yr R /W(Calculated from the SMART report using logical sectors R /W divided by total hours)  This figure is slightly worrying has spec is 500? TB/yr. And the truly worrying figure is the g sense error at 100+ with one at \\~700 :O\n\nSo with that information should I be worried about theses couple of drives with high R /W opps and g sense error. I see nothing in the SMART report about a Read/Write error. So could the g sensor be faulty? As I would expect a Read/Write error in the SMART log accompanying a g sense error, especially when I have \\~700 on one drive. I have written about 70TB to the drives and no complaints from ZFS or SMART. I perform daily short test and biweekly long with a ZFS scrub on the alternating week. All drives are in a raidz2(I am not brave enough to run raidz1)\n\nHere is a pastebin of the SMART report on the worse offending drive [https://pastebin.com/2jBZ8BRy](https://pastebin.com/2jBZ8BRy) if anyone wants to take a gander. I hope this is the right place to post this, I am just looking for feedback or maybe some other tool I can use to see if these drives will fail sooner than the others. As some of theses drive were abused and defiantly was not looked after.\n\nOne final question it sounds like a bag rocks on write operations(No metal on metal or bearing failure sounds). I know enterprise drives run loud but I had some 6TB WD Reds that run quiet as a mouse. Is this normal for a Toshiba?", "author_fullname": "t2_eifwg1c96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Toshiba Enterprise HDD Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atifbm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708220747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708220560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one run theses in bulk or have experience with them? I bought some &amp;quot;refurbished&amp;quot; at about $10/TB. 80% are barley old with only 200hours on them with less than 100 load cycles. The other 20% I am worried about. I ran a SMART long test before I put any drive in service and all passed. But the 20% are 5k hours+(Not really an issue seeing the specs are 1.5mil or something, memory fails to recall) ~300 load cycles, and 1500+ TB/yr R /W(Calculated from the SMART report using logical sectors R /W divided by total hours)  This figure is slightly worrying has spec is 500? TB/yr. And the truly worrying figure is the g sense error at 100+ with one at ~700 :O&lt;/p&gt;\n\n&lt;p&gt;So with that information should I be worried about theses couple of drives with high R /W opps and g sense error. I see nothing in the SMART report about a Read/Write error. So could the g sensor be faulty? As I would expect a Read/Write error in the SMART log accompanying a g sense error, especially when I have ~700 on one drive. I have written about 70TB to the drives and no complaints from ZFS or SMART. I perform daily short test and biweekly long with a ZFS scrub on the alternating week. All drives are in a raidz2(I am not brave enough to run raidz1)&lt;/p&gt;\n\n&lt;p&gt;Here is a pastebin of the SMART report on the worse offending drive &lt;a href=\"https://pastebin.com/2jBZ8BRy\"&gt;https://pastebin.com/2jBZ8BRy&lt;/a&gt; if anyone wants to take a gander. I hope this is the right place to post this, I am just looking for feedback or maybe some other tool I can use to see if these drives will fail sooner than the others. As some of theses drive were abused and defiantly was not looked after.&lt;/p&gt;\n\n&lt;p&gt;One final question it sounds like a bag rocks on write operations(No metal on metal or bearing failure sounds). I know enterprise drives run loud but I had some 6TB WD Reds that run quiet as a mouse. Is this normal for a Toshiba?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?auto=webp&amp;s=b9f5c4e4867fbffb2c1ff45dd70aa338d1e3f40c", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1atifbm", "is_robot_indexable": true, "report_reasons": null, "author": "poptrek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1atifbm/toshiba_enterprise_hdd_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1atifbm/toshiba_enterprise_hdd_questions/", "subreddit_subscribers": 732982, "created_utc": 1708220560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm under the impression that IDrive is notorious on their over-storage charges, so I'm wondering if IDrive has features like data de-duplication, compression (on the cloud storage) or block-level incremental? I couldn't be sure from my initial search and from their website.\n\nThe compression on the cloud storage is key here. It doesn't help if IDrive still calculate the storage size before compression.\n\nI found on one small forum [a user tested it and the result is negative](https://backupforum.progforums.com/BackupForum/viewtopic.php?pid=521#p521).\n\nI believe CrashPlan has those features, but it is the transfer speed to/from their sever that broke the deal for me.", "author_fullname": "t2_fcb1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does IDrive cloud backup has data de-duplication, compression or block-level incremental backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ati3e2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708219544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m under the impression that IDrive is notorious on their over-storage charges, so I&amp;#39;m wondering if IDrive has features like data de-duplication, compression (on the cloud storage) or block-level incremental? I couldn&amp;#39;t be sure from my initial search and from their website.&lt;/p&gt;\n\n&lt;p&gt;The compression on the cloud storage is key here. It doesn&amp;#39;t help if IDrive still calculate the storage size before compression.&lt;/p&gt;\n\n&lt;p&gt;I found on one small forum &lt;a href=\"https://backupforum.progforums.com/BackupForum/viewtopic.php?pid=521#p521\"&gt;a user tested it and the result is negative&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I believe CrashPlan has those features, but it is the transfer speed to/from their sever that broke the deal for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1ati3e2", "is_robot_indexable": true, "report_reasons": null, "author": "AlienBoy_tw", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1ati3e2/does_idrive_cloud_backup_has_data_deduplication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1ati3e2/does_idrive_cloud_backup_has_data_deduplication/", "subreddit_subscribers": 732982, "created_utc": 1708219544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\nI am looking for another 3 x 4TB SSDs for my torrenting purpose and was thinking to go the same Samsung route as they seem to be the most reliable.\n\nBut price went up from \u00a3180 to over \u00a3300. Why is that?\n\nAny other option other than this model?\n\nWas using Crucial BX500 in the past (the one with no cache) and was so bad, if I wanted to torrent and watch a 4K movie from that drive, VLC couldn't do it. Would buying the cached MX500 model solve that problem?\n\nAnother option is WD Blue 4TB.\n\nIf anyone has a diffent option, I would gladly consider it, just looking for something reliable with big TBW.\n\nThank you!", "author_fullname": "t2_opgrro6xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 870 EVO 4TB price goes brrr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1athqo7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708218506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I am looking for another 3 x 4TB SSDs for my torrenting purpose and was thinking to go the same Samsung route as they seem to be the most reliable.&lt;/p&gt;\n\n&lt;p&gt;But price went up from \u00a3180 to over \u00a3300. Why is that?&lt;/p&gt;\n\n&lt;p&gt;Any other option other than this model?&lt;/p&gt;\n\n&lt;p&gt;Was using Crucial BX500 in the past (the one with no cache) and was so bad, if I wanted to torrent and watch a 4K movie from that drive, VLC couldn&amp;#39;t do it. Would buying the cached MX500 model solve that problem?&lt;/p&gt;\n\n&lt;p&gt;Another option is WD Blue 4TB.&lt;/p&gt;\n\n&lt;p&gt;If anyone has a diffent option, I would gladly consider it, just looking for something reliable with big TBW.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1athqo7", "is_robot_indexable": true, "report_reasons": null, "author": "morningwarmbed", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1athqo7/samsung_870_evo_4tb_price_goes_brrr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1athqo7/samsung_870_evo_4tb_price_goes_brrr/", "subreddit_subscribers": 732982, "created_utc": 1708218506.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}