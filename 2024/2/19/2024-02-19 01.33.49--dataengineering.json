{"kind": "Listing", "data": {"after": "t3_1au3vk2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nOur team is currently in the process of evaluating various ETL/ELT platforms to enhance our data integration and transformation capabilities with Google BigQuery. We've been using Skyvia but are looking for something more scalable and robust.\nWe\u2019ve compiled a comparison chart of several platforms (Informatica, Microsoft, Oracle, Qlik, SAP, and Talend) with various features such as ease of use, scalability, cost, performance, security, resources, strengths, and weaknesses.\nBased on your experience, which of these platforms would you recommend for use with BigQuery? I\u2019m particularly interested in scalability and performance. If you've used any of these platforms, I\u2019d love to hear your thoughts and experiences and integration with BigQuery.\nYour insights and experiences would be invaluable in helping us make an informed decision. Thank you in advance!", "author_fullname": "t2_di68hd87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on ETL/ELT Platforms \u2013 Your Experiences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "name": "t3_1atq6yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AGH1_W7nb4WG1VplZQtXkKI3aq_fNdejtmUvMRerJzk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708248263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nOur team is currently in the process of evaluating various ETL/ELT platforms to enhance our data integration and transformation capabilities with Google BigQuery. We&amp;#39;ve been using Skyvia but are looking for something more scalable and robust.\nWe\u2019ve compiled a comparison chart of several platforms (Informatica, Microsoft, Oracle, Qlik, SAP, and Talend) with various features such as ease of use, scalability, cost, performance, security, resources, strengths, and weaknesses.\nBased on your experience, which of these platforms would you recommend for use with BigQuery? I\u2019m particularly interested in scalability and performance. If you&amp;#39;ve used any of these platforms, I\u2019d love to hear your thoughts and experiences and integration with BigQuery.\nYour insights and experiences would be invaluable in helping us make an informed decision. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jssiglo4bbjc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?auto=webp&amp;s=cea186fe4588d0b54ee2648e9131da305df17054", "width": 932, "height": 684}, "resolutions": [{"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4ba58a7661c8f58795cbd66bf974cb922e3087b", "width": 108, "height": 79}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=85ef5e17c7ba7ecee686d107943d63dc52538fcc", "width": 216, "height": 158}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75e94ecb0f9def2a9d5330eaed95a6fce225a2d9", "width": 320, "height": 234}, {"url": "https://preview.redd.it/jssiglo4bbjc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6123534c8f06365f7026cc086fe59b285afe8e42", "width": 640, "height": 469}], "variants": {}, "id": "geddnMcoZyxKt3HLjtyxX9qwSlhp15jujsgWhBXLheo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atq6yq", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Okra222", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atq6yq/seeking_advice_on_etlelt_platforms_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jssiglo4bbjc1.jpeg", "subreddit_subscribers": 161783, "created_utc": 1708248263.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi !\n\nI am a junior Data Engineer, still doing my master degree in big data &amp; IA and try to make some data stuff outside work/courses.\n\nAs I struggle to found a cool project to run, I was wondering if you guys are doing Data Engineering, and data things, ouside work ? Is like a hobby for you ? Do you found enough fun in doing that, to run such a project ?", "author_fullname": "t2_15zkxv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing data engineering as a hobby ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atz3j8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708276413.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi !&lt;/p&gt;\n\n&lt;p&gt;I am a junior Data Engineer, still doing my master degree in big data &amp;amp; IA and try to make some data stuff outside work/courses.&lt;/p&gt;\n\n&lt;p&gt;As I struggle to found a cool project to run, I was wondering if you guys are doing Data Engineering, and data things, ouside work ? Is like a hobby for you ? Do you found enough fun in doing that, to run such a project ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atz3j8", "is_robot_indexable": true, "report_reasons": null, "author": "oOGreenFantasyOo", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atz3j8/doing_data_engineering_as_a_hobby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atz3j8/doing_data_engineering_as_a_hobby/", "subreddit_subscribers": 161783, "created_utc": 1708276413.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been toying around with all of the mentioned products in the last weeks as we are looking to rebuild our data stack from the existing mix of shell scripts, python and talend jobs. We are running on AWS. One thing I noticed is that none of these tools support configurations that are pretty common in enterprise settings like ours. For example, we enforce KMS encryption on S3 buckets via service control policies. None of the tools seem to support this, which also prevents loading of data into Redshift. While I am thinking \"yes, this is open source, I could add this myself\" I am wondering what else is in store, if no one in a larger org seems to have run into this. Same goes for things like support for dynamic AWS credentials (we don't allow IAM users) and probably other surprises. I'm not necessarily blaming the tools, as there are a bunch of AWS' own services that don't support this (looking at you Datasync) either.", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are the \"cool\" tools (Meltano, dlt, sling, Airbyte, etc) really production ready", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atqm34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708249949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been toying around with all of the mentioned products in the last weeks as we are looking to rebuild our data stack from the existing mix of shell scripts, python and talend jobs. We are running on AWS. One thing I noticed is that none of these tools support configurations that are pretty common in enterprise settings like ours. For example, we enforce KMS encryption on S3 buckets via service control policies. None of the tools seem to support this, which also prevents loading of data into Redshift. While I am thinking &amp;quot;yes, this is open source, I could add this myself&amp;quot; I am wondering what else is in store, if no one in a larger org seems to have run into this. Same goes for things like support for dynamic AWS credentials (we don&amp;#39;t allow IAM users) and probably other surprises. I&amp;#39;m not necessarily blaming the tools, as there are a bunch of AWS&amp;#39; own services that don&amp;#39;t support this (looking at you Datasync) either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atqm34", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atqm34/are_the_cool_tools_meltano_dlt_sling_airbyte_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atqm34/are_the_cool_tools_meltano_dlt_sling_airbyte_etc/", "subreddit_subscribers": 161783, "created_utc": 1708249949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).\n\nI figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a [GitHub repo](https://github.com/nydasco/data-pipeline-demo), and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).\n\nAs an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.\n\nIt was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.\n\nThe README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.\n\nWould be great to get some feedback.\n\nEdit: [Link to first LinkedIn article](https://www.linkedin.com/pulse/building-modern-data-pipeline-journey-from-api-insight-andy-sawyer-nx1oc%3FtrackingId=x%252Fesg5MtRZSbov%252FoWswC0g%253D%253D/?trackingId=x%2Fesg5MtRZSbov%2FoWswC0g%3D%3D)", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline Demo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ato76r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708297327.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708240154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a post the other day asking for suggestions on a demo pipeline. I\u2019d suggested building something that hit an API and then persisted the data in an object store (MinIO).&lt;/p&gt;\n\n&lt;p&gt;I figured I should \u2018eat my own dog food\u2019. So I built the pipeline myself. I\u2019ve published it to a &lt;a href=\"https://github.com/nydasco/data-pipeline-demo\"&gt;GitHub repo&lt;/a&gt;, and I\u2019m intending to post a series of LinkedIn articles that walk through the code base (I\u2019ll link to them in the comments as I publish them).&lt;/p&gt;\n\n&lt;p&gt;As an overview, it spins up in Docker, orchestrated with Airflow, with data moved around and transformed using Polars. The data are persisted across a series of S3 buckets in MinIO, and there is a Jupyter front end to look at the final fact and dimension tables.&lt;/p&gt;\n\n&lt;p&gt;It was an educational experience building this, and there is lots of room for improvement. But I hope that it is useful to some of you to get an idea of a pipeline.&lt;/p&gt;\n\n&lt;p&gt;The README.md steps through everything you need to do to get it running, and I\u2019ve done my best to comment the code well.&lt;/p&gt;\n\n&lt;p&gt;Would be great to get some feedback.&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;a href=\"https://www.linkedin.com/pulse/building-modern-data-pipeline-journey-from-api-insight-andy-sawyer-nx1oc%3FtrackingId=x%252Fesg5MtRZSbov%252FoWswC0g%253D%253D/?trackingId=x%2Fesg5MtRZSbov%2FoWswC0g%3D%3D\"&gt;Link to first LinkedIn article&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?auto=webp&amp;s=7b230c66253ca3313f8fbf400a60be75ad2fffa7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1aa45a1a0be6a4e267550f2ca1b07a7f9297410", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3068f2ce2fa79d11a7007802d7d3f8f1461d8660", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f368c74a838a47e110b847d3a0b6a49c9fc27c1b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae14cd04851ad514a9932ca95e4d86de49d7dc6b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=27f98159a7f54a886feb93cd17e1558462a5af41", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0w4O_joCXKgB-8d8huiymMdVL9eiN-VfqyRvxRQhulY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90631702985d5da5250877d8f16473458f06056b", "width": 1080, "height": 540}], "variants": {}, "id": "wnCrxe-TM2VFXPruIurfbhMUz_x3VePwsOO5CivxLj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1ato76r", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ato76r/data_pipeline_demo/", "subreddit_subscribers": 161783, "created_utc": 1708240154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAs I prepare to depart from my current company in a month, I'm encountering a common data engineering challenge:\n\n- I had two months to build pipelines for staging, as well as fact and model tables for a project, which I drafted on paper (not submitted yet).\n- The team spent a month configuring dbt and related tools using terraform, and the source tables were only provided to me afterward, with minimal documentation available, except for an Excel sheet with some columns.\n- The data engineer responsible for setting up the source tables took a three-week vacation, the day after he pushed the source tables, leaving behind minimal documentation.\n\nNow, with just one month left, scarce documentation, and no stakeholder support, what would you do?", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From a Data Engineer to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atsmdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708257865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;As I prepare to depart from my current company in a month, I&amp;#39;m encountering a common data engineering challenge:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I had two months to build pipelines for staging, as well as fact and model tables for a project, which I drafted on paper (not submitted yet).&lt;/li&gt;\n&lt;li&gt;The team spent a month configuring dbt and related tools using terraform, and the source tables were only provided to me afterward, with minimal documentation available, except for an Excel sheet with some columns.&lt;/li&gt;\n&lt;li&gt;The data engineer responsible for setting up the source tables took a three-week vacation, the day after he pushed the source tables, leaving behind minimal documentation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now, with just one month left, scarce documentation, and no stakeholder support, what would you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atsmdv", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atsmdv/from_a_data_engineer_to_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atsmdv/from_a_data_engineer_to_another/", "subreddit_subscribers": 161783, "created_utc": 1708257865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently using Athena to upsert data to a Iceberg table in AWS. \n\nThere are 5-20 upserts per hour using MERGE INTO query in Athena. \n\nI am doing maintenance using a scheduled Glue job which does data rewrite, snapshot expiration, orphan file removals using [Spark Procedures](https://iceberg.apache.org/docs/latest/spark-procedures/#spark-procedures)\n\n  \nHere are the issues that I'm facing  \n\\- A lot of position delete files **not** being cleared.  \n\\- Remove orphan files procedure becomes very slow \\[Unusable\\]  \n\\- [rewrite\\_position\\_delete\\_files](https://iceberg.apache.org/docs/latest/spark-procedures/#rewrite_position_delete_files) Not supported  \n\n\n[VACUUM](https://docs.aws.amazon.com/athena/latest/ug/vacuum-statement.html) works fine compared to spark procedure for removing orphan files. But it also doesn't do [rewrite\\_position\\_delete\\_files](https://iceberg.apache.org/docs/latest/spark-procedures/#rewrite_position_delete_files).  \n\n\nI would greatly appreciate your insights and advice.", "author_fullname": "t2_tpq1kiki6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena MERGE INTO + Iceberg in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atxo57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708272890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using Athena to upsert data to a Iceberg table in AWS. &lt;/p&gt;\n\n&lt;p&gt;There are 5-20 upserts per hour using MERGE INTO query in Athena. &lt;/p&gt;\n\n&lt;p&gt;I am doing maintenance using a scheduled Glue job which does data rewrite, snapshot expiration, orphan file removals using &lt;a href=\"https://iceberg.apache.org/docs/latest/spark-procedures/#spark-procedures\"&gt;Spark Procedures&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are the issues that I&amp;#39;m facing&lt;br/&gt;\n- A lot of position delete files &lt;strong&gt;not&lt;/strong&gt; being cleared.&lt;br/&gt;\n- Remove orphan files procedure becomes very slow [Unusable]&lt;br/&gt;\n- &lt;a href=\"https://iceberg.apache.org/docs/latest/spark-procedures/#rewrite_position_delete_files\"&gt;rewrite_position_delete_files&lt;/a&gt; Not supported  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.aws.amazon.com/athena/latest/ug/vacuum-statement.html\"&gt;VACUUM&lt;/a&gt; works fine compared to spark procedure for removing orphan files. But it also doesn&amp;#39;t do &lt;a href=\"https://iceberg.apache.org/docs/latest/spark-procedures/#rewrite_position_delete_files\"&gt;rewrite_position_delete_files&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate your insights and advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atxo57", "is_robot_indexable": true, "report_reasons": null, "author": "softwaredemonlord", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atxo57/athena_merge_into_iceberg_in_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atxo57/athena_merge_into_iceberg_in_aws/", "subreddit_subscribers": 161783, "created_utc": 1708272890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Running Doom on random things will always amuse me. (Not my article)", "author_fullname": "t2_v7fvlqc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Doom on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1au3t2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WgXLJTG2pa_K3qLr7Du72IiC-ZPeT9D38ayIWT88K5I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708287844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arecadata.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Running Doom on random things will always amuse me. (Not my article)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arecadata.com/running-doom-on-snowflake/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?auto=webp&amp;s=3da9e0b85f62d31ecbde6b96ede37513734765a4", "width": 1200, "height": 651}, "resolutions": [{"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c99b9df6158a8dd12eacc1c55fd8f8423c5fdfc9", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa5d13ac6e6dbb428009e135f3dc4d10feb92150", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f1facc2d3c3978dbf5d9c075466380dc8af3939", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81348fe59f55668bbc34505a0cfdc6653e76d21e", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3d5ab77c5e1144c5959c9bf8f6b9afc23aa64c4", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/6oWjOJV-nsTH5Fl_QZNeNoB-Lhmxnk5Gmu_pPtaJhi8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac60e90a6fb3be6e81a59ad1f1b2cd36cb721667", "width": 1080, "height": 585}], "variants": {}, "id": "J05-ZJ5DV3zhbmZJpbrj25Yessx-OZEatX9tI4J_iOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1au3t2l", "is_robot_indexable": true, "report_reasons": null, "author": "on_the_mark_data", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1au3t2l/running_doom_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arecadata.com/running-doom-on-snowflake/", "subreddit_subscribers": 161783, "created_utc": 1708287844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In today's blog we will talk about Azure Data Factory (ADF).\n\n**Step 1**: Short introduction about how ADF is used.\n\nADF can be used in two of the following ways:\n\n1. As an **orchestrator**.\n2. As an **ETL tool**.\n\nThe choice of how it is being used will differ with each organization. In my preparation I practiced it as an orchestrator, the reason behind it being two fold:\n\n1. Data Flows go out of the picture, thereby reducing the learning curve.\n2. PySpark does exactly that with us having a lot more control and I personally loved the idea of using my own codes for transformations than using a almost code-less GUI.\n\nHow you perceive ADF amongst the two choices, I will leave it up to you guys. \n\n**Step 2**: How to go about preparing for ADF. \n\n1. You should first understand what ADF is.\n2. You need to understand top level concepts like Linked Services, Datasets, Activities, Pipelines, and Triggers. By this I mean you should know that such things exist and what they do.\n3. Go through the different kinds of activities available, through YouTube tutorials and Microsoft documentations. At this point you should be aware about the different activities available to us and what they do.\n4. Go through the different triggers available and understand when to use what.\n5. Learn to make your pipelines dynamic by avoiding hard-coding values in your pipelines and by using variables and parameters. This will also introduce you to a service called Key Vault.\n6. Learn about error-handling in your pipelines (different methods of error handling) and various ways to send notifications about failures (web/webhook activities using logic apps, using alerts from data studio).\n7. How to troubleshoot your pipelines, how to retain logs for different time frames, how to restart from a certain point if your pipeline fails, how to debug.\n8. What is CI/CD. How to implement CI/CD in your data factories and how to work using it, by this I mean to say you should be comfortable with: creating feature branches, publishing from main branch, creating artifacts and builds.\n9. How to integrate ADF with Databricks.\n\n**Step 3**: Resources I used to prepare\n\n1. Go through the ADF videos in [this channel](https://www.youtube.com/@TybulOnAzure), I have already shared it in my first blog, he has taught really well. At least watch his error handling video, you can ignore the CI/CD videos as I found other video more easy than that approach.\n2. Go through [this playlist](https://www.youtube.com/watch?v=Mc9JAra8WZU&amp;list=PLMWaZteqtEaLTJffbbBzVOv9C0otal1FO). You can ignore the data flow videos if you plan to use ADF as an orchestrator. This will give a very good idea about all the points in step 2 except point 6 and 8.   \n**Tip:** He does spend a lot of time creating linked services and datasets in every video, so once you are comfortable you can just skip those parts and watch at 1.5x speed to save a lot of time.\n3. Now go through the [Microsoft documentations](https://learn.microsoft.com/en-us/azure/data-factory/) to really get in what you have learned so far. You have to skim through it, don't spend a lot of time on that.\n\n**Step 4**: Practical\n\nYou can open a Azure account and practice side by side along with the tutorials. This will get a lot of hate in comments but I personally would recommend to wait a bit on this part, first understand pyspark, Azure Databricks, how to integrate with ADF, etc and then start practicing as Azure is free for only 1 month and services like Databricks will cost a bit. \n\nI would recommend first understanding the stack, creating a rough idea about real life data flow, then opening your account and creating different projects for your learnings. \n\n[Link To Table Of Content](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nPlease do let me know in comments if you have any feedback on this blog or feel I should add anything, also interactive comments helps me in understanding that people are going through this and engaging with it so it motivates me to spend the time to bring that content to you.\n\nLastly, please do upvote the blog as it helps in reaching to other people and wider audience and tells me that people are engaging with it and need it, otherwise it'll be posting something which people don't need :)\n\nThank You..!!", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog 3 - Let's talk ADF!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atuvav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708271689.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708265245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In today&amp;#39;s blog we will talk about Azure Data Factory (ADF).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Short introduction about how ADF is used.&lt;/p&gt;\n\n&lt;p&gt;ADF can be used in two of the following ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As an &lt;strong&gt;orchestrator&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;As an &lt;strong&gt;ETL tool&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The choice of how it is being used will differ with each organization. In my preparation I practiced it as an orchestrator, the reason behind it being two fold:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Flows go out of the picture, thereby reducing the learning curve.&lt;/li&gt;\n&lt;li&gt;PySpark does exactly that with us having a lot more control and I personally loved the idea of using my own codes for transformations than using a almost code-less GUI.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How you perceive ADF amongst the two choices, I will leave it up to you guys. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: How to go about preparing for ADF. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You should first understand what ADF is.&lt;/li&gt;\n&lt;li&gt;You need to understand top level concepts like Linked Services, Datasets, Activities, Pipelines, and Triggers. By this I mean you should know that such things exist and what they do.&lt;/li&gt;\n&lt;li&gt;Go through the different kinds of activities available, through YouTube tutorials and Microsoft documentations. At this point you should be aware about the different activities available to us and what they do.&lt;/li&gt;\n&lt;li&gt;Go through the different triggers available and understand when to use what.&lt;/li&gt;\n&lt;li&gt;Learn to make your pipelines dynamic by avoiding hard-coding values in your pipelines and by using variables and parameters. This will also introduce you to a service called Key Vault.&lt;/li&gt;\n&lt;li&gt;Learn about error-handling in your pipelines (different methods of error handling) and various ways to send notifications about failures (web/webhook activities using logic apps, using alerts from data studio).&lt;/li&gt;\n&lt;li&gt;How to troubleshoot your pipelines, how to retain logs for different time frames, how to restart from a certain point if your pipeline fails, how to debug.&lt;/li&gt;\n&lt;li&gt;What is CI/CD. How to implement CI/CD in your data factories and how to work using it, by this I mean to say you should be comfortable with: creating feature branches, publishing from main branch, creating artifacts and builds.&lt;/li&gt;\n&lt;li&gt;How to integrate ADF with Databricks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Resources I used to prepare&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go through the ADF videos in &lt;a href=\"https://www.youtube.com/@TybulOnAzure\"&gt;this channel&lt;/a&gt;, I have already shared it in my first blog, he has taught really well. At least watch his error handling video, you can ignore the CI/CD videos as I found other video more easy than that approach.&lt;/li&gt;\n&lt;li&gt;Go through &lt;a href=\"https://www.youtube.com/watch?v=Mc9JAra8WZU&amp;amp;list=PLMWaZteqtEaLTJffbbBzVOv9C0otal1FO\"&gt;this playlist&lt;/a&gt;. You can ignore the data flow videos if you plan to use ADF as an orchestrator. This will give a very good idea about all the points in step 2 except point 6 and 8.&lt;br/&gt;\n&lt;strong&gt;Tip:&lt;/strong&gt; He does spend a lot of time creating linked services and datasets in every video, so once you are comfortable you can just skip those parts and watch at 1.5x speed to save a lot of time.&lt;/li&gt;\n&lt;li&gt;Now go through the &lt;a href=\"https://learn.microsoft.com/en-us/azure/data-factory/\"&gt;Microsoft documentations&lt;/a&gt; to really get in what you have learned so far. You have to skim through it, don&amp;#39;t spend a lot of time on that.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Practical&lt;/p&gt;\n\n&lt;p&gt;You can open a Azure account and practice side by side along with the tutorials. This will get a lot of hate in comments but I personally would recommend to wait a bit on this part, first understand pyspark, Azure Databricks, how to integrate with ADF, etc and then start practicing as Azure is free for only 1 month and services like Databricks will cost a bit. &lt;/p&gt;\n\n&lt;p&gt;I would recommend first understanding the stack, creating a rough idea about real life data flow, then opening your account and creating different projects for your learnings. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Link To Table Of Content&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please do let me know in comments if you have any feedback on this blog or feel I should add anything, also interactive comments helps me in understanding that people are going through this and engaging with it so it motivates me to spend the time to bring that content to you.&lt;/p&gt;\n\n&lt;p&gt;Lastly, please do upvote the blog as it helps in reaching to other people and wider audience and tells me that people are engaging with it and need it, otherwise it&amp;#39;ll be posting something which people don&amp;#39;t need :)&lt;/p&gt;\n\n&lt;p&gt;Thank You..!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?auto=webp&amp;s=b89998b8009f6ed380e4ddb97e465419e3896c64", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca01c758d5fa5b44264cfb0498b961588cdb4208", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=16334c3bf943dfe4ace7ad968d2dc11b89a61eae", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da069911f236610bd9b661cde061bbdda31797a8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f6bc143bf4207f73245b25a20e679cacc056cae", "width": 640, "height": 640}], "variants": {}, "id": "Y_tM-phmGdqUx-mShSmQnJRgDvBVdeU-A9Y4N-Jk4yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atuvav", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/", "subreddit_subscribers": 161783, "created_utc": 1708265245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I want to create my own data architecture with it revolving Azure tools. in my mind it will be like source &gt; ingestion &gt; storage &gt; integration &gt; visualization.. what kind of Azure tools should i use for each component? and my data processing pipeline is source &gt; rest api &gt; Azure function &gt; sql server. am i in the right direction?", "author_fullname": "t2_gengojk0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atvwhu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708268231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to create my own data architecture with it revolving Azure tools. in my mind it will be like source &amp;gt; ingestion &amp;gt; storage &amp;gt; integration &amp;gt; visualization.. what kind of Azure tools should i use for each component? and my data processing pipeline is source &amp;gt; rest api &amp;gt; Azure function &amp;gt; sql server. am i in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atvwhu", "is_robot_indexable": true, "report_reasons": null, "author": "shinon_7652", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvwhu/data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvwhu/data_architecture/", "subreddit_subscribers": 161783, "created_utc": 1708268231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nMy team is currently in the migration our data platform from on-prem to azure cloud. Our current setup is dbt core orchestrated with airflow and we asked to move to databricks/ADF. Which of the adoption is more feasible dbt core or dbt cloud  with databricks in your opinion/experience or would you consider any other options...", "author_fullname": "t2_je149dlo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "databricks + dbt core or dbt cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atvon9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708267630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My team is currently in the migration our data platform from on-prem to azure cloud. Our current setup is dbt core orchestrated with airflow and we asked to move to databricks/ADF. Which of the adoption is more feasible dbt core or dbt cloud  with databricks in your opinion/experience or would you consider any other options...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atvon9", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Bad1906", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvon9/databricks_dbt_core_or_dbt_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvon9/databricks_dbt_core_or_dbt_cloud/", "subreddit_subscribers": 161783, "created_utc": 1708267630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I see a lot of folks here asking how to break into Data Engineering, and I wanted to offer some advice beyond the fundamentals of learning tool X. I've hired and trained dozens of people in this field, and at this point I've got a pretty solid sense of what makes someone successful in it. This is what I'd personally recommend.\n\n\n1. Focus on SWE fundamentals. The algorithms and algebra you learned in school can feel a little impractical for day-to-day work, but they're the core of the powerful distributed processing engines you work with in DE. Moving data around efficiently requires a strong understanding of hardware behavior and memory management. Orchestration tools like Airflow are just regular applications with servers and API's like anything else. Realistically, you're not going to walk into your first DE job with experience with DE tools, but you can reason through solutions based on what you know about software in general. The rest will come with time and training.\n\n\n2. Learn battle-tested modeling and architecture patterns and where to apply them. Again, the fundamentals will serve you very well here. Data teams are often tasked with handling data from all over the company, across many contexts and business domains. Trying to keep all of that straight and building bespoke solutions for each one will not only drive you insane, but will end up wasting a ton of time and money reinventing the wheel and reverse-engineering long-forgotten one-offs. Using durable, repeatable patterns is one way to avoid that. Get some books on the subject and start reading.\n\n\n3. Have a clear Definition of Done for your projects that includes quality controls and ongoing monitoring. Data pipelines are uniquely vulnerable to changes entirely outside of your control, since it's highly unlikely that you are the producer of the input data. Think carefully about how eventual changes in upstream data would affect your workload - where are the fragile points, and how you can build resiliency into them. You don't have to (and realistically can't) account for every scenario upfront, but you can take simple steps to catch issues before they reach the CEO's dashboard.\n\n\n4. This is a team sport. Empathy for stakeholders and teammates, in particular assuming good intentions and that previous decisions were made for a good reason, is the #1 thing I look for in a candidate outside of reasoning skills. I have disqualified candidates for off-handed comments about colleagues \"not knowing what they're talking about\", or dragging previous work when talking about refactoring a pipeline. Your job as a steward for the data platform is to understand your stakeholders and build something that allows them to safely and effectively interact with it. It's a unique and complex system which they likely don't, and shouldn't have to, have as deep an understanding of as you do. Behave accordingly. \n\n\n5. Understand what responsible data stewardship looks like. Data is often one of, if not the most, expensive line item for a company. As a DE you are being trusted with the thing that can make or break a company's success both from a cost and legal liability perspective. In my role I regularly make architecture decisions that will cost or pay someone's salary - while it will probably take you a long time to get to that point, being conscientious of the financial impact/risk of your projects makes the jobs of people who do have to make those decisions (the ones who hire and promote you) much easier. \n\n\n6. Beware hype trains and silver bullets. Again, I have disqualified candidates of all levels for falling into this trap. Every tool, language, and framework was built (at least initially) to solve a specific problem, and when you choose to use it you should understand what that problem is. You're absolutely allowed to have a preferred toolbox, but over-indexing on one solution is an indicator that you don't really understand the problem space or the pitfalls of that thing. I've noticed a significant uptick in this problem with the recent popularity of AI; if you're going to use/advocate for it, you'd better be prepared to also speak to the implications and drawbacks.\n\n\nHonorable mention: this may be controversial but I strongly caution against inflating your work experience in this field. Trust me, they'll know. It's okay and expected that you don't have big data experience when you're starting out - it would be ridiculous for me to expect you to know how to scale a Spark pipeline without access to an enterprise system. Just show enthusiasm for learning and use what you've got to your advantage.\n\n\nI believe in you! You got this.", "author_fullname": "t2_v9jthku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DE advice from a Principal", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1au9s4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708303013.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708302646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I see a lot of folks here asking how to break into Data Engineering, and I wanted to offer some advice beyond the fundamentals of learning tool X. I&amp;#39;ve hired and trained dozens of people in this field, and at this point I&amp;#39;ve got a pretty solid sense of what makes someone successful in it. This is what I&amp;#39;d personally recommend.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Focus on SWE fundamentals. The algorithms and algebra you learned in school can feel a little impractical for day-to-day work, but they&amp;#39;re the core of the powerful distributed processing engines you work with in DE. Moving data around efficiently requires a strong understanding of hardware behavior and memory management. Orchestration tools like Airflow are just regular applications with servers and API&amp;#39;s like anything else. Realistically, you&amp;#39;re not going to walk into your first DE job with experience with DE tools, but you can reason through solutions based on what you know about software in general. The rest will come with time and training.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Learn battle-tested modeling and architecture patterns and where to apply them. Again, the fundamentals will serve you very well here. Data teams are often tasked with handling data from all over the company, across many contexts and business domains. Trying to keep all of that straight and building bespoke solutions for each one will not only drive you insane, but will end up wasting a ton of time and money reinventing the wheel and reverse-engineering long-forgotten one-offs. Using durable, repeatable patterns is one way to avoid that. Get some books on the subject and start reading.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have a clear Definition of Done for your projects that includes quality controls and ongoing monitoring. Data pipelines are uniquely vulnerable to changes entirely outside of your control, since it&amp;#39;s highly unlikely that you are the producer of the input data. Think carefully about how eventual changes in upstream data would affect your workload - where are the fragile points, and how you can build resiliency into them. You don&amp;#39;t have to (and realistically can&amp;#39;t) account for every scenario upfront, but you can take simple steps to catch issues before they reach the CEO&amp;#39;s dashboard.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This is a team sport. Empathy for stakeholders and teammates, in particular assuming good intentions and that previous decisions were made for a good reason, is the #1 thing I look for in a candidate outside of reasoning skills. I have disqualified candidates for off-handed comments about colleagues &amp;quot;not knowing what they&amp;#39;re talking about&amp;quot;, or dragging previous work when talking about refactoring a pipeline. Your job as a steward for the data platform is to understand your stakeholders and build something that allows them to safely and effectively interact with it. It&amp;#39;s a unique and complex system which they likely don&amp;#39;t, and shouldn&amp;#39;t have to, have as deep an understanding of as you do. Behave accordingly. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Understand what responsible data stewardship looks like. Data is often one of, if not the most, expensive line item for a company. As a DE you are being trusted with the thing that can make or break a company&amp;#39;s success both from a cost and legal liability perspective. In my role I regularly make architecture decisions that will cost or pay someone&amp;#39;s salary - while it will probably take you a long time to get to that point, being conscientious of the financial impact/risk of your projects makes the jobs of people who do have to make those decisions (the ones who hire and promote you) much easier. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Beware hype trains and silver bullets. Again, I have disqualified candidates of all levels for falling into this trap. Every tool, language, and framework was built (at least initially) to solve a specific problem, and when you choose to use it you should understand what that problem is. You&amp;#39;re absolutely allowed to have a preferred toolbox, but over-indexing on one solution is an indicator that you don&amp;#39;t really understand the problem space or the pitfalls of that thing. I&amp;#39;ve noticed a significant uptick in this problem with the recent popularity of AI; if you&amp;#39;re going to use/advocate for it, you&amp;#39;d better be prepared to also speak to the implications and drawbacks.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Honorable mention: this may be controversial but I strongly caution against inflating your work experience in this field. Trust me, they&amp;#39;ll know. It&amp;#39;s okay and expected that you don&amp;#39;t have big data experience when you&amp;#39;re starting out - it would be ridiculous for me to expect you to know how to scale a Spark pipeline without access to an enterprise system. Just show enthusiasm for learning and use what you&amp;#39;ve got to your advantage.&lt;/p&gt;\n\n&lt;p&gt;I believe in you! You got this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1au9s4s", "is_robot_indexable": true, "report_reasons": null, "author": "ithinkiboughtadingo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1au9s4s/new_de_advice_from_a_principal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1au9s4s/new_de_advice_from_a_principal/", "subreddit_subscribers": 161783, "created_utc": 1708302646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello there!\n\nI am currently designing a new model for our Data Warehouse (BigQuery in this case and I am leaning towards Star Schema). The structure of the problem / data for which I want to design the model for is giving me some headache though and I would love to get some opinions on it to check that I am making the best possible design.\n\nEssentially the problem is that the data consists of nested scores. Image the following structure:\n\n* Record\n   * Part A\n      * Score A\n      * Score B\n   * Part B\n      * Score C\n      * Score D\n\nIn a nutshell we have a single record that is split into different parts. Each part is then scored according to some criteria (different ones for each part). The scores are then combined to a single score for a part. Then the combined scores for each part are further combined to compute a score for the entire record. The calculation of the \"combining\" might not be a simple sum, therefore it shouldn't be computed on the fly and instead already when transforming the data.\n\nTo make matters worse the nesting could go deeper than this, for example Part C could consist of Part C1 and Part C2 with their own scores. Also the granularity of the parts might not be 1:1 - a record might have 2 or 3 Part D(s).\n\nAs a side note, each record has an ID, which I would have put into a dimension table (there is some other metadata associated with the ID). This dimension could be connected to all parts, as it might be of interest to break down only the scores of a certain part by some of the metadata associated with that ID.\n\nI know this is probably quite tricky to work with - especially with that limited information I can give you guys, sorry. Would you also choose Star Schema or something else? If yes, would you create a fact table per Part (e.g. fact\\_part\\_c1) and then a fact table for each level of combination (e.g. fact\\_part\\_c and then fact\\_record)? How would you deal with the different granularities that might exist? By pre-aggregating them to bring everything to the same granularity? Would you try to create a very large fact table, like an OBT and just work with many different columns?\n\nI know that it might be hard to answer some of this without the exact use-cases and requirements at hand but just imagine that we are actually interested in gaining a deep understanding of the different scores, therefore we might be interested to investigate and breakdown the scores only for a given part or subpart.\n\nLet me know what you would do and thank for the help in advance!", "author_fullname": "t2_1u69ldaj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star Schema: How to best model nested data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au3l53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708287314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there!&lt;/p&gt;\n\n&lt;p&gt;I am currently designing a new model for our Data Warehouse (BigQuery in this case and I am leaning towards Star Schema). The structure of the problem / data for which I want to design the model for is giving me some headache though and I would love to get some opinions on it to check that I am making the best possible design.&lt;/p&gt;\n\n&lt;p&gt;Essentially the problem is that the data consists of nested scores. Image the following structure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Record\n\n&lt;ul&gt;\n&lt;li&gt;Part A\n\n&lt;ul&gt;\n&lt;li&gt;Score A&lt;/li&gt;\n&lt;li&gt;Score B&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Part B\n\n&lt;ul&gt;\n&lt;li&gt;Score C&lt;/li&gt;\n&lt;li&gt;Score D&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In a nutshell we have a single record that is split into different parts. Each part is then scored according to some criteria (different ones for each part). The scores are then combined to a single score for a part. Then the combined scores for each part are further combined to compute a score for the entire record. The calculation of the &amp;quot;combining&amp;quot; might not be a simple sum, therefore it shouldn&amp;#39;t be computed on the fly and instead already when transforming the data.&lt;/p&gt;\n\n&lt;p&gt;To make matters worse the nesting could go deeper than this, for example Part C could consist of Part C1 and Part C2 with their own scores. Also the granularity of the parts might not be 1:1 - a record might have 2 or 3 Part D(s).&lt;/p&gt;\n\n&lt;p&gt;As a side note, each record has an ID, which I would have put into a dimension table (there is some other metadata associated with the ID). This dimension could be connected to all parts, as it might be of interest to break down only the scores of a certain part by some of the metadata associated with that ID.&lt;/p&gt;\n\n&lt;p&gt;I know this is probably quite tricky to work with - especially with that limited information I can give you guys, sorry. Would you also choose Star Schema or something else? If yes, would you create a fact table per Part (e.g. fact_part_c1) and then a fact table for each level of combination (e.g. fact_part_c and then fact_record)? How would you deal with the different granularities that might exist? By pre-aggregating them to bring everything to the same granularity? Would you try to create a very large fact table, like an OBT and just work with many different columns?&lt;/p&gt;\n\n&lt;p&gt;I know that it might be hard to answer some of this without the exact use-cases and requirements at hand but just imagine that we are actually interested in gaining a deep understanding of the different scores, therefore we might be interested to investigate and breakdown the scores only for a given part or subpart.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you would do and thank for the help in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1au3l53", "is_robot_indexable": true, "report_reasons": null, "author": "YannickAlex07", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1au3l53/star_schema_how_to_best_model_nested_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1au3l53/star_schema_how_to_best_model_nested_data/", "subreddit_subscribers": 161783, "created_utc": 1708287314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on streamlining data analysis for a few non-technical clients who are currently grappling with data processing using spreadsheets and third-party tools. As a developer, I'm on a quest to find a user-friendly tool that allows me to configure data pipelines, seamlessly connect with third-party tools, and visualize both the results and intermediate steps.\n\nWhile I appreciate the simplicity of tools like SQL and Excel, they fall short of meeting my requirements for this particular task. I'm curious to know if anyone has come across a more intuitive and easy-to-use solution that strikes the right balance between simplicity and functionality.\n\nYour insights and recommendations would be immensely valuable!", "author_fullname": "t2_5uf7plm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking user-friendly data analysis tool for non-technical clients", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atsxpt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708258986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on streamlining data analysis for a few non-technical clients who are currently grappling with data processing using spreadsheets and third-party tools. As a developer, I&amp;#39;m on a quest to find a user-friendly tool that allows me to configure data pipelines, seamlessly connect with third-party tools, and visualize both the results and intermediate steps.&lt;/p&gt;\n\n&lt;p&gt;While I appreciate the simplicity of tools like SQL and Excel, they fall short of meeting my requirements for this particular task. I&amp;#39;m curious to know if anyone has come across a more intuitive and easy-to-use solution that strikes the right balance between simplicity and functionality.&lt;/p&gt;\n\n&lt;p&gt;Your insights and recommendations would be immensely valuable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atsxpt", "is_robot_indexable": true, "report_reasons": null, "author": "moshestv", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atsxpt/seeking_userfriendly_data_analysis_tool_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atsxpt/seeking_userfriendly_data_analysis_tool_for/", "subreddit_subscribers": 161783, "created_utc": 1708258986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_e2n97hil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Fundamentals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atqdps", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708249023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tontinton.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tontinton.com/posts/database-fundementals", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atqdps", "is_robot_indexable": true, "report_reasons": null, "author": "youmarye", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atqdps/database_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tontinton.com/posts/database-fundementals", "subreddit_subscribers": 161783, "created_utc": 1708249023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI'm really a newbie and trying to get into a Data Engineering field. I'm trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.\n\nThank so much in advance ", "author_fullname": "t2_i6yfsp2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Labs for learning Spark or Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atpbay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708244596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really a newbie and trying to get into a Data Engineering field. I&amp;#39;m trying to learn Pyspark or Airflow and would like to know whether there is any online platform that also provides labs online to practice.&lt;/p&gt;\n\n&lt;p&gt;Thank so much in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atpbay", "is_robot_indexable": true, "report_reasons": null, "author": "BarberCultural4665", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atpbay/labs_for_learning_spark_or_airflow/", "subreddit_subscribers": 161783, "created_utc": 1708244596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone in this sub work with healthcare provider data? \n\nI started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.\n\nCurrently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.\n\nI\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. \n\nFeel free to DM me as well.", "author_fullname": "t2_n2poi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage healthcare provider data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atkfkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708226802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone in this sub work with healthcare provider data? &lt;/p&gt;\n\n&lt;p&gt;I started a job recently in health insurance and have been handed a project to manage/insert/update provider data for the company. Currently the whole data set is in a commercial low code data management system, that is really difficult to work with (no version control, no documentation, deployment and maintenance is a pain). Our contract for the commercial software we use is up for renewal this year and likely will not be renewed.&lt;/p&gt;\n\n&lt;p&gt;Currently, the system we have is a data management platform. We load data from multiple internal and external sources, merge all the data into a master record, then export any updates/inserts to other internal systems.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m new to healthcare and I am wondering if there is a platform or architecture that is commonly used to manage this type of data. The company I work for is small and I am not confident that management really knows what the industry standards are (I certainly do not). If anyone here has any experience in this domain and could point me in the right direction I would appreciate it. &lt;/p&gt;\n\n&lt;p&gt;Feel free to DM me as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atkfkl", "is_robot_indexable": true, "report_reasons": null, "author": "Elmopo74", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atkfkl/how_to_manage_healthcare_provider_data/", "subreddit_subscribers": 161783, "created_utc": 1708226802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been passively searching for a new job for the last year but in the last couple of weeks it's become a more active search. I'd like to find remote work (currently on-site and hate it), but I'm also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I've started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.\n\nI have a lot of experience with .NET, SQL Server, and the \"Microsoft stack\" in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!\n\nI think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested [Databricks Certified Associate Developer for Apache Spark](https://www.databricks.com/learn/certification/apache-spark-developer-associate), but I'm wondering if [Databricks Certified Data Engineer Associate](https://www.databricks.com/learn/certification/data-engineer-associate) would be more worth my time. I am leaning towards the \"Developer for Apache Spark\" one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?\n\nThanks so much!", "author_fullname": "t2_tu8mg1tj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Databricks certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atipxm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708221462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been passively searching for a new job for the last year but in the last couple of weeks it&amp;#39;s become a more active search. I&amp;#39;d like to find remote work (currently on-site and hate it), but I&amp;#39;m also interested in making a career shift towards data engineering and away (?) from a background as a backend software developer. Without any good way of getting professional experience with modern data engineering tools in my current role, I&amp;#39;ve started working towards certifications to bolster my r\u00e9sum\u00e9 and learn more about the skill set required.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of experience with .NET, SQL Server, and the &amp;quot;Microsoft stack&amp;quot; in general but almost no experience with cloud technologies, so I thought it would be natural to start with a Microsoft Azure certification. I just passed the exam for Azure Data Engineer Associate and I am very proud and excited!&lt;/p&gt;\n\n&lt;p&gt;I think next I would like to pursue a Databricks certification, but I am unsure which is the best fit for my interests and for the interests of potential employers. I saw a post on Medium that suggested &lt;a href=\"https://www.databricks.com/learn/certification/apache-spark-developer-associate\"&gt;Databricks Certified Associate Developer for Apache Spark&lt;/a&gt;, but I&amp;#39;m wondering if &lt;a href=\"https://www.databricks.com/learn/certification/data-engineer-associate\"&gt;Databricks Certified Data Engineer Associate&lt;/a&gt; would be more worth my time. I am leaning towards the &amp;quot;Developer for Apache Spark&amp;quot; one, as it seems like it would be more suited for proving skill with developing for Spark in a general sense, but does anyone have more insight into which would be best for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?auto=webp&amp;s=757df626a63f83d9b1b9f06f8d8ba2d8237cc58f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a16398fc492032970830de9a00c08df10b34b4f6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a41a848b62534920ef03a38d809533173e246992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40a54960ac2c4744e4b273083930feb825efabd6", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=014f43b3314169ca926f88efd3b0ee45aa5e04c5", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=35758df6b7e49498041bec685a142e08302fa089", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/Q3J_5tNQxoVKYB7KkTYPB72RKx1ZM5aY7do8m8eyJrY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6db5a3a3031ae960d05d7a0590693b1808444daf", "width": 1080, "height": 565}], "variants": {}, "id": "HwfMz-OF8GV89jc_qQVRVxc8W8oTe2mVUnnrYLKLhX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1atipxm", "is_robot_indexable": true, "report_reasons": null, "author": "Le-Melancolique", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atipxm/questions_about_databricks_certifications/", "subreddit_subscribers": 161783, "created_utc": 1708221462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a complete newbie in Data Engineering, but I have some knowledge about programming so I decided to jump into big project and I think it destroyed me at in the beggining. I have an API that takes data from steam, but I want to automate it so I decided to run this on Apache Airflow on EC2 machine. I created an instance, connected, but the first problem appeared when I wanted to initialize airflow. I have read bunch of tutorials, watched youtube videos, but every person does it different way. some of them uses airflow standalone, some airflow db init.. etc. According to Airflow Documentation I should use the version with constraints:  \n[https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html)  \nBut some people use the python venv, some of them no. Can anyone provide not outdated source of information where I can learn it? Thank you very much &lt;3", "author_fullname": "t2_81vhqati", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow + AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atvls5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708267403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a complete newbie in Data Engineering, but I have some knowledge about programming so I decided to jump into big project and I think it destroyed me at in the beggining. I have an API that takes data from steam, but I want to automate it so I decided to run this on Apache Airflow on EC2 machine. I created an instance, connected, but the first problem appeared when I wanted to initialize airflow. I have read bunch of tutorials, watched youtube videos, but every person does it different way. some of them uses airflow standalone, some airflow db init.. etc. According to Airflow Documentation I should use the version with constraints:&lt;br/&gt;\n&lt;a href=\"https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html\"&gt;https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html&lt;/a&gt;&lt;br/&gt;\nBut some people use the python venv, some of them no. Can anyone provide not outdated source of information where I can learn it? Thank you very much &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atvls5", "is_robot_indexable": true, "report_reasons": null, "author": "GlZM0O", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atvls5/apache_airflow_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atvls5/apache_airflow_aws/", "subreddit_subscribers": 161783, "created_utc": 1708267403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI learn best when I follow along and implement projects. I have been successful learning a lot of data science through it and would\u2019ve loved to do so at the engineering aspects. Here is an example https://sagemaker-examples.readthedocs.io/en/latest/ \n\n\nMost of the resources I have been recommended are very theoretical. And I don\u2019t have the luxury to practice at work. \n\nI looked at Acloudguru and databricks academy. Both look promising. I am trying to widen the net with respect to technology so that I can pick up best practices and intuitions. \n\nAre there other hands on guides you recommend?\n\nThank you all. \n", "author_fullname": "t2_lcc2eyxje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner here. How do I learn by practicing? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1attvja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708262227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I learn best when I follow along and implement projects. I have been successful learning a lot of data science through it and would\u2019ve loved to do so at the engineering aspects. Here is an example &lt;a href=\"https://sagemaker-examples.readthedocs.io/en/latest/\"&gt;https://sagemaker-examples.readthedocs.io/en/latest/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Most of the resources I have been recommended are very theoretical. And I don\u2019t have the luxury to practice at work. &lt;/p&gt;\n\n&lt;p&gt;I looked at Acloudguru and databricks academy. Both look promising. I am trying to widen the net with respect to technology so that I can pick up best practices and intuitions. &lt;/p&gt;\n\n&lt;p&gt;Are there other hands on guides you recommend?&lt;/p&gt;\n\n&lt;p&gt;Thank you all. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1attvja", "is_robot_indexable": true, "report_reasons": null, "author": "20231027", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1attvja/beginner_here_how_do_i_learn_by_practicing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1attvja/beginner_here_how_do_i_learn_by_practicing/", "subreddit_subscribers": 161783, "created_utc": 1708262227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the data team of a very large organisation and containing cloud cost is my goal for this year. We have been doing some optimisation like identifying under utilised storage/  clusters but I am exploring SaaS products which recommends optimisation  opportunities and also help assign them to respective teams. Has anyone come across a product like this? I believe pepper data , accel data , IBMs products are out there. Any thoughts on them?", "author_fullname": "t2_cf14lum7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use any tool to optimise your overall data enablement  cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atyted", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708275732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the data team of a very large organisation and containing cloud cost is my goal for this year. We have been doing some optimisation like identifying under utilised storage/  clusters but I am exploring SaaS products which recommends optimisation  opportunities and also help assign them to respective teams. Has anyone come across a product like this? I believe pepper data , accel data , IBMs products are out there. Any thoughts on them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atyted", "is_robot_indexable": true, "report_reasons": null, "author": "Parking-Army3092", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atyted/do_you_use_any_tool_to_optimise_your_overall_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atyted/do_you_use_any_tool_to_optimise_your_overall_data/", "subreddit_subscribers": 161783, "created_utc": 1708275732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everybody,\n\nI happen to be stuck on an awful position at my workplace right now. Our data engineering / data science / data analytics team, has lost a not experienced senior engineer. Now I'm stuck as the only \"real\" junior engineer with two other juniors with one year experience. \n\nAnyways I'm also clueless and seek professional advice, because I only read bad things about Talend.\n\nOur company right now uses several MSSQL on premise databases, hosted on a local server. We have implemented an EL inside of mssql, loading incrementally from a Microsoft Navision Databases directly into our DWH. There's a small SSIS package we use to trigger the incremental load process. Else we also developed a couple python scripts for certain requirements/reports, but most of the transformation is made inside of mssql as stored procedures. Else we use Tableau &amp; Excel as the visual tool / reporting tool. Tableau contract will be terminated, because of the new high pricing There's a lot of shit made by the predeccesors and we spend a lot of time maintaining all the instable processes.\n\nOur next task is to move to the cloud, because of reasons. \n\nWe are also getting pressure from the head quarters, which has no internal technical engineers and is only kept together by external service provider. So I'm questioning the competence of the further upcomming decisions, since the job of external service providers is getting as much money as possible from a customer. Additionally they are trying to sell Talend as an ETL-Tool right now and Quicksight as a Visulisation Tool.\n\nLong text, short question:   \nHow would you proceed with the migration of an old MSSQL DWH to the Cloud (AWS/Azure) and which tools would you suggest using, to build a stable &amp; good DWH. And how can we prevent damage caused by bad decisions.", "author_fullname": "t2_z6w29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior Data Engineer needs urgent advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atwofy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708270300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt;\n\n&lt;p&gt;I happen to be stuck on an awful position at my workplace right now. Our data engineering / data science / data analytics team, has lost a not experienced senior engineer. Now I&amp;#39;m stuck as the only &amp;quot;real&amp;quot; junior engineer with two other juniors with one year experience. &lt;/p&gt;\n\n&lt;p&gt;Anyways I&amp;#39;m also clueless and seek professional advice, because I only read bad things about Talend.&lt;/p&gt;\n\n&lt;p&gt;Our company right now uses several MSSQL on premise databases, hosted on a local server. We have implemented an EL inside of mssql, loading incrementally from a Microsoft Navision Databases directly into our DWH. There&amp;#39;s a small SSIS package we use to trigger the incremental load process. Else we also developed a couple python scripts for certain requirements/reports, but most of the transformation is made inside of mssql as stored procedures. Else we use Tableau &amp;amp; Excel as the visual tool / reporting tool. Tableau contract will be terminated, because of the new high pricing There&amp;#39;s a lot of shit made by the predeccesors and we spend a lot of time maintaining all the instable processes.&lt;/p&gt;\n\n&lt;p&gt;Our next task is to move to the cloud, because of reasons. &lt;/p&gt;\n\n&lt;p&gt;We are also getting pressure from the head quarters, which has no internal technical engineers and is only kept together by external service provider. So I&amp;#39;m questioning the competence of the further upcomming decisions, since the job of external service providers is getting as much money as possible from a customer. Additionally they are trying to sell Talend as an ETL-Tool right now and Quicksight as a Visulisation Tool.&lt;/p&gt;\n\n&lt;p&gt;Long text, short question:&lt;br/&gt;\nHow would you proceed with the migration of an old MSSQL DWH to the Cloud (AWS/Azure) and which tools would you suggest using, to build a stable &amp;amp; good DWH. And how can we prevent damage caused by bad decisions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atwofy", "is_robot_indexable": true, "report_reasons": null, "author": "IamCoolerThanYoux3", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atwofy/junior_data_engineer_needs_urgent_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atwofy/junior_data_engineer_needs_urgent_advice/", "subreddit_subscribers": 161783, "created_utc": 1708270300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dgovt4x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The DBT of AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atrusw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1708254998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "matsmoll.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://matsmoll.github.io/posts/the-dbt-of-ai", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1atrusw", "is_robot_indexable": true, "report_reasons": null, "author": "FewComfort75", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atrusw/the_dbt_of_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://matsmoll.github.io/posts/the-dbt-of-ai", "subreddit_subscribers": 161783, "created_utc": 1708254998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,\n\n@dlt.view\ndef taxi_raw():\n  return spark.read.format(\"json\").load(\"/databricks-datasets/nyctaxi/sample/json/\")\n\nThe request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.\n", "author_fullname": "t2_zb0y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For loop in Delta live table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atjppu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708224484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i am new to Databricks and been having some challenges figuring out how to use for loop inside delta live table pipeline. Here\u2019s the problem I have. Assume I create this view,&lt;/p&gt;\n\n&lt;p&gt;@dlt.view\ndef taxi_raw():\n  return spark.read.format(&amp;quot;json&amp;quot;).load(&amp;quot;/databricks-datasets/nyctaxi/sample/json/&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;The request is to use (for loop) or any other possible way to iterate over each row and then use values from column 1 and 2 to pass them into another function. I can do it in normal notebook using collect(), but it seems collect() function is not supported in DLT. Any help or suggestions please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1atjppu", "is_robot_indexable": true, "report_reasons": null, "author": "stock_daddy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atjppu/for_loop_in_delta_live_table/", "subreddit_subscribers": 161783, "created_utc": 1708224484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. \n\nAfter listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. \n\nIs that right?\n\nhttps://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1", "author_fullname": "t2_a452rbie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache datafusion question - primary use cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1atj40m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708222627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I just went through part 1 of 3 parts regarding Apache data fusion because it has come up at work. &lt;/p&gt;\n\n&lt;p&gt;After listening to this, however, it seems like as a query engine its primary use case is for building domain specific databases. &lt;/p&gt;\n\n&lt;p&gt;Is that right?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1\"&gt;https://share.highersignal.xyz/compaction/apache-arrow-datafusion-architecture-part-1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?auto=webp&amp;s=d6f1ac4360cf49229134f99c3d153108689c59c2", "width": 640, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9aa6a892393ee232f0eb45a7ea58d22c4c3e6146", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d003e541b5fea5344745c480962668e3b833264", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ae4191256e575310a08a80dfdc57ec7bf6197bc", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/KL5Cz2PmXya8k0SRcXesQLVTRbckAWzy_JpAq3LB2Z4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b53852171fad55deabc07ee33f2d1864f8b7dd8", "width": 640, "height": 480}], "variants": {}, "id": "01KqIsa7WqHwmoOZOln6aLV18Tj-fG_SKL1QeSNhoc8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1atj40m", "is_robot_indexable": true, "report_reasons": null, "author": "Double-Code1902", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1atj40m/apache_datafusion_question_primary_use_cases/", "subreddit_subscribers": 161783, "created_utc": 1708222627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am trying to implement a role based auth system to connecting to a PostgreSql DB system. I have routine functions and triggers that I created that perform select statements and such on the database. How do I create a Role that can only perform the actions in the Executable Functions, while also not having general Select functions on the table? I originally gave the role execute only permissions, but they give a permission error since the functions require a select  statement on certain tables.\n\n\nThank you!", "author_fullname": "t2_39utoy9a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Roles in PostgreSql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1au3vk2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708288007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am trying to implement a role based auth system to connecting to a PostgreSql DB system. I have routine functions and triggers that I created that perform select statements and such on the database. How do I create a Role that can only perform the actions in the Executable Functions, while also not having general Select functions on the table? I originally gave the role execute only permissions, but they give a permission error since the functions require a select  statement on certain tables.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1au3vk2", "is_robot_indexable": true, "report_reasons": null, "author": "scuffed12s", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1au3vk2/question_on_roles_in_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1au3vk2/question_on_roles_in_postgresql/", "subreddit_subscribers": 161783, "created_utc": 1708288007.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}