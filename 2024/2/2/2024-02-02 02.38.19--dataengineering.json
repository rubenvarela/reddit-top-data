{"kind": "Listing", "data": {"after": "t3_1ag9anf", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?", "author_fullname": "t2_5bpuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a flight this weekend, which do I read first?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1aggfae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 183, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 183, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fkp7Vi_KUwsffn0f-CQp6ImM1t_tLY7rbFUtNaCXdoo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706808494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?auto=webp&amp;s=0bb941bc45a50426758995677a5b66ade92d4487", "width": 4284, "height": 4284}, "resolutions": [{"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c142ee9e88145d514baf92c943cb4782a02fe1fd", "width": 108, "height": 108}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dd4cfeb0dea340a652f5cbedc437c8af3b7d383", "width": 216, "height": 216}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=799a87e4b938ce32637470e436697ccef761995b", "width": 320, "height": 320}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d4f5e17cc504cc6fceb83bcc7598f1a0e9acde7", "width": 640, "height": 640}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87b404df2c257e27d4cd12c14dc6a1815c8d8229", "width": 960, "height": 960}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e5d546b223a59bbb785a129044ec15075d289f4", "width": 1080, "height": 1080}], "variants": {}, "id": "RB47IfFEb96kxJx90GTTv2lUGE-jR20k6IwvMCivNjY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aggfae", "is_robot_indexable": true, "report_reasons": null, "author": "cheanerman", "discussion_type": null, "num_comments": 93, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aggfae/got_a_flight_this_weekend_which_do_i_read_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "subreddit_subscribers": 157581, "created_utc": 1706808494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.\n\nIt\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c\n\nThe behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f\n\nIm calling it SQRUFF :)", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Portable SQL Linter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1agijyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S-kVxl0yQ0zf6MOW3PZbmZsw0mFsesa5vKjPHCqg0WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706813857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;The behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f&lt;/p&gt;\n\n&lt;p&gt;Im calling it SQRUFF :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/quarylabs/sqruff", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?auto=webp&amp;s=87a0b3113d8a8e8a4fad2901b55e98449c340d4a", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e09230469f5ffa3e0cd2651e18d860042ba4e457", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f7839ea5bdb3e5e564f2e5afa389d6990bfb89", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fda65169a18d5a214619e0464afdb2852107fa47", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57c64448d9e4256e89bfc92a1e03627f1e1c4050", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cde8788c99759e9bf6399fa46456d0e851962e6", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c224395a06778cde7bd43e5c28b6fab324950f9", "width": 1080, "height": 1080}], "variants": {}, "id": "yi1H-2cV1KrS4mf3uf9SP2lDbruuU_3Qx50MFRPXxz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agijyw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agijyw/open_source_portable_sql_linter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/quarylabs/sqruff", "subreddit_subscribers": 157581, "created_utc": 1706813857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "#### Background\n\nMy company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. \n\n#### Question\nWhich library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? \n\n#### More Contexts\nOn the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. \n\nPreviously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. \n\n#### TLDR\nSo which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Portable and Cleanest Way to Integrate BigQuery with Python Dataframe Manipulation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1afzpri", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706754014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706753532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h4&gt;Background&lt;/h4&gt;\n\n&lt;p&gt;My company recently migrated from AWS to Google cloud, and we started using Big Query for backend data warehouse. Now, my job, as a machine learning engineer, is to build some feature engineering and model training/inference pipelines on the data. &lt;/p&gt;\n\n&lt;h4&gt;Question&lt;/h4&gt;\n\n&lt;p&gt;Which library provides the cleanest and portable interface to ingest data from Big Query in the most pythonic way? &lt;/p&gt;\n\n&lt;h4&gt;More Contexts&lt;/h4&gt;\n\n&lt;p&gt;On the python side, once the data is loaded, I will use a combination of polars (for preprocessing) and tensorflow (for modelling) before inserting back some dataframes to big query. I have to do a lot of SQL-like operations (joins, groupby, aggregation etc.) on the data, and that best be done on the server side rather than inside my code. &lt;/p&gt;\n\n&lt;p&gt;Previously, I was using PyAthena and SQL Alchemy ORM (to interface with Athena). I am under the impression that the Alchemy framework offers the highest portability and clean interface for SQL-like operation irrespective of the exact database. However, I also read that BigQuery is not a database at all, and Google has its own libraries with python binding for interfacing with bigquery. &lt;/p&gt;\n\n&lt;h4&gt;TLDR&lt;/h4&gt;\n\n&lt;p&gt;So which one is the recommended practice here to get data to and from BQ? SQL alchemy (assuming it is possible) with better portability and reusability of modules? Or locking myself more to Google libraries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1afzpri", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1afzpri/most_portable_and_cleanest_way_to_integrate/", "subreddit_subscribers": 157581, "created_utc": 1706753532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does any company or data engineers really using Dremio?", "author_fullname": "t2_fta9agm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3wih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does any company or data engineers really using Dremio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3wih", "is_robot_indexable": true, "report_reasons": null, "author": "luqmancrit69", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3wih/dremio_lakehouse/", "subreddit_subscribers": 157581, "created_utc": 1706766620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spend the last few months using dbt  to model and analyze historical NBA data sets. [The project](https://github.com/paradime-io/paradime-dbt-nba-data-challenge) has been so fun that I'm releasing it to data folks as a competition!\n\nIn this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!\n\nHere's how it works:\n\nUpon registration, Participants will gain access to:  \n\ud83d\udc49 Paradime for SQL &amp; dbt\u2122 development.  \n\u2744\ufe0f Snowflake for computing and storage.  \n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.  \n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023\n\nFrom there, participants will create insightful analyses and visualizations, and submit them for a chance to win! \n\nIf you're curious, learn more below!\n\n[https://www.paradime.io/dbt-data-modeling-challenge-nba-edition](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt\u2122 data modeling Challenge - NBA Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agj7rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706815476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spend the last few months using dbt  to model and analyze historical NBA data sets. &lt;a href=\"https://github.com/paradime-io/paradime-dbt-nba-data-challenge\"&gt;The project&lt;/a&gt; has been so fun that I&amp;#39;m releasing it to data folks as a competition!&lt;/p&gt;\n\n&lt;p&gt;In this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;p&gt;Upon registration, Participants will gain access to:&lt;br/&gt;\n\ud83d\udc49 Paradime for SQL &amp;amp; dbt\u2122 development.&lt;br/&gt;\n\u2744\ufe0f Snowflake for computing and storage.&lt;br/&gt;\n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.&lt;br/&gt;\n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023&lt;/p&gt;\n\n&lt;p&gt;From there, participants will create insightful analyses and visualizations, and submit them for a chance to win! &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, learn more below!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;https://www.paradime.io/dbt-data-modeling-challenge-nba-edition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?auto=webp&amp;s=41fdb09a495622cce5c1f05123e1900fbad6458f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0bfcb7fbab608cc41fe9498b564e93d489c6b27", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a40dbe345268ba354342895cc4ca9607f43f87be", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=785b1c83ea7863ff2de6c16a16e12de4c8f1e39b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed0083d4fe841bfb13478442010389f368cc1d31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2aeaaee50e003c299bde24ad70d7ca810d0e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8f90192aba6736b970c5fc77523839360fd4610", "width": 1080, "height": 540}], "variants": {}, "id": "vW2rNRLOsGuCMv-C4hnHQbNq81LUjPoWMDxQDE6o6jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agj7rh", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "subreddit_subscribers": 157581, "created_utc": 1706815476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey Data Engineering community,\n\nI'm currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I'm using SQL PostgreSQL to fetch products by type. I have a 'products' table with intrinsic information like title, URL, image URL, etc., and sub-tables such as 'books' and 'movies,' each containing specific fields for authors, editors, and movie details.\n\nHowever, I'm facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.\n\nThese products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.\n\nI've heard suggestions that a NoSQL database could be a good fit for this use case. As I'm working exclusively with Python, I'd appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.\n\nThanks a lot for your valuable insights!", "author_fullname": "t2_3mvaeyfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-commerce Products storing (SQL / no sql )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag7kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706781655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineering community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I&amp;#39;m using SQL PostgreSQL to fetch products by type. I have a &amp;#39;products&amp;#39; table with intrinsic information like title, URL, image URL, etc., and sub-tables such as &amp;#39;books&amp;#39; and &amp;#39;movies,&amp;#39; each containing specific fields for authors, editors, and movie details.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.&lt;/p&gt;\n\n&lt;p&gt;These products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard suggestions that a NoSQL database could be a good fit for this use case. As I&amp;#39;m working exclusively with Python, I&amp;#39;d appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your valuable insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ag7kh1", "is_robot_indexable": true, "report_reasons": null, "author": "Saa3dLfachil", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "subreddit_subscribers": 157581, "created_utc": 1706781655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.   \nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  \n\n\nI took Alan Rodrigues course in udemy and Ramesh retnasamy's ADF course .  \n[https://www.udemy.com/course/data-engineering-on-microsoft-azure/](https://www.udemy.com/course/data-engineering-on-microsoft-azure/)  \n[https://www.udemy.com/course/learn-azure-data-factory-from-scratch/](https://www.udemy.com/course/learn-azure-data-factory-from-scratch/)\n\nAnd following Tybul on azure youtube play list.   \n[https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg](https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg)  \n\n\nPlease share any study materials or notes for new syllabus. \n\n&amp;#x200B;", "author_fullname": "t2_pdssduhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP -203 Exam study materials help..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agiyq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706814845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.&lt;br/&gt;\nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  &lt;/p&gt;\n\n&lt;p&gt;I took Alan Rodrigues course in udemy and Ramesh retnasamy&amp;#39;s ADF course .&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/data-engineering-on-microsoft-azure/\"&gt;https://www.udemy.com/course/data-engineering-on-microsoft-azure/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/learn-azure-data-factory-from-scratch/\"&gt;https://www.udemy.com/course/learn-azure-data-factory-from-scratch/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And following Tybul on azure youtube play list.&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg\"&gt;https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Please share any study materials or notes for new syllabus. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agiyq1", "is_robot_indexable": true, "report_reasons": null, "author": "bmkmanojkumar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "subreddit_subscribers": 157581, "created_utc": 1706814845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.\n\nThe problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I'm thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our \"data warehouse\", and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.\n\nIs this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don't really have that much data in the OLTP database right now, it's about 140k rows.\n\n&amp;#x200B;\n\nEdit: I doesn't necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives for data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agbigy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706795682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.&lt;/p&gt;\n\n&lt;p&gt;The problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I&amp;#39;m thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our &amp;quot;data warehouse&amp;quot;, and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.&lt;/p&gt;\n\n&lt;p&gt;Is this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don&amp;#39;t really have that much data in the OLTP database right now, it&amp;#39;s about 140k rows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I doesn&amp;#39;t necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agbigy", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "subreddit_subscribers": 157581, "created_utc": 1706795682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is fundamental to databases like Clickhouse, Pinot or Druid that the same can't be achieved using MPP?   \nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I'm thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?  \nI want an understanding of why MPP's don't fit well for user analytical use cases?", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Does MPP's not support high QPS, low latency queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag4ehz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706768368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is fundamental to databases like Clickhouse, Pinot or Druid that the same can&amp;#39;t be achieved using MPP?&lt;br/&gt;\nHypothetically, if a Snowflake warehouse with a good cache setting is setup, why would it still not be able to provide quick response to high QPS? I&amp;#39;m thinking that if somehow the entire table (columnar format) is loaded into the warehouse memory - 30-40 MB, that in a way is in-memory OLAP?&lt;br/&gt;\nI want an understanding of why MPP&amp;#39;s don&amp;#39;t fit well for user analytical use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag4ehz", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag4ehz/why_does_mpps_not_support_high_qps_low_latency/", "subreddit_subscribers": 157581, "created_utc": 1706768368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How was the switch for you, and what steps did you take to make the switch?", "author_fullname": "t2_55l23zh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here switched to DE from Technical Project Management/Product Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglecn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706820952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How was the switch for you, and what steps did you take to make the switch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aglecn", "is_robot_indexable": true, "report_reasons": null, "author": "egg_boi56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "subreddit_subscribers": 157581, "created_utc": 1706820952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data \u201ccontract\u201d actual implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agkong", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706819160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agkong", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "subreddit_subscribers": 157581, "created_utc": 1706819160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Feb 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n", "t3_1agfqy9"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1706806830.354, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agfqy9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706806830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agfqy9", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "subreddit_subscribers": 157581, "created_utc": 1706806830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. \n\nIm trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. \n\nHence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.", "author_fullname": "t2_6nb4026n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there Python libraries that define and parametrize etl jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglv35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706822120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Im trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. &lt;/p&gt;\n\n&lt;p&gt;Hence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aglv35", "is_robot_indexable": true, "report_reasons": null, "author": "armAssembledx86", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "subreddit_subscribers": 157581, "created_utc": 1706822120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database \u201cAPI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agifx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agifx8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agifx8/database_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agifx8/database_api/", "subreddit_subscribers": 157581, "created_utc": 1706813572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. [https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs](https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs)", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Five Useful Queries to Get BigQuery Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agh025", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706809949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. &lt;a href=\"https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs\"&gt;https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?auto=webp&amp;s=2c376e4c51790abdb77e784f486631d6e609bf89", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=624487499dff65b0038a6c79404f8cb230d1a7fc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=957574fe4817ae688d5220549723b418efbd73b2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a83e577e94738c1cb9f73585da0e1bb35b288f7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11affefba0bc6c9c3308849bfea98546e488beb4", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a74bcd4439a79a120286cee87d6cf904a597e79b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56bf0012cdb3fb9322b7eb47cdf976897786096b", "width": 1080, "height": 567}], "variants": {}, "id": "KzLetsP8vZqCBImB3BNcFmeWS5rzEuEkGvHPTrzwTtw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agh025", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "subreddit_subscribers": 157581, "created_utc": 1706809949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you, people, approach new projects?\n\nIn my experience, most data and analytics engineers do the following:\n\n1. Collect all the information they need initially\n2. Start digging and working in the background\n3. Occasionally ask additional questions\n4. Show everything when ready\n\nI don't like that approach. In my opinion, there are too many risks.\n\nInstead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bold Projects vs MVPs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag3rhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706766104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you, people, approach new projects?&lt;/p&gt;\n\n&lt;p&gt;In my experience, most data and analytics engineers do the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Collect all the information they need initially&lt;/li&gt;\n&lt;li&gt;Start digging and working in the background&lt;/li&gt;\n&lt;li&gt;Occasionally ask additional questions&lt;/li&gt;\n&lt;li&gt;Show everything when ready&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I don&amp;#39;t like that approach. In my opinion, there are too many risks.&lt;/p&gt;\n\n&lt;p&gt;Instead, usually go with Minimum Viable Products. That often means, Google Sheets. It means a shorter time to market, less development time, and many other benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Manager", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ag3rhm", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag3rhm/bold_projects_vs_mvps/", "subreddit_subscribers": 157581, "created_utc": 1706766104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don't want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.", "author_fullname": "t2_dla93s85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharing data with non-technical teams for review and approval", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag0ir4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706755856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table in Postgres that I need to share with non-technical business people. They aim to review and approve the data and request changes if something looks incorrect. I don&amp;#39;t want to extract and share the data over XLSX or CSV. The table needs to be the source of truth. I also need the option to filter the data by certain fields. If you were designing a solution, what would you put in front of the Postgres table and why? I have experience with Metabase, Superset or Grafana, so I am considering those,  but I am keen to hear thoughts from other DEs for this use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag0ir4", "is_robot_indexable": true, "report_reasons": null, "author": "FooFighter_V", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag0ir4/sharing_data_with_nontechnical_teams_for_review/", "subreddit_subscribers": 157581, "created_utc": 1706755856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a career dilemma and would really appreciate some outside perspectives. I'm currently working at a small startup with a peaceful work culture and a great manager. The work is predictable, without much surprises or challenges. However, it's a remote job, which I really enjoy.\n\nRecently, I received an offer for a senior position in a larger, well-established company (not a FAANG company, but still significant). This role would double my current salary, which is obviously a huge plus. However, it comes with its own set of challenges:\n\nHybrid Work Model: I currently work fully remotely\nIncreased Responsibilities: The senior role would demand more from me in terms of responsibilities and decision-making.\nTime Flexibility: I'd have to manage evening calls with offshore teams and early morning meetings, which could be a significant change from my current routine.\nIs the stress and change in work-life balance worth the salary increase and senior title? Would love to hear from anyone who's been in a similar situation or can offer insights into making such a decision.\n\nI have total experience 6-7 years, and masters. I am in my 30s.", "author_fullname": "t2_t526hbv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Torn Between a High-Paying Senior Role and a Comfortable Current Job - Need Advice!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agroto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706837694.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706837410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a career dilemma and would really appreciate some outside perspectives. I&amp;#39;m currently working at a small startup with a peaceful work culture and a great manager. The work is predictable, without much surprises or challenges. However, it&amp;#39;s a remote job, which I really enjoy.&lt;/p&gt;\n\n&lt;p&gt;Recently, I received an offer for a senior position in a larger, well-established company (not a FAANG company, but still significant). This role would double my current salary, which is obviously a huge plus. However, it comes with its own set of challenges:&lt;/p&gt;\n\n&lt;p&gt;Hybrid Work Model: I currently work fully remotely\nIncreased Responsibilities: The senior role would demand more from me in terms of responsibilities and decision-making.\nTime Flexibility: I&amp;#39;d have to manage evening calls with offshore teams and early morning meetings, which could be a significant change from my current routine.\nIs the stress and change in work-life balance worth the salary increase and senior title? Would love to hear from anyone who&amp;#39;s been in a similar situation or can offer insights into making such a decision.&lt;/p&gt;\n\n&lt;p&gt;I have total experience 6-7 years, and masters. I am in my 30s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agroto", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Ticket6016", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agroto/torn_between_a_highpaying_senior_role_and_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agroto/torn_between_a_highpaying_senior_role_and_a/", "subreddit_subscribers": 157581, "created_utc": 1706837410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks", "author_fullname": "t2_ened0cec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi, BI developer to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1agqyza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706835367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agqyza", "is_robot_indexable": true, "report_reasons": null, "author": "sleepy_bored_eternal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "subreddit_subscribers": 157581, "created_utc": 1706835367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you had to start a modern data strategy from scratch which approach would you use?", "author_fullname": "t2_3rz22c0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agp6oy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706830539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you had to start a modern data strategy from scratch which approach would you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agp6oy", "is_robot_indexable": true, "report_reasons": null, "author": "tipdrllr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agp6oy/data_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agp6oy/data_strategy/", "subreddit_subscribers": 157581, "created_utc": 1706830539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long time data engineer here. Recently working with legacy SSIS (SQL Server Integration Services.)  I'm in search of an SSIS expert to help with a deployment error. Anyone know of good resources for this?", "author_fullname": "t2_a2c7z181", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agoad2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706828230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time data engineer here. Recently working with legacy SSIS (SQL Server Integration Services.)  I&amp;#39;m in search of an SSIS expert to help with a deployment error. Anyone know of good resources for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agoad2", "is_robot_indexable": true, "report_reasons": null, "author": "-Xenophon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agoad2/etl_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agoad2/etl_help/", "subreddit_subscribers": 157581, "created_utc": 1706828230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nAt my company, which is a scale up in the utilities sector, we re planning on building an Azure based data lakehouse/warehouse.\n\nIts primary goal is to allow billing of annual utility data (15m gas and electricity usage) for about 100,000 connections.\n\nI want these pricing calculations, to go as swift as possible. What kind of architecture would you recommend? Note that although the data is 15min data, it is not streaming data. We collect/receive previous days\u2019 usage data in batch at the end of each day.\n\nIm particularly interested whether one would go with a time series versus relational solution, and which services one would recommend within Azure.\n\nThanks in advance for any advise.", "author_fullname": "t2_2irne35v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Utilities datawarehouse Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agmcf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706823355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;At my company, which is a scale up in the utilities sector, we re planning on building an Azure based data lakehouse/warehouse.&lt;/p&gt;\n\n&lt;p&gt;Its primary goal is to allow billing of annual utility data (15m gas and electricity usage) for about 100,000 connections.&lt;/p&gt;\n\n&lt;p&gt;I want these pricing calculations, to go as swift as possible. What kind of architecture would you recommend? Note that although the data is 15min data, it is not streaming data. We collect/receive previous days\u2019 usage data in batch at the end of each day.&lt;/p&gt;\n\n&lt;p&gt;Im particularly interested whether one would go with a time series versus relational solution, and which services one would recommend within Azure.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agmcf9", "is_robot_indexable": true, "report_reasons": null, "author": "buzzie13", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agmcf9/utilities_datawarehouse_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agmcf9/utilities_datawarehouse_azure/", "subreddit_subscribers": 157581, "created_utc": 1706823355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a tax transformation group at a large accounting firm, where we do data transformation, data analytics and process automation for tax departments.\n\nTax departments are not generally tech savvy, and a lot of our work is just replacing awful, manual monthly excel processes with low-code solutions, generally using Alteryx or Microsoft's Power Platform (PowerQuery, PowerApps, etc.) \n\nMost of my bosses and staff are accountants who learned tech concepts along the way, and most of my clients are tax accountants with very limited tech skills. \n\nOur most 'sophisticated' clients are using tools like Alteryx, Microsoft SQL databases and REST APIs to push/pull data from software systems.  \n\nThere's a lot of resistance to using \"code\" because clients don't understand it, and staff turnover is constant. It's \"easier\" to teach people to maintain, review and update low code solutions. \n\nThis usually works fine due to low volume, but occasionally cracks begin to show and I can tell we are doing things in a way that isn't ideal. \n\nFor example, I've got a project right now where I need to process 100m rows of data and store the resulting 50m records somewhere it can be accessed by a PowerBI dashboard in the client's tenant. \n\nI could process that data quickly in batches with Alteryx, but it seems like they want to use Power Platform since the client may already have licenses, so I might have to try to process it using a PowerApps Dataflow, which is basically cloud-based PowerQuery. \n\nI don't think the client has any actual databases, so we may have to store the outputs in a Microsoft Dataverse table, but I'm having trouble even assessing the potential storage cost and performance of storing 50m+ records in a dataverse table and interacting with the data via PowerBI. I could also potentially store a giant CSV on SharePoint and try to open that through PowerBI, but I'm not sure if that would even work. \n\nI don't think these issues are specific to my firm either, there is a whole industry of \"tax technology\" and \"transformation\" that seem to basically be doing data engineering work inefficiently, using suboptimal tools. \n\nDoes anyone have recommendations on how we can do things in more sophisticated ways that are more in line with data engineering best practices? \n\nI'd love to understand the \"right\" way to do some of these tasks, and how I can potentially educate firm leadership and steer things in a better direction. \n\nThank you for any advice you may have!", "author_fullname": "t2_a1kvwds9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I help make things better? (Accounting firm data work)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agi84m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a tax transformation group at a large accounting firm, where we do data transformation, data analytics and process automation for tax departments.&lt;/p&gt;\n\n&lt;p&gt;Tax departments are not generally tech savvy, and a lot of our work is just replacing awful, manual monthly excel processes with low-code solutions, generally using Alteryx or Microsoft&amp;#39;s Power Platform (PowerQuery, PowerApps, etc.) &lt;/p&gt;\n\n&lt;p&gt;Most of my bosses and staff are accountants who learned tech concepts along the way, and most of my clients are tax accountants with very limited tech skills. &lt;/p&gt;\n\n&lt;p&gt;Our most &amp;#39;sophisticated&amp;#39; clients are using tools like Alteryx, Microsoft SQL databases and REST APIs to push/pull data from software systems.  &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of resistance to using &amp;quot;code&amp;quot; because clients don&amp;#39;t understand it, and staff turnover is constant. It&amp;#39;s &amp;quot;easier&amp;quot; to teach people to maintain, review and update low code solutions. &lt;/p&gt;\n\n&lt;p&gt;This usually works fine due to low volume, but occasionally cracks begin to show and I can tell we are doing things in a way that isn&amp;#39;t ideal. &lt;/p&gt;\n\n&lt;p&gt;For example, I&amp;#39;ve got a project right now where I need to process 100m rows of data and store the resulting 50m records somewhere it can be accessed by a PowerBI dashboard in the client&amp;#39;s tenant. &lt;/p&gt;\n\n&lt;p&gt;I could process that data quickly in batches with Alteryx, but it seems like they want to use Power Platform since the client may already have licenses, so I might have to try to process it using a PowerApps Dataflow, which is basically cloud-based PowerQuery. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think the client has any actual databases, so we may have to store the outputs in a Microsoft Dataverse table, but I&amp;#39;m having trouble even assessing the potential storage cost and performance of storing 50m+ records in a dataverse table and interacting with the data via PowerBI. I could also potentially store a giant CSV on SharePoint and try to open that through PowerBI, but I&amp;#39;m not sure if that would even work. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think these issues are specific to my firm either, there is a whole industry of &amp;quot;tax technology&amp;quot; and &amp;quot;transformation&amp;quot; that seem to basically be doing data engineering work inefficiently, using suboptimal tools. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have recommendations on how we can do things in more sophisticated ways that are more in line with data engineering best practices? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to understand the &amp;quot;right&amp;quot; way to do some of these tasks, and how I can potentially educate firm leadership and steer things in a better direction. &lt;/p&gt;\n\n&lt;p&gt;Thank you for any advice you may have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agi84m", "is_robot_indexable": true, "report_reasons": null, "author": "comments_egg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agi84m/how_can_i_help_make_things_better_accounting_firm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agi84m/how_can_i_help_make_things_better_accounting_firm/", "subreddit_subscribers": 157581, "created_utc": 1706813042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source and the Data Lakehouse: Apache Arrow, Apache Iceberg, Nessie and Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agce9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1706798072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1agce9z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agce9z/open_source_and_the_data_lakehouse_apache_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/open-source-and-the-data-lakehouse-apache-arrow-apache-iceberg-nessie-and-dremio/", "subreddit_subscribers": 157581, "created_utc": 1706798072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn't find anything resembling what I wanted to get. I want to get metadata rather than the data itself.", "author_fullname": "t2_mekatj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to extract data sources from a PBI dashboard in .pbix?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag9anf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706788577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to scrape information about the data sources (databases, schemas, table names etc) from an offline PBI dashboard (basically, a .pbix file) that uses ms sql server views as data sources? I tried to unzip .pbix, and didn&amp;#39;t find anything resembling what I wanted to get. I want to get metadata rather than the data itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ag9anf", "is_robot_indexable": true, "report_reasons": null, "author": "thiophosgene", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag9anf/is_there_a_way_to_extract_data_sources_from_a_pbi/", "subreddit_subscribers": 157581, "created_utc": 1706788577.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}