{"kind": "Listing", "data": {"after": "t3_1agtmsp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?", "author_fullname": "t2_5bpuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a flight this weekend, which do I read first?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1aggfae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 259, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 259, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fkp7Vi_KUwsffn0f-CQp6ImM1t_tLY7rbFUtNaCXdoo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706808494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an Analytics Engineer who is experienced doing SQL ETL\u2019s. Looking to grow my skillset. I plan to read both but is there a better one to start with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?auto=webp&amp;s=0bb941bc45a50426758995677a5b66ade92d4487", "width": 4284, "height": 4284}, "resolutions": [{"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c142ee9e88145d514baf92c943cb4782a02fe1fd", "width": 108, "height": 108}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dd4cfeb0dea340a652f5cbedc437c8af3b7d383", "width": 216, "height": 216}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=799a87e4b938ce32637470e436697ccef761995b", "width": 320, "height": 320}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d4f5e17cc504cc6fceb83bcc7598f1a0e9acde7", "width": 640, "height": 640}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=87b404df2c257e27d4cd12c14dc6a1815c8d8229", "width": 960, "height": 960}, {"url": "https://preview.redd.it/9a5y3tgyd0gc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e5d546b223a59bbb785a129044ec15075d289f4", "width": 1080, "height": 1080}], "variants": {}, "id": "RB47IfFEb96kxJx90GTTv2lUGE-jR20k6IwvMCivNjY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1aggfae", "is_robot_indexable": true, "report_reasons": null, "author": "cheanerman", "discussion_type": null, "num_comments": 107, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aggfae/got_a_flight_this_weekend_which_do_i_read_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9a5y3tgyd0gc1.jpeg", "subreddit_subscribers": 157650, "created_utc": 1706808494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.\n\nIt\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c\n\nThe behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f\n\nIm calling it SQRUFF :)", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Portable SQL Linter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1agijyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S-kVxl0yQ0zf6MOW3PZbmZsw0mFsesa5vKjPHCqg0WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706813857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;The behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f&lt;/p&gt;\n\n&lt;p&gt;Im calling it SQRUFF :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/quarylabs/sqruff", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?auto=webp&amp;s=87a0b3113d8a8e8a4fad2901b55e98449c340d4a", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e09230469f5ffa3e0cd2651e18d860042ba4e457", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f7839ea5bdb3e5e564f2e5afa389d6990bfb89", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fda65169a18d5a214619e0464afdb2852107fa47", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57c64448d9e4256e89bfc92a1e03627f1e1c4050", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cde8788c99759e9bf6399fa46456d0e851962e6", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c224395a06778cde7bd43e5c28b6fab324950f9", "width": 1080, "height": 1080}], "variants": {}, "id": "yi1H-2cV1KrS4mf3uf9SP2lDbruuU_3Qx50MFRPXxz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agijyw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agijyw/open_source_portable_sql_linter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/quarylabs/sqruff", "subreddit_subscribers": 157650, "created_utc": 1706813857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nWe have one table named A in SQL server database which is hosted on Azure Managed VM.\nand for some testing, I wanted to copy the data to our ADLS gen 2.\n\nCurrently the SQL table has no partitions. And super slow even to find the count of rows and has close to 500+ columns.\n\nI have used copy activity with SQL as source and parquet format in ADLS destination. But it is going at the rate of 100k rows per minute.\n\nI have tried tinkering with parallelism and it didn't help much.\nI'm thinking pulling in just selected columns as a start tomorrow.\n\nJust trying to see if you have guys f have better ideas?", "author_fullname": "t2_kz99f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to copy 3 billion rows from SQL Server to ADLS. What would be the fastest way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agw6kc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706851432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;We have one table named A in SQL server database which is hosted on Azure Managed VM.\nand for some testing, I wanted to copy the data to our ADLS gen 2.&lt;/p&gt;\n\n&lt;p&gt;Currently the SQL table has no partitions. And super slow even to find the count of rows and has close to 500+ columns.&lt;/p&gt;\n\n&lt;p&gt;I have used copy activity with SQL as source and parquet format in ADLS destination. But it is going at the rate of 100k rows per minute.&lt;/p&gt;\n\n&lt;p&gt;I have tried tinkering with parallelism and it didn&amp;#39;t help much.\nI&amp;#39;m thinking pulling in just selected columns as a start tomorrow.&lt;/p&gt;\n\n&lt;p&gt;Just trying to see if you have guys f have better ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agw6kc", "is_robot_indexable": true, "report_reasons": null, "author": "jerrie86", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agw6kc/trying_to_copy_3_billion_rows_from_sql_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agw6kc/trying_to_copy_3_billion_rows_from_sql_server_to/", "subreddit_subscribers": 157650, "created_utc": 1706851432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spend the last few months using dbt  to model and analyze historical NBA data sets. [The project](https://github.com/paradime-io/paradime-dbt-nba-data-challenge) has been so fun that I'm releasing it to data folks as a competition!\n\nIn this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!\n\nHere's how it works:\n\nUpon registration, Participants will gain access to:  \n\ud83d\udc49 Paradime for SQL &amp; dbt\u2122 development.  \n\u2744\ufe0f Snowflake for computing and storage.  \n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.  \n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023\n\nFrom there, participants will create insightful analyses and visualizations, and submit them for a chance to win! \n\nIf you're curious, learn more below!\n\n[https://www.paradime.io/dbt-data-modeling-challenge-nba-edition](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt\u2122 data modeling Challenge - NBA Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agj7rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706815476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spend the last few months using dbt  to model and analyze historical NBA data sets. &lt;a href=\"https://github.com/paradime-io/paradime-dbt-nba-data-challenge\"&gt;The project&lt;/a&gt; has been so fun that I&amp;#39;m releasing it to data folks as a competition!&lt;/p&gt;\n\n&lt;p&gt;In this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;p&gt;Upon registration, Participants will gain access to:&lt;br/&gt;\n\ud83d\udc49 Paradime for SQL &amp;amp; dbt\u2122 development.&lt;br/&gt;\n\u2744\ufe0f Snowflake for computing and storage.&lt;br/&gt;\n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.&lt;br/&gt;\n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023&lt;/p&gt;\n\n&lt;p&gt;From there, participants will create insightful analyses and visualizations, and submit them for a chance to win! &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, learn more below!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;https://www.paradime.io/dbt-data-modeling-challenge-nba-edition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?auto=webp&amp;s=41fdb09a495622cce5c1f05123e1900fbad6458f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0bfcb7fbab608cc41fe9498b564e93d489c6b27", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a40dbe345268ba354342895cc4ca9607f43f87be", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=785b1c83ea7863ff2de6c16a16e12de4c8f1e39b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed0083d4fe841bfb13478442010389f368cc1d31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2aeaaee50e003c299bde24ad70d7ca810d0e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8f90192aba6736b970c5fc77523839360fd4610", "width": 1080, "height": 540}], "variants": {}, "id": "vW2rNRLOsGuCMv-C4hnHQbNq81LUjPoWMDxQDE6o6jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agj7rh", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "subreddit_subscribers": 157650, "created_utc": 1706815476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I recently caught up with a friend of mine who is a experienced data engineer and works for a marketing startup. While we were chatting up, he told me that he has taken 100s of interviews and his main filter is always leetcode. Only candidates who are able to solve different medium level leetcode problems across multiple rounds are considered for hiring.\n\nI was a bit surprised by that because wouldn't it be easy to lose out on a lot of good candidates but he said that leetcode type problems help him understand how smart a candidate is and how well he can come up with ideas and tackle DE problems. What are your thoughts on this? How would you choose to interview potential candidates and does leetcode type competitive coding questions have any bearing?", "author_fullname": "t2_q979mh0vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you think Leetcode type questions is a good metric for data engineering skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agxov7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706857017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently caught up with a friend of mine who is a experienced data engineer and works for a marketing startup. While we were chatting up, he told me that he has taken 100s of interviews and his main filter is always leetcode. Only candidates who are able to solve different medium level leetcode problems across multiple rounds are considered for hiring.&lt;/p&gt;\n\n&lt;p&gt;I was a bit surprised by that because wouldn&amp;#39;t it be easy to lose out on a lot of good candidates but he said that leetcode type problems help him understand how smart a candidate is and how well he can come up with ideas and tackle DE problems. What are your thoughts on this? How would you choose to interview potential candidates and does leetcode type competitive coding questions have any bearing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1agxov7", "is_robot_indexable": true, "report_reasons": null, "author": "ShaliniMalhotra9512", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agxov7/do_you_think_leetcode_type_questions_is_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agxov7/do_you_think_leetcode_type_questions_is_a_good/", "subreddit_subscribers": 157650, "created_utc": 1706857017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey Data Engineering community,\n\nI'm currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I'm using SQL PostgreSQL to fetch products by type. I have a 'products' table with intrinsic information like title, URL, image URL, etc., and sub-tables such as 'books' and 'movies,' each containing specific fields for authors, editors, and movie details.\n\nHowever, I'm facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.\n\nThese products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.\n\nI've heard suggestions that a NoSQL database could be a good fit for this use case. As I'm working exclusively with Python, I'd appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.\n\nThanks a lot for your valuable insights!", "author_fullname": "t2_3mvaeyfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-commerce Products storing (SQL / no sql )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ag7kh1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706781655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineering community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on an application that involves AI for recommendation systems, utilizing product data scraped from various e-commerce websites. Currently, I&amp;#39;m using SQL PostgreSQL to fetch products by type. I have a &amp;#39;products&amp;#39; table with intrinsic information like title, URL, image URL, etc., and sub-tables such as &amp;#39;books&amp;#39; and &amp;#39;movies,&amp;#39; each containing specific fields for authors, editors, and movie details.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m facing a challenge in deciding how to store the scraped products in the database since different websites have variations in their fields. For instance, scraping from Amazon and eBay may result in slightly different fields for the same type of product.&lt;/p&gt;\n\n&lt;p&gt;These products are essential for training an NLP recommender system (which is working fine without any issues). They are accessed through a Django API, connecting to the database for rapid operations. The data is updated weekly to add new products to the database and ensure the recommender system stays up-to-date. The system is designed for efficient reading to display product information on a mobile app.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard suggestions that a NoSQL database could be a good fit for this use case. As I&amp;#39;m working exclusively with Python, I&amp;#39;d appreciate your advice on whether NoSQL is the right direction and, if so, recommendations on frameworks/tools that align with Python.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your valuable insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ag7kh1", "is_robot_indexable": true, "report_reasons": null, "author": "Saa3dLfachil", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ag7kh1/ecommerce_products_storing_sql_no_sql/", "subreddit_subscribers": 157650, "created_utc": 1706781655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I got a chance to revamp a data pipeline in my company that ingest event log data in JSON format. Currently, the pipeline will perform a full refresh for every table stage (raw, transformation/curated, and aggregation layer), and due to constant increase in the volume, it takes too much resources and running hours. All of the tables are not partitioned based on date column, so basically we store the table snapshot on each date partition. I am thinking of using incremental update for each stage so the computing cost should be lower.\n\nRevamp the part from raw JSON into tables was done and the raw table are properly partitioned based on \\`created\\_date\\` column and ingesting the delta from cloud storage. But I got stuck in the transformation layer, where there is one column where we need to create a row\\_number based on \\`uid\\` and \\`created\\_date\\` to create some kind of \\`session\\` column, and there is a possibility that this \\`session\\` can span for more than a days, which means I need to consider all of the date partition for each \\`uid\\` no matter what.\n\nDo you folks have any idea to make incremental update for this kind of scenario? Currently I consider to cluster the \\`uid\\` column so it will optimze the step to populate the  \\`session\\` column a bit when it scan all of the dates in the table. Thanks in advance.", "author_fullname": "t2_1q71lc7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating full refresh pipeline into incremental update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agxp69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706857054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I got a chance to revamp a data pipeline in my company that ingest event log data in JSON format. Currently, the pipeline will perform a full refresh for every table stage (raw, transformation/curated, and aggregation layer), and due to constant increase in the volume, it takes too much resources and running hours. All of the tables are not partitioned based on date column, so basically we store the table snapshot on each date partition. I am thinking of using incremental update for each stage so the computing cost should be lower.&lt;/p&gt;\n\n&lt;p&gt;Revamp the part from raw JSON into tables was done and the raw table are properly partitioned based on `created_date` column and ingesting the delta from cloud storage. But I got stuck in the transformation layer, where there is one column where we need to create a row_number based on `uid` and `created_date` to create some kind of `session` column, and there is a possibility that this `session` can span for more than a days, which means I need to consider all of the date partition for each `uid` no matter what.&lt;/p&gt;\n\n&lt;p&gt;Do you folks have any idea to make incremental update for this kind of scenario? Currently I consider to cluster the `uid` column so it will optimze the step to populate the  `session` column a bit when it scan all of the dates in the table. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agxp69", "is_robot_indexable": true, "report_reasons": null, "author": "srodinger18", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agxp69/migrating_full_refresh_pipeline_into_incremental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agxp69/migrating_full_refresh_pipeline_into_incremental/", "subreddit_subscribers": 157650, "created_utc": 1706857054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does the need for ingestion latency or data freshness impact this choice? \n\n[View Poll](https://www.reddit.com/poll/1agw2fp)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which pattern do you use when ingesting data into lakehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agw2fp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706851043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does the need for ingestion latency or data freshness impact this choice? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1agw2fp\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agw2fp", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1707455843617, "options": [{"text": "Land data in object storage in csv, json. Then run a job to write into Delta / Hudi / Iceberg tables", "id": "26948894"}, {"text": "Directly write into Delta /  Iceberg / Hudi tables", "id": "26948895"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 35, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agw2fp/which_pattern_do_you_use_when_ingesting_data_into/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1agw2fp/which_pattern_do_you_use_when_ingesting_data_into/", "subreddit_subscribers": 157650, "created_utc": 1706851043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.   \nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  \n\n\nI took Alan Rodrigues course in udemy and Ramesh retnasamy's ADF course .  \n[https://www.udemy.com/course/data-engineering-on-microsoft-azure/](https://www.udemy.com/course/data-engineering-on-microsoft-azure/)  \n[https://www.udemy.com/course/learn-azure-data-factory-from-scratch/](https://www.udemy.com/course/learn-azure-data-factory-from-scratch/)\n\nAnd following Tybul on azure youtube play list.   \n[https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg](https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg)  \n\n\nPlease share any study materials or notes for new syllabus. \n\n&amp;#x200B;", "author_fullname": "t2_pdssduhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP -203 Exam study materials help..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agiyq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706814845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.&lt;br/&gt;\nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  &lt;/p&gt;\n\n&lt;p&gt;I took Alan Rodrigues course in udemy and Ramesh retnasamy&amp;#39;s ADF course .&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/data-engineering-on-microsoft-azure/\"&gt;https://www.udemy.com/course/data-engineering-on-microsoft-azure/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/learn-azure-data-factory-from-scratch/\"&gt;https://www.udemy.com/course/learn-azure-data-factory-from-scratch/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And following Tybul on azure youtube play list.&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg\"&gt;https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Please share any study materials or notes for new syllabus. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agiyq1", "is_robot_indexable": true, "report_reasons": null, "author": "bmkmanojkumar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "subreddit_subscribers": 157650, "created_utc": 1706814845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.\n\nThe problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I'm thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our \"data warehouse\", and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.\n\nIs this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don't really have that much data in the OLTP database right now, it's about 140k rows.\n\n&amp;#x200B;\n\nEdit: I doesn't necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives for data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agbigy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706795682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m working in a startup that has one major client asking for analytics and reporting functionalities on the website, I suggested we start implementing and pipelining data to Snowflake and use it as our data warehouse, the advantage is that we could scale it as we get more clients/more data.&lt;/p&gt;\n\n&lt;p&gt;The problem is the amount of money that we would spend to implement this, especially given the fact that currently we only have one client. As an alternative I&amp;#39;m thinking about using a postgresSQL instance on AWS RDS for that purpose, the costs would be much lower and it would serve, for the time being, as our &amp;quot;data warehouse&amp;quot;, and later on we could just migrate the data, and change the ETL pipelines sink to Snowflake or whatever other source we choose to use.&lt;/p&gt;\n\n&lt;p&gt;Is this a good approach in this case, or should I think of an alternative? The biggest problem is the ROI in this case, and keep in mind that we don&amp;#39;t really have that much data in the OLTP database right now, it&amp;#39;s about 140k rows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I doesn&amp;#39;t necessarily have to be a Postgres Database, it could any other SQL database for that matter, I just mentioned it as an example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agbigy", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agbigy/alternatives_for_data_warehouses/", "subreddit_subscribers": 157650, "created_utc": 1706795682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI have an interview home assignment where I need to decode hexadecimal payload. I have been trying to decode the payload but not able to produce correct output specifically for the time field. I would appreciate assistance in decoding it.\n\n**Description about the task:**\n\nIn order to monitor the health of the battery, we decided to send data from the battery to the cloud. This data is transmitted in hexadecimal format and received in our AWS account. \n\nThe data is transmitted as a hexadecimal string. Every payload consists of 8 bytes. Due to space optimization, the information is not byte-aligned. A field can start in the middle of a byte. We therefore need bit operations to decode the payload. The payload is not signed and encoded in little Endian. The following table describes the data fields contained in the payload and their bit positions.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9a2hor5nj3gc1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=0d62b085471d2ab5a134701139b22f254206bcd1\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9zl6pcopj3gc1.png?width=1104&amp;format=png&amp;auto=webp&amp;s=55261006470072c0e9933dadc5d59115751de0f3\n\nFor instance, type is encoded on 4 bits in the first byte. state of charge is encoded on 8 bits (1 byte) on the 6th byte.\n\n**Time:** time represents the timestamp of the data. It is defined in seconds since UNIX epoch.\n\n**State:** state is a string, with the following corresponding values:\n\n0: \"power off\"\n\n1: \"power on\"\n\n2: \"discharge\"\n\n3: \"charge\"\n\n4: \"charge complete\"\n\n5: \"host mode\"\n\n6: \"shutdown\"\n\n7: \"error\"\n\n8: \"undefined\"\n\n**State of charge**: state of charge represents the charge of the battery. It is a float with values between 0 and 100 and a 0.5 precision. To store it as an integer, it was multiplied by 2.\n\n**Battery temperature:** battery temperature represents the temperature of the battery. Values can vary between -20 and 100. The precision is 0.5. To store it as an integer we added 20 and multiplied it by 2.\n\n **Sample Test data** \n\ninput: F1E6E63676C75000 \n\noutput: {  \"time\": 1668181615,  \"state\": \"error\",  \"state\\_of\\_charge\": 99.5,  \"temperature\": 20.0 } \n\nMy Script:\n\n&amp;#x200B;\n\n    import base64\n    import struct\n    import json\n    from datetime import datetime\n    \n    \n    def lambda_handler(event):\n        # Extract device and payload from the event\n        device = event[\"device\"]\n        payload_hex = event[\"payload\"]\n    \n        # Convert hexadecimal payload to bytes\n        payload_bytes = bytes.fromhex(payload_hex)\n    \n        # Unpack the payload using struct\n        unpacked_data = struct.unpack('&lt;I2Bh', payload_bytes)\n    \n        # Extract individual fields\n        time = unpacked_data[0]\n    \n        state = unpacked_data[1]\n        state = (state &gt;&gt; 4) &amp; 0x0F\n    \n        state_of_charge = unpacked_data[2] / 2.0\n        temperature = (unpacked_data[3] / 2.0) - 20.0\n    \n        # Mapping state values to corresponding strings\n        state_mapping = {\n            0: \"power off\",\n            1: \"power on\",\n            2: \"discharge\",\n            3: \"charge\",\n            4: \"charge complete\",\n            5: \"host mode\",\n            6: \"shutdown\",\n            7: \"error\",\n            8: \"undefined\"\n        }\n    \n        # Create the output dictionary\n        output_data = {\n            \"device\": device,\n            \"time\": time,\n            \"state\": state_mapping.get(state, \"unknown\"),\n            \"state_of_charge\": round(state_of_charge, 1),\n            \"temperature\": round(temperature, 1)\n        }\n    \n        # Log the output data to stdout\n        print(json.dumps(output_data))\n    \n    event = {'device': 'device1', 'payload': '6188293726C75C00'}\n    lambda_handler(event)\n\nI am struggling to get the correct output for the time which is not just dependent on the unpacked\\_data\\[0\\] based on the above logic.\n\n&amp;#x200B;", "author_fullname": "t2_uj6cs26m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hexadecimal format payload decoding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 64, "top_awarded_type": null, "hide_score": false, "media_metadata": {"9zl6pcopj3gc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d9e648ae4e3ec9f9b6476e457f5334d486b5100"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3fb6cda5fb619033ca27ba5788fd7f045cfa962"}, {"y": 102, "x": 320, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c34166f838f20ff7e9c24c9df834d96f92692c80"}, {"y": 205, "x": 640, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e7c997bbd763720ab02239d3ef29f79cbe0799"}, {"y": 308, "x": 960, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=517b5fab9b880e3d885cd9d9b3fc5db7ccfda9b3"}, {"y": 347, "x": 1080, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=755ddbd4f19215cb3fc073c697d6bcf926ea076b"}], "s": {"y": 355, "x": 1104, "u": "https://preview.redd.it/9zl6pcopj3gc1.png?width=1104&amp;format=png&amp;auto=webp&amp;s=55261006470072c0e9933dadc5d59115751de0f3"}, "id": "9zl6pcopj3gc1"}, "9a2hor5nj3gc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9226cd9bc668947a45659b0372127b5d3b1c8034"}, {"y": 99, "x": 216, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9b9f0a0248d363bd410db3090b1802dd0804419"}, {"y": 147, "x": 320, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e8ac5db81d347238245dee287b99b48853a5f60"}, {"y": 294, "x": 640, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=25fd21426a00f01c6879a17b3801a660bf9b9139"}, {"y": 441, "x": 960, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=541aa7ac17809096292434b0ae63813d31cf5f38"}], "s": {"y": 472, "x": 1027, "u": "https://preview.redd.it/9a2hor5nj3gc1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=0d62b085471d2ab5a134701139b22f254206bcd1"}, "id": "9a2hor5nj3gc1"}}, "name": "t3_1agvb8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ko9tlb8ckFBfUfEWr3Djt1ICPQzJryn56VbSSXn3KC0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706848528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I have an interview home assignment where I need to decode hexadecimal payload. I have been trying to decode the payload but not able to produce correct output specifically for the time field. I would appreciate assistance in decoding it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Description about the task:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In order to monitor the health of the battery, we decided to send data from the battery to the cloud. This data is transmitted in hexadecimal format and received in our AWS account. &lt;/p&gt;\n\n&lt;p&gt;The data is transmitted as a hexadecimal string. Every payload consists of 8 bytes. Due to space optimization, the information is not byte-aligned. A field can start in the middle of a byte. We therefore need bit operations to decode the payload. The payload is not signed and encoded in little Endian. The following table describes the data fields contained in the payload and their bit positions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9a2hor5nj3gc1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d62b085471d2ab5a134701139b22f254206bcd1\"&gt;https://preview.redd.it/9a2hor5nj3gc1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d62b085471d2ab5a134701139b22f254206bcd1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9zl6pcopj3gc1.png?width=1104&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55261006470072c0e9933dadc5d59115751de0f3\"&gt;https://preview.redd.it/9zl6pcopj3gc1.png?width=1104&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55261006470072c0e9933dadc5d59115751de0f3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For instance, type is encoded on 4 bits in the first byte. state of charge is encoded on 8 bits (1 byte) on the 6th byte.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Time:&lt;/strong&gt; time represents the timestamp of the data. It is defined in seconds since UNIX epoch.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;State:&lt;/strong&gt; state is a string, with the following corresponding values:&lt;/p&gt;\n\n&lt;p&gt;0: &amp;quot;power off&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;1: &amp;quot;power on&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;2: &amp;quot;discharge&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;3: &amp;quot;charge&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;4: &amp;quot;charge complete&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;5: &amp;quot;host mode&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;6: &amp;quot;shutdown&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;7: &amp;quot;error&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;8: &amp;quot;undefined&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;State of charge&lt;/strong&gt;: state of charge represents the charge of the battery. It is a float with values between 0 and 100 and a 0.5 precision. To store it as an integer, it was multiplied by 2.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Battery temperature:&lt;/strong&gt; battery temperature represents the temperature of the battery. Values can vary between -20 and 100. The precision is 0.5. To store it as an integer we added 20 and multiplied it by 2.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Sample Test data&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;input: F1E6E63676C75000 &lt;/p&gt;\n\n&lt;p&gt;output: {  &amp;quot;time&amp;quot;: 1668181615,  &amp;quot;state&amp;quot;: &amp;quot;error&amp;quot;,  &amp;quot;state_of_charge&amp;quot;: 99.5,  &amp;quot;temperature&amp;quot;: 20.0 } &lt;/p&gt;\n\n&lt;p&gt;My Script:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import base64\nimport struct\nimport json\nfrom datetime import datetime\n\n\ndef lambda_handler(event):\n    # Extract device and payload from the event\n    device = event[&amp;quot;device&amp;quot;]\n    payload_hex = event[&amp;quot;payload&amp;quot;]\n\n    # Convert hexadecimal payload to bytes\n    payload_bytes = bytes.fromhex(payload_hex)\n\n    # Unpack the payload using struct\n    unpacked_data = struct.unpack(&amp;#39;&amp;lt;I2Bh&amp;#39;, payload_bytes)\n\n    # Extract individual fields\n    time = unpacked_data[0]\n\n    state = unpacked_data[1]\n    state = (state &amp;gt;&amp;gt; 4) &amp;amp; 0x0F\n\n    state_of_charge = unpacked_data[2] / 2.0\n    temperature = (unpacked_data[3] / 2.0) - 20.0\n\n    # Mapping state values to corresponding strings\n    state_mapping = {\n        0: &amp;quot;power off&amp;quot;,\n        1: &amp;quot;power on&amp;quot;,\n        2: &amp;quot;discharge&amp;quot;,\n        3: &amp;quot;charge&amp;quot;,\n        4: &amp;quot;charge complete&amp;quot;,\n        5: &amp;quot;host mode&amp;quot;,\n        6: &amp;quot;shutdown&amp;quot;,\n        7: &amp;quot;error&amp;quot;,\n        8: &amp;quot;undefined&amp;quot;\n    }\n\n    # Create the output dictionary\n    output_data = {\n        &amp;quot;device&amp;quot;: device,\n        &amp;quot;time&amp;quot;: time,\n        &amp;quot;state&amp;quot;: state_mapping.get(state, &amp;quot;unknown&amp;quot;),\n        &amp;quot;state_of_charge&amp;quot;: round(state_of_charge, 1),\n        &amp;quot;temperature&amp;quot;: round(temperature, 1)\n    }\n\n    # Log the output data to stdout\n    print(json.dumps(output_data))\n\nevent = {&amp;#39;device&amp;#39;: &amp;#39;device1&amp;#39;, &amp;#39;payload&amp;#39;: &amp;#39;6188293726C75C00&amp;#39;}\nlambda_handler(event)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am struggling to get the correct output for the time which is not just dependent on the unpacked_data[0] based on the above logic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agvb8x", "is_robot_indexable": true, "report_reasons": null, "author": "Huge_Jicama_3087", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agvb8x/hexadecimal_format_payload_decoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agvb8x/hexadecimal_format_payload_decoding/", "subreddit_subscribers": 157650, "created_utc": 1706848528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had a DE Doordash interview? Haven't been able to find to much information specific to what DE interview questions are like. \n\nWas told there would be 4 SQL and 1 Python question and to practice with Leetcode from the recruiter, but nothing past that. \n\nI've been referencing this curated list of [questions](https://leetcode.com/discuss/interview-question/1583430/Doordash-Questions-Consolidated) for the Python/DSA portion. \n\nWhat has your experience been like?", "author_fullname": "t2_19klta65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Doordash Phone Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agsg83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706839598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had a DE Doordash interview? Haven&amp;#39;t been able to find to much information specific to what DE interview questions are like. &lt;/p&gt;\n\n&lt;p&gt;Was told there would be 4 SQL and 1 Python question and to practice with Leetcode from the recruiter, but nothing past that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been referencing this curated list of &lt;a href=\"https://leetcode.com/discuss/interview-question/1583430/Doordash-Questions-Consolidated\"&gt;questions&lt;/a&gt; for the Python/DSA portion. &lt;/p&gt;\n\n&lt;p&gt;What has your experience been like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1agsg83", "is_robot_indexable": true, "report_reasons": null, "author": "DRUKSTOP", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agsg83/upcoming_doordash_phone_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agsg83/upcoming_doordash_phone_interview/", "subreddit_subscribers": 157650, "created_utc": 1706839598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data \u201ccontract\u201d actual implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agkong", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706819160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agkong", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "subreddit_subscribers": 157650, "created_utc": 1706819160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Feb 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c", "t3_17lfedu", "t3_188grkl", "t3_18w0y5n", "t3_1agfqy9"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1706806830.354, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agfqy9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706806830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agfqy9", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1agfqy9/monthly_general_discussion_feb_2024/", "subreddit_subscribers": 157650, "created_utc": 1706806830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How to structure a data pipeline repo for pyspark jupyter notebooks?\n\nI am planning to build a data pipeline for a new project from scratch, which would be in pyspark sagemaker notebooks Technologies used as below\nOrchestration: Airlfow\nStorage: S3\nFinal transformed tables will be created in athena.\n\nHow would you structure a git repo that's written in pyspark notebooks and with a dag folder. We are also looking to implement CI/CD in the future. It also should have a proper logging mechanism.\n\nWould like to hear all your suggestions and any github repo examples would be highly appreciated.\n Thanks!", "author_fullname": "t2_6n03d0sf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline creation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agv56b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706847963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to structure a data pipeline repo for pyspark jupyter notebooks?&lt;/p&gt;\n\n&lt;p&gt;I am planning to build a data pipeline for a new project from scratch, which would be in pyspark sagemaker notebooks Technologies used as below\nOrchestration: Airlfow\nStorage: S3\nFinal transformed tables will be created in athena.&lt;/p&gt;\n\n&lt;p&gt;How would you structure a git repo that&amp;#39;s written in pyspark notebooks and with a dag folder. We are also looking to implement CI/CD in the future. It also should have a proper logging mechanism.&lt;/p&gt;\n\n&lt;p&gt;Would like to hear all your suggestions and any github repo examples would be highly appreciated.\n Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agv56b", "is_robot_indexable": true, "report_reasons": null, "author": "arunrajan96", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agv56b/data_pipeline_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agv56b/data_pipeline_creation/", "subreddit_subscribers": 157650, "created_utc": 1706847963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI was trying to create a server that allow users to create and execute notebook for data exploration, data processing and training model\n\nFor now I am using JupyterHub on Kubernetes which will create a pod to execute notebook each time a user log in and terminate it after they stop their session. However, I assume when there are 1000 users at the same time then there will be 1000 pods, which is very resource-consuming. Is there any other way to do this job? Am I going in the right direction? Does anyone know which tools big company like kaggle, google colab, databrick, ... use to manage and execute user notebooks in large scale? Please give me some information\n\nThanks for your reading", "author_fullname": "t2_oj5xmc65m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting Notebook server in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agulfg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706846150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I was trying to create a server that allow users to create and execute notebook for data exploration, data processing and training model&lt;/p&gt;\n\n&lt;p&gt;For now I am using JupyterHub on Kubernetes which will create a pod to execute notebook each time a user log in and terminate it after they stop their session. However, I assume when there are 1000 users at the same time then there will be 1000 pods, which is very resource-consuming. Is there any other way to do this job? Am I going in the right direction? Does anyone know which tools big company like kaggle, google colab, databrick, ... use to manage and execute user notebooks in large scale? Please give me some information&lt;/p&gt;\n\n&lt;p&gt;Thanks for your reading&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agulfg", "is_robot_indexable": true, "report_reasons": null, "author": "resrrdttrt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agulfg/hosting_notebook_server_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agulfg/hosting_notebook_server_in_production/", "subreddit_subscribers": 157650, "created_utc": 1706846150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a large enterprise and got hired to re engineer our HR data. \n\nWe get our data from Kafka and from that the current development team made a database and tried to create a clean layer. I think they used mostly SAS. \n\nFrom that product they made went on to create some sub data bases for various analytic customers with some custom sql transformations.\n\nBecause the enterprise has evolved since that\u2019s happened - they created another database as an \u201cimprovement\u201d to the first one but didn\u2019t migrate any of the customer data sets.\n\nI feel like I want to convince the C suite to use Python to do a mass consolidation and create a semantic layer and connect everything with APIs.\n\nComing to this reddit I\u2019m seeing a lot of \u201cPython bad\u201d. \n\nWhat am I missing?", "author_fullname": "t2_4dovkjca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My First Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agtr1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706843472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a large enterprise and got hired to re engineer our HR data. &lt;/p&gt;\n\n&lt;p&gt;We get our data from Kafka and from that the current development team made a database and tried to create a clean layer. I think they used mostly SAS. &lt;/p&gt;\n\n&lt;p&gt;From that product they made went on to create some sub data bases for various analytic customers with some custom sql transformations.&lt;/p&gt;\n\n&lt;p&gt;Because the enterprise has evolved since that\u2019s happened - they created another database as an \u201cimprovement\u201d to the first one but didn\u2019t migrate any of the customer data sets.&lt;/p&gt;\n\n&lt;p&gt;I feel like I want to convince the C suite to use Python to do a mass consolidation and create a semantic layer and connect everything with APIs.&lt;/p&gt;\n\n&lt;p&gt;Coming to this reddit I\u2019m seeing a lot of \u201cPython bad\u201d. &lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agtr1x", "is_robot_indexable": true, "report_reasons": null, "author": "necrohobo", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agtr1x/my_first_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agtr1x/my_first_day/", "subreddit_subscribers": 157650, "created_utc": 1706843472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks", "author_fullname": "t2_ened0cec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi, BI developer to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agqyza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706835367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agqyza", "is_robot_indexable": true, "report_reasons": null, "author": "sleepy_bored_eternal", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "subreddit_subscribers": 157650, "created_utc": 1706835367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. \n\nIm trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. \n\nHence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.", "author_fullname": "t2_6nb4026n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there Python libraries that define and parametrize etl jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglv35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706822120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Im trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. &lt;/p&gt;\n\n&lt;p&gt;Hence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aglv35", "is_robot_indexable": true, "report_reasons": null, "author": "armAssembledx86", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "subreddit_subscribers": 157650, "created_utc": 1706822120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How was the switch for you, and what steps did you take to make the switch?", "author_fullname": "t2_55l23zh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here switched to DE from Technical Project Management/Product Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglecn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706820952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How was the switch for you, and what steps did you take to make the switch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aglecn", "is_robot_indexable": true, "report_reasons": null, "author": "egg_boi56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "subreddit_subscribers": 157650, "created_utc": 1706820952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database \u201cAPI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agifx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agifx8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agifx8/database_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agifx8/database_api/", "subreddit_subscribers": 157650, "created_utc": 1706813572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. [https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs](https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs)", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Five Useful Queries to Get BigQuery Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agh025", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706809949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A topic that comes up in every customer engagement is Data Warehouse costs. In spirit of that we are publishing our first blog on Five Useful Queries to Get BigQuery Costs. &lt;a href=\"https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs\"&gt;https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?auto=webp&amp;s=2c376e4c51790abdb77e784f486631d6e609bf89", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=624487499dff65b0038a6c79404f8cb230d1a7fc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=957574fe4817ae688d5220549723b418efbd73b2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a83e577e94738c1cb9f73585da0e1bb35b288f7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11affefba0bc6c9c3308849bfea98546e488beb4", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a74bcd4439a79a120286cee87d6cf904a597e79b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/OGd3eO22aksYTefQpHEte0JXIMXcC7B2-vv4N2Q94Es.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56bf0012cdb3fb9322b7eb47cdf976897786096b", "width": 1080, "height": 567}], "variants": {}, "id": "KzLetsP8vZqCBImB3BNcFmeWS5rzEuEkGvHPTrzwTtw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agh025", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agh025/five_useful_queries_to_get_bigquery_costs/", "subreddit_subscribers": 157650, "created_utc": 1706809949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product Data Teams 101", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": true, "name": "t3_1agyc09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xK7yt6q8iT8AZBnUiQ0X0CfFONvGoWQYpJODiUAFQ_A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706859675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "svenbalnojan.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://svenbalnojan.medium.com/product-data-teams-101-bf0e42c2000d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?auto=webp&amp;s=02b330f1c8f6daa0db15d4cdc76da5eb0658e699", "width": 1200, "height": 857}, "resolutions": [{"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44a44a9c5658b23e982c52f2e892809d84a0594d", "width": 108, "height": 77}, {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db6f81739a92a0a8005ef7cd26531a075237b977", "width": 216, "height": 154}, {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ddd8561bdc1954eae2e3fd4cdf754ed6768fd30", "width": 320, "height": 228}, {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=506d3017ba35a5f6709984d0bd10a1784291a4a7", "width": 640, "height": 457}, {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30294e1b92197a70dc1709673b4c4e36f506a5d8", "width": 960, "height": 685}, {"url": "https://external-preview.redd.it/w6yc_iQiJ3gOBLfPwRFZ_WQfW-nL9kqkwLd8amDYhEo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42ea305ce180a5e34dc13f231adcce5b82309fe4", "width": 1080, "height": 771}], "variants": {}, "id": "KpBCqnyNSWGVn4rs7W3B_Uku0Gh68u1CJeo_Fj40PAg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agyc09", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agyc09/product_data_teams_101/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://svenbalnojan.medium.com/product-data-teams-101-bf0e42c2000d", "subreddit_subscribers": 157650, "created_utc": 1706859675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I'm in networking so work usually goes something like\n\n1. Get ticket saying host is down/in a bad state\n2. Do stuff to fix \n3. Test to confirm fix\n4. Send ticket back, showing confirmed fix\n\nWhat's the workflow like with DE?", "author_fullname": "t2_1zk4yufi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's work assigned (tickets?) and later verified \"complete\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agy1fc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706858410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m in networking so work usually goes something like&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get ticket saying host is down/in a bad state&lt;/li&gt;\n&lt;li&gt;Do stuff to fix &lt;/li&gt;\n&lt;li&gt;Test to confirm fix&lt;/li&gt;\n&lt;li&gt;Send ticket back, showing confirmed fix&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What&amp;#39;s the workflow like with DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agy1fc", "is_robot_indexable": true, "report_reasons": null, "author": "9070932767", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agy1fc/hows_work_assigned_tickets_and_later_verified/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agy1fc/hows_work_assigned_tickets_and_later_verified/", "subreddit_subscribers": 157650, "created_utc": 1706858410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hola all data engineers,\n\nmay I ask about your experiences in time series forecast, like do you explain the most important factors of the swing? Something happened at my work was (I'm on the business side, more like a business analyst) :\n\ndata engineering team came up with a time series forecast of our revenue using Facebook prophet. the model is based on the past revenue day-by-day at customer level, meaning we have lots of data points. Based on the time series projection, the future quarters' movement could range between **2%-5% every week we extract our data from the time series model**.\n\nHow do you usually explain such swing ? I can break them down by customer for explaining such swings. However earlier when I built a financial modelling , I could always tell which factor is the main driver but now with FB prophet it's kind of a black box. For example, I couldn't tell **whether each time period in the past has equal weight in the final projection**, or does FB prophet weighs more on the recent history of revenue like last 3 months , or that depends on how one changes parameters etc.\n\nI could check with the data engineering team too however based on past interaction with them, I'd rather know about a few things myself first, in order to validate their points or dig deeper\n\nMuch appreciated!", "author_fullname": "t2_2lo2gcta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drivers of movement in time series forecast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agtmsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706843123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hola all data engineers,&lt;/p&gt;\n\n&lt;p&gt;may I ask about your experiences in time series forecast, like do you explain the most important factors of the swing? Something happened at my work was (I&amp;#39;m on the business side, more like a business analyst) :&lt;/p&gt;\n\n&lt;p&gt;data engineering team came up with a time series forecast of our revenue using Facebook prophet. the model is based on the past revenue day-by-day at customer level, meaning we have lots of data points. Based on the time series projection, the future quarters&amp;#39; movement could range between &lt;strong&gt;2%-5% every week we extract our data from the time series model&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;How do you usually explain such swing ? I can break them down by customer for explaining such swings. However earlier when I built a financial modelling , I could always tell which factor is the main driver but now with FB prophet it&amp;#39;s kind of a black box. For example, I couldn&amp;#39;t tell &lt;strong&gt;whether each time period in the past has equal weight in the final projection&lt;/strong&gt;, or does FB prophet weighs more on the recent history of revenue like last 3 months , or that depends on how one changes parameters etc.&lt;/p&gt;\n\n&lt;p&gt;I could check with the data engineering team too however based on past interaction with them, I&amp;#39;d rather know about a few things myself first, in order to validate their points or dig deeper&lt;/p&gt;\n\n&lt;p&gt;Much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agtmsp", "is_robot_indexable": true, "report_reasons": null, "author": "clooneyge", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agtmsp/drivers_of_movement_in_time_series_forecast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agtmsp/drivers_of_movement_in_time_series_forecast/", "subreddit_subscribers": 157650, "created_utc": 1706843123.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}