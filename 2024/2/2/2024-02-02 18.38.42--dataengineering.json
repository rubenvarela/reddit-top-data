{"kind": "Listing", "data": {"after": "t3_1ah5j9x", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I recently caught up with a friend of mine who is a experienced data engineer and works for a marketing startup. While we were chatting up, he told me that he has taken 100s of interviews and his main filter is always leetcode. Only candidates who are able to solve different medium level leetcode problems across multiple rounds are considered for hiring.\n\nI was a bit surprised by that because wouldn't it be easy to lose out on a lot of good candidates but he said that leetcode type problems help him understand how smart a candidate is and how well he can come up with ideas and tackle DE problems. What are your thoughts on this? How would you choose to interview potential candidates and does leetcode type competitive coding questions have any bearing?", "author_fullname": "t2_q979mh0vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you think Leetcode type questions is a good metric for data engineering skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agxov7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706857017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently caught up with a friend of mine who is a experienced data engineer and works for a marketing startup. While we were chatting up, he told me that he has taken 100s of interviews and his main filter is always leetcode. Only candidates who are able to solve different medium level leetcode problems across multiple rounds are considered for hiring.&lt;/p&gt;\n\n&lt;p&gt;I was a bit surprised by that because wouldn&amp;#39;t it be easy to lose out on a lot of good candidates but he said that leetcode type problems help him understand how smart a candidate is and how well he can come up with ideas and tackle DE problems. What are your thoughts on this? How would you choose to interview potential candidates and does leetcode type competitive coding questions have any bearing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1agxov7", "is_robot_indexable": true, "report_reasons": null, "author": "ShaliniMalhotra9512", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agxov7/do_you_think_leetcode_type_questions_is_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agxov7/do_you_think_leetcode_type_questions_is_a_good/", "subreddit_subscribers": 157761, "created_utc": 1706857017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.\n\nIt\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c\n\nThe behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f\n\nIm calling it SQRUFF :)", "author_fullname": "t2_dr38sa99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Portable SQL Linter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1agijyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S-kVxl0yQ0zf6MOW3PZbmZsw0mFsesa5vKjPHCqg0WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706813857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s early days but I should be done with the first version of it next week - feel free to star it for updates and I can\u2019t wait to give back to the community \ud83d\ude4c&lt;/p&gt;\n\n&lt;p&gt;The behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn\u2019t need python to run :) it will literally run natively on my phone which is crazy \ud83e\udd2f&lt;/p&gt;\n\n&lt;p&gt;Im calling it SQRUFF :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/quarylabs/sqruff", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?auto=webp&amp;s=87a0b3113d8a8e8a4fad2901b55e98449c340d4a", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e09230469f5ffa3e0cd2651e18d860042ba4e457", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f7839ea5bdb3e5e564f2e5afa389d6990bfb89", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fda65169a18d5a214619e0464afdb2852107fa47", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57c64448d9e4256e89bfc92a1e03627f1e1c4050", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cde8788c99759e9bf6399fa46456d0e851962e6", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c224395a06778cde7bd43e5c28b6fab324950f9", "width": 1080, "height": 1080}], "variants": {}, "id": "yi1H-2cV1KrS4mf3uf9SP2lDbruuU_3Qx50MFRPXxz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agijyw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Call6280", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agijyw/open_source_portable_sql_linter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/quarylabs/sqruff", "subreddit_subscribers": 157761, "created_utc": 1706813857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nWe have one table named A in SQL server database which is hosted on Azure Managed VM.\nand for some testing, I wanted to copy the data to our ADLS gen 2.\n\nCurrently the SQL table has no partitions. And super slow even to find the count of rows and has close to 500+ columns.\n\nI have used copy activity with SQL as source and parquet format in ADLS destination. But it is going at the rate of 100k rows per minute.\n\nI have tried tinkering with parallelism and it didn't help much.\nI'm thinking pulling in just selected columns as a start tomorrow.\n\nJust trying to see if you have guys f have better ideas?", "author_fullname": "t2_kz99f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to copy 3 billion rows from SQL Server to ADLS. What would be the fastest way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agw6kc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706851432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;We have one table named A in SQL server database which is hosted on Azure Managed VM.\nand for some testing, I wanted to copy the data to our ADLS gen 2.&lt;/p&gt;\n\n&lt;p&gt;Currently the SQL table has no partitions. And super slow even to find the count of rows and has close to 500+ columns.&lt;/p&gt;\n\n&lt;p&gt;I have used copy activity with SQL as source and parquet format in ADLS destination. But it is going at the rate of 100k rows per minute.&lt;/p&gt;\n\n&lt;p&gt;I have tried tinkering with parallelism and it didn&amp;#39;t help much.\nI&amp;#39;m thinking pulling in just selected columns as a start tomorrow.&lt;/p&gt;\n\n&lt;p&gt;Just trying to see if you have guys f have better ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agw6kc", "is_robot_indexable": true, "report_reasons": null, "author": "jerrie86", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agw6kc/trying_to_copy_3_billion_rows_from_sql_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agw6kc/trying_to_copy_3_billion_rows_from_sql_server_to/", "subreddit_subscribers": 157761, "created_utc": 1706851432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spend the last few months using dbt  to model and analyze historical NBA data sets. [The project](https://github.com/paradime-io/paradime-dbt-nba-data-challenge) has been so fun that I'm releasing it to data folks as a competition!\n\nIn this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!\n\nHere's how it works:\n\nUpon registration, Participants will gain access to:  \n\ud83d\udc49 Paradime for SQL &amp; dbt\u2122 development.  \n\u2744\ufe0f Snowflake for computing and storage.  \n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.  \n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023\n\nFrom there, participants will create insightful analyses and visualizations, and submit them for a chance to win! \n\nIf you're curious, learn more below!\n\n[https://www.paradime.io/dbt-data-modeling-challenge-nba-edition](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt\u2122 data modeling Challenge - NBA Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agj7rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706815476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spend the last few months using dbt  to model and analyze historical NBA data sets. &lt;a href=\"https://github.com/paradime-io/paradime-dbt-nba-data-challenge\"&gt;The project&lt;/a&gt; has been so fun that I&amp;#39;m releasing it to data folks as a competition!&lt;/p&gt;\n\n&lt;p&gt;In this competition, data. folks across the globe will have the opportunity to demonstrate their expertise in SQL, dbt, and analytics to not only extract meaningful insights from NBA data, but also win a $500 - $ 1500 Amazon gift cards!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works:&lt;/p&gt;\n\n&lt;p&gt;Upon registration, Participants will gain access to:&lt;br/&gt;\n\ud83d\udc49 Paradime for SQL &amp;amp; dbt\u2122 development.&lt;br/&gt;\n\u2744\ufe0f Snowflake for computing and storage.&lt;br/&gt;\n\ud83e\udd16 \ud835\udc06\ud835\udc22\ud835\udc2d\ud835\udc07\ud835\udc2e\ud835\udc1b repository to showcase your work and insights.&lt;br/&gt;\n\ud83c\udfc0 Seven historical \ud835\udc0d\ud835\udc01\ud835\udc00 \ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc2c\ud835\udc1e\ud835\udc2d\ud835\udc2c, ranging from 1946-2023&lt;/p&gt;\n\n&lt;p&gt;From there, participants will create insightful analyses and visualizations, and submit them for a chance to win! &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, learn more below!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;https://www.paradime.io/dbt-data-modeling-challenge-nba-edition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?auto=webp&amp;s=41fdb09a495622cce5c1f05123e1900fbad6458f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0bfcb7fbab608cc41fe9498b564e93d489c6b27", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a40dbe345268ba354342895cc4ca9607f43f87be", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=785b1c83ea7863ff2de6c16a16e12de4c8f1e39b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed0083d4fe841bfb13478442010389f368cc1d31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2aeaaee50e003c299bde24ad70d7ca810d0e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Thsu6DiTNB6o35Olu0RK2mAv7xdwetIiVXo4ZejVNiI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8f90192aba6736b970c5fc77523839360fd4610", "width": 1080, "height": 540}], "variants": {}, "id": "vW2rNRLOsGuCMv-C4hnHQbNq81LUjPoWMDxQDE6o6jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1agj7rh", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agj7rh/dbt_data_modeling_challenge_nba_edition/", "subreddit_subscribers": 157761, "created_utc": 1706815476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I got a chance to revamp a data pipeline in my company that ingest event log data in JSON format. Currently, the pipeline will perform a full refresh for every table stage (raw, transformation/curated, and aggregation layer), and due to constant increase in the volume, it takes too much resources and running hours. All of the tables are not partitioned based on date column, so basically we store the table snapshot on each date partition. I am thinking of using incremental update for each stage so the computing cost should be lower.\n\nRevamp the part from raw JSON into tables was done and the raw table are properly partitioned based on \\`created\\_date\\` column and ingesting the delta from cloud storage. But I got stuck in the transformation layer, where there is one column where we need to create a row\\_number based on \\`uid\\` and \\`created\\_date\\` to create some kind of \\`session\\` column, and there is a possibility that this \\`session\\` can span for more than a days, which means I need to consider all of the date partition for each \\`uid\\` no matter what.\n\nDo you folks have any idea to make incremental update for this kind of scenario? Currently I consider to cluster the \\`uid\\` column so it will optimze the step to populate the  \\`session\\` column a bit when it scan all of the dates in the table. Thanks in advance.", "author_fullname": "t2_1q71lc7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating full refresh pipeline into incremental update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agxp69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706857054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I got a chance to revamp a data pipeline in my company that ingest event log data in JSON format. Currently, the pipeline will perform a full refresh for every table stage (raw, transformation/curated, and aggregation layer), and due to constant increase in the volume, it takes too much resources and running hours. All of the tables are not partitioned based on date column, so basically we store the table snapshot on each date partition. I am thinking of using incremental update for each stage so the computing cost should be lower.&lt;/p&gt;\n\n&lt;p&gt;Revamp the part from raw JSON into tables was done and the raw table are properly partitioned based on `created_date` column and ingesting the delta from cloud storage. But I got stuck in the transformation layer, where there is one column where we need to create a row_number based on `uid` and `created_date` to create some kind of `session` column, and there is a possibility that this `session` can span for more than a days, which means I need to consider all of the date partition for each `uid` no matter what.&lt;/p&gt;\n\n&lt;p&gt;Do you folks have any idea to make incremental update for this kind of scenario? Currently I consider to cluster the `uid` column so it will optimze the step to populate the  `session` column a bit when it scan all of the dates in the table. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agxp69", "is_robot_indexable": true, "report_reasons": null, "author": "srodinger18", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agxp69/migrating_full_refresh_pipeline_into_incremental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agxp69/migrating_full_refresh_pipeline_into_incremental/", "subreddit_subscribers": 157761, "created_utc": 1706857054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had a DE Doordash interview? Haven't been able to find to much information specific to what DE interview questions are like. \n\nWas told there would be 4 SQL and 1 Python question and to practice with Leetcode from the recruiter, but nothing past that. \n\nI've been referencing this curated list of [questions](https://leetcode.com/discuss/interview-question/1583430/Doordash-Questions-Consolidated) for the Python/DSA portion. \n\nWhat has your experience been like?", "author_fullname": "t2_19klta65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Doordash Phone Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agsg83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706839598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had a DE Doordash interview? Haven&amp;#39;t been able to find to much information specific to what DE interview questions are like. &lt;/p&gt;\n\n&lt;p&gt;Was told there would be 4 SQL and 1 Python question and to practice with Leetcode from the recruiter, but nothing past that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been referencing this curated list of &lt;a href=\"https://leetcode.com/discuss/interview-question/1583430/Doordash-Questions-Consolidated\"&gt;questions&lt;/a&gt; for the Python/DSA portion. &lt;/p&gt;\n\n&lt;p&gt;What has your experience been like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1agsg83", "is_robot_indexable": true, "report_reasons": null, "author": "DRUKSTOP", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agsg83/upcoming_doordash_phone_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agsg83/upcoming_doordash_phone_interview/", "subreddit_subscribers": 157761, "created_utc": 1706839598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks", "author_fullname": "t2_ened0cec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi, BI developer to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agqyza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706835367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a BI developer where SQL, Tableau, QlikView and in limited capacity Python being my tool/tech stack. But given I have always been on the low-code side, my natural inclination is towards learning dbt, snowflake and Power BI. I am attending the Zoomcamp, but finding it a bit difficult. Can someone guide me into which path makes more sense. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agqyza", "is_robot_indexable": true, "report_reasons": null, "author": "sleepy_bored_eternal", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agqyza/hi_bi_developer_to_de/", "subreddit_subscribers": 157761, "created_utc": 1706835367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a large enterprise and got hired to re engineer our HR data. \n\nWe get our data from Kafka and from that the current development team made a database and tried to create a clean layer. I think they used mostly SAS. \n\nFrom that product they made went on to create some sub data bases for various analytic customers with some custom sql transformations.\n\nBecause the enterprise has evolved since that\u2019s happened - they created another database as an \u201cimprovement\u201d to the first one but didn\u2019t migrate any of the customer data sets.\n\nI feel like I want to convince the C suite to use Python to do a mass consolidation and create a semantic layer and connect everything with APIs.\n\nComing to this reddit I\u2019m seeing a lot of \u201cPython bad\u201d. \n\nWhat am I missing?", "author_fullname": "t2_4dovkjca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My First Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agtr1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706843472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a large enterprise and got hired to re engineer our HR data. &lt;/p&gt;\n\n&lt;p&gt;We get our data from Kafka and from that the current development team made a database and tried to create a clean layer. I think they used mostly SAS. &lt;/p&gt;\n\n&lt;p&gt;From that product they made went on to create some sub data bases for various analytic customers with some custom sql transformations.&lt;/p&gt;\n\n&lt;p&gt;Because the enterprise has evolved since that\u2019s happened - they created another database as an \u201cimprovement\u201d to the first one but didn\u2019t migrate any of the customer data sets.&lt;/p&gt;\n\n&lt;p&gt;I feel like I want to convince the C suite to use Python to do a mass consolidation and create a semantic layer and connect everything with APIs.&lt;/p&gt;\n\n&lt;p&gt;Coming to this reddit I\u2019m seeing a lot of \u201cPython bad\u201d. &lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agtr1x", "is_robot_indexable": true, "report_reasons": null, "author": "necrohobo", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agtr1x/my_first_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agtr1x/my_first_day/", "subreddit_subscribers": 157761, "created_utc": 1706843472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.   \nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  \n\n\nI took Alan Rodrigues course in udemy and Ramesh retnasamy's ADF course .  \n[https://www.udemy.com/course/data-engineering-on-microsoft-azure/](https://www.udemy.com/course/data-engineering-on-microsoft-azure/)  \n[https://www.udemy.com/course/learn-azure-data-factory-from-scratch/](https://www.udemy.com/course/learn-azure-data-factory-from-scratch/)\n\nAnd following Tybul on azure youtube play list.   \n[https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg](https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg)  \n\n\nPlease share any study materials or notes for new syllabus. \n\n&amp;#x200B;", "author_fullname": "t2_pdssduhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP -203 Exam study materials help..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agiyq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706814845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one recently took DP -203 (After Nov -2023). I heard there is change in syllabus from November and also its open book exam.&lt;br/&gt;\nI have scheduled my exam for next Monday. But still am not feeling confident about my exam since the syllabus is vast. This courses are enough or do i need to follow  any other updated materials.  &lt;/p&gt;\n\n&lt;p&gt;I took Alan Rodrigues course in udemy and Ramesh retnasamy&amp;#39;s ADF course .&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/data-engineering-on-microsoft-azure/\"&gt;https://www.udemy.com/course/data-engineering-on-microsoft-azure/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.udemy.com/course/learn-azure-data-factory-from-scratch/\"&gt;https://www.udemy.com/course/learn-azure-data-factory-from-scratch/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And following Tybul on azure youtube play list.&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg\"&gt;https://www.youtube.com/channel/UCLnXq-Fr-6rAsCitq9nYiGg&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Please share any study materials or notes for new syllabus. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agiyq1", "is_robot_indexable": true, "report_reasons": null, "author": "bmkmanojkumar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agiyq1/dp_203_exam_study_materials_help/", "subreddit_subscribers": 157761, "created_utc": 1706814845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company Im in the accounting side of the application. Often Im required to run some reports which take an awful loads of time like 3hrs. I have a table which has a column of type JSON(not JSONB). In one of the report Im supposed to select some fields from the json like the following\n\n&amp;#x200B;\n\nSELECT \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_truck\\_tractor\\_value' as \"ORIGINAL TRUCK TRACTOR VALUE\",    \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_trailer\\_value' as \"ORIGINAL TRAILER VALUE\",        \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_truck\\_tractor\\_value' as \"CURRENT TRUCK TRACTOR VALUE\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_trailer\\_value' as \"CURRENT TRAILER VALUE\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_unit\\_count' as \"ORIGINAL UNIT COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_driver\\_count' as \"ORIGINAL DRIVER COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_unit\\_count' as \"CURRENT UNIT COUNT\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_trailer\\_count' as \"CURRENT TRAILER COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_driver\\_count' as \"CURRENT DRIVER COUNT\"\n\nFROM blah blah blah.....  \n\n\nLike the above query but in production we have like 60-70 fields to be selected from json which is making the query tooo slow. All other parts of the query are running in less than half a minute. Can anyone suggest a way to speed up this query . All the joins im making are inevitable and cannot be optimized as the postgres optimizer is doing it job well and fine. i've seperated the query into two parts . one with all the json fields accessing query and others with normal column accessing. The normal query is running in &lt;30s. While this json accessing query is taking a loads of time.  \n\n\nPlease suggest a way to speed up this query.  \nthank you\n\n  \n", "author_fullname": "t2_344hea7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accessing large json columns in postgresql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah6reu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706896172.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706888924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company Im in the accounting side of the application. Often Im required to run some reports which take an awful loads of time like 3hrs. I have a table which has a column of type JSON(not JSONB). In one of the report Im supposed to select some fields from the json like the following&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SELECT &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_truck\\_tractor\\_value&amp;#39; as &amp;quot;ORIGINAL TRUCK TRACTOR VALUE&amp;quot;,    \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_trailer\\_value&amp;#39; as &amp;quot;ORIGINAL TRAILER VALUE&amp;quot;,        \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_truck\\_tractor\\_value&amp;#39; as &amp;quot;CURRENT TRUCK TRACTOR VALUE&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_trailer\\_value&amp;#39; as &amp;quot;CURRENT TRAILER VALUE&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_unit\\_count&amp;#39; as &amp;quot;ORIGINAL UNIT COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_driver\\_count&amp;#39; as &amp;quot;ORIGINAL DRIVER COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_unit\\_count&amp;#39; as &amp;quot;CURRENT UNIT COUNT&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_trailer\\_count&amp;#39; as &amp;quot;CURRENT TRAILER COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_driver\\_count&amp;#39; as &amp;quot;CURRENT DRIVER COUNT&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;FROM blah blah blah.....  &lt;/p&gt;\n\n&lt;p&gt;Like the above query but in production we have like 60-70 fields to be selected from json which is making the query tooo slow. All other parts of the query are running in less than half a minute. Can anyone suggest a way to speed up this query . All the joins im making are inevitable and cannot be optimized as the postgres optimizer is doing it job well and fine. i&amp;#39;ve seperated the query into two parts . one with all the json fields accessing query and others with normal column accessing. The normal query is running in &amp;lt;30s. While this json accessing query is taking a loads of time.  &lt;/p&gt;\n\n&lt;p&gt;Please suggest a way to speed up this query.&lt;br/&gt;\nthank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ah6reu", "is_robot_indexable": true, "report_reasons": null, "author": "Ashu6410", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah6reu/accessing_large_json_columns_in_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah6reu/accessing_large_json_columns_in_postgresql/", "subreddit_subscribers": 157761, "created_utc": 1706888924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does the need for ingestion latency or data freshness impact this choice? \n\n[View Poll](https://www.reddit.com/poll/1agw2fp)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which pattern do you use when ingesting data into lakehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agw2fp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706851043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does the need for ingestion latency or data freshness impact this choice? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1agw2fp\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agw2fp", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1707455843617, "options": [{"text": "Land data in object storage in csv, json. Then run a job to write into Delta / Hudi / Iceberg tables", "id": "26948894"}, {"text": "Directly write into Delta /  Iceberg / Hudi tables", "id": "26948895"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 82, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agw2fp/which_pattern_do_you_use_when_ingesting_data_into/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1agw2fp/which_pattern_do_you_use_when_ingesting_data_into/", "subreddit_subscribers": 157761, "created_utc": 1706851043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a career dilemma and would really appreciate some outside perspectives. I'm currently working at a small startup with a peaceful work culture and a great manager. The work is predictable, without much surprises or challenges. However, it's a remote job, which I really enjoy.\n\nRecently, I received an offer for a senior position in a larger, well-established company (not a FAANG company, but still significant). This role would double my current salary, which is obviously a huge plus. However, it comes with its own set of challenges:\n\nHybrid Work Model: I currently work fully remotely\nIncreased Responsibilities: The senior role would demand more from me in terms of responsibilities and decision-making.\nTime Flexibility: I'd have to manage evening calls with offshore teams and early morning meetings, which could be a significant change from my current routine.\nIs the stress and change in work-life balance worth the salary increase and senior title? Would love to hear from anyone who's been in a similar situation or can offer insights into making such a decision.\n\nI have total experience 6-7 years, and masters. I am in my 30s.", "author_fullname": "t2_t526hbv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Torn Between a High-Paying Senior Role and a Comfortable Current Job - Need Advice!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agroto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706837694.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706837410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a career dilemma and would really appreciate some outside perspectives. I&amp;#39;m currently working at a small startup with a peaceful work culture and a great manager. The work is predictable, without much surprises or challenges. However, it&amp;#39;s a remote job, which I really enjoy.&lt;/p&gt;\n\n&lt;p&gt;Recently, I received an offer for a senior position in a larger, well-established company (not a FAANG company, but still significant). This role would double my current salary, which is obviously a huge plus. However, it comes with its own set of challenges:&lt;/p&gt;\n\n&lt;p&gt;Hybrid Work Model: I currently work fully remotely\nIncreased Responsibilities: The senior role would demand more from me in terms of responsibilities and decision-making.\nTime Flexibility: I&amp;#39;d have to manage evening calls with offshore teams and early morning meetings, which could be a significant change from my current routine.\nIs the stress and change in work-life balance worth the salary increase and senior title? Would love to hear from anyone who&amp;#39;s been in a similar situation or can offer insights into making such a decision.&lt;/p&gt;\n\n&lt;p&gt;I have total experience 6-7 years, and masters. I am in my 30s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agroto", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Ticket6016", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agroto/torn_between_a_highpaying_senior_role_and_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agroto/torn_between_a_highpaying_senior_role_and_a/", "subreddit_subscribers": 157761, "created_utc": 1706837410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im trying to achieve 5NF, I believe it\u2019s at 2NF at the moment, I\u2019m getting different answers from google and chatgpt, but it\u2019s saying since source_name is dependent on source_id it\u2019s in 2Nf ,  and it talks about how their can\u2019t be any join dependencies for it to be in 5NF I don\u2019t understand can someone help ?", "author_fullname": "t2_ab7jqmfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I achieve 5NF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": true, "name": "t3_1ah8nki", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RJa57J6G8CqJDz6zmuy9QVWv0aSEexUCRQPi_xvhoZQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706893713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to achieve 5NF, I believe it\u2019s at 2NF at the moment, I\u2019m getting different answers from google and chatgpt, but it\u2019s saying since source_name is dependent on source_id it\u2019s in 2Nf ,  and it talks about how their can\u2019t be any join dependencies for it to be in 5NF I don\u2019t understand can someone help ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/u8g6rvscf7gc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?auto=webp&amp;s=9ad18cacbabbffc4494ca7e3560a4f162d296051", "width": 570, "height": 340}, "resolutions": [{"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b66981351fce766b60cdded8a5cfad51a01109ac", "width": 108, "height": 64}, {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=548f66ef894660333cb665afa2cda681b7d1a88d", "width": 216, "height": 128}, {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=77b839ec9bf4db3ff55aa76d808a4aba914e52e9", "width": 320, "height": 190}], "variants": {}, "id": "yYuvFeH9VQSoCRlQ7gATg7Qe4St1eeCjrY5zdzO71TY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah8nki", "is_robot_indexable": true, "report_reasons": null, "author": "Mother-Finance-8431", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah8nki/how_can_i_achieve_5nf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/u8g6rvscf7gc1.jpeg", "subreddit_subscribers": 157761, "created_utc": 1706893713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\n&amp;#x200B;\n\nI'm venturing into the realm of data management research and aiming to construct a data lakehouse on my PC. However, I'm facing some dilemmas and could use your expertise to navigate through them.\n\n&amp;#x200B;\n\nFirstly, I'm torn between two options: Iceberg and Delta Lake. Both seem promising for effectively managing data lakes, but I'm unsure which one would suit my needs better. If anyone has experience with either or both of these platforms, I'd greatly appreciate insights into their strengths, weaknesses, and practical applications.\n\n&amp;#x200B;\n\nSecondly, I'm grappling with the choice of storage infrastructure. Would it be feasible to utilize a Hadoop Distributed File System (HDFS) for local storage, or would it be better to opt for cloud storage solutions? While local storage might offer more control and privacy, cloud storage could potentially provide scalability and accessibility. Any advice or experiences in this regard would be immensely helpful.\n\n&amp;#x200B;\n\nAdditionally, I'm seeking to understand how to construct a lakehouse on my PC. Any resources, tutorials, or tips on setting up a data lakehouse environment locally would be greatly appreciated.\n\n&amp;#x200B;\n\nUltimately, my goal is to create a robust data environment conducive to research and experimentation. Any tips, recommendations, or cautionary tales from those who have ventured down a similar path would be invaluable.\n\n&amp;#x200B;\n\nThank you in advance for your guidance and support!", "author_fullname": "t2_fxresopqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance for Building a Data Lakehouse for Research Purposes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah3o15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706880521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m venturing into the realm of data management research and aiming to construct a data lakehouse on my PC. However, I&amp;#39;m facing some dilemmas and could use your expertise to navigate through them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Firstly, I&amp;#39;m torn between two options: Iceberg and Delta Lake. Both seem promising for effectively managing data lakes, but I&amp;#39;m unsure which one would suit my needs better. If anyone has experience with either or both of these platforms, I&amp;#39;d greatly appreciate insights into their strengths, weaknesses, and practical applications.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Secondly, I&amp;#39;m grappling with the choice of storage infrastructure. Would it be feasible to utilize a Hadoop Distributed File System (HDFS) for local storage, or would it be better to opt for cloud storage solutions? While local storage might offer more control and privacy, cloud storage could potentially provide scalability and accessibility. Any advice or experiences in this regard would be immensely helpful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;m seeking to understand how to construct a lakehouse on my PC. Any resources, tutorials, or tips on setting up a data lakehouse environment locally would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ultimately, my goal is to create a robust data environment conducive to research and experimentation. Any tips, recommendations, or cautionary tales from those who have ventured down a similar path would be invaluable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your guidance and support!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah3o15", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Goat3978", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah3o15/seeking_guidance_for_building_a_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah3o15/seeking_guidance_for_building_a_data_lakehouse/", "subreddit_subscribers": 157761, "created_utc": 1706880521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI was trying to create a server that allow users to create and execute notebook for data exploration, data processing and training model\n\nFor now I am using JupyterHub on Kubernetes which will create a pod to execute notebook each time a user log in and terminate it after they stop their session. However, I assume when there are 1000 users at the same time then there will be 1000 pods, which is very resource-consuming. Is there any other way to do this job? Am I going in the right direction? Does anyone know which tools big company like kaggle, google colab, databrick, ... use to manage and execute user notebooks in large scale? Please give me some information\n\nThanks for your reading", "author_fullname": "t2_oj5xmc65m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting Notebook server in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agulfg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706846150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I was trying to create a server that allow users to create and execute notebook for data exploration, data processing and training model&lt;/p&gt;\n\n&lt;p&gt;For now I am using JupyterHub on Kubernetes which will create a pod to execute notebook each time a user log in and terminate it after they stop their session. However, I assume when there are 1000 users at the same time then there will be 1000 pods, which is very resource-consuming. Is there any other way to do this job? Am I going in the right direction? Does anyone know which tools big company like kaggle, google colab, databrick, ... use to manage and execute user notebooks in large scale? Please give me some information&lt;/p&gt;\n\n&lt;p&gt;Thanks for your reading&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1agulfg", "is_robot_indexable": true, "report_reasons": null, "author": "resrrdttrt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agulfg/hosting_notebook_server_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agulfg/hosting_notebook_server_in_production/", "subreddit_subscribers": 157761, "created_utc": 1706846150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked as Abinitio dev whole my career of 10 years. However , i did few certifications in azure and databricks . Really frustrated with my manager and the company policies. Started giving interviews and got offer in one of the big health care company in azure + databricks(pyspark) tech. As i dont have any hands-on experience on python or databricks , wanted to know if this is right move to join the company. Will it create any big problem learning on job ?", "author_fullname": "t2_t0zkmfpy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agsn9a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706840171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked as Abinitio dev whole my career of 10 years. However , i did few certifications in azure and databricks . Really frustrated with my manager and the company policies. Started giving interviews and got offer in one of the big health care company in azure + databricks(pyspark) tech. As i dont have any hands-on experience on python or databricks , wanted to know if this is right move to join the company. Will it create any big problem learning on job ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1agsn9a", "is_robot_indexable": true, "report_reasons": null, "author": "Terrible_Mud5318", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agsn9a/need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agsn9a/need_advice/", "subreddit_subscribers": 157761, "created_utc": 1706840171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. \n\nIm trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. \n\nHence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.", "author_fullname": "t2_6nb4026n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there Python libraries that define and parametrize etl jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglv35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706822120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The startup im in runs a bunch of aws glue jobs that take data from s3, transform it, and load it into a database. &lt;/p&gt;\n\n&lt;p&gt;Im trying to create a \u2018commons\u2019 package for us that includes often repeated code in these jobs:  combining data, common transformations, db loading etc.. &lt;/p&gt;\n\n&lt;p&gt;Hence im looking for a library that can help me define parametrized \u2018steps\u2019 which i can put together in different orders for each glue job. Ive looked into Airflow but i think its an overkill since I dont need the whole ecosystem just the DAG definition part. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aglv35", "is_robot_indexable": true, "report_reasons": null, "author": "armAssembledx86", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglv35/are_there_python_libraries_that_define_and/", "subreddit_subscribers": 157761, "created_utc": 1706822120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data \u201ccontract\u201d actual implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agkong", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706819160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you actually implement data types/schemas so the producer and consumer can both utilize it? I thought of defining a python package with python dataclasses/pydantic models which could then be imported into producer/consumer app. I am just unsure about versioning(upgrading schema). I also bumped into Confluent schema registry for avro schemas but no idea how is that implemented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agkong", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agkong/data_contract_actual_implementation/", "subreddit_subscribers": 157761, "created_utc": 1706819160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database \u201cAPI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agifx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706813572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to provide REST API access to data in our database. How would you design such solution? Custom FastAPI/Flask app with custom queries defined? Or something like Postgrest? Or is there any industry standard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agifx8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agifx8/database_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agifx8/database_api/", "subreddit_subscribers": 157761, "created_utc": 1706813572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How to structure a data pipeline repo for pyspark jupyter notebooks?\n\nI am planning to build a data pipeline for a new project from scratch, which would be in pyspark sagemaker notebooks Technologies used as below\nOrchestration: Airlfow\nStorage: S3\nFinal transformed tables will be created in athena.\n\nHow would you structure a git repo that's written in pyspark notebooks and with a dag folder. We are also looking to implement CI/CD in the future. It also should have a proper logging mechanism.\n\nWould like to hear all your suggestions and any github repo examples would be highly appreciated.\n Thanks!", "author_fullname": "t2_6n03d0sf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline creation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1agv56b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706847963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to structure a data pipeline repo for pyspark jupyter notebooks?&lt;/p&gt;\n\n&lt;p&gt;I am planning to build a data pipeline for a new project from scratch, which would be in pyspark sagemaker notebooks Technologies used as below\nOrchestration: Airlfow\nStorage: S3\nFinal transformed tables will be created in athena.&lt;/p&gt;\n\n&lt;p&gt;How would you structure a git repo that&amp;#39;s written in pyspark notebooks and with a dag folder. We are also looking to implement CI/CD in the future. It also should have a proper logging mechanism.&lt;/p&gt;\n\n&lt;p&gt;Would like to hear all your suggestions and any github repo examples would be highly appreciated.\n Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1agv56b", "is_robot_indexable": true, "report_reasons": null, "author": "arunrajan96", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1agv56b/data_pipeline_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1agv56b/data_pipeline_creation/", "subreddit_subscribers": 157761, "created_utc": 1706847963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How was the switch for you, and what steps did you take to make the switch?", "author_fullname": "t2_55l23zh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here switched to DE from Technical Project Management/Product Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aglecn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706820952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How was the switch for you, and what steps did you take to make the switch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1aglecn", "is_robot_indexable": true, "report_reasons": null, "author": "egg_boi56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aglecn/has_anyone_here_switched_to_de_from_technical/", "subreddit_subscribers": 157761, "created_utc": 1706820952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine I have a stock data - when tx happened and at what price. I want to track the all all time high of that data, without spikes. Right now what im doing is   \n\n\n```\n`WITH RankedTxs AS (\n            SELECT\n                stock_id, price, timestamp,\n                ROW_NUMBER() OVER (PARTITION BY stock_id ORDER BY price DESC) AS row_num\n                FROM\n                \"Txs\"\n                ts &gt;= ${} AND ts &lt;= ${} \n            )\n            SELECT\n                *\n            FROM\n                RankedSwaps\n            WHERE\n                row_num = 1;\n        ;`)\n```\n\nThis works, but when there is a spike i.e someone bought a lot and then there are sells it creates an ATH that it's way to high. \n\nAlso the Txs tables is quite large ~20M of rows, so I should avoid going into the history of raw txs.\n\nWhat's the proper way to handle this ? I have timescale plugin that probably can create buckets for ath and then write a query and filter peaks. What would be the proper way ?", "author_fullname": "t2_3ppayh15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effectively tracking all time high in postgres with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ah9le1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706896109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine I have a stock data - when tx happened and at what price. I want to track the all all time high of that data, without spikes. Right now what im doing is   &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n`WITH RankedTxs AS (\n            SELECT\n                stock_id, price, timestamp,\n                ROW_NUMBER() OVER (PARTITION BY stock_id ORDER BY price DESC) AS row_num\n                FROM\n                &amp;quot;Txs&amp;quot;\n                ts &amp;gt;= ${} AND ts &amp;lt;= ${} \n            )\n            SELECT\n                *\n            FROM\n                RankedSwaps\n            WHERE\n                row_num = 1;\n        ;`)\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This works, but when there is a spike i.e someone bought a lot and then there are sells it creates an ATH that it&amp;#39;s way to high. &lt;/p&gt;\n\n&lt;p&gt;Also the Txs tables is quite large ~20M of rows, so I should avoid going into the history of raw txs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the proper way to handle this ? I have timescale plugin that probably can create buckets for ath and then write a query and filter peaks. What would be the proper way ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah9le1", "is_robot_indexable": true, "report_reasons": null, "author": "dotaleaker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah9le1/effectively_tracking_all_time_high_in_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah9le1/effectively_tracking_all_time_high_in_postgres/", "subreddit_subscribers": 157761, "created_utc": 1706896109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nWe're leveraging MySQL for OLTP, streaming logs to S3 via Kafka, and using AWS Glue for transforming data into Delta Lake format stored in 'silver' and 'gold' S3 buckets. Given the high cost of maintaining terabytes of historical transactions in MySQL, we plan to retain only the last 3 years of data and rely on Delta Lake for analytics.\n\n**Challenge:** We need a fail-safe disaster recovery strategy for our Delta Lake in S3, especially if we can't rely on MySQL for full data recovery due to deletion of older transactions. Our main concern is ensuring data recovery if the S3 buckets or their contents are accidentally deleted.\n\n**Disaster Recovery Considerations:**\n\n* **S3 File Versioning:** Found not recommended for Delta Lake as per Databricks' advice due to potential performance issues.\n* **S3 Bucket Replication:** Requires enabling S3 file versioning, which brings us back to the same concern.\n* **AWS Backup service:** [https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/](https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/)\n\nHas anyone here managed disaster recovery for Delta Lake on AWS and handled these issues? Looking for insights or alternative strategies that have worked for you.\n\nThanks in advance!", "author_fullname": "t2_1rqmub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake in AWS disaster recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ah8yn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706899023.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706894499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re leveraging MySQL for OLTP, streaming logs to S3 via Kafka, and using AWS Glue for transforming data into Delta Lake format stored in &amp;#39;silver&amp;#39; and &amp;#39;gold&amp;#39; S3 buckets. Given the high cost of maintaining terabytes of historical transactions in MySQL, we plan to retain only the last 3 years of data and rely on Delta Lake for analytics.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; We need a fail-safe disaster recovery strategy for our Delta Lake in S3, especially if we can&amp;#39;t rely on MySQL for full data recovery due to deletion of older transactions. Our main concern is ensuring data recovery if the S3 buckets or their contents are accidentally deleted.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Disaster Recovery Considerations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;S3 File Versioning:&lt;/strong&gt; Found not recommended for Delta Lake as per Databricks&amp;#39; advice due to potential performance issues.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;S3 Bucket Replication:&lt;/strong&gt; Requires enabling S3 file versioning, which brings us back to the same concern.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AWS Backup service:&lt;/strong&gt; &lt;a href=\"https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/\"&gt;https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone here managed disaster recovery for Delta Lake on AWS and handled these issues? Looking for insights or alternative strategies that have worked for you.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?auto=webp&amp;s=61c9098f4fdfde5094c36b617d58e9bdfef2a473", "width": 374, "height": 186}, "resolutions": [{"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7dc54cb6e7696044b9a9600137afca76c123f9d4", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5013cd8396dff572dd46eea4dd9aba67c089e700", "width": 216, "height": 107}, {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=572bb1376f50894df9c51aeabe5ca7bad4698c19", "width": 320, "height": 159}], "variants": {}, "id": "_cMfF1AUEjTkRGdbXXJuaqM3kgKtguwKSFdUfATSu1w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah8yn3", "is_robot_indexable": true, "report_reasons": null, "author": "EatDirty", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah8yn3/delta_lake_in_aws_disaster_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah8yn3/delta_lake_in_aws_disaster_recovery/", "subreddit_subscribers": 157761, "created_utc": 1706894499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you even go about designing a platform that needs to potentially support \\~5,000 entities of data? Data needs to come in from source systems and be validated, cleaned, and transformed into dimensional models to support data warehouse/BI initiatives. This data may also need to be streamed into backend databases for data applications (which is outside of the warehouse layer). Some data assets can be updated in batches, but other need have low-latency updates. (I am defining a data asset as some table in a warehouse layer or a database backend for an application). I couldn't just batch everything once day, the applications need low-latency updates, manufacturing reporting needs to up-to-date within minutes. Things likes sales and financial reporting can be daily batched.\n\nFor perspective, we have 12 SAP ERP instances for different business areas/regions (it a mess). We have identified \\~75-100 standard tables that exist in all instances that we consider critical. This alone is 900-1200 tables. We have nearly 100 factories, each with a local installation of a manufacturing database system for tracking all production data. I don't have a table count, but even at 30 tables per factory that bring the table could to nearly 4,000. This doesn't include all of the IoT Devices installed on nearly every machine in a plant, video streams, API's from SaaS products we use, or external sources. It is not unreasonable to say there could be 5000-6000 data entities in total.\n\nIf the purpose of an analytical platform, like a lakehouse, is to integrate data from all the sources into a common data model that represents the organization for analytics, I don't see any way to manage such a volume of entities. How could you possibly begin to clean, transform, govern, and orchestrate all of this data across storage layers in a data platform. There is no possible way any team could write cleaning and transformation rules for every single one of the thousands of tables.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Thousands of Tables in a Data Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah7ujv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706891705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you even go about designing a platform that needs to potentially support ~5,000 entities of data? Data needs to come in from source systems and be validated, cleaned, and transformed into dimensional models to support data warehouse/BI initiatives. This data may also need to be streamed into backend databases for data applications (which is outside of the warehouse layer). Some data assets can be updated in batches, but other need have low-latency updates. (I am defining a data asset as some table in a warehouse layer or a database backend for an application). I couldn&amp;#39;t just batch everything once day, the applications need low-latency updates, manufacturing reporting needs to up-to-date within minutes. Things likes sales and financial reporting can be daily batched.&lt;/p&gt;\n\n&lt;p&gt;For perspective, we have 12 SAP ERP instances for different business areas/regions (it a mess). We have identified ~75-100 standard tables that exist in all instances that we consider critical. This alone is 900-1200 tables. We have nearly 100 factories, each with a local installation of a manufacturing database system for tracking all production data. I don&amp;#39;t have a table count, but even at 30 tables per factory that bring the table could to nearly 4,000. This doesn&amp;#39;t include all of the IoT Devices installed on nearly every machine in a plant, video streams, API&amp;#39;s from SaaS products we use, or external sources. It is not unreasonable to say there could be 5000-6000 data entities in total.&lt;/p&gt;\n\n&lt;p&gt;If the purpose of an analytical platform, like a lakehouse, is to integrate data from all the sources into a common data model that represents the organization for analytics, I don&amp;#39;t see any way to manage such a volume of entities. How could you possibly begin to clean, transform, govern, and orchestrate all of this data across storage layers in a data platform. There is no possible way any team could write cleaning and transformation rules for every single one of the thousands of tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ah7ujv", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah7ujv/managing_thousands_of_tables_in_a_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah7ujv/managing_thousands_of_tables_in_a_data_platform/", "subreddit_subscribers": 157761, "created_utc": 1706891705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers,\nWhich tool do you use to handle schema creation/modifications.\n\nDjango is working well for now, but we are considering liquibase.\nLet me know if you have faced issues with the above two techniques + any additional help is appreciated.", "author_fullname": "t2_j9pcxbcbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about schema migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah5j9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706885629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers,\nWhich tool do you use to handle schema creation/modifications.&lt;/p&gt;\n\n&lt;p&gt;Django is working well for now, but we are considering liquibase.\nLet me know if you have faced issues with the above two techniques + any additional help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah5j9x", "is_robot_indexable": true, "report_reasons": null, "author": "Human-Failure-99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah5j9x/question_about_schema_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah5j9x/question_about_schema_migration/", "subreddit_subscribers": 157761, "created_utc": 1706885629.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}