{"kind": "Listing", "data": {"after": "t3_1ahck2r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So many posts are about the latest and greatest cloud tech. If you don't work for an org that uses AWS or databricks or what not, you'll be told to look for a new job (in this market? yeah right). \n\nI'm an analyst working in a purely Microsoft team, everything is Access+Excel. My role invovles bandaging legacy systems together with Python. Anyone else relate? Where are my folks who work with these techs? I'm guessing most of us are healthcare, government, or some old school corporation. Do you ever wish you worked in a modern company with a modern data stack? Or are you happy where you are?", "author_fullname": "t2_lfyfd0fn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers working with suboptimal tech?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah9z7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706897053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many posts are about the latest and greatest cloud tech. If you don&amp;#39;t work for an org that uses AWS or databricks or what not, you&amp;#39;ll be told to look for a new job (in this market? yeah right). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an analyst working in a purely Microsoft team, everything is Access+Excel. My role invovles bandaging legacy systems together with Python. Anyone else relate? Where are my folks who work with these techs? I&amp;#39;m guessing most of us are healthcare, government, or some old school corporation. Do you ever wish you worked in a modern company with a modern data stack? Or are you happy where you are?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ah9z7z", "is_robot_indexable": true, "report_reasons": null, "author": "velimino", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah9z7z/any_data_engineers_working_with_suboptimal_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah9z7z/any_data_engineers_working_with_suboptimal_tech/", "subreddit_subscribers": 157960, "created_utc": 1706897053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im trying to achieve 5NF, I believe it\u2019s at 2NF at the moment, I\u2019m getting different answers from google and chatgpt, but it\u2019s saying since source_name is dependent on source_id it\u2019s in 2Nf ,  and it talks about how their can\u2019t be any join dependencies for it to be in 5NF I don\u2019t understand can someone help ?", "author_fullname": "t2_ab7jqmfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I achieve 5NF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah8nki", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RJa57J6G8CqJDz6zmuy9QVWv0aSEexUCRQPi_xvhoZQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706893713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to achieve 5NF, I believe it\u2019s at 2NF at the moment, I\u2019m getting different answers from google and chatgpt, but it\u2019s saying since source_name is dependent on source_id it\u2019s in 2Nf ,  and it talks about how their can\u2019t be any join dependencies for it to be in 5NF I don\u2019t understand can someone help ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/u8g6rvscf7gc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?auto=webp&amp;s=9ad18cacbabbffc4494ca7e3560a4f162d296051", "width": 570, "height": 340}, "resolutions": [{"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b66981351fce766b60cdded8a5cfad51a01109ac", "width": 108, "height": 64}, {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=548f66ef894660333cb665afa2cda681b7d1a88d", "width": 216, "height": 128}, {"url": "https://preview.redd.it/u8g6rvscf7gc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=77b839ec9bf4db3ff55aa76d808a4aba914e52e9", "width": 320, "height": 190}], "variants": {}, "id": "yYuvFeH9VQSoCRlQ7gATg7Qe4St1eeCjrY5zdzO71TY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah8nki", "is_robot_indexable": true, "report_reasons": null, "author": "Mother-Finance-8431", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah8nki/how_can_i_achieve_5nf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/u8g6rvscf7gc1.jpeg", "subreddit_subscribers": 157960, "created_utc": 1706893713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a Data Analyst but want to change to Data Engineering. However, several of the jobs I want to apply to require proficiency in SQL. I am proficient in Python, since it's what we use in my company, but I have no experience with SQL so I want to add some projects to my GitHub to show that I can write SQL queries and make data analysis with it. Do you have any recommendation?", "author_fullname": "t2_ulshlx1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you prove SQL proficiency with projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahjv8r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706922851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a Data Analyst but want to change to Data Engineering. However, several of the jobs I want to apply to require proficiency in SQL. I am proficient in Python, since it&amp;#39;s what we use in my company, but I have no experience with SQL so I want to add some projects to my GitHub to show that I can write SQL queries and make data analysis with it. Do you have any recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ahjv8r", "is_robot_indexable": true, "report_reasons": null, "author": "__academic__", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahjv8r/how_do_you_prove_sql_proficiency_with_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahjv8r/how_do_you_prove_sql_proficiency_with_projects/", "subreddit_subscribers": 157960, "created_utc": 1706922851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7waxarsz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The less they know, the more they pay. Keep them hooked on stupid.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1aht9m3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p7h1O2SS3jdWO9LWY4LyR-onm2ueVraiQypiA9gfW_o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706956384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/y5ctytm4lcgc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/y5ctytm4lcgc1.png?auto=webp&amp;s=9fa5c35f3c61bd8d721cfdd3e0065eb28de36985", "width": 954, "height": 1256}, "resolutions": [{"url": "https://preview.redd.it/y5ctytm4lcgc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4834b62bde36a2f238dd99fd56a8c3130e455ca", "width": 108, "height": 142}, {"url": "https://preview.redd.it/y5ctytm4lcgc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc8042cb7e338d23e099a9981153665508b4cb45", "width": 216, "height": 284}, {"url": "https://preview.redd.it/y5ctytm4lcgc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=493f4133ccba57904b6997f990d359c5be4fcf58", "width": 320, "height": 421}, {"url": "https://preview.redd.it/y5ctytm4lcgc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ebe2bd0a2d3549bee28583c4e416fff9a90f17c", "width": 640, "height": 842}], "variants": {}, "id": "IvF2dOYaO_0FtRfoTDqouyGTSixVP4hTvng9NbwJByg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1aht9m3", "is_robot_indexable": true, "report_reasons": null, "author": "almost-mushroom", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aht9m3/the_less_they_know_the_more_they_pay_keep_them/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/y5ctytm4lcgc1.png", "subreddit_subscribers": 157960, "created_utc": 1706956384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is really one of the things that make me scratch my head.\n\nI have nothing against Python, but I feel that for most data engineering problems, you could heavily benefit from type safety and reliable multi threading/parallel processing. Not to mention from scalability in terms of codebase (in my opinion, python projects don't tend to scale well, I don't mean at a performance level, but at structure level)\n\nBut I'm surprised because a lot of people here who say they have a DE role also say they work primarily in Python.\n\nSure, you got PySpark, but even then, that's just an API interacting with a JVM running an actual Scala Spark process.\n\nThe only reason I can think of is that Python has very relaxed syntax, with very few reserved keywords or extremely complex constructs, but of course that simplicity comes at the cost of performance and safety measures.", "author_fullname": "t2_s75gwyxxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm just gonna go ahead and ask: why is Python so popular for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahud3x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706960755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is really one of the things that make me scratch my head.&lt;/p&gt;\n\n&lt;p&gt;I have nothing against Python, but I feel that for most data engineering problems, you could heavily benefit from type safety and reliable multi threading/parallel processing. Not to mention from scalability in terms of codebase (in my opinion, python projects don&amp;#39;t tend to scale well, I don&amp;#39;t mean at a performance level, but at structure level)&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m surprised because a lot of people here who say they have a DE role also say they work primarily in Python.&lt;/p&gt;\n\n&lt;p&gt;Sure, you got PySpark, but even then, that&amp;#39;s just an API interacting with a JVM running an actual Scala Spark process.&lt;/p&gt;\n\n&lt;p&gt;The only reason I can think of is that Python has very relaxed syntax, with very few reserved keywords or extremely complex constructs, but of course that simplicity comes at the cost of performance and safety measures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahud3x", "is_robot_indexable": true, "report_reasons": null, "author": "yourAvgSE", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahud3x/im_just_gonna_go_ahead_and_ask_why_is_python_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahud3x/im_just_gonna_go_ahead_and_ask_why_is_python_so/", "subreddit_subscribers": 157960, "created_utc": 1706960755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm running it in a t3.medium ec2 instance to get data from a few sources into a google big query data warehouse.  \n\n\nI'm running 4 or 5 ELT jobs once a day in the morning, on very small sets of data.  \n\n\nI started to wonder why I am running an ec2 instance all the time for this? It'd be much better to stand something up, bring in the data, and tear it down again. I originally chose to use airbyte because it was easy to set up, had all the connections I needed out of the box and I wanted to have a good centralized standard across all my ELT jobs.  \n\n\nI've been using Dagster a lot recently and I realize that I think I went the wrong direction with Airbyte. Airbyte is great as a platform but I'd preferably only want it do have the connections and ELT jobs. I'd rather have Dagster handle the orchestration - bring up the ELT job - run it and tear it down. I wish I could keep Airbyte as a library and keep its standard with configs somehow - while either running the jobs in the dagster instance itself or via a instance that dagster brings up for a job.  \n\n\nThat also brings me to the fact that in Airbyte:\n\n* It seems hard to handle IAC\n* Hard to setup ELT as code or config managed in github. Seems way more UI based. I like how in Dagster the UI is limited to only turning things off and on.\n\nI guess my preference would be to have:\n\n* Airbyte connections in config\n* A dagster op that uses that connection and the Airbyte library to run the ELT job. (Not airbyte as a separate service)\n* All orchestration and knowledge on cursors/previous runs handled by dagster \n\nI understand how Airbyte offers a UI more specific to ELT jobs - and is probably good to run as a separate service when you have tons of data and need to keep running ELT often, but I am only running them once a day and I think Dagster offers most of the features I need from a UI already.  \n\n\nJust curious about other peoples thoughts on this.", "author_fullname": "t2_62zo8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've been using Airbyte for a while - wondering if I really need it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahlljr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706928131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running it in a t3.medium ec2 instance to get data from a few sources into a google big query data warehouse.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running 4 or 5 ELT jobs once a day in the morning, on very small sets of data.  &lt;/p&gt;\n\n&lt;p&gt;I started to wonder why I am running an ec2 instance all the time for this? It&amp;#39;d be much better to stand something up, bring in the data, and tear it down again. I originally chose to use airbyte because it was easy to set up, had all the connections I needed out of the box and I wanted to have a good centralized standard across all my ELT jobs.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Dagster a lot recently and I realize that I think I went the wrong direction with Airbyte. Airbyte is great as a platform but I&amp;#39;d preferably only want it do have the connections and ELT jobs. I&amp;#39;d rather have Dagster handle the orchestration - bring up the ELT job - run it and tear it down. I wish I could keep Airbyte as a library and keep its standard with configs somehow - while either running the jobs in the dagster instance itself or via a instance that dagster brings up for a job.  &lt;/p&gt;\n\n&lt;p&gt;That also brings me to the fact that in Airbyte:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It seems hard to handle IAC&lt;/li&gt;\n&lt;li&gt;Hard to setup ELT as code or config managed in github. Seems way more UI based. I like how in Dagster the UI is limited to only turning things off and on.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I guess my preference would be to have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Airbyte connections in config&lt;/li&gt;\n&lt;li&gt;A dagster op that uses that connection and the Airbyte library to run the ELT job. (Not airbyte as a separate service)&lt;/li&gt;\n&lt;li&gt;All orchestration and knowledge on cursors/previous runs handled by dagster &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I understand how Airbyte offers a UI more specific to ELT jobs - and is probably good to run as a separate service when you have tons of data and need to keep running ELT often, but I am only running them once a day and I think Dagster offers most of the features I need from a UI already.  &lt;/p&gt;\n\n&lt;p&gt;Just curious about other peoples thoughts on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahlljr", "is_robot_indexable": true, "report_reasons": null, "author": "rhiyo", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahlljr/ive_been_using_airbyte_for_a_while_wondering_if_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahlljr/ive_been_using_airbyte_for_a_while_wondering_if_i/", "subreddit_subscribers": 157960, "created_utc": 1706928131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019ve been getting conflicting answers on this. \n\nAccording to Linkedin, for Kimball\u2019s dimensional model \u201cThe dimensional model, also known as the star schema or the snowflake schema\u201d. [Source](https://www.linkedin.com/advice/0/what-pros-cons-using-dimensional-model-kimball-vs?utm_source=share&amp;utm_medium=member_ios&amp;utm_campaign=share_via)\n\n\nUntil now, my understanding was:\n\nKimball = dimensional = star = 2NF\n\nInmon = 3NF = Snowflake\n\nIs my above understanding correct? Or Kimball could also have snowflake schema?\n\nEdit: Thanks for the awesome answers. I understand there are other aspects to the data models other than schema, like Inmon follows top down approach (first planning the enterprise data warehouse and then departmental data marta) opposed to Kimball which follows the ground up approach. The schemas for both got me confusing.", "author_fullname": "t2_563z04ou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inmon vs Kimball - Snowflake vs star?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahebky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706915303.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706908119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve been getting conflicting answers on this. &lt;/p&gt;\n\n&lt;p&gt;According to Linkedin, for Kimball\u2019s dimensional model \u201cThe dimensional model, also known as the star schema or the snowflake schema\u201d. &lt;a href=\"https://www.linkedin.com/advice/0/what-pros-cons-using-dimensional-model-kimball-vs?utm_source=share&amp;amp;utm_medium=member_ios&amp;amp;utm_campaign=share_via\"&gt;Source&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Until now, my understanding was:&lt;/p&gt;\n\n&lt;p&gt;Kimball = dimensional = star = 2NF&lt;/p&gt;\n\n&lt;p&gt;Inmon = 3NF = Snowflake&lt;/p&gt;\n\n&lt;p&gt;Is my above understanding correct? Or Kimball could also have snowflake schema?&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks for the awesome answers. I understand there are other aspects to the data models other than schema, like Inmon follows top down approach (first planning the enterprise data warehouse and then departmental data marta) opposed to Kimball which follows the ground up approach. The schemas for both got me confusing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?auto=webp&amp;s=434e4d8fad80f2d6557225fbf0856f19dba6f526", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5268542435ad9c3cbafee9ed90d34ca6dad9481e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=060dd202ec23064267df1849bfc67332faf2b561", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8683759234772ddde1cfc55058e5c4c7948f0d1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aec116fa7fff13afa999b99d8bebcb97a0bec957", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=07fa0d8a0d54feae275c4978be3a2a74167692a3", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/R4ivYY_C99HXD4prDb9vQCq5i8raDWP7dHwn-Bxw1lg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf90f89919aff424f2200e7348b8a5b017261986", "width": 1080, "height": 607}], "variants": {}, "id": "5D_1SU7Uk7TLzMw2nzqbmJy6unw8aO30yNg8chkwY5Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahebky", "is_robot_indexable": true, "report_reasons": null, "author": "life_is_enjoy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahebky/inmon_vs_kimball_snowflake_vs_star/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahebky/inmon_vs_kimball_snowflake_vs_star/", "subreddit_subscribers": 157960, "created_utc": 1706908119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI have more then 25 years experience in IT but right now I want to put myself into Data Engineering.\n\nWhich course you can suggest for me on Coursera platform. I find this two but maybe from your experience I can get another suggestion. Final target to find position Data Engineer in IBM\n\n1: [https://www.coursera.org/professional-certificates/ibm-data-engineer#courses](https://www.coursera.org/professional-certificates/ibm-data-engineer#courses)\n\n2: [https://www.coursera.org/professional-certificates/data-warehouse-engineering#courses](https://www.coursera.org/professional-certificates/data-warehouse-engineering#courses)", "author_fullname": "t2_fzvchhkq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Best set online courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahar22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706899038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I have more then 25 years experience in IT but right now I want to put myself into Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;Which course you can suggest for me on Coursera platform. I find this two but maybe from your experience I can get another suggestion. Final target to find position Data Engineer in IBM&lt;/p&gt;\n\n&lt;p&gt;1: &lt;a href=\"https://www.coursera.org/professional-certificates/ibm-data-engineer#courses\"&gt;https://www.coursera.org/professional-certificates/ibm-data-engineer#courses&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;2: &lt;a href=\"https://www.coursera.org/professional-certificates/data-warehouse-engineering#courses\"&gt;https://www.coursera.org/professional-certificates/data-warehouse-engineering#courses&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?auto=webp&amp;s=54a372d63f783b3975eb29731b94b5836e5ea649", "width": 1772, "height": 928}, "resolutions": [{"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df5ab1e671e652979109d922cbc56fd2fa6221e7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=820f0f00f7fd59c7ab4e201cd437b20bbb9c20df", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8bb3b9b3a036a3cfe963b984faadb5add1bcf83d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a5595108c898729b35709da56f4bdcc587466b74", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d515a8b33cb3faff34946641984e393e4ba6206", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/BiVmwR97KKnaZQdr0A3kRf5rEoBCnu1ATN_d6FHEUYQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a187bf4d0181beaf1a6ce988c7ebbeb5ac666766", "width": 1080, "height": 565}], "variants": {}, "id": "Sy8YR-35ze1EvH-MJapkQSX2HlBbsaru9SRfTDExjAk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ahar22", "is_robot_indexable": true, "report_reasons": null, "author": "_DukeDu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahar22/data_engineering_best_set_online_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahar22/data_engineering_best_set_online_courses/", "subreddit_subscribers": 157960, "created_utc": 1706899038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "W's\n\nIts cool that you can create folders and share it with others\n\nThere's a code version control (but this is also a L that's later explained)\n\nThere's a python worksheet, which is cool, not needed because you can always wrap it in a stored proc (and that's what it does under the hood anyway), but the package manager is cool nonetheless\n\nThe charts is a nice feature, wouldn't say its at the point where you can use it to report, but very nice to get a quick summary\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nL's (Some are not snowsight specific)\n\nThere's just 1 folder level, its not a major problem but I would like to create a 'recycle bin folder' and move everything in it without having to sort through it every now and then\n\nWith the Snowsight UI, a lot of times the log just disappears especially when you use a python worksheet\n\nRunning a bunch of queries often makes the code stuck in UI initializing\n\nIf you run a lot of SQL commands and the page refreshes, it just executes the last command and stops - this is incredibly frustrating\n\nVersion control is nightmare, especially if you have 2+ people working on it, and the saves aren't as intuitive as they are in databricks for some reason\n\nQuery history is far less that the classic UI, so many a times you actually have to go to the classic UI to see your query history\n\nThere's no way to see in which worksheet I have run my last query - this would be an amazing feature since if you run 100+ queries a day for adhoc business requests you often lose track of what's where\n\nUnlike databricks every python worksheet is like 1 cell in databricks, but what if i want to run 1 cell, then another cell, then another etc. That feels more intuitive when you are running data science experiments.\n\nsaving anything like a model object, pca object etc, means you have to keep it in a stage but that means you have to store it in a internal '/tmp/' folder first then upload to a stage, it just seems hacky imho. Not sure if other use the same proccess\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_reopsquch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Snowsight (Snowflake's new UI) a big L or big W?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahnp1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706934835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;W&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;Its cool that you can create folders and share it with others&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a code version control (but this is also a L that&amp;#39;s later explained)&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a python worksheet, which is cool, not needed because you can always wrap it in a stored proc (and that&amp;#39;s what it does under the hood anyway), but the package manager is cool nonetheless&lt;/p&gt;\n\n&lt;p&gt;The charts is a nice feature, wouldn&amp;#39;t say its at the point where you can use it to report, but very nice to get a quick summary&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;L&amp;#39;s (Some are not snowsight specific)&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s just 1 folder level, its not a major problem but I would like to create a &amp;#39;recycle bin folder&amp;#39; and move everything in it without having to sort through it every now and then&lt;/p&gt;\n\n&lt;p&gt;With the Snowsight UI, a lot of times the log just disappears especially when you use a python worksheet&lt;/p&gt;\n\n&lt;p&gt;Running a bunch of queries often makes the code stuck in UI initializing&lt;/p&gt;\n\n&lt;p&gt;If you run a lot of SQL commands and the page refreshes, it just executes the last command and stops - this is incredibly frustrating&lt;/p&gt;\n\n&lt;p&gt;Version control is nightmare, especially if you have 2+ people working on it, and the saves aren&amp;#39;t as intuitive as they are in databricks for some reason&lt;/p&gt;\n\n&lt;p&gt;Query history is far less that the classic UI, so many a times you actually have to go to the classic UI to see your query history&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s no way to see in which worksheet I have run my last query - this would be an amazing feature since if you run 100+ queries a day for adhoc business requests you often lose track of what&amp;#39;s where&lt;/p&gt;\n\n&lt;p&gt;Unlike databricks every python worksheet is like 1 cell in databricks, but what if i want to run 1 cell, then another cell, then another etc. That feels more intuitive when you are running data science experiments.&lt;/p&gt;\n\n&lt;p&gt;saving anything like a model object, pca object etc, means you have to keep it in a stage but that means you have to store it in a internal &amp;#39;/tmp/&amp;#39; folder first then upload to a stage, it just seems hacky imho. Not sure if other use the same proccess&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahnp1q", "is_robot_indexable": true, "report_reasons": null, "author": "Moist-Comedian5033", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahnp1q/is_snowsight_snowflakes_new_ui_a_big_l_or_big_w/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahnp1q/is_snowsight_snowflakes_new_ui_a_big_l_or_big_w/", "subreddit_subscribers": 157960, "created_utc": 1706934835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a young adult entering a computer science college in october, I'm scared that it's gonna hard to get a job despite efforts and having a internship. I'm mostly concentrated on data engineering, other than data science/analysis, so I don't know surely if its a saturated subdomain yet like web development. I heard people applying to 300 applications and getting only one job accepted. Should I still concentrate on this, hope for the better or just learn trades?  \n\n\nIf this post isn't unrelated to the subreddit, feel free to ignore it or diss me, I'll take it down", "author_fullname": "t2_m3cgbvw3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "As a new person into DE, should I still chase this despite the market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahggrt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706913613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a young adult entering a computer science college in october, I&amp;#39;m scared that it&amp;#39;s gonna hard to get a job despite efforts and having a internship. I&amp;#39;m mostly concentrated on data engineering, other than data science/analysis, so I don&amp;#39;t know surely if its a saturated subdomain yet like web development. I heard people applying to 300 applications and getting only one job accepted. Should I still concentrate on this, hope for the better or just learn trades?  &lt;/p&gt;\n\n&lt;p&gt;If this post isn&amp;#39;t unrelated to the subreddit, feel free to ignore it or diss me, I&amp;#39;ll take it down&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahggrt", "is_robot_indexable": true, "report_reasons": null, "author": "xX_GamErPRO2000_Xx", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahggrt/as_a_new_person_into_de_should_i_still_chase_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahggrt/as_a_new_person_into_de_should_i_still_chase_this/", "subreddit_subscribers": 157960, "created_utc": 1706913613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have no idea whether this is going to be useful for anyone, but I've created a bare bones repo that provides an example of using Terraform to manage a Snowflake instance.\n\nIt creates users, warehouses, databases (and schemas), and assigns team roles to users, permission roles to schemas, and grants team roles access to the permission roles.\n\nThere are lots of ways to structure a Terraform repo. This is just one. Let me know if it's helpful.\n\n[https://github.com/nydasco/snowflake-terraform-demo](https://github.com/nydasco/snowflake-terraform-demo)", "author_fullname": "t2_ojr03vx2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bare bones IaC Terraform for Snowflake example", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahnkyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706934471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have no idea whether this is going to be useful for anyone, but I&amp;#39;ve created a bare bones repo that provides an example of using Terraform to manage a Snowflake instance.&lt;/p&gt;\n\n&lt;p&gt;It creates users, warehouses, databases (and schemas), and assigns team roles to users, permission roles to schemas, and grants team roles access to the permission roles.&lt;/p&gt;\n\n&lt;p&gt;There are lots of ways to structure a Terraform repo. This is just one. Let me know if it&amp;#39;s helpful.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/nydasco/snowflake-terraform-demo\"&gt;https://github.com/nydasco/snowflake-terraform-demo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?auto=webp&amp;s=cde19c8cdd4a0fe76397cdbccb74834370732aa7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cea28de7fd9f001374ce0aee2af700273e987c84", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c6840ac69d687c413fc2358151e545849dcdb33", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=56e26a08f6a75e81c9edf0411b7b66468aacefc7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac229206935f64c5e4321a861a99e9301e59e76a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ddf11bf9feb79b8b959437328aa8c7444c08b32", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/p6a8_hc6XJhRGVq-uJjqXWOgXsOMHjjVNdNW3LhoH58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41b907645337d8852632c3ab35294c8cb4093470", "width": 1080, "height": 540}], "variants": {}, "id": "32cljpCcBBDNWgyzwnL_0TMKpyADq2EXLfaS-xORIAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1ahnkyi", "is_robot_indexable": true, "report_reasons": null, "author": "nydasco", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahnkyi/bare_bones_iac_terraform_for_snowflake_example/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahnkyi/bare_bones_iac_terraform_for_snowflake_example/", "subreddit_subscribers": 157960, "created_utc": 1706934471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A question for those of you who have experience with large enterprise data warehouses - what data modeling techniques/best practices do you use and why in your DWHs?\n\nFrom my experience it is common to have the last - consumer-facing layer - be in star/snowflake schema optionally with some OBTs (as that is what most BI tools support well). Other layers in our case mostly were 1:1 to source + intermediary transformations. No specific data modeling approach was used for those. In your case:\n\n1. **Consumer-facing layer (gold)**: Is dimensional modeling (star/snowflake schema) what you use for your consumer-facing layer? If not, why and what do you use instead?\n2. **Raw &amp; intermediary layer (brozne, silver)**: How are your raw and intermediate data layers organized? If you use a specific data modeling technique - like a Data Vault - why did you pick it and how well is it working for you?\n3. **History**: Do you store full history of all data (even tables you don\u2019t have any historical reporting requirements for yet) somewhere? Where?\n4. **If you could start from scratch**: would you structure your dwh differently and why?\n\nThanks!", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling for DWH - what techniques do you use and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahkpuc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706925465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A question for those of you who have experience with large enterprise data warehouses - what data modeling techniques/best practices do you use and why in your DWHs?&lt;/p&gt;\n\n&lt;p&gt;From my experience it is common to have the last - consumer-facing layer - be in star/snowflake schema optionally with some OBTs (as that is what most BI tools support well). Other layers in our case mostly were 1:1 to source + intermediary transformations. No specific data modeling approach was used for those. In your case:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Consumer-facing layer (gold)&lt;/strong&gt;: Is dimensional modeling (star/snowflake schema) what you use for your consumer-facing layer? If not, why and what do you use instead?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raw &amp;amp; intermediary layer (brozne, silver)&lt;/strong&gt;: How are your raw and intermediate data layers organized? If you use a specific data modeling technique - like a Data Vault - why did you pick it and how well is it working for you?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;History&lt;/strong&gt;: Do you store full history of all data (even tables you don\u2019t have any historical reporting requirements for yet) somewhere? Where?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;If you could start from scratch&lt;/strong&gt;: would you structure your dwh differently and why?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahkpuc", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahkpuc/data_modeling_for_dwh_what_techniques_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahkpuc/data_modeling_for_dwh_what_techniques_do_you_use/", "subreddit_subscribers": 157960, "created_utc": 1706925465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning to process streaming data on azure. I prepared a simple workflow on Miro. I have also built and tested most of this on Azure. I would like to know if this is how pipeline are actually made. Each of these blocks in themselves have a lot of configuration and setup, and so it's not easy in any way. Took me around a week to learn about the streaming services on azure and build out a barebones prototype on azure (Still WIP). Any Feedback is much appreciated.\n\nContext - Building a prototype of an IOT data processing pipeline for a part of a gas plant.\n\nLink to the Miro Board : [https://miro.com/app/board/uXjVNxtPWog=/?share\\_link\\_id=230769379583](https://miro.com/app/board/uXjVNxtPWog=/?share_link_id=230769379583)\n\nhttps://preview.redd.it/qtrde5x3t8gc1.png?width=1966&amp;format=png&amp;auto=webp&amp;s=7ac1f94e90694bdfca941767a531115bcba40ab0\n\n&amp;#x200B;", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can this streaming Workflow be improved(On Azure)? In terms of Efficiency, best practices and cost.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qtrde5x3t8gc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44401ffa9011be099f44fd22cc575dff10b20bc7"}, {"y": 124, "x": 216, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4795fe0e93592637c862df07aa9fe0787724d9d3"}, {"y": 184, "x": 320, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e556324a61bbe64d2967ab17d131c4a69feb7e96"}, {"y": 368, "x": 640, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4393ab048eda6d2b508b13bdbf1f8f7486e16204"}, {"y": 552, "x": 960, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b6b66c8f71d8874150cef4b8f797ec8b9ad794b2"}, {"y": 621, "x": 1080, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4545e0dd215587587ab1897a5958682cf9221b87"}], "s": {"y": 1132, "x": 1966, "u": "https://preview.redd.it/qtrde5x3t8gc1.png?width=1966&amp;format=png&amp;auto=webp&amp;s=7ac1f94e90694bdfca941767a531115bcba40ab0"}, "id": "qtrde5x3t8gc1"}}, "name": "t3_1ahf8if", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9JMz8USo2VCT9gWFcaYk1hyYgPm_pCSwjnSQyisUXvc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706910465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning to process streaming data on azure. I prepared a simple workflow on Miro. I have also built and tested most of this on Azure. I would like to know if this is how pipeline are actually made. Each of these blocks in themselves have a lot of configuration and setup, and so it&amp;#39;s not easy in any way. Took me around a week to learn about the streaming services on azure and build out a barebones prototype on azure (Still WIP). Any Feedback is much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Context - Building a prototype of an IOT data processing pipeline for a part of a gas plant.&lt;/p&gt;\n\n&lt;p&gt;Link to the Miro Board : &lt;a href=\"https://miro.com/app/board/uXjVNxtPWog=/?share_link_id=230769379583\"&gt;https://miro.com/app/board/uXjVNxtPWog=/?share_link_id=230769379583&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qtrde5x3t8gc1.png?width=1966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ac1f94e90694bdfca941767a531115bcba40ab0\"&gt;https://preview.redd.it/qtrde5x3t8gc1.png?width=1966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ac1f94e90694bdfca941767a531115bcba40ab0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahf8if", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahf8if/how_can_this_streaming_workflow_be_improvedon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahf8if/how_can_this_streaming_workflow_be_improvedon/", "subreddit_subscribers": 157960, "created_utc": 1706910465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company Im in the accounting side of the application. Often Im required to run some reports which take an awful loads of time like 3hrs. I have a table which has a column of type JSON(not JSONB). In one of the report Im supposed to select some fields from the json like the following\n\n&amp;#x200B;\n\nSELECT \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_truck\\_tractor\\_value' as \"ORIGINAL TRUCK TRACTOR VALUE\",    \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_trailer\\_value' as \"ORIGINAL TRAILER VALUE\",        \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_truck\\_tractor\\_value' as \"CURRENT TRUCK TRACTOR VALUE\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_trailer\\_value' as \"CURRENT TRAILER VALUE\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_unit\\_count' as \"ORIGINAL UNIT COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'og\\_driver\\_count' as \"ORIGINAL DRIVER COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_unit\\_count' as \"CURRENT UNIT COUNT\",\n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_trailer\\_count' as \"CURRENT TRAILER COUNT\",             \n\n\t(ijd.quote\\_submit\\_data-&gt;'uwReviewInfo')-&gt;&gt;'cr\\_driver\\_count' as \"CURRENT DRIVER COUNT\"\n\nFROM blah blah blah.....  \n\n\nLike the above query but in production we have like 60-70 fields to be selected from json which is making the query tooo slow. All other parts of the query are running in less than half a minute. Can anyone suggest a way to speed up this query . All the joins im making are inevitable and cannot be optimized as the postgres optimizer is doing it job well and fine. i've seperated the query into two parts . one with all the json fields accessing query and others with normal column accessing. The normal query is running in &lt;30s. While this json accessing query is taking a loads of time.  \n\n\nPlease suggest a way to speed up this query.  \nthank you\n\n  \n", "author_fullname": "t2_344hea7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accessing large json columns in postgresql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah6reu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706896172.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706888924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company Im in the accounting side of the application. Often Im required to run some reports which take an awful loads of time like 3hrs. I have a table which has a column of type JSON(not JSONB). In one of the report Im supposed to select some fields from the json like the following&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SELECT &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_truck\\_tractor\\_value&amp;#39; as &amp;quot;ORIGINAL TRUCK TRACTOR VALUE&amp;quot;,    \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_trailer\\_value&amp;#39; as &amp;quot;ORIGINAL TRAILER VALUE&amp;quot;,        \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_truck\\_tractor\\_value&amp;#39; as &amp;quot;CURRENT TRUCK TRACTOR VALUE&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_trailer\\_value&amp;#39; as &amp;quot;CURRENT TRAILER VALUE&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_unit\\_count&amp;#39; as &amp;quot;ORIGINAL UNIT COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;og\\_driver\\_count&amp;#39; as &amp;quot;ORIGINAL DRIVER COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_unit\\_count&amp;#39; as &amp;quot;CURRENT UNIT COUNT&amp;quot;,\n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_trailer\\_count&amp;#39; as &amp;quot;CURRENT TRAILER COUNT&amp;quot;,             \n\n(ijd.quote\\_submit\\_data-&amp;gt;&amp;#39;uwReviewInfo&amp;#39;)-&amp;gt;&amp;gt;&amp;#39;cr\\_driver\\_count&amp;#39; as &amp;quot;CURRENT DRIVER COUNT&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;FROM blah blah blah.....  &lt;/p&gt;\n\n&lt;p&gt;Like the above query but in production we have like 60-70 fields to be selected from json which is making the query tooo slow. All other parts of the query are running in less than half a minute. Can anyone suggest a way to speed up this query . All the joins im making are inevitable and cannot be optimized as the postgres optimizer is doing it job well and fine. i&amp;#39;ve seperated the query into two parts . one with all the json fields accessing query and others with normal column accessing. The normal query is running in &amp;lt;30s. While this json accessing query is taking a loads of time.  &lt;/p&gt;\n\n&lt;p&gt;Please suggest a way to speed up this query.&lt;br/&gt;\nthank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ah6reu", "is_robot_indexable": true, "report_reasons": null, "author": "Ashu6410", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah6reu/accessing_large_json_columns_in_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah6reu/accessing_large_json_columns_in_postgresql/", "subreddit_subscribers": 157960, "created_utc": 1706888924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to get my team to adapt Parquet format for the data stored on AWS S3 instead of CSV. Our data engineering pipeline consists of Lambda triggers on S3 file drop.\n\nUnfortunately, I could not package pyarrow due to the 250 MB Lambda layer size limitation, so I ended up packaging fastparquet to read and writes parquet files.\n\nI created two identical lambda functions\n1. Read csv from s3 into a pandas dataframe -&gt; aggregation -&gt; write df to csv back to s3\n2. Read parque from s3 into a pandas dataframe -&gt;  aggregation -&gt; write df to parquet back to s3\n\nBoth the functions were processing the exact same dataset, just different file formats.\n\nThe read-parquet-write-parquet lambda consumes way more memory than the read-csv-write-csv lambda, for the same dataset, in some cases almost double. \n\nI tried the above tests with different sized datasets and different memory allocated to both lambda functions, got the same results. The benefit gained read and write speed of parquet data and the s3 storage size compared to csv, is negligible compared to the AWS lambda memory usage cost as parquet df consumes way more RAM than csv df ?\n\n\nWhat am I doing wrong? Could this be specific to fastparquet engine? Is there a way I can try the same test using Pyarrow (since I am not about to package pyarrow to AWS lambda)?", "author_fullname": "t2_v8qhw4wd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Lambda Parquet consumes more memory than csv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahw2qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706967486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706966798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to get my team to adapt Parquet format for the data stored on AWS S3 instead of CSV. Our data engineering pipeline consists of Lambda triggers on S3 file drop.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I could not package pyarrow due to the 250 MB Lambda layer size limitation, so I ended up packaging fastparquet to read and writes parquet files.&lt;/p&gt;\n\n&lt;p&gt;I created two identical lambda functions\n1. Read csv from s3 into a pandas dataframe -&amp;gt; aggregation -&amp;gt; write df to csv back to s3\n2. Read parque from s3 into a pandas dataframe -&amp;gt;  aggregation -&amp;gt; write df to parquet back to s3&lt;/p&gt;\n\n&lt;p&gt;Both the functions were processing the exact same dataset, just different file formats.&lt;/p&gt;\n\n&lt;p&gt;The read-parquet-write-parquet lambda consumes way more memory than the read-csv-write-csv lambda, for the same dataset, in some cases almost double. &lt;/p&gt;\n\n&lt;p&gt;I tried the above tests with different sized datasets and different memory allocated to both lambda functions, got the same results. The benefit gained read and write speed of parquet data and the s3 storage size compared to csv, is negligible compared to the AWS lambda memory usage cost as parquet df consumes way more RAM than csv df ?&lt;/p&gt;\n\n&lt;p&gt;What am I doing wrong? Could this be specific to fastparquet engine? Is there a way I can try the same test using Pyarrow (since I am not about to package pyarrow to AWS lambda)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahw2qp", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Love_648", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahw2qp/aws_lambda_parquet_consumes_more_memory_than_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahw2qp/aws_lambda_parquet_consumes_more_memory_than_csv/", "subreddit_subscribers": 157960, "created_utc": 1706966798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company's data, something that is quite important for our analytics is what 'roles' have been assigned to our users. There are currently about 50 different roles, and each user can have many roles. These roles can represent different settings that have been selected by the user, or things like whether we are giving them early access to a new feature in our app.\n\nIn our application database, this is modelled roughly how you'd expect. We have a users table (one row per user), a roles tables (one row per role), and a user\\_roles table, which maps the users to each of the roles they have.\n\nI'm wondering if there is some best practice on how this should be modelled in a data warehouse. My predecessor wanted to de-normalize a lot of the data to make it easier to use for analytics (which in *general* I support), but in this case it meant that they created new columns in the our dim\\_user table in the datawarehouse for whether the user had each of the given roles. i.e.: columns for has\\_role1, has\\_role2 etc. I wonder if this is something people would generally support as a good idea, or if it might actually be better keeping this as a separate table. Would one approach be easier for keeping track of changes in the roles each user has?", "author_fullname": "t2_173s1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model user roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ahx0t2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706969648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company&amp;#39;s data, something that is quite important for our analytics is what &amp;#39;roles&amp;#39; have been assigned to our users. There are currently about 50 different roles, and each user can have many roles. These roles can represent different settings that have been selected by the user, or things like whether we are giving them early access to a new feature in our app.&lt;/p&gt;\n\n&lt;p&gt;In our application database, this is modelled roughly how you&amp;#39;d expect. We have a users table (one row per user), a roles tables (one row per role), and a user_roles table, which maps the users to each of the roles they have.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if there is some best practice on how this should be modelled in a data warehouse. My predecessor wanted to de-normalize a lot of the data to make it easier to use for analytics (which in &lt;em&gt;general&lt;/em&gt; I support), but in this case it meant that they created new columns in the our dim_user table in the datawarehouse for whether the user had each of the given roles. i.e.: columns for has_role1, has_role2 etc. I wonder if this is something people would generally support as a good idea, or if it might actually be better keeping this as a separate table. Would one approach be easier for keeping track of changes in the roles each user has?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ahx0t2", "is_robot_indexable": true, "report_reasons": null, "author": "Pancakeman123000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahx0t2/how_to_model_user_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahx0t2/how_to_model_user_roles/", "subreddit_subscribers": 157960, "created_utc": 1706969648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've been working with dbt for a long time. While it greatly improves delivery speed and provides many QOL improvements, I have noticed some patterns that have significantly enhanced dbt development workflows.\n\nWith this in mind, I wrote an article (with fully working code) that goes over some tools &amp; techniques that I have seen improve dbt development speed by enabling.\n\n1. Reproducible environment\n2. Reducing feedback loop time\n3. Using existing dbt-packages\n4. Streamlining commonly run tasks\n\nI've also tried to specify any caveats/tradeoffs with the tools/approaches. I hope this helps someone looking to speed up their dbt development process with some good ideas.\n\nBlog: [Uplevel dbt](https://www.startdataengineering.com/post/uplevel-dbt-workflow/)\n\nCode: [dbt project](https://github.com/josephmachado/simple_dbt_project)\n\nI appreciate any questions, feedback, or comments. I hope this helps someone.", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steps to enhance dbt(data build tool) dev workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ahwltd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706968459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working with dbt for a long time. While it greatly improves delivery speed and provides many QOL improvements, I have noticed some patterns that have significantly enhanced dbt development workflows.&lt;/p&gt;\n\n&lt;p&gt;With this in mind, I wrote an article (with fully working code) that goes over some tools &amp;amp; techniques that I have seen improve dbt development speed by enabling.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Reproducible environment&lt;/li&gt;\n&lt;li&gt;Reducing feedback loop time&lt;/li&gt;\n&lt;li&gt;Using existing dbt-packages&lt;/li&gt;\n&lt;li&gt;Streamlining commonly run tasks&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried to specify any caveats/tradeoffs with the tools/approaches. I hope this helps someone looking to speed up their dbt development process with some good ideas.&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://www.startdataengineering.com/post/uplevel-dbt-workflow/\"&gt;Uplevel dbt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/josephmachado/simple_dbt_project\"&gt;dbt project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I appreciate any questions, feedback, or comments. I hope this helps someone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?auto=webp&amp;s=d3f46b7a5e182a2295460628dc1f041a7de1f3e1", "width": 1262, "height": 707}, "resolutions": [{"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f2e7ced329f037447de0b6362a694be68e73aea", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b1356076b1f7bb65859d5e658e01dfb6ada9332", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c2db5be5707398ffac36961f3bdd57d03489ace", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7c9e184380a63b5eb0b3b80174abec522da14663", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1007b5769638af0aaf4dc1c5b8fcbd9c0899b3f8", "width": 960, "height": 537}, {"url": "https://external-preview.redd.it/09N95gjFm6Xzi6PIlSRJrKNMJO7GHw1stEbE_vD3RBk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90d8445fbdfbb3b97082c19cfb56b6b0bd43d94c", "width": 1080, "height": 605}], "variants": {}, "id": "t2uBlsTIaigUxxCU2fngqLcShkJLkDZmzsVkj3LAbvU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ahwltd", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahwltd/steps_to_enhance_dbtdata_build_tool_dev_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahwltd/steps_to_enhance_dbtdata_build_tool_dev_workflow/", "subreddit_subscribers": 157960, "created_utc": 1706968459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can I artificially create a transactional data on some topic to build a oltp database out of it?", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generate Artificial Transactional Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahkfbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706924566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I artificially create a transactional data on some topic to build a oltp database out of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahkfbd", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahkfbd/generate_artificial_transactional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahkfbd/generate_artificial_transactional_data/", "subreddit_subscribers": 157960, "created_utc": 1706924566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All, I am an aspring data engineer and I would like to setup a distributed system setup in GCP with 1 master node and 2 worker nodes for airflow and spark and 1 for mysql metadata storage. I want to use celery executor for airflow. I am trying to follow guides online but they are confusing and I would appreciate any feedback or suggestions on how to do this.\n\nMy project concept is taking data from bucket use airflow with pyspark to make transformations and store in bigquery and use tableau to make some visualizations.", "author_fullname": "t2_7bo4ark1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Setup in GCP environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahg5jn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706912826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All, I am an aspring data engineer and I would like to setup a distributed system setup in GCP with 1 master node and 2 worker nodes for airflow and spark and 1 for mysql metadata storage. I want to use celery executor for airflow. I am trying to follow guides online but they are confusing and I would appreciate any feedback or suggestions on how to do this.&lt;/p&gt;\n\n&lt;p&gt;My project concept is taking data from bucket use airflow with pyspark to make transformations and store in bigquery and use tableau to make some visualizations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahg5jn", "is_robot_indexable": true, "report_reasons": null, "author": "iT0X1Ni", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahg5jn/distributed_setup_in_gcp_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahg5jn/distributed_setup_in_gcp_environment/", "subreddit_subscribers": 157960, "created_utc": 1706912826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy, I am an analyst (aspiring DE, one day)\n\nMy team which their primary function is reporting. Maintains around 100/200 reports.\n\nThe data that we are pulling these reports from mostly do no live in our Database (we use snowflake.)\n\nDoes anyone know of a way in which I could attempt to replace a lot of these field names programmatically?ks in SF on our Database, that pull from the other teams DB.\n\nDoes anyone know of a way in which I could attempt to programmatically replace a lot of these field names?\n\nDoes anyone know of a way in which I could attempt to replace a lot of these field names programmatically?\n\nAnother caveat is a lot of these databases that the other team owns are replicated by End date, so a monthly table, archive table, etc so they will share column names that are changing. \n\nI am also pretty limited on tools because I am an analyst, So I have Snowflake, Excel, Access, VBA, Excel. Potentially get Python if I prove a working solution.\n\n&amp;#x200B;\n\nI am also pretty limited on tools because I am an analyst, So I have Snowflake, Excel, Access, VBA, and Excel. Potentially get Python if I prove a working solution.", "author_fullname": "t2_5w8en", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help regarding Mass Collumn/Field Name changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahf33q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706910076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy, I am an analyst (aspiring DE, one day)&lt;/p&gt;\n\n&lt;p&gt;My team which their primary function is reporting. Maintains around 100/200 reports.&lt;/p&gt;\n\n&lt;p&gt;The data that we are pulling these reports from mostly do no live in our Database (we use snowflake.)&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way in which I could attempt to replace a lot of these field names programmatically?ks in SF on our Database, that pull from the other teams DB.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way in which I could attempt to programmatically replace a lot of these field names?&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way in which I could attempt to replace a lot of these field names programmatically?&lt;/p&gt;\n\n&lt;p&gt;Another caveat is a lot of these databases that the other team owns are replicated by End date, so a monthly table, archive table, etc so they will share column names that are changing. &lt;/p&gt;\n\n&lt;p&gt;I am also pretty limited on tools because I am an analyst, So I have Snowflake, Excel, Access, VBA, Excel. Potentially get Python if I prove a working solution.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am also pretty limited on tools because I am an analyst, So I have Snowflake, Excel, Access, VBA, and Excel. Potentially get Python if I prove a working solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahf33q", "is_robot_indexable": true, "report_reasons": null, "author": "1nteger", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahf33q/help_regarding_mass_collumnfield_name_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahf33q/help_regarding_mass_collumnfield_name_changes/", "subreddit_subscribers": 157960, "created_utc": 1706910076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to implement some Python code to showcase the fundamentals of data quality checking and do this without using any specific packages. \n\nThinking of implementing\n1. Range checks/anomaly detection\n2. data type checks (e..g consistent date-times)\n3. Volume checks (number of rows,columns, entries)\n4. Null checks. \n\nPlanning to wrap this output into a dashboard for presentation purposes. \n\nAny additional checks that you think I should write in? Hoping to use this as a resource moving forward as an intro to data quality for early career staff", "author_fullname": "t2_jso5n8jj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahdnr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706906470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to implement some Python code to showcase the fundamentals of data quality checking and do this without using any specific packages. &lt;/p&gt;\n\n&lt;p&gt;Thinking of implementing\n1. Range checks/anomaly detection\n2. data type checks (e..g consistent date-times)\n3. Volume checks (number of rows,columns, entries)\n4. Null checks. &lt;/p&gt;\n\n&lt;p&gt;Planning to wrap this output into a dashboard for presentation purposes. &lt;/p&gt;\n\n&lt;p&gt;Any additional checks that you think I should write in? Hoping to use this as a resource moving forward as an intro to data quality for early career staff&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ahdnr6", "is_robot_indexable": true, "report_reasons": null, "author": "RobDoesData", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahdnr6/data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahdnr6/data_quality_checks/", "subreddit_subscribers": 157960, "created_utc": 1706906470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4js2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental Database Computations | Feldera", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 49, "top_awarded_type": null, "hide_score": false, "name": "t3_1aha8of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/G4578cPyTd2mEY6NVQ7nTFoNXtuU_CKAKOs2HmIstDo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1706897713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "feldera.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.feldera.com/blog/incremental-database-computations/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Wtw26b0HG9Z-p5WJrAoaBnPdsSJs97JVYoMO70mSAnQ.jpg?auto=webp&amp;s=58452f391d40a2f299da496d9ef3946f5a28c797", "width": 895, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/Wtw26b0HG9Z-p5WJrAoaBnPdsSJs97JVYoMO70mSAnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9129d1404c13a1e186fc789d7ab4a3204e6cdeb", "width": 108, "height": 38}, {"url": "https://external-preview.redd.it/Wtw26b0HG9Z-p5WJrAoaBnPdsSJs97JVYoMO70mSAnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=347dc0e25756b3e0b6a9682a7c1c0f32a05ed499", "width": 216, "height": 76}, {"url": "https://external-preview.redd.it/Wtw26b0HG9Z-p5WJrAoaBnPdsSJs97JVYoMO70mSAnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=05bb0e84922e03ee6a99810aa6c44ac7ec91e730", "width": 320, "height": 112}, {"url": "https://external-preview.redd.it/Wtw26b0HG9Z-p5WJrAoaBnPdsSJs97JVYoMO70mSAnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa3353443e6d9835eb782ea65eda8552197cb17f", "width": 640, "height": 225}], "variants": {}, "id": "2ehk8BQ9oyV1jaNzXzf3HcqAWW51WDxmYOrnCjmMAbI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aha8of", "is_robot_indexable": true, "report_reasons": null, "author": "mww09", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aha8of/incremental_database_computations_feldera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.feldera.com/blog/incremental-database-computations/", "subreddit_subscribers": 157960, "created_utc": 1706897713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nWe're leveraging MySQL for OLTP, streaming logs to S3 via Kafka, and using AWS Glue for transforming data into Delta Lake format stored in 'silver' and 'gold' S3 buckets. Given the high cost of maintaining terabytes of historical transactions in MySQL, we plan to retain only the last 3 years of data and rely on Delta Lake for analytics.\n\n**Challenge:** We need a fail-safe disaster recovery strategy for our Delta Lake in S3, especially if we can't rely on MySQL for full data recovery due to deletion of older transactions. Our main concern is ensuring data recovery if the S3 buckets or their contents are accidentally deleted.\n\n**Disaster Recovery Considerations:**\n\n* **S3 File Versioning:** Found not recommended for Delta Lake as per Databricks' advice due to potential performance issues.\n* **S3 Bucket Replication:** Requires enabling S3 file versioning, which brings us back to the same concern.\n* **AWS Backup service:** [https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/](https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/)\n\nHas anyone here managed disaster recovery for Delta Lake on AWS and handled these issues? Looking for insights or alternative strategies that have worked for you.\n\nThanks in advance!", "author_fullname": "t2_1rqmub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake in AWS disaster recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah8yn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1706899023.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1706894499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re leveraging MySQL for OLTP, streaming logs to S3 via Kafka, and using AWS Glue for transforming data into Delta Lake format stored in &amp;#39;silver&amp;#39; and &amp;#39;gold&amp;#39; S3 buckets. Given the high cost of maintaining terabytes of historical transactions in MySQL, we plan to retain only the last 3 years of data and rely on Delta Lake for analytics.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; We need a fail-safe disaster recovery strategy for our Delta Lake in S3, especially if we can&amp;#39;t rely on MySQL for full data recovery due to deletion of older transactions. Our main concern is ensuring data recovery if the S3 buckets or their contents are accidentally deleted.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Disaster Recovery Considerations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;S3 File Versioning:&lt;/strong&gt; Found not recommended for Delta Lake as per Databricks&amp;#39; advice due to potential performance issues.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;S3 Bucket Replication:&lt;/strong&gt; Requires enabling S3 file versioning, which brings us back to the same concern.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AWS Backup service:&lt;/strong&gt; &lt;a href=\"https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/\"&gt;https://aws.amazon.com/blogs/storage/best-practices-for-data-lake-protection-with-aws-backup/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone here managed disaster recovery for Delta Lake on AWS and handled these issues? Looking for insights or alternative strategies that have worked for you.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?auto=webp&amp;s=61c9098f4fdfde5094c36b617d58e9bdfef2a473", "width": 374, "height": 186}, "resolutions": [{"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7dc54cb6e7696044b9a9600137afca76c123f9d4", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5013cd8396dff572dd46eea4dd9aba67c089e700", "width": 216, "height": 107}, {"url": "https://external-preview.redd.it/LSsTTt3f1A_42ho37WOkmWDRgBSVRGErlrpOeWweKcg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=572bb1376f50894df9c51aeabe5ca7bad4698c19", "width": 320, "height": 159}], "variants": {}, "id": "_cMfF1AUEjTkRGdbXXJuaqM3kgKtguwKSFdUfATSu1w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah8yn3", "is_robot_indexable": true, "report_reasons": null, "author": "EatDirty", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah8yn3/delta_lake_in_aws_disaster_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah8yn3/delta_lake_in_aws_disaster_recovery/", "subreddit_subscribers": 157960, "created_utc": 1706894499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine I have a stock data - when tx happened and at what price. I want to track the all all time high of that data, without spikes. Right now what im doing is   \n\n\n```\n`WITH RankedTxs AS (\n            SELECT\n                stock_id, price, timestamp,\n                ROW_NUMBER() OVER (PARTITION BY stock_id ORDER BY price DESC) AS row_num\n                FROM\n                \"Txs\"\n                ts &gt;= ${} AND ts &lt;= ${} \n            )\n            SELECT\n                *\n            FROM\n                RankedSwaps\n            WHERE\n                row_num = 1;\n        ;`)\n```\n\nThis works, but when there is a spike i.e someone bought a lot and then there are sells it creates an ATH that it's way to high. \n\nAlso the Txs tables is quite large ~20M of rows, so I should avoid going into the history of raw txs.\n\nWhat's the proper way to handle this ? I have timescale plugin that probably can create buckets for ath and then write a query and filter peaks. What would be the proper way ?", "author_fullname": "t2_3ppayh15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effectively tracking all time high in postgres with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ah9le1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706896109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine I have a stock data - when tx happened and at what price. I want to track the all all time high of that data, without spikes. Right now what im doing is   &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n`WITH RankedTxs AS (\n            SELECT\n                stock_id, price, timestamp,\n                ROW_NUMBER() OVER (PARTITION BY stock_id ORDER BY price DESC) AS row_num\n                FROM\n                &amp;quot;Txs&amp;quot;\n                ts &amp;gt;= ${} AND ts &amp;lt;= ${} \n            )\n            SELECT\n                *\n            FROM\n                RankedSwaps\n            WHERE\n                row_num = 1;\n        ;`)\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This works, but when there is a spike i.e someone bought a lot and then there are sells it creates an ATH that it&amp;#39;s way to high. &lt;/p&gt;\n\n&lt;p&gt;Also the Txs tables is quite large ~20M of rows, so I should avoid going into the history of raw txs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the proper way to handle this ? I have timescale plugin that probably can create buckets for ath and then write a query and filter peaks. What would be the proper way ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ah9le1", "is_robot_indexable": true, "report_reasons": null, "author": "dotaleaker", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ah9le1/effectively_tracking_all_time_high_in_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ah9le1/effectively_tracking_all_time_high_in_postgres/", "subreddit_subscribers": 157960, "created_utc": 1706896109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can I become a Data Architect without working as a Data Engineer?", "author_fullname": "t2_ht40qce8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architect without working as a Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ahck2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.28, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1706903632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I become a Data Architect without working as a Data Engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ahck2r", "is_robot_indexable": true, "report_reasons": null, "author": "phoot_in_the_door", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ahck2r/data_architect_without_working_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ahck2r/data_architect_without_working_as_a_data_engineer/", "subreddit_subscribers": 157960, "created_utc": 1706903632.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}