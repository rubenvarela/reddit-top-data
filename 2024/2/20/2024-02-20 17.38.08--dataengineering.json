{"kind": "Listing", "data": {"after": "t3_1auzk3d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently got hired as a data engineering analyst at a big fortune 100 company. It's not FAANG But it's a pretty big company, and One reason I was excited about this position is because I've done lots of data engineering functions in my past positions, but I've never worked in the actual data engineering capacity. The position is great and I am being challenged. But I feel like there's no mentoring. I spent 3 weeks trying to write an SQL query to retrieve data for ETL, and was given complete radio silence. Anytime I ask a question about it to my manager, it was short replies and very little information...\n\n\nNow, several weeks later, find myself getting grilled about how the SQL script is poorly written, there's major issues with it, We need to change XYZ. And it's just like, I've been asking you about this for several weeks, and I let you know specifically that I wanted additional training and SQL so I could learn the data stack, which is Teradata. Got almost no support. So it's kind of like a guess what you need to learn, go learn it yourself, and hope you hit the mark along the way otherwise you failed.\n\n\nLike, how do you mentor yourself to get better?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It really sucks having to mentor yourself with no support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avgvna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 96, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 96, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708433388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently got hired as a data engineering analyst at a big fortune 100 company. It&amp;#39;s not FAANG But it&amp;#39;s a pretty big company, and One reason I was excited about this position is because I&amp;#39;ve done lots of data engineering functions in my past positions, but I&amp;#39;ve never worked in the actual data engineering capacity. The position is great and I am being challenged. But I feel like there&amp;#39;s no mentoring. I spent 3 weeks trying to write an SQL query to retrieve data for ETL, and was given complete radio silence. Anytime I ask a question about it to my manager, it was short replies and very little information...&lt;/p&gt;\n\n&lt;p&gt;Now, several weeks later, find myself getting grilled about how the SQL script is poorly written, there&amp;#39;s major issues with it, We need to change XYZ. And it&amp;#39;s just like, I&amp;#39;ve been asking you about this for several weeks, and I let you know specifically that I wanted additional training and SQL so I could learn the data stack, which is Teradata. Got almost no support. So it&amp;#39;s kind of like a guess what you need to learn, go learn it yourself, and hope you hit the mark along the way otherwise you failed.&lt;/p&gt;\n\n&lt;p&gt;Like, how do you mentor yourself to get better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avgvna", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avgvna/it_really_sucks_having_to_mentor_yourself_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avgvna/it_really_sucks_having_to_mentor_yourself_with_no/", "subreddit_subscribers": 162248, "created_utc": 1708433388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\"Data Engineer\" is a horribly esoteric job title which tends to result in a lot of blank stares and confused looks from all the \"normal\" people in my life. How do you describe data engineering to normal folk in as few sentences as possible?\n\nMy favourite: \"I'm like a plumber for data, i.e. I do for number and computers what a plumber does for water and toilets.\"", "author_fullname": "t2_6mxelrxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you describe your job to normies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auwziw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708372901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Data Engineer&amp;quot; is a horribly esoteric job title which tends to result in a lot of blank stares and confused looks from all the &amp;quot;normal&amp;quot; people in my life. How do you describe data engineering to normal folk in as few sentences as possible?&lt;/p&gt;\n\n&lt;p&gt;My favourite: &amp;quot;I&amp;#39;m like a plumber for data, i.e. I do for number and computers what a plumber does for water and toilets.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1auwziw", "is_robot_indexable": true, "report_reasons": null, "author": "Creyke", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auwziw/how_do_you_describe_your_job_to_normies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auwziw/how_do_you_describe_your_job_to_normies/", "subreddit_subscribers": 162248, "created_utc": 1708372901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I start, this is hooks (pre_hooks and post_hooks) allowing to run any sql code on your DB !", "author_fullname": "t2_ie6cij4nf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the DBT function you discovered recently and you use everywhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auwjek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708371894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I start, this is hooks (pre_hooks and post_hooks) allowing to run any sql code on your DB !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1auwjek", "is_robot_indexable": true, "report_reasons": null, "author": "Advanced_Addition321", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auwjek/what_is_the_dbt_function_you_discovered_recently/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auwjek/what_is_the_dbt_function_you_discovered_recently/", "subreddit_subscribers": 162248, "created_utc": 1708371894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Last week I was hired as a Data Analyst at a small 200 employees company and the expectations for my role are to automate a lot of manually generated reports, and also build dashboards to provide insights to Sales, Marketing, Product teams. \n\nI found out there is absolutely **no data and analytics infrastructure**!! The reporting is based solely on Salesforce reports exported to Gsheets and a lot of manual formulas and calcs. \n\nWhat would be a low-cost tool or tools that let me build ETLs pulling data mostly from Salesforce and build custom queries for dashboards that I would build in Looker? The main source of data will be Salesforce, but would want to include others like Gainsight, NetSuite, HubSpot, Google Analytics, etc.\n\nIn my experience I have worked with already built datawarehouses in Snowflake, Redshift, PostgreSQL, etc. but I have no experience setting them up!\n\nNote: a guy from finance gave me access to Fivetran but I have absolutely no idea of how it works. Would it be useful to continue using it or should I pick another one?", "author_fullname": "t2_i9w9s17d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hired as Data Analyst but the Company has no Data Infrastructure/Strategy/Governance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av26rw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708385017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week I was hired as a Data Analyst at a small 200 employees company and the expectations for my role are to automate a lot of manually generated reports, and also build dashboards to provide insights to Sales, Marketing, Product teams. &lt;/p&gt;\n\n&lt;p&gt;I found out there is absolutely &lt;strong&gt;no data and analytics infrastructure&lt;/strong&gt;!! The reporting is based solely on Salesforce reports exported to Gsheets and a lot of manual formulas and calcs. &lt;/p&gt;\n\n&lt;p&gt;What would be a low-cost tool or tools that let me build ETLs pulling data mostly from Salesforce and build custom queries for dashboards that I would build in Looker? The main source of data will be Salesforce, but would want to include others like Gainsight, NetSuite, HubSpot, Google Analytics, etc.&lt;/p&gt;\n\n&lt;p&gt;In my experience I have worked with already built datawarehouses in Snowflake, Redshift, PostgreSQL, etc. but I have no experience setting them up!&lt;/p&gt;\n\n&lt;p&gt;Note: a guy from finance gave me access to Fivetran but I have absolutely no idea of how it works. Would it be useful to continue using it or should I pick another one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1av26rw", "is_robot_indexable": true, "report_reasons": null, "author": "JaviReds", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av26rw/hired_as_data_analyst_but_the_company_has_no_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av26rw/hired_as_data_analyst_but_the_company_has_no_data/", "subreddit_subscribers": 162248, "created_utc": 1708385017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data team has been working really hard on our platform. We know many teams that develop pipelines using many technologies, we wanted to create our platform in order to decentralize our data team\n\nHere\u2019s a quick overview of what our platform brings to the table:\n\n\t\u2022\tHarnesses the power of Spark clusters over Kubernetes for scalability and efficiency.\n\t\u2022\tSeamlessly integrates with Hive or Glue for robust data storage and management. At the moment we use s3 and iceberg as our table format, but it could be gcs/adls2.\n\t\u2022\tExecutes DBT commands effortlessly via an integrated image with DBT and Spark clusters.\n\t\u2022\tAll of the jobs created on the platform, run on ephemeral clusters to optimize resource utilization and cut down costs.\n\t\u2022\tOffers Airflow integration for easy pipeline scheduling, configurable through a user-friendly interface.\n\t\u2022\twe can download data from Kafka topics, to the lake or We can extract data from any database using our connectors.\n\t\u2022\tEnables resource management by assigning distinct resources to different groups within organizations. \n\nWe use permissions, a data catalogue is integrated and you can create spark jobs (python , java, sparksql) in order to create transformations or ml model training. You can run on a normal cluster or one with gpu's. \n\nAre we missing something? Would you use something like this? Or is it over engineered?", "author_fullname": "t2_4alt9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We've created a Data Engineering Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av7fdf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708400012.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708399067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data team has been working really hard on our platform. We know many teams that develop pipelines using many technologies, we wanted to create our platform in order to decentralize our data team&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s a quick overview of what our platform brings to the table:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2022 Harnesses the power of Spark clusters over Kubernetes for scalability and efficiency.\n\u2022 Seamlessly integrates with Hive or Glue for robust data storage and management. At the moment we use s3 and iceberg as our table format, but it could be gcs/adls2.\n\u2022 Executes DBT commands effortlessly via an integrated image with DBT and Spark clusters.\n\u2022 All of the jobs created on the platform, run on ephemeral clusters to optimize resource utilization and cut down costs.\n\u2022 Offers Airflow integration for easy pipeline scheduling, configurable through a user-friendly interface.\n\u2022 we can download data from Kafka topics, to the lake or We can extract data from any database using our connectors.\n\u2022 Enables resource management by assigning distinct resources to different groups within organizations. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We use permissions, a data catalogue is integrated and you can create spark jobs (python , java, sparksql) in order to create transformations or ml model training. You can run on a normal cluster or one with gpu&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;Are we missing something? Would you use something like this? Or is it over engineered?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1av7fdf", "is_robot_indexable": true, "report_reasons": null, "author": "Matunguito", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av7fdf/weve_created_a_data_engineering_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av7fdf/weve_created_a_data_engineering_platform/", "subreddit_subscribers": 162248, "created_utc": 1708399067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When your data sources and ETL pipelines live in AWS for examples as S3, Glue Jobs and RDS, then when you want to intagre it with a data warehouse, isn't the choice of Snowflake a bad decision cost-wise? Does it not cost extra to move data between AWS Cloud and Snowflake Cloud? Does it not make more sense to use Redshift then to avoid moving data via public internet?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Isn't Snowflake expensive when your other infrastructure is on AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ausru6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708363185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When your data sources and ETL pipelines live in AWS for examples as S3, Glue Jobs and RDS, then when you want to intagre it with a data warehouse, isn&amp;#39;t the choice of Snowflake a bad decision cost-wise? Does it not cost extra to move data between AWS Cloud and Snowflake Cloud? Does it not make more sense to use Redshift then to avoid moving data via public internet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ausru6", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ausru6/isnt_snowflake_expensive_when_your_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ausru6/isnt_snowflake_expensive_when_your_other/", "subreddit_subscribers": 162248, "created_utc": 1708363185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it's hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What was your biggest challenge getting into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avj2k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708439683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it&amp;#39;s hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avj2k2", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/", "subreddit_subscribers": 162248, "created_utc": 1708439683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently firing off a few POCs on data catalog/data discovery tools for my organization. I am not a data engineer, but I work on the security side of things. Our goal is to have max visibility into the data we have, so we can ensure security best practices are being followed. That being said, we are rolling this out to our data engineering teams for their use as well. After polling our data teams, several features were listed as high in demand, such as data lineage, data glossary, and the ability to collaborate between data assets owned by different teams. Perhaps you folks could shed some light as to what you would like to see in a catalog/discovery tool?\n\n&amp;#x200B;\n\nEdit: I kant spel.", "author_fullname": "t2_dq4h698", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some key features of a data catalog would engineers find useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auzzx9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708379878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently firing off a few POCs on data catalog/data discovery tools for my organization. I am not a data engineer, but I work on the security side of things. Our goal is to have max visibility into the data we have, so we can ensure security best practices are being followed. That being said, we are rolling this out to our data engineering teams for their use as well. After polling our data teams, several features were listed as high in demand, such as data lineage, data glossary, and the ability to collaborate between data assets owned by different teams. Perhaps you folks could shed some light as to what you would like to see in a catalog/discovery tool?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I kant spel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1auzzx9", "is_robot_indexable": true, "report_reasons": null, "author": "Wentz_ylvania", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auzzx9/what_are_some_key_features_of_a_data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auzzx9/what_are_some_key_features_of_a_data_catalog/", "subreddit_subscribers": 162248, "created_utc": 1708379878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you or are you implementing a lakehouse?\n\nIf so what have been some of the biggest hurdles or things you\u2019ve learned?\n\nWhat were big decisions you were happy with or would do differently if you could start over.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most important parts of implementing a lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avhmkt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708435683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you or are you implementing a lakehouse?&lt;/p&gt;\n\n&lt;p&gt;If so what have been some of the biggest hurdles or things you\u2019ve learned?&lt;/p&gt;\n\n&lt;p&gt;What were big decisions you were happy with or would do differently if you could start over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avhmkt", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avhmkt/most_important_parts_of_implementing_a_lakehouse/", "subreddit_subscribers": 162248, "created_utc": 1708435683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just curious what is out there in the real world.\n\nMine has grown to 320 models with about 4000 tests across 5 hops and the required thought/impact analysis that is put into changes has multiplied by 10x than what it was with 100 models.  Within the next year, there will be a hundred or so more models and I expect the complexity to continue to rise.\n\nWhat do you use to maintain your DBT project?  Do you manually build out the models or is there a tool you use?", "author_fullname": "t2_33sjvqg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How big are your DBT projects and how do you build/manage them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av9i90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708405649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just curious what is out there in the real world.&lt;/p&gt;\n\n&lt;p&gt;Mine has grown to 320 models with about 4000 tests across 5 hops and the required thought/impact analysis that is put into changes has multiplied by 10x than what it was with 100 models.  Within the next year, there will be a hundred or so more models and I expect the complexity to continue to rise.&lt;/p&gt;\n\n&lt;p&gt;What do you use to maintain your DBT project?  Do you manually build out the models or is there a tool you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1av9i90", "is_robot_indexable": true, "report_reasons": null, "author": "Captain_Coffee_III", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av9i90/how_big_are_your_dbt_projects_and_how_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av9i90/how_big_are_your_dbt_projects_and_how_do_you/", "subreddit_subscribers": 162248, "created_utc": 1708405649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So my first job was a a cloud engineer with database administration in which I learned a lot because of my seniors. Now my current job (3 months) is as a data engineer where we run our pipelines (python scripts) in a VM with task scheduler. We are slowly migrating our things to azure. My senior is leaving the company leaving me alone until they find a replacement (he came from a data scientist background and is self taught)\n\n&amp;#x200B;\n\nAm I missing a lot from not having a mentor and being on a company where data engineering lacks maturity? When I came to this company I made it clear my goal was to learn the ropes as a data engineer and most of what is keeping me is to not have a job with only 3 months in my CV.  \n\n\nFor the most experienced how common are companies like this? Should I look for other options?", "author_fullname": "t2_scb32bzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How important is mentoring for a junior data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auzjli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708378805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my first job was a a cloud engineer with database administration in which I learned a lot because of my seniors. Now my current job (3 months) is as a data engineer where we run our pipelines (python scripts) in a VM with task scheduler. We are slowly migrating our things to azure. My senior is leaving the company leaving me alone until they find a replacement (he came from a data scientist background and is self taught)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Am I missing a lot from not having a mentor and being on a company where data engineering lacks maturity? When I came to this company I made it clear my goal was to learn the ropes as a data engineer and most of what is keeping me is to not have a job with only 3 months in my CV.  &lt;/p&gt;\n\n&lt;p&gt;For the most experienced how common are companies like this? Should I look for other options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1auzjli", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyKey4744", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auzjli/how_important_is_mentoring_for_a_junior_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auzjli/how_important_is_mentoring_for_a_junior_data/", "subreddit_subscribers": 162248, "created_utc": 1708378805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired 2 years ago to migrate my organization's data from absolute shit-show format (random spreadsheets, a legacy php/sql web applications, written forms, and literal smartphone photos of computer screens) into a more standard format using commercial industry database applications.\n\nMy field is extremely specialized and there are only a small handful of options available that fit this criteria (had to be commercial software for purchase, had to be specific to our industry, had to have x, y, and z features). I made a judgement call with a specific vendor and although it started off to a rocky start they're openness/willingness to collaborate has paid off.\n\nBut it's become increasingly clear that this project was doomed from the start (it was planned and budgeted for before I was employed). The data going into this new platform is inconsistent at best and completely lacking integrity at worst. The new platform also sports many features that our users do not need and lead to confusion, but it was also the only platform to host the features we need.\n\nWithout going into further detail, this has just become a mess with no end in sight. On one hand I relish the freedom/independence I'm given in my role, on the other hand I am frustrated that I have no support. Additionally, the project has no defined outcomes. Basically it just has to pass a general \"vibe check\" from management to determine when the migration is complete. This is a double edged sword as it's basically impossible for me to fail, but also to a certain extent it's impossible for me succeed.\n\nHas anyone dealt projects like this? How did you handle them/yourself while going through them. I'm encountering a fair amount of mental distress as this never-ending projects drags from months into years.", "author_fullname": "t2_hrrbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on dealing with \"impossible\" projects.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avccpj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708415981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired 2 years ago to migrate my organization&amp;#39;s data from absolute shit-show format (random spreadsheets, a legacy php/sql web applications, written forms, and literal smartphone photos of computer screens) into a more standard format using commercial industry database applications.&lt;/p&gt;\n\n&lt;p&gt;My field is extremely specialized and there are only a small handful of options available that fit this criteria (had to be commercial software for purchase, had to be specific to our industry, had to have x, y, and z features). I made a judgement call with a specific vendor and although it started off to a rocky start they&amp;#39;re openness/willingness to collaborate has paid off.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s become increasingly clear that this project was doomed from the start (it was planned and budgeted for before I was employed). The data going into this new platform is inconsistent at best and completely lacking integrity at worst. The new platform also sports many features that our users do not need and lead to confusion, but it was also the only platform to host the features we need.&lt;/p&gt;\n\n&lt;p&gt;Without going into further detail, this has just become a mess with no end in sight. On one hand I relish the freedom/independence I&amp;#39;m given in my role, on the other hand I am frustrated that I have no support. Additionally, the project has no defined outcomes. Basically it just has to pass a general &amp;quot;vibe check&amp;quot; from management to determine when the migration is complete. This is a double edged sword as it&amp;#39;s basically impossible for me to fail, but also to a certain extent it&amp;#39;s impossible for me succeed.&lt;/p&gt;\n\n&lt;p&gt;Has anyone dealt projects like this? How did you handle them/yourself while going through them. I&amp;#39;m encountering a fair amount of mental distress as this never-ending projects drags from months into years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1avccpj", "is_robot_indexable": true, "report_reasons": null, "author": "shittypaintjpeg", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avccpj/advice_on_dealing_with_impossible_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avccpj/advice_on_dealing_with_impossible_projects/", "subreddit_subscribers": 162248, "created_utc": 1708415981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data analyst, mostly developing ETL pipelines with Python and SQL, lots of data transformations with pandas. I don't generate any business insights with my work. In accounting/audit, there's not much insight to derive from. I think I would like a more business analytics role with stats, but I also enjoy pure coding and database work. Both data engineering and data science at the highest level fascinate me, at this rate I think I'm going down the data engineering path. Can anyone share some insight, anyone in similar situations? Does anyone ever think about going for a more client/business facing role as opposed to the behind the scenes data engineering work?", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why data engineering over business insights?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av4a9j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708390348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analyst, mostly developing ETL pipelines with Python and SQL, lots of data transformations with pandas. I don&amp;#39;t generate any business insights with my work. In accounting/audit, there&amp;#39;s not much insight to derive from. I think I would like a more business analytics role with stats, but I also enjoy pure coding and database work. Both data engineering and data science at the highest level fascinate me, at this rate I think I&amp;#39;m going down the data engineering path. Can anyone share some insight, anyone in similar situations? Does anyone ever think about going for a more client/business facing role as opposed to the behind the scenes data engineering work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1av4a9j", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av4a9j/why_data_engineering_over_business_insights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av4a9j/why_data_engineering_over_business_insights/", "subreddit_subscribers": 162248, "created_utc": 1708390348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to build my own data lake for some DE personal projects in AWS. Really just to upskill in data lakes and related technologies (i.e. Iceberg) since I don't have good opportunities to work on these things in my day job.\n\nI found two pretty thorough articles, [one from from Microsoft](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/best-practices/data-lake-zones) and [one from AWS](https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html) on basic data lake organization and infrastructure, and I was wondering whether the community had any opinion on which of these would be the better model to follow when building out a new data lake architecture?\n\nBoth are very similar in design, but I like how the Microsoft design includes the conformance container for data passing QC and converted into the lake format. Other then which design is better, my only other questions are:\n\n1. How many S3 buckets should I use to build this out? 1 per layer? Also in the Microsoft design what's the difference between a lake and a container??\n2. How important is it to actually have a curated/analytics layer? I'm struggling to see how the enriched/stage layer differs from the curated/analytics layer, and I'm hesitant to add complexity before it's needed...", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help deciding on new data lake design for DE personal projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auxvo3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708374961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to build my own data lake for some DE personal projects in AWS. Really just to upskill in data lakes and related technologies (i.e. Iceberg) since I don&amp;#39;t have good opportunities to work on these things in my day job.&lt;/p&gt;\n\n&lt;p&gt;I found two pretty thorough articles, &lt;a href=\"https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/best-practices/data-lake-zones\"&gt;one from from Microsoft&lt;/a&gt; and &lt;a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html\"&gt;one from AWS&lt;/a&gt; on basic data lake organization and infrastructure, and I was wondering whether the community had any opinion on which of these would be the better model to follow when building out a new data lake architecture?&lt;/p&gt;\n\n&lt;p&gt;Both are very similar in design, but I like how the Microsoft design includes the conformance container for data passing QC and converted into the lake format. Other then which design is better, my only other questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How many S3 buckets should I use to build this out? 1 per layer? Also in the Microsoft design what&amp;#39;s the difference between a lake and a container??&lt;/li&gt;\n&lt;li&gt;How important is it to actually have a curated/analytics layer? I&amp;#39;m struggling to see how the enriched/stage layer differs from the curated/analytics layer, and I&amp;#39;m hesitant to add complexity before it&amp;#39;s needed...&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1auxvo3", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auxvo3/need_help_deciding_on_new_data_lake_design_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auxvo3/need_help_deciding_on_new_data_lake_design_for_de/", "subreddit_subscribers": 162248, "created_utc": 1708374961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ctd40uoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Podcast: Using Trino And Iceberg As The Foundation Of Your Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1auxetw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-4Dik-FyeSJpSLw-jbXD2GjKV7cWZOWsUwh84W2XrY0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708373870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringpodcast.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dataengineeringpodcast.com/starburst-trino-iceberg-data-lakehouse-episode-413", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?auto=webp&amp;s=2120aa9314091ca630e69926832b5fbc04ca756f", "width": 1400, "height": 1400}, "resolutions": [{"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aaf640634e9da8c03b7edc9f09ca7e31f6aee921", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d88217a29fa71c5c662c7d324035df8ba72fecca", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0034038dbb6753ab8fb8490b14b6bf2819656cf3", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8b617a7c6583a41fb510d3334de668dec2a38e3", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=235cd7229206b22708fdca688c95c3648c246479", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/wrAtDXyezXa8uJlV1VdXTIX8bGWtYFscPm9kyLMGhTg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9448afa1d602f1ff1c49cf168a73076130d1cb10", "width": 1080, "height": 1080}], "variants": {}, "id": "T69lnyzJjP6GC8oxXuuJPHNQnXgo_9t63cJnvtec6xw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1auxetw", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Relationship-207", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auxetw/data_engineering_podcast_using_trino_and_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dataengineeringpodcast.com/starburst-trino-iceberg-data-lakehouse-episode-413", "subreddit_subscribers": 162248, "created_utc": 1708373870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was thinking about a POC for AWS MSK and it sort of raised a question in mind: what are your use cases for real time data? If it helps, expand the question to using Kafka in general. \n\nLet me give some quick background and where I\u2019m coming from. I built real-time pipelines a long ago (8+ years). My use case initially was real time product analytics; we had nightly batch jobs, so, at the time, this was a significant improvement. \n\nHowever, since then, we have tools like Segment that handle most use cases. Other than bringing Segment in-house to help reduce costs, why would one build their own realtime pipeline instead of leveraging existing tools? I thought about multiple consumers in Kafka, but Segment takes care of that as well, e.g. you can use Segment to send data to your DW, to Amplitude and to your CRM tool, all in near real-time. \n\nOther than a couple of companies I worked at, the data volume is not that significant, i.e. 100\u2019s of TBs, not PBs, so the tools generally work fine. Consequently, as a leader, I tend to lean more towards using tools as they help increase team velocity and I have the team focus on stakeholder needs &amp; deriving insights from the data, more than the data pipelines themselves. \n\nGiven all that, I\u2019m curious about your use cases and why you chose to build a realtime pipeline yourself instead of leveraging an existing tool like Segment?", "author_fullname": "t2_16b1dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your use case for a realtime data pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av0oth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708381460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about a POC for AWS MSK and it sort of raised a question in mind: what are your use cases for real time data? If it helps, expand the question to using Kafka in general. &lt;/p&gt;\n\n&lt;p&gt;Let me give some quick background and where I\u2019m coming from. I built real-time pipelines a long ago (8+ years). My use case initially was real time product analytics; we had nightly batch jobs, so, at the time, this was a significant improvement. &lt;/p&gt;\n\n&lt;p&gt;However, since then, we have tools like Segment that handle most use cases. Other than bringing Segment in-house to help reduce costs, why would one build their own realtime pipeline instead of leveraging existing tools? I thought about multiple consumers in Kafka, but Segment takes care of that as well, e.g. you can use Segment to send data to your DW, to Amplitude and to your CRM tool, all in near real-time. &lt;/p&gt;\n\n&lt;p&gt;Other than a couple of companies I worked at, the data volume is not that significant, i.e. 100\u2019s of TBs, not PBs, so the tools generally work fine. Consequently, as a leader, I tend to lean more towards using tools as they help increase team velocity and I have the team focus on stakeholder needs &amp;amp; deriving insights from the data, more than the data pipelines themselves. &lt;/p&gt;\n\n&lt;p&gt;Given all that, I\u2019m curious about your use cases and why you chose to build a realtime pipeline yourself instead of leveraging an existing tool like Segment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1av0oth", "is_robot_indexable": true, "report_reasons": null, "author": "jawabdey", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av0oth/what_is_your_use_case_for_a_realtime_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av0oth/what_is_your_use_case_for_a_realtime_data_pipeline/", "subreddit_subscribers": 162248, "created_utc": 1708381460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? ", "author_fullname": "t2_17a3gi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you understand by \u201eSQL Basics\u201d and \u201ePython Basics\u201d in CV, what exact skills would you expect from that person?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avi583", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708437174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for internships/entry-level/junior positions in various office jobs, exact positions are not important right now. In my CV I have listed \u201eSQL Basics\u201d and \u201ePython Basics\u201d under my skills section, I am still learning. What would you understand by that, what exact skills would you expect from me, and what you wouldn\u2019t require from someone with \u201ebasic\u201d skills? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1avi583", "is_robot_indexable": true, "report_reasons": null, "author": "ItsGonnaBeGreatYear", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avi583/what_would_you_understand_by_sql_basics_and/", "subreddit_subscribers": 162248, "created_utc": 1708437174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nSo, I am currently working on a problem where I have to find duplicates in my data. The data has columns C1, C2, C3 and T. T stores timestamps. The received data is not sorted by timestamp. A duplicate is defined as when columns C1, C2 and C3 have same value but column T should have time difference of less than 120 seconds. The duplicates should be returned in groups.\n\nE.g.\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xyx\", C2:\"aba\", C3: \"ded\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:01.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:01:05.000\"`\n\n`C1:\"aaa\", C2:\"bbb\", C3: \"ccc\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:02:10.000\"`\n\nThe output for this would be:\n\n`[[C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:00:00.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:01:05.000\"`\n\n`C1:\"xyz\", C2:\"abc\", C3: \"def\", \"T:2024-02-10T12:02:10.000\"]`\n\n`[C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:00.000\"`\n\n`C1:\"xxx\", C2:\"yyy\", C3: \"zzz\", \"T:2024-02-10T12:05:01.000\"]]`\n\nMy current solution for small data does not use Pandas or Spark and works by sorting the data over T column, then grouping them on (C1, C2, C3) and removing any duplicates which do not uphold the time difference criteria. And finally,, sorting the discovered duplicate groups over T again.\n\nHow would I approach this problem when I have say 1 billion rows? Should I use Spark or process with pandas in batches? I am not able to get an intuition of how sorting and grouping would work when the data is distributed or processed in batches?\n\n", "author_fullname": "t2_bmxda7ioc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach this problem for very large datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1averfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708430799.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708425818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;So, I am currently working on a problem where I have to find duplicates in my data. The data has columns C1, C2, C3 and T. T stores timestamps. The received data is not sorted by timestamp. A duplicate is defined as when columns C1, C2 and C3 have same value but column T should have time difference of less than 120 seconds. The duplicates should be returned in groups.&lt;/p&gt;\n\n&lt;p&gt;E.g.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyx&amp;quot;, C2:&amp;quot;aba&amp;quot;, C3: &amp;quot;ded&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:01.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:01:05.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;aaa&amp;quot;, C2:&amp;quot;bbb&amp;quot;, C3: &amp;quot;ccc&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:02:10.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The output for this would be:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[[C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:00:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:01:05.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xyz&amp;quot;, C2:&amp;quot;abc&amp;quot;, C3: &amp;quot;def&amp;quot;, &amp;quot;T:2024-02-10T12:02:10.000&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:00.000&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;C1:&amp;quot;xxx&amp;quot;, C2:&amp;quot;yyy&amp;quot;, C3: &amp;quot;zzz&amp;quot;, &amp;quot;T:2024-02-10T12:05:01.000&amp;quot;]]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;My current solution for small data does not use Pandas or Spark and works by sorting the data over T column, then grouping them on (C1, C2, C3) and removing any duplicates which do not uphold the time difference criteria. And finally,, sorting the discovered duplicate groups over T again.&lt;/p&gt;\n\n&lt;p&gt;How would I approach this problem when I have say 1 billion rows? Should I use Spark or process with pandas in batches? I am not able to get an intuition of how sorting and grouping would work when the data is distributed or processed in batches?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1averfs", "is_robot_indexable": true, "report_reasons": null, "author": "Even_Work_7995", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1averfs/how_would_you_approach_this_problem_for_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1averfs/how_would_you_approach_this_problem_for_very/", "subreddit_subscribers": 162248, "created_utc": 1708425818.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks!   \n[https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)  \n\n\n I'm Subin, co-founder at Multiwoven .Multiwoven is a OSS reverse ETL platform that helps dev &amp; data teams to sync data from databases to business tools. Multiwoven is built using Ruby on Rails . Our data sync orchestration is built on top of Temporal using temporal-ruby SDK.I would greatly appreciate any feedback. Our codebase is available at Github. Please star us to get updates.", "author_fullname": "t2_9bibppzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiwoven - Open-source reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avew2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708426332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks!&lt;br/&gt;\n&lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;https://github.com/Multiwoven/multiwoven&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Subin, co-founder at Multiwoven .Multiwoven is a OSS reverse ETL platform that helps dev &amp;amp; data teams to sync data from databases to business tools. Multiwoven is built using Ruby on Rails . Our data sync orchestration is built on top of Temporal using temporal-ruby SDK.I would greatly appreciate any feedback. Our codebase is available at Github. Please star us to get updates.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?auto=webp&amp;s=7bee046b00e43a80d51ec06b5b03fab8fe50e8a6", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c820e531bd69e73f8ab0a6ae0beacd99c2b46eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c80be54ff0c4e74ab2f7726675fcebd3a845b46", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ac265f606906d37f473536da1c0f1402f557f04", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f31b53a48c8d26b2abdbd514797bd69ea9539bcf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d338fcd1ab24aecad9b1dcf39d5649f7cff2a29", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aom3cnBNH14gDW_dvebtl5pxJssgJVZB1OrLyKDYO9s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3be250d32cd9b8a61ac25ca44aa1a3ab73cb70a", "width": 1080, "height": 540}], "variants": {}, "id": "KXdCRFA3PFw6uZLyGfHzBPQBTSPbN50XvrsQE2m5iOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1avew2o", "is_robot_indexable": true, "report_reasons": null, "author": "BobcatOutrageous2152", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avew2o/multiwoven_opensource_reverse_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avew2o/multiwoven_opensource_reverse_etl/", "subreddit_subscribers": 162248, "created_utc": 1708426332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interesting writeup from Grab on the evolution of their architecture and what drove it, also covers the tool stack\n\n[https://engineering.grab.com/attribution-platform](https://engineering.grab.com/attribution-platform)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ad-attribution toolstack and architecture evolution Kappa -&gt; Lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ave5me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708423386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interesting writeup from Grab on the evolution of their architecture and what drove it, also covers the tool stack&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://engineering.grab.com/attribution-platform\"&gt;https://engineering.grab.com/attribution-platform&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?auto=webp&amp;s=d7f3d72292945fabad7e425d6dac8116bc8d6e8e", "width": 820, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb2280b5de657ab5aa7ccf3f8db3ffe8fa589c64", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9eb75e1d7b5d9afd7cc95933569c8da0e34d9cbb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f45878784b7396c1a4b9f8a18f3451d77b8aaabc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dus5HaHSWGwIWNsjetAVnCinorLaf2mk2zT9edtnT6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b9b9331c902ce00b15be8ab905704d531385cf7", "width": 640, "height": 320}], "variants": {}, "id": "NcKcYtUrZALKPPo2SThdpWDUuQIC8J5ZFrY4ANzdEiA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ave5me", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ave5me/adattribution_toolstack_and_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ave5me/adattribution_toolstack_and_architecture/", "subreddit_subscribers": 162248, "created_utc": 1708423386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your suggestions for \"exceptional\" requirements gathering and test cases for data tables and analytical views for a GCP implementation? At a superficial level, I understand both to be straightforward.\n\n**Requirements gathering**: I work with end-users to document their needs. The documentation is used to develop some portion of the data architecture. In my case, we're using the requirements to build analytical views.\n\n**Test-case development**: This is a bit trickier for me as I'm accustomed to developing unit tests. I understand that a different approach is required with data lakes and data pipelines.\n\nI believe that these two tasks are frequently underappreciated. What should I do to ensure that my team/I perform these tasks at an above-average level? What details are commonly overlooked?\n\n&amp;#x200B;\n\nP.S. I asked this question on a subreddit, but I am curious to hear other's thoughts\n\n&amp;#x200B;\n\nEdit: Link to my initial question and ithinkiboughtadingo's insightful response - [https://www.reddit.com/r/dataengineering/comments/1au9s4s/comment/kr6p7a6/?context=3](https://www.reddit.com/r/dataengineering/comments/1au9s4s/comment/kr6p7a6/?context=3)", "author_fullname": "t2_6cc9wqbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for Requirements Gathering and Test Cases in Building/Testing Data Tables and Analytical Views for Cloud Migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1av5w74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708441009.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708394778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your suggestions for &amp;quot;exceptional&amp;quot; requirements gathering and test cases for data tables and analytical views for a GCP implementation? At a superficial level, I understand both to be straightforward.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Requirements gathering&lt;/strong&gt;: I work with end-users to document their needs. The documentation is used to develop some portion of the data architecture. In my case, we&amp;#39;re using the requirements to build analytical views.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Test-case development&lt;/strong&gt;: This is a bit trickier for me as I&amp;#39;m accustomed to developing unit tests. I understand that a different approach is required with data lakes and data pipelines.&lt;/p&gt;\n\n&lt;p&gt;I believe that these two tasks are frequently underappreciated. What should I do to ensure that my team/I perform these tasks at an above-average level? What details are commonly overlooked?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. I asked this question on a subreddit, but I am curious to hear other&amp;#39;s thoughts&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: Link to my initial question and ithinkiboughtadingo&amp;#39;s insightful response - &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1au9s4s/comment/kr6p7a6/?context=3\"&gt;https://www.reddit.com/r/dataengineering/comments/1au9s4s/comment/kr6p7a6/?context=3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1av5w74", "is_robot_indexable": true, "report_reasons": null, "author": "Cultured_dude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1av5w74/suggestions_for_requirements_gathering_and_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1av5w74/suggestions_for_requirements_gathering_and_test/", "subreddit_subscribers": 162248, "created_utc": 1708394778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n['](https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01)\n\n**Multiwoven** is built for **data teams** \\- *engineers*, *analysts*, *ops, etc*. - to easily prepare and sync data to business tools.\n\nWe\u2019re **open-source** (so you don\u2019t have to pay to move your own data to your own tools), **self-hosted** (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of **Multiwoven**, for your own needs)\n\n[**https://github.com/Multiwoven/multiwoven**](https://github.com/Multiwoven/multiwoven)\n\nStar it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) ", "author_fullname": "t2_rtrd3q97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched Multiwoven - an open-source Reverse ETL platform!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ec2bl2oberjc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe86d0b5ca53c1dd9e0ad579158f73eb36bdc3c"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62c60bf3aa7c430b5662e778781118fcdd5844f6"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=efc980e0a347d5d8e365eb1f775fef20d188f4cf"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3758c4feeef9986b3896ee00413668180b7d165b"}, {"y": 480, "x": 960, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3322c7e225a2a3d82e63ba5637c27f349d05bff9"}, {"y": 540, "x": 1080, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6701a1f01a4abff464adae7daa870e6d0422fbcb"}], "s": {"y": 1280, "x": 2560, "u": "https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=31fbcafee400f17cba29297c7360a598feacef01"}, "id": "ec2bl2oberjc1"}}, "name": "t3_1avkgw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ic6Skc0tts236_84heUlo76cJufFHxdjWGqEdY50b1M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708443255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ec2bl2oberjc1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31fbcafee400f17cba29297c7360a598feacef01\"&gt;&amp;#39;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Multiwoven&lt;/strong&gt; is built for &lt;strong&gt;data teams&lt;/strong&gt; - &lt;em&gt;engineers&lt;/em&gt;, &lt;em&gt;analysts&lt;/em&gt;, &lt;em&gt;ops, etc&lt;/em&gt;. - to easily prepare and sync data to business tools.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re &lt;strong&gt;open-source&lt;/strong&gt; (so you don\u2019t have to pay to move your own data to your own tools), &lt;strong&gt;self-hosted&lt;/strong&gt; (so you are in full control of your own customers\u2019 data), and easily extensible (so you can modify or build on top of &lt;strong&gt;Multiwoven&lt;/strong&gt;, for your own needs)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;&lt;strong&gt;https://github.com/Multiwoven/multiwoven&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Star it \u2b50 and try it out - we\u2019re eager for feedback (documentation, ease of deployment, use cases, source and destination connectors, etc.) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1avkgw5", "is_robot_indexable": true, "report_reasons": null, "author": "nagstler", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avkgw5/we_just_launched_multiwoven_an_opensource_reverse/", "subreddit_subscribers": 162248, "created_utc": 1708443255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Versioning, Cataloging, and Decommissioning Data Products", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1avh111", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gB_ZjJKC3197XhodvkkYWTDrjINSJ_rc0q8f7ibLLaQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708433867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?auto=webp&amp;s=b45c9b290ad8b6c7c2763d4ba3fc9f7d69a4d9d8", "width": 700, "height": 378}, "resolutions": [{"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e1fbb5dbacab22ce5c5bdb3a0ac1d16a6ee6be", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=226b1f43fa2918ebe8b0a3dd752f37c180ad5c67", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9a0ba30cb74687bc66030c20d31517a641216bf", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/MmyYDSrycbY7HMY_ZiBcFTZzei7DLAEN2Maxh8CzP88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=133772bc0f8d87f8a49b52a21cd9f6da38b18adc", "width": 640, "height": 345}], "variants": {}, "id": "pJ8tPpgSlQhmlrsiqx50_VPVXrM4UCN658AcFCzyXbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1avh111", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avh111/versioning_cataloging_and_decommissioning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/managing-the-evolving-data-products-landscape-p2", "subreddit_subscribers": 162248, "created_utc": 1708433867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to achieve at least once processing guarantee while consuming from kafka without checkpointing ?Duplicate messages are accepted in sink.The only requiremere here is no data loss. Like when the consumer polls the record from kafka and it fails in between without processing completely, then the record should get replayed.Is there a way we can achieve without checkpointing enabled ?\n\nEdit:\n\nFlink can guarantee exactly-once state updates to user-defined state only when the source participates in the snapshotting mechanismsource flink doc: [https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/connectors/guarantees.html](https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/connectors/guarantees.html)Does this mean we can achieve at least once without checkpointing ? I didn't find any specific requirement for at least once processing.", "author_fullname": "t2_aqmxwdoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink at least once Processing without checkpointing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1avap0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708413443.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708409734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to achieve at least once processing guarantee while consuming from kafka without checkpointing ?Duplicate messages are accepted in sink.The only requiremere here is no data loss. Like when the consumer polls the record from kafka and it fails in between without processing completely, then the record should get replayed.Is there a way we can achieve without checkpointing enabled ?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Flink can guarantee exactly-once state updates to user-defined state only when the source participates in the snapshotting mechanismsource flink doc: &lt;a href=\"https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/connectors/guarantees.html\"&gt;https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/connectors/guarantees.html&lt;/a&gt;Does this mean we can achieve at least once without checkpointing ? I didn&amp;#39;t find any specific requirement for at least once processing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1avap0i", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingParsnip89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1avap0i/flink_at_least_once_processing_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1avap0i/flink_at_least_once_processing_without/", "subreddit_subscribers": 162248, "created_utc": 1708409734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for suggestions on an (ideally opens source) tool to help setup an extract and load process between two databases. I have a JDBC driver for the source database, and the target database is postgresql.\n\nThe number of tables is quite large, though the volume of data itself is not that large. Additionally, the schema in the source database will regularly change. Is there any tool that will auto-create and update the schema on the second database? Ideally with options for incremental/complete replication?", "author_fullname": "t2_dei36", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract and load tool with target table creation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1auzk3d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708378834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for suggestions on an (ideally opens source) tool to help setup an extract and load process between two databases. I have a JDBC driver for the source database, and the target database is postgresql.&lt;/p&gt;\n\n&lt;p&gt;The number of tables is quite large, though the volume of data itself is not that large. Additionally, the schema in the source database will regularly change. Is there any tool that will auto-create and update the schema on the second database? Ideally with options for incremental/complete replication?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1auzk3d", "is_robot_indexable": true, "report_reasons": null, "author": "corycorycory09", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1auzk3d/extract_and_load_tool_with_target_table_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1auzk3d/extract_and_load_tool_with_target_table_creation/", "subreddit_subscribers": 162248, "created_utc": 1708378834.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}