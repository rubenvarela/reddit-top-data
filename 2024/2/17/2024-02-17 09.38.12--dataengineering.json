{"kind": "Listing", "data": {"after": "t3_1asa273", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I created this Jupyter Notebook when I started learning PySpark, intended as a cheat sheet for me when working with it. Since I started learning PySpark with the book \"Data Analysis with Python and PySpark\", this notebook can be seen as my learning notes focused on practical coding. If you want to fully understand PySpark, I highly recommend reading the book. Originally, I put it on Kaggle. I rediscovered it recently and want to share it. It can be a good starting point for beginners who want to learn PySpark.\n\nLink to notebook: [https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377](https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377)\n\nSome related posts that expand/explain the notebook:\n\n[PySpark common operations](https://note.datengineer.dev/posts/a-practical-pyspark-tutorial-for-beginners-in-jupyter-notebook/)\n\n[PySpark UDFs](https://note.datengineer.dev/posts/pyspark-udfs-a-comprehensive-guide-to-unlock-pyspark-potential/)", "author_fullname": "t2_19ex2hl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Tutorial in Jupyter Notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asc1gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708100908.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708099128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created this Jupyter Notebook when I started learning PySpark, intended as a cheat sheet for me when working with it. Since I started learning PySpark with the book &amp;quot;Data Analysis with Python and PySpark&amp;quot;, this notebook can be seen as my learning notes focused on practical coding. If you want to fully understand PySpark, I highly recommend reading the book. Originally, I put it on Kaggle. I rediscovered it recently and want to share it. It can be a good starting point for beginners who want to learn PySpark.&lt;/p&gt;\n\n&lt;p&gt;Link to notebook: &lt;a href=\"https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377\"&gt;https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some related posts that expand/explain the notebook:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://note.datengineer.dev/posts/a-practical-pyspark-tutorial-for-beginners-in-jupyter-notebook/\"&gt;PySpark common operations&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://note.datengineer.dev/posts/pyspark-udfs-a-comprehensive-guide-to-unlock-pyspark-potential/\"&gt;PySpark UDFs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?auto=webp&amp;s=9ae035fbdcd6bb503ab0b4a605b8db6de46647ee", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9bcab7b79864ff27bf48116cb335a6f825bfb124", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4e925345605c644eebe8abd69916915fc4fbcf7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=614b06d5b40c890a59e355191a6e2d75cdf50789", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5f4a30974a8e6bad0d617a79935bc70c954e3e8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=476793be11eaac4604b6b0c938b45c7c3b52d450", "width": 1080, "height": 540}], "variants": {}, "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1asc1gj", "is_robot_indexable": true, "report_reasons": null, "author": "ImportantA", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asc1gj/pyspark_tutorial_in_jupyter_notebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asc1gj/pyspark_tutorial_in_jupyter_notebook/", "subreddit_subscribers": 161292, "created_utc": 1708099128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Leetcode SQL problems? Review concepts like window functions, joins, etc?\n\nI wrote tons of SQL in a past job but it was data modeling heavy and more \"practical\" than what these interview seem to be asking.", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you prep for SQL heavy technical rounds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asim63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708115119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Leetcode SQL problems? Review concepts like window functions, joins, etc?&lt;/p&gt;\n\n&lt;p&gt;I wrote tons of SQL in a past job but it was data modeling heavy and more &amp;quot;practical&amp;quot; than what these interview seem to be asking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1asim63", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asim63/how_do_you_prep_for_sql_heavy_technical_rounds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asim63/how_do_you_prep_for_sql_heavy_technical_rounds/", "subreddit_subscribers": 161292, "created_utc": 1708115119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is a lot of chaos in DE field with so many tech stacks and alternatives available it gets overwhelming so the purpose of this blog is to simplify just that.\n\n**Tech Stack Needed:**\n\n1. SQL\n2. Azure Data Factory (ADF)\n3. Spark Theoretical Knowledge\n4. Python (On a basic level)\n5. PySpark (Java and Scala Variants will also do)\n6. Power BI (Optional, some companies ask but it's not a mandatory must know thing, you'll be fine even if you don't know)\n\nThe tech stack I mentioned above is the order in which I feel you should learn things and you will find the reason about that below along with that let's also see what we'll be using those components for to get an idea about how much time we should spend studying them.\n\n**Tech Stack Use Cases and no. of days to be spent learning:**\n\n1. **SQL**: SQL is the core of DE, whatever transformations you are going to do, even if you are using pyspark, you will need to know SQL. So I will recommend solving at least 1 SQL problem everyday and really understand the logic behind them, trust me good query writing skills in SQL is a must! **\\[No. of days to learn: Keep practicing till you get a new job\\]**  \n\n2. **ADF**: This will be used just as an orchestration tool, so I will recommend just going through the videos initially, understand high level concepts like Integration runtime, linked services, datasets, activities, trigger types, parameterization of flow and on a very high level get an idea about the different relevant activities available. I highly recommend not going through the data flow videos as almost no one uses them or asks about them, so you'll be wasting your time.**\\[No. of days to learn: Initially 1-2 weeks should be enough to get a high level understanding\\]**  \n\n3. **Spark Theoretical Knowledge**: Your entire big data flow will be handled by spark and its clusters so understanding how spark internal works is more important before learning how to write queries in pyspark. Concepts such as spark architecture, catalyst optimizer, AQE, data skew and how to handle it, join strategies, how to optimize or troubleshoot long running queries are a must know for you to clear your interviews. **\\[No. of days to learn: 2-3 weeks\\]**  \n\n4. **Python**: You do not need to know OOP or have a excellent hand at writing code, but basic things like functions, variables, loops, inbuilt data structures like list, tuple, dictionary, set are a must know. Solving string and list based question should also be done on a regular basis. After that we can move on to some modules, file handling, exception handling, etc. **\\[No. of days to learn: 2 weeks\\]**  \n\n5. **PySpark**: Finally start writing queries in pyspark. It's almost SQL just with a couple of dot notations so once you get familiar with syntax and after couple of days of writing queries in this you should be comfortable working in it. **\\[No. of days to learn: 2 weeks\\]**  \n\n6. **Other Components**: CI/CD, DataBricks, ADLS, monitoring, etc, this can be covered on ad hoc basis and I'll make a detailed post on this later.\n\nPlease note the number of days mentioned will vary for each individual and this is just a high level plan to get you comfortable with the components. Once you are comfortable you will need to revise and practice so you don't forget things and feel really comfortable. Also, this blog is just an overview at a very high level, I will get into details of each component along with resources in the upcoming blogs.\n\n&amp;#x200B;\n\nBonus: [https://www.youtube.com/@TybulOnAzure](https://www.youtube.com/@TybulOnAzure)Above channel is a gold mine for data engineers, it may be a DP-203 playlist but his videos will be of immense help as he really teaches things on a grass root level so highly recommend following him.\n\n[Original Post link to get to other blogs](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\n&amp;#x200B;\n\nPlease do let me know how you felt about this blog, if there are any improvements you would like to see or if there is anything you would like me to post about.\n\nThank You..!!", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog 1 - Structured Way to Study and Get into Azure DE role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asegcy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708106553.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708104912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a lot of chaos in DE field with so many tech stacks and alternatives available it gets overwhelming so the purpose of this blog is to simplify just that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tech Stack Needed:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL&lt;/li&gt;\n&lt;li&gt;Azure Data Factory (ADF)&lt;/li&gt;\n&lt;li&gt;Spark Theoretical Knowledge&lt;/li&gt;\n&lt;li&gt;Python (On a basic level)&lt;/li&gt;\n&lt;li&gt;PySpark (Java and Scala Variants will also do)&lt;/li&gt;\n&lt;li&gt;Power BI (Optional, some companies ask but it&amp;#39;s not a mandatory must know thing, you&amp;#39;ll be fine even if you don&amp;#39;t know)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The tech stack I mentioned above is the order in which I feel you should learn things and you will find the reason about that below along with that let&amp;#39;s also see what we&amp;#39;ll be using those components for to get an idea about how much time we should spend studying them.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tech Stack Use Cases and no. of days to be spent learning:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;SQL&lt;/strong&gt;: SQL is the core of DE, whatever transformations you are going to do, even if you are using pyspark, you will need to know SQL. So I will recommend solving at least 1 SQL problem everyday and really understand the logic behind them, trust me good query writing skills in SQL is a must! &lt;strong&gt;[No. of days to learn: Keep practicing till you get a new job]&lt;/strong&gt;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;ADF&lt;/strong&gt;: This will be used just as an orchestration tool, so I will recommend just going through the videos initially, understand high level concepts like Integration runtime, linked services, datasets, activities, trigger types, parameterization of flow and on a very high level get an idea about the different relevant activities available. I highly recommend not going through the data flow videos as almost no one uses them or asks about them, so you&amp;#39;ll be wasting your time.&lt;strong&gt;[No. of days to learn: Initially 1-2 weeks should be enough to get a high level understanding]&lt;/strong&gt;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Spark Theoretical Knowledge&lt;/strong&gt;: Your entire big data flow will be handled by spark and its clusters so understanding how spark internal works is more important before learning how to write queries in pyspark. Concepts such as spark architecture, catalyst optimizer, AQE, data skew and how to handle it, join strategies, how to optimize or troubleshoot long running queries are a must know for you to clear your interviews. &lt;strong&gt;[No. of days to learn: 2-3 weeks]&lt;/strong&gt;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;: You do not need to know OOP or have a excellent hand at writing code, but basic things like functions, variables, loops, inbuilt data structures like list, tuple, dictionary, set are a must know. Solving string and list based question should also be done on a regular basis. After that we can move on to some modules, file handling, exception handling, etc. &lt;strong&gt;[No. of days to learn: 2 weeks]&lt;/strong&gt;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;PySpark&lt;/strong&gt;: Finally start writing queries in pyspark. It&amp;#39;s almost SQL just with a couple of dot notations so once you get familiar with syntax and after couple of days of writing queries in this you should be comfortable working in it. &lt;strong&gt;[No. of days to learn: 2 weeks]&lt;/strong&gt;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Other Components&lt;/strong&gt;: CI/CD, DataBricks, ADLS, monitoring, etc, this can be covered on ad hoc basis and I&amp;#39;ll make a detailed post on this later.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please note the number of days mentioned will vary for each individual and this is just a high level plan to get you comfortable with the components. Once you are comfortable you will need to revise and practice so you don&amp;#39;t forget things and feel really comfortable. Also, this blog is just an overview at a very high level, I will get into details of each component along with resources in the upcoming blogs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Bonus: &lt;a href=\"https://www.youtube.com/@TybulOnAzure\"&gt;https://www.youtube.com/@TybulOnAzure&lt;/a&gt;Above channel is a gold mine for data engineers, it may be a DP-203 playlist but his videos will be of immense help as he really teaches things on a grass root level so highly recommend following him.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Original Post link to get to other blogs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Please do let me know how you felt about this blog, if there are any improvements you would like to see or if there is anything you would like me to post about.&lt;/p&gt;\n\n&lt;p&gt;Thank You..!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?auto=webp&amp;s=b89998b8009f6ed380e4ddb97e465419e3896c64", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca01c758d5fa5b44264cfb0498b961588cdb4208", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=16334c3bf943dfe4ace7ad968d2dc11b89a61eae", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da069911f236610bd9b661cde061bbdda31797a8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/4bf4SF2ZgZLMTGq_l0ci3YLAnxFA32UNO2kwP_yDmV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f6bc143bf4207f73245b25a20e679cacc056cae", "width": 640, "height": 640}], "variants": {}, "id": "Y_tM-phmGdqUx-mShSmQnJRgDvBVdeU-A9Y4N-Jk4yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1asegcy", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asegcy/blog_1_structured_way_to_study_and_get_into_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asegcy/blog_1_structured_way_to_study_and_get_into_azure/", "subreddit_subscribers": 161292, "created_utc": 1708104912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. As per the management, they have a healthy balance sheet, it\u2019s just that the company isn\u2019t big enough to participate in larger deals, hence the acquisition.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current company got acquired by bigger company. Should I be worried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as7t57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708087805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. As per the management, they have a healthy balance sheet, it\u2019s just that the company isn\u2019t big enough to participate in larger deals, hence the acquisition.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1as7t57", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as7t57/current_company_got_acquired_by_bigger_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as7t57/current_company_got_acquired_by_bigger_company/", "subreddit_subscribers": 161292, "created_utc": 1708087805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\nI'm currently working on a project where we're utilizing Azure Data Lake Storage (ADLS) Gen 2 within Databricks. We've set up our mount points for ADLS Gen 2 using DBFS functions, and we're using abfss for the path in those functions.\n\nHowever I think abfss might be more faster and efficient than dbfs since we're majorly using adla.\n\nI'd love to hear from the community about your experiences and insights:\n\nHave you worked with both ADFSS and DBFS for ADLS Gen 2 in Databricks?\nWhat are the pros and cons you've encountered with each approach?\nIf abfss is faster than how should I use it effectively to give better results than dbfs or vice-versa.\n\nI can't seem to find many articles on that. chatgpt and gemini advanced both dont seem to convince my senior to go with any of that\nThanks in advance for your help!", "author_fullname": "t2_95vfkzya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is better abfss or dbfs in azure databricks for ADLS Gen2?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asbdf4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708097442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI&amp;#39;m currently working on a project where we&amp;#39;re utilizing Azure Data Lake Storage (ADLS) Gen 2 within Databricks. We&amp;#39;ve set up our mount points for ADLS Gen 2 using DBFS functions, and we&amp;#39;re using abfss for the path in those functions.&lt;/p&gt;\n\n&lt;p&gt;However I think abfss might be more faster and efficient than dbfs since we&amp;#39;re majorly using adla.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from the community about your experiences and insights:&lt;/p&gt;\n\n&lt;p&gt;Have you worked with both ADFSS and DBFS for ADLS Gen 2 in Databricks?\nWhat are the pros and cons you&amp;#39;ve encountered with each approach?\nIf abfss is faster than how should I use it effectively to give better results than dbfs or vice-versa.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t seem to find many articles on that. chatgpt and gemini advanced both dont seem to convince my senior to go with any of that\nThanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asbdf4", "is_robot_indexable": true, "report_reasons": null, "author": "TH3R4PIST", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asbdf4/which_is_better_abfss_or_dbfs_in_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asbdf4/which_is_better_abfss_or_dbfs_in_azure_databricks/", "subreddit_subscribers": 161292, "created_utc": 1708097442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wrote this up the other day after talking with a business analyst early in his career looking to get into the data field (either data engineering or data analyst) - focusing on SQL &amp; Python for now. Also, glad to tweak this and make it more useful, so roast my Wiki!", "author_fullname": "t2_8ay6w1hr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting Started with Data Engineering (wiki)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1as8nxe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/WeyGeFlrwxT52btBR7xls17rhIcdranWnuUBDEBzId8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708090307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wrote this up the other day after talking with a business analyst early in his career looking to get into the data field (either data engineering or data analyst) - focusing on SQL &amp;amp; Python for now. Also, glad to tweak this and make it more useful, so roast my Wiki!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/bbrewington/data-tools/wiki/Getting-Started-with-Data-Engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?auto=webp&amp;s=37fedcf34578289a4d02585dc0fc54ceb3355846", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8527a33ec2a467ab6fd1d3d1ea28bfa9e456285", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4317810b19323f5551787695ec2136361d82963", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92aae002ff7a87f11edf6d608ab8974d670d6db5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c801b7329b19bb2fd8861dac8f45c34106db2b7d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d54dd58a2d2e51f689e15b4dd48e15831f096c99", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mu-aHECxSd5xUXAGGsL0vz8QZ6Ggcj7JnELvpcxYuEM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb71aea1d6544270361fffcd050c611e02b222c5", "width": 1080, "height": 540}], "variants": {}, "id": "vhE74cbFHXyowQ-PbIfOgDBv_l3kvB-lx7P7ugk1bQw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1as8nxe", "is_robot_indexable": true, "report_reasons": null, "author": "brent_brewington", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as8nxe/getting_started_with_data_engineering_wiki/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/bbrewington/data-tools/wiki/Getting-Started-with-Data-Engineering", "subreddit_subscribers": 161292, "created_utc": 1708090307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I am a software developer that got put on some analytics tasking. I'm finding myself enjoying the process and enjoying the data side more than just web development. I am trying to \nDesign my own databases now for some ideas have. But I keep getting stuck on where the line is in terms of normalizing the DB. \n\nI.e. store has an address. Create an address table and store that ID in the store table. But addresses have a state field, should I just add the state there or would you create a new state table and store that ID in the address table as state Id?", "author_fullname": "t2_3m3oxyaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much normalization is too much normalization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ashy7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708113484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am a software developer that got put on some analytics tasking. I&amp;#39;m finding myself enjoying the process and enjoying the data side more than just web development. I am trying to \nDesign my own databases now for some ideas have. But I keep getting stuck on where the line is in terms of normalizing the DB. &lt;/p&gt;\n\n&lt;p&gt;I.e. store has an address. Create an address table and store that ID in the store table. But addresses have a state field, should I just add the state there or would you create a new state table and store that ID in the address table as state Id?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ashy7w", "is_robot_indexable": true, "report_reasons": null, "author": "darin_thompson", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ashy7w/how_much_normalization_is_too_much_normalization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ashy7w/how_much_normalization_is_too_much_normalization/", "subreddit_subscribers": 161292, "created_utc": 1708113484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spent the last few months collecting and analyzing historical data from the NBA API. It contains high-quality, real-world data that's both interesting to analyze and great to practice with. \n\nThe experience has been so fun that I turned the project into a publicly available competition!\n\nHere's how the competition works: Participants utilize real NBA data to craft SQL queries, develop dbt\u2122 models, and derive insights, all for a chance to win a $1,500 Amazon gift card.\u00a0  \n\n\nFor more details, check out my corny video below, and register to participate [here](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)!  \n\n\nhttps://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Data Modeling Competition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fyjdyy9w00jc1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/1asi2e5/asset/fyjdyy9w00jc1/DASHPlaylist.mpd?a=1710754691%2CZjI0Y2Q2NDBmZmUxOGM2OWExM2FjZWZhNDViMWY0MjVlNWM2YTljZjU0YTc0MDlkMWQ2OWQ2YTg3YTE4ODBkYw%3D%3D&amp;v=1&amp;f=sd", "x": 1920, "y": 1080, "hlsUrl": "https://v.redd.it/link/1asi2e5/asset/fyjdyy9w00jc1/HLSPlaylist.m3u8?a=1710754691%2CMmU0Njg2ZTkwMmRhYjBiNjU2M2ZiN2VjOTlkODdmZGNlZjQ1MWY3N2IwYjNjYWY1NDZmYmMwMDAyZDY4NmYwYg%3D%3D&amp;v=1&amp;f=sd", "id": "fyjdyy9w00jc1", "isGif": false}}, "name": "t3_1asi2e5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LB1zzxQ7TXcjIPuXMaifM8Hd7JWBsoXtu5fUnZ_t8BI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1708113753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spent the last few months collecting and analyzing historical data from the NBA API. It contains high-quality, real-world data that&amp;#39;s both interesting to analyze and great to practice with. &lt;/p&gt;\n\n&lt;p&gt;The experience has been so fun that I turned the project into a publicly available competition!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how the competition works: Participants utilize real NBA data to craft SQL queries, develop dbt\u2122 models, and derive insights, all for a chance to win a $1,500 Amazon gift card.\u00a0  &lt;/p&gt;\n\n&lt;p&gt;For more details, check out my corny video below, and register to participate &lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;here&lt;/a&gt;!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player\"&gt;https://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?auto=webp&amp;s=0e6a69a173bc7a517e4426fa77b5ce63ca357210", "width": 1376, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa9033a0520a160ebc57d0e3354dcf7483f121d1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d671d735ac9482c5e580511d7090f11baa4e01f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d53e2037de8a232ed72eb0082d7d4dca2f4c4adb", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0df826dfb886bdf8ffa209deb21e0b8c7574fc56", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6aac07197f3a4df7afdbf3bcb69112354ee0e9ab", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efa695d775899cd3edee7829becbbc9d4107af68", "width": 1080, "height": 565}], "variants": {}, "id": "gMycMBUMB23ezrHe91vhkZqeB9cNGVRk3G4g0CuHC7Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1asi2e5", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asi2e5/dbt_data_modeling_competition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asi2e5/dbt_data_modeling_competition/", "subreddit_subscribers": 161292, "created_utc": 1708113753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am thinking about doing either the new DP-600 (Fabric Analytics Engineer) or DP-203 (Azure Data Engineer). Would the DP-600 exam be a good choice since Microsoft seems to be trying to push Fabric over Synapse? Or would the DP-203 be a better choice? Would like to hear your opinions.\n", "author_fullname": "t2_3iljgzjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP-600 vs DP-203: Which one would be better choice? Need advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ast2g5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708145723.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708143428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am thinking about doing either the new DP-600 (Fabric Analytics Engineer) or DP-203 (Azure Data Engineer). Would the DP-600 exam be a good choice since Microsoft seems to be trying to push Fabric over Synapse? Or would the DP-203 be a better choice? Would like to hear your opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ast2g5", "is_robot_indexable": true, "report_reasons": null, "author": "bass581", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ast2g5/dp600_vs_dp203_which_one_would_be_better_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ast2g5/dp600_vs_dp203_which_one_would_be_better_choice/", "subreddit_subscribers": 161292, "created_utc": 1708143428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "These are some questions that I have been thinking about for some time now. This all centers around what to do in the data warehouse vs what to do in the BI Layer. For the BI layer, I am sure this applies to all BI tools but I am primarily referring to PBI.   \n**I am giving very simple examples here but in reality, the data has many granularities, more metrics, and more reports need to be built.**\n\n&amp;#x200B;\n\n1. Do you create YRAGO, 2YRAGO metrics in DBT?\n   1. Let's say you have a table of Date, Item, and Sale Amount (*Appendix Table A*). Would it make sense to create another column for YRAGO sale amount for that item on that date? I typically do not like to do that and have it done in the PBI layer. What are your thoughts?\n2. Do you create aggregate tables for each major granularity?\n   1. Lets say you have a table of Date, Store, Item, and Sale Amount (*Appendix Table B*). You need to create 3 reports for Sales by Item, Store, and Date. Would you make views/tables that aggregate to each granularity?\n\n**Things I like to optimize for:**\n\n\\- Keeping the BI layer as light as possible, reusability in the DW, not keeping logic in the BI layer.\n\n&amp;#x200B;\n\nAppendix:\n\nTable A:\n\n|Date|Item ID|Sale Amount|\n|:-|:-|:-|\n|1/1/2023|a|100|\n|1/1/2023|b|50|\n|1/2/2023|a|76|\n\nTable B:\n\n|Date|Store ID|Item ID|Sale Amount|\n|:-|:-|:-|:-|\n|1/1/2023|z|a|100|\n|1/1/2023|q|b|50|\n|1/2/2023|w|a|76|\n\n&amp;#x200B;", "author_fullname": "t2_uww96dnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling: What logic to put in DBT vs in BI Layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asq126", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708134277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These are some questions that I have been thinking about for some time now. This all centers around what to do in the data warehouse vs what to do in the BI Layer. For the BI layer, I am sure this applies to all BI tools but I am primarily referring to PBI.&lt;br/&gt;\n&lt;strong&gt;I am giving very simple examples here but in reality, the data has many granularities, more metrics, and more reports need to be built.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you create YRAGO, 2YRAGO metrics in DBT?\n\n&lt;ol&gt;\n&lt;li&gt;Let&amp;#39;s say you have a table of Date, Item, and Sale Amount (&lt;em&gt;Appendix Table A&lt;/em&gt;). Would it make sense to create another column for YRAGO sale amount for that item on that date? I typically do not like to do that and have it done in the PBI layer. What are your thoughts?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Do you create aggregate tables for each major granularity?\n\n&lt;ol&gt;\n&lt;li&gt;Lets say you have a table of Date, Store, Item, and Sale Amount (&lt;em&gt;Appendix Table B&lt;/em&gt;). You need to create 3 reports for Sales by Item, Store, and Date. Would you make views/tables that aggregate to each granularity?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Things I like to optimize for:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Keeping the BI layer as light as possible, reusability in the DW, not keeping logic in the BI layer.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Appendix:&lt;/p&gt;\n\n&lt;p&gt;Table A:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Item ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Sale Amount&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/2/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Table B:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Store ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Item ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Sale Amount&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;z&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;q&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/2/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;w&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asq126", "is_robot_indexable": true, "report_reasons": null, "author": "anon_data_person", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asq126/data_modeling_what_logic_to_put_in_dbt_vs_in_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asq126/data_modeling_what_logic_to_put_in_dbt_vs_in_bi/", "subreddit_subscribers": 161292, "created_utc": 1708134277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im looking to implement an OLAP db in my company. The idea is to pull data from our MongoDB, ETL that shit with mage.ai and push it to an OLAP db where BI can consume it.\n\nData is not really big. Less than 1GB total. May be bigger in the future but not by much.\n\nWhich provider would fit given the costs and without being overkill? Should I just stick with BigQuery?", "author_fullname": "t2_4rpxclpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help deciding an OLAP db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aspdhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708132421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking to implement an OLAP db in my company. The idea is to pull data from our MongoDB, ETL that shit with mage.ai and push it to an OLAP db where BI can consume it.&lt;/p&gt;\n\n&lt;p&gt;Data is not really big. Less than 1GB total. May be bigger in the future but not by much.&lt;/p&gt;\n\n&lt;p&gt;Which provider would fit given the costs and without being overkill? Should I just stick with BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aspdhk", "is_robot_indexable": true, "report_reasons": null, "author": "RedBlueWhiteBlack", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aspdhk/need_help_deciding_an_olap_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aspdhk/need_help_deciding_an_olap_db/", "subreddit_subscribers": 161292, "created_utc": 1708132421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like I may be going down the wrong route to I'm here to ask. I have a lot of situations where I'd like to encrypt a file before storing it on s3. I went ahead and wrote a fastapi endpoint that takes a file upload and a recipient email then returns an encrypted file. This would be available internally as a service over https. \n\nIs there an easier way of centrally managing keys / encryption?", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encryption micro service?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asnwgy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708128441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I may be going down the wrong route to I&amp;#39;m here to ask. I have a lot of situations where I&amp;#39;d like to encrypt a file before storing it on s3. I went ahead and wrote a fastapi endpoint that takes a file upload and a recipient email then returns an encrypted file. This would be available internally as a service over https. &lt;/p&gt;\n\n&lt;p&gt;Is there an easier way of centrally managing keys / encryption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asnwgy", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asnwgy/encryption_micro_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asnwgy/encryption_micro_service/", "subreddit_subscribers": 161292, "created_utc": 1708128441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!  \nI need a bit of help figuring out how to run a docker container in a manner that I used to, but carelessly never documented. As it turns out, it used to be the perfect way to experiment with various Spark jobs.  \nEnvironment: docker on WSL, Windows used for IDE  \nWhat I know for sure is that I used the apache/spark-py image and the container settings allowed me to interact with the spark UI for jobs and everything similar. The container could be run indefinitely detached. It could network with a local postgres database also run in a docker container with an attached volume.\n\nThe setup was a single node cluster, ran exclusively on the wsl docker without docker compose or explicit worker-master relationships.\n\nRunning the spark container produced the spark logo, however my latest attempt does not.\n\nIf anyone out there has any clue about this, please let me know!", "author_fullname": "t2_n75h7s91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot recreate PySpark docker container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asj9op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708116733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI need a bit of help figuring out how to run a docker container in a manner that I used to, but carelessly never documented. As it turns out, it used to be the perfect way to experiment with various Spark jobs.&lt;br/&gt;\nEnvironment: docker on WSL, Windows used for IDE&lt;br/&gt;\nWhat I know for sure is that I used the apache/spark-py image and the container settings allowed me to interact with the spark UI for jobs and everything similar. The container could be run indefinitely detached. It could network with a local postgres database also run in a docker container with an attached volume.&lt;/p&gt;\n\n&lt;p&gt;The setup was a single node cluster, ran exclusively on the wsl docker without docker compose or explicit worker-master relationships.&lt;/p&gt;\n\n&lt;p&gt;Running the spark container produced the spark logo, however my latest attempt does not.&lt;/p&gt;\n\n&lt;p&gt;If anyone out there has any clue about this, please let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1asj9op", "is_robot_indexable": true, "report_reasons": null, "author": "Cheeky-owlet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asj9op/cannot_recreate_pyspark_docker_container/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asj9op/cannot_recreate_pyspark_docker_container/", "subreddit_subscribers": 161292, "created_utc": 1708116733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have been tasked with creating a data warehousing solution (over the next 2 years) for my company. We are a full Microsoft shop, and currently push data around with SSIS (I hate SSIS!). We have multiple db's and some flat file sources, and may leverage API's. I  have never built out a data warehouse from scratch,  so I've been lurking here while doing a bunch of research on various tools. \n\nWhile doing my research I came across this framework: [https://docs.massstreet.net/v/data-warehouse-etl-framework/](https://docs.massstreet.net/v/data-warehouse-etl-framework/), that I think would be a relatively straight forward approach, centered around T-SQL Agent Jobs and Python. However, as I don't have experience with this, I feel like it would be foolish not to at least ask the opinions of how others would approach the job.   \n\n\nAnother tool that has particularly piqued my interest is DBT. From my (admittedly limited) understanding, this a transformation tool that allows for versioning of SQL? Would this be a good addition to my project?   \n\n\nBeyond the tooling, what approach would you take to this project? How would you approach data governance/master data management? (I'm sure I'm overlooking things, so feel free to add any wisdom that I haven't asked for/about specifically!)  \n\n\nPre-emptive Thank You!", "author_fullname": "t2_qx8om", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach creating an on-premises data warehouse in SQL Server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as9hp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708092659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been tasked with creating a data warehousing solution (over the next 2 years) for my company. We are a full Microsoft shop, and currently push data around with SSIS (I hate SSIS!). We have multiple db&amp;#39;s and some flat file sources, and may leverage API&amp;#39;s. I  have never built out a data warehouse from scratch,  so I&amp;#39;ve been lurking here while doing a bunch of research on various tools. &lt;/p&gt;\n\n&lt;p&gt;While doing my research I came across this framework: &lt;a href=\"https://docs.massstreet.net/v/data-warehouse-etl-framework/\"&gt;https://docs.massstreet.net/v/data-warehouse-etl-framework/&lt;/a&gt;, that I think would be a relatively straight forward approach, centered around T-SQL Agent Jobs and Python. However, as I don&amp;#39;t have experience with this, I feel like it would be foolish not to at least ask the opinions of how others would approach the job.   &lt;/p&gt;\n\n&lt;p&gt;Another tool that has particularly piqued my interest is DBT. From my (admittedly limited) understanding, this a transformation tool that allows for versioning of SQL? Would this be a good addition to my project?   &lt;/p&gt;\n\n&lt;p&gt;Beyond the tooling, what approach would you take to this project? How would you approach data governance/master data management? (I&amp;#39;m sure I&amp;#39;m overlooking things, so feel free to add any wisdom that I haven&amp;#39;t asked for/about specifically!)  &lt;/p&gt;\n\n&lt;p&gt;Pre-emptive Thank You!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?auto=webp&amp;s=a24341b98b29b76e320b1bfd493b52ea9f39305c", "width": 2560, "height": 663}, "resolutions": [{"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27841e7f690f25f15160a798ef0beece1c89b889", "width": 108, "height": 27}, {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=357ac3441c3ec205fe9dc5243f1e005731dce189", "width": 216, "height": 55}, {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=520cf6fe4b417f5e905df123859ea10ed7575838", "width": 320, "height": 82}, {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1699378cf02116efc50a9d991cc396a60ede4383", "width": 640, "height": 165}, {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=248596218da30a2adc73fa119d55f3ab8cdc0295", "width": 960, "height": 248}, {"url": "https://external-preview.redd.it/LYszz7_d-uDRAqtPpysc5qDxWX2bhV79eN2Wg4gGk_c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=858c223fb80e70ef2d924904ac042ae8d3290d93", "width": 1080, "height": 279}], "variants": {}, "id": "QvkIa5luhoF7dWuC9UnAMmsS_cGm6kOFo42KPr2tUHI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1as9hp3", "is_robot_indexable": true, "report_reasons": null, "author": "jtdubbs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as9hp3/how_would_you_approach_creating_an_onpremises/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as9hp3/how_would_you_approach_creating_an_onpremises/", "subreddit_subscribers": 161292, "created_utc": 1708092659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometime back, I came across this [post](https://www.reddit.com/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/). I did mention in the [reply](https://www.reddit.com/r/dataengineering/comments/1apkw7y/comment/kq74hfy/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) that I had an unfinished blog on DE and would try to complete it.\n\nSo I started to write and instead of a blog, I decided to make it an open book.\n\n[https://paperplaneflyr.gitbook.io/data-engineering-first-contact/](https://paperplaneflyr.gitbook.io/data-engineering-first-contact/)\n\nMy agenda was to make this for anyone who is just starting in Data Engineering, a seasoned software developer who was pushed into Data Engineering, or students who want to become Data Engineers.\n\nYou may find tons of information on the internet about DE, but how much of it applies to your project? Of course, there is always a give and take of tools and methods but at the core, some principles remain the same.\n\nThis book is about those principles and a bit on the tools. Tools always have documentation, principles are created with experience.\n\nIt's not complete, suggestions are welcome. ", "author_fullname": "t2_aryc45smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering - First Contact", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1aswgfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708155541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometime back, I came across this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/\"&gt;post&lt;/a&gt;. I did mention in the &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1apkw7y/comment/kq74hfy/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;reply&lt;/a&gt; that I had an unfinished blog on DE and would try to complete it.&lt;/p&gt;\n\n&lt;p&gt;So I started to write and instead of a blog, I decided to make it an open book.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://paperplaneflyr.gitbook.io/data-engineering-first-contact/\"&gt;https://paperplaneflyr.gitbook.io/data-engineering-first-contact/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My agenda was to make this for anyone who is just starting in Data Engineering, a seasoned software developer who was pushed into Data Engineering, or students who want to become Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;You may find tons of information on the internet about DE, but how much of it applies to your project? Of course, there is always a give and take of tools and methods but at the core, some principles remain the same.&lt;/p&gt;\n\n&lt;p&gt;This book is about those principles and a bit on the tools. Tools always have documentation, principles are created with experience.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not complete, suggestions are welcome. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?auto=webp&amp;s=1b3d15e2eab54160d1eb8341a8e8a8f8d488df87", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3f121b8a75d093d8ca58faee2a92edcb1ac215d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9a3839d812d45bc7418fef80aff696f5b5d48c8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=092cde4fa2061aa37e79e35e3bf665e47aeff21f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=970aca83490e5c0d671932eee55fa0ae5be9bc0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61ef971cbb4e56f2eaa692513120d5944ac87e86", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62b2aca4584a495a22733bd20205e93294b0200e", "width": 1080, "height": 567}], "variants": {}, "id": "Zypq0-kjIoHeIOLgkPEic9po2a5RmVdTkDYH8npVrHY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aswgfv", "is_robot_indexable": true, "report_reasons": null, "author": "Paperplaneflyr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aswgfv/data_engineering_first_contact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aswgfv/data_engineering_first_contact/", "subreddit_subscribers": 161292, "created_utc": 1708155541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI am looking for some guidance for my containerized Flask app API that I am hosting in Azure. Previously I have hosted internal APIs with Azure Functions and ACR/ACI but never an external API and had some questions. \n\nI am using bash in my console as a makeshift CICD pipeline until I get the infrastructure squared out. So far, once I commit, the script pushes to Docker Hub and Azure Container Registry (ACR) while tagging the image with the git commit ID. It then deletes the existing container group in Azure Container Instance (ACI) and recreates it using the same FQDN.\n\nI want to introduce Azure API Management (APIM) and Front Door (AFD) now that this will be an external facing IP. The idea here is that I can manage authentication/security/rate limits as well as use a custom DNS and point to a prod/test/dev API.\n\nMy issue is that I don't see a way to maintain a static IP on the ACI and of course if I delete and recreate it the IP will change. Then I will have to reconfigure in the APIM every time. Am I going about this wrong? It doesn't seem I can point to the FQDN in this case unfortunately. Ideally, I can rebuild the ACI without losing the IP. If not, I suppose the best practice would be to query the IP and update the corresponding setting in APIM. This won't be too difficult, but it felt like there may be a simpler way.\n\nI am also open to advice on splitting the prod/test/dev in this case. My initial thought was to split at the resource group level so each environment would have their own instance of each service i.e. ACR/ACI/APIM/AFD. This is what I do for internal APIs. ChatGPT is telling me a single instance of APIM and AFD would suffice but I am taking it with a grain of salt.\n\nI understand this may be more of a platform engineering question, but I am in one of those DE positions where I build the data services including the infrastructure and DevOps. Thank you all in advance!", "author_fullname": "t2_326jvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Container Instance IP CICD best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asmjjc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708124914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I am looking for some guidance for my containerized Flask app API that I am hosting in Azure. Previously I have hosted internal APIs with Azure Functions and ACR/ACI but never an external API and had some questions. &lt;/p&gt;\n\n&lt;p&gt;I am using bash in my console as a makeshift CICD pipeline until I get the infrastructure squared out. So far, once I commit, the script pushes to Docker Hub and Azure Container Registry (ACR) while tagging the image with the git commit ID. It then deletes the existing container group in Azure Container Instance (ACI) and recreates it using the same FQDN.&lt;/p&gt;\n\n&lt;p&gt;I want to introduce Azure API Management (APIM) and Front Door (AFD) now that this will be an external facing IP. The idea here is that I can manage authentication/security/rate limits as well as use a custom DNS and point to a prod/test/dev API.&lt;/p&gt;\n\n&lt;p&gt;My issue is that I don&amp;#39;t see a way to maintain a static IP on the ACI and of course if I delete and recreate it the IP will change. Then I will have to reconfigure in the APIM every time. Am I going about this wrong? It doesn&amp;#39;t seem I can point to the FQDN in this case unfortunately. Ideally, I can rebuild the ACI without losing the IP. If not, I suppose the best practice would be to query the IP and update the corresponding setting in APIM. This won&amp;#39;t be too difficult, but it felt like there may be a simpler way.&lt;/p&gt;\n\n&lt;p&gt;I am also open to advice on splitting the prod/test/dev in this case. My initial thought was to split at the resource group level so each environment would have their own instance of each service i.e. ACR/ACI/APIM/AFD. This is what I do for internal APIs. ChatGPT is telling me a single instance of APIM and AFD would suffice but I am taking it with a grain of salt.&lt;/p&gt;\n\n&lt;p&gt;I understand this may be more of a platform engineering question, but I am in one of those DE positions where I build the data services including the infrastructure and DevOps. Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1asmjjc", "is_robot_indexable": true, "report_reasons": null, "author": "ShouldHaveWentBio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asmjjc/azure_container_instance_ip_cicd_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asmjjc/azure_container_instance_ip_cicd_best_practices/", "subreddit_subscribers": 161292, "created_utc": 1708124914.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So basically there's a project requirement on finding a tool/service that can automate business rules transformation.\nThe expectation in detail -\nThere should be a UI through which a business user will submit an excel mapping sheet. Which will then be sent to a storage service and executed to run the ETL based on the requirements.\nThere can be multiple scenarios of business rules (i.e. simple set of mapping from multiple tables to xyz - if x column = a then u column =b, there can be a simple select query which can be used with where clause and then there can be a complex SQL query that can be directly stored to a storage location (s3)\n\nNow is there any tool or services in market which provides this kind of capabilities to execute transformations based on business requirements?", "author_fullname": "t2_7c310271", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3rd party tools for business rules transformation engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asgrtz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708110546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So basically there&amp;#39;s a project requirement on finding a tool/service that can automate business rules transformation.\nThe expectation in detail -\nThere should be a UI through which a business user will submit an excel mapping sheet. Which will then be sent to a storage service and executed to run the ETL based on the requirements.\nThere can be multiple scenarios of business rules (i.e. simple set of mapping from multiple tables to xyz - if x column = a then u column =b, there can be a simple select query which can be used with where clause and then there can be a complex SQL query that can be directly stored to a storage location (s3)&lt;/p&gt;\n\n&lt;p&gt;Now is there any tool or services in market which provides this kind of capabilities to execute transformations based on business requirements?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asgrtz", "is_robot_indexable": true, "report_reasons": null, "author": "Next_Alternative9492", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asgrtz/3rd_party_tools_for_business_rules_transformation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asgrtz/3rd_party_tools_for_business_rules_transformation/", "subreddit_subscribers": 161292, "created_utc": 1708110546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am part of ODSC.com, one of the largest AI and Data Science conferences in the world. It is a great opportunity for training and networking! If anyone is interested in the upcoming Open Data Science Conference and Data Engineering Summit in Boston (or virtual), April 23rd-25th. Happy to share more info for discounts on registration or corporate sponsorship. Thanks!\n", "author_fullname": "t2_tgi8zsk6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODSC East and Data Engineering Summit ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asfqd1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708107978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am part of ODSC.com, one of the largest AI and Data Science conferences in the world. It is a great opportunity for training and networking! If anyone is interested in the upcoming Open Data Science Conference and Data Engineering Summit in Boston (or virtual), April 23rd-25th. Happy to share more info for discounts on registration or corporate sponsorship. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asfqd1", "is_robot_indexable": true, "report_reasons": null, "author": "Jennibird1220", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asfqd1/odsc_east_and_data_engineering_summit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asfqd1/odsc_east_and_data_engineering_summit/", "subreddit_subscribers": 161292, "created_utc": 1708107978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nSo I have a case where data comes to Storage account. However, the catch here is that there are around 2000+ columns in the mapping table and the files sent might not contain all of the columns all the times as different parameters are tracked. The end table should be delta table in Databricks. Any one had similar situations? I have not seen a table with 2K columns. Would it be okay to create such a huge table and then map values to columns as they arrive? What would be good approach here? \n\nThanks! \n", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Table - Lots of Columns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asf5da", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708112377.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708106557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;So I have a case where data comes to Storage account. However, the catch here is that there are around 2000+ columns in the mapping table and the files sent might not contain all of the columns all the times as different parameters are tracked. The end table should be delta table in Databricks. Any one had similar situations? I have not seen a table with 2K columns. Would it be okay to create such a huge table and then map values to columns as they arrive? What would be good approach here? &lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asf5da", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asf5da/delta_table_lots_of_columns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asf5da/delta_table_lots_of_columns/", "subreddit_subscribers": 161292, "created_utc": 1708106557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all... would like to know if you have implemented monitoring or observability over your Glue Jobs and Crawlers. What tool did you use (CloudWatch, Glue API calls) ? What are the relevant metrics to observe ? Would you be willing to share your dashboard so I can have an idea on what to start ? Cheers.", "author_fullname": "t2_791vaf17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you implemented monitoring or observability on Glue Jobs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asbxsm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708098863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all... would like to know if you have implemented monitoring or observability over your Glue Jobs and Crawlers. What tool did you use (CloudWatch, Glue API calls) ? What are the relevant metrics to observe ? Would you be willing to share your dashboard so I can have an idea on what to start ? Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asbxsm", "is_robot_indexable": true, "report_reasons": null, "author": "BlueAcronis", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asbxsm/have_you_implemented_monitoring_or_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asbxsm/have_you_implemented_monitoring_or_observability/", "subreddit_subscribers": 161292, "created_utc": 1708098863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,  \n\n\nI need some help transforming the op field - The op field is currently populated with values like u,r and d but I need the value to show 1 for u and r, and 0 for d.  \n\n\nCan I do this transformation in the connector settings or do I have to use a smt? (smt seems a bit confusing atm)  \n\n\nThe whole project is in docker. Thanks all", "author_fullname": "t2_crdnm9xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transform __op field postgres --&gt; debezuim --&gt; Kaftka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as79pc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708086042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,  &lt;/p&gt;\n\n&lt;p&gt;I need some help transforming the op field - The op field is currently populated with values like u,r and d but I need the value to show 1 for u and r, and 0 for d.  &lt;/p&gt;\n\n&lt;p&gt;Can I do this transformation in the connector settings or do I have to use a smt? (smt seems a bit confusing atm)  &lt;/p&gt;\n\n&lt;p&gt;The whole project is in docker. Thanks all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1as79pc", "is_robot_indexable": true, "report_reasons": null, "author": "Agile_Trash_470", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as79pc/how_to_transform_op_field_postgres_debezuim_kaftka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as79pc/how_to_transform_op_field_postgres_debezuim_kaftka/", "subreddit_subscribers": 161292, "created_utc": 1708086042.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, \n\nI've hit a bit of a snag and could really use your collective wisdom. We've got this database, **Semantic**, filled with tables that only a select group of users can access. Now, we're trying to set up a separate database for external users. This one would have views that provide access to certain data from the **Semantic** database tables, but without letting these external users directly touch the actual tables.\n\nHere's where I'm stuck: for the views in the new database to work properly and access the tables in the Semantic database, it looks like we need to grant these external users access to the tables. But that's a no-go for data security reasons.\n\nDoes anyone know if there's a way in MS SQL to pull this off? Like, some mechanism or approach that lets us give access to the views without exposing the underlying tables? I'd really appreciate any info. \n\nThanks a ton in advance!", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "View-Only Access in MS SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1as5qle", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708080539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve hit a bit of a snag and could really use your collective wisdom. We&amp;#39;ve got this database, &lt;strong&gt;Semantic&lt;/strong&gt;, filled with tables that only a select group of users can access. Now, we&amp;#39;re trying to set up a separate database for external users. This one would have views that provide access to certain data from the &lt;strong&gt;Semantic&lt;/strong&gt; database tables, but without letting these external users directly touch the actual tables.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s where I&amp;#39;m stuck: for the views in the new database to work properly and access the tables in the Semantic database, it looks like we need to grant these external users access to the tables. But that&amp;#39;s a no-go for data security reasons.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know if there&amp;#39;s a way in MS SQL to pull this off? Like, some mechanism or approach that lets us give access to the views without exposing the underlying tables? I&amp;#39;d really appreciate any info. &lt;/p&gt;\n\n&lt;p&gt;Thanks a ton in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1as5qle", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1as5qle/viewonly_access_in_ms_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1as5qle/viewonly_access_in_ms_sql/", "subreddit_subscribers": 161292, "created_utc": 1708080539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nBeen working as a data engineer in the APS for a year now, getting about 79k AUD + 15% super (so about 90k total compensation).\n\nMy question is, should I be looking to switch to the private sector and what kind of salary would i realistically be looking at? Or, should I stay in my current position and gain more experience before thinking of moving to the private sector. Located in Perth.\n\nThanks!", "author_fullname": "t2_nsy66b5tt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Australia data engineering salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asw21j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708153964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Been working as a data engineer in the APS for a year now, getting about 79k AUD + 15% super (so about 90k total compensation).&lt;/p&gt;\n\n&lt;p&gt;My question is, should I be looking to switch to the private sector and what kind of salary would i realistically be looking at? Or, should I stay in my current position and gain more experience before thinking of moving to the private sector. Located in Perth.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1asw21j", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-z", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asw21j/australia_data_engineering_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asw21j/australia_data_engineering_salary/", "subreddit_subscribers": 161292, "created_utc": 1708153964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI\u2019m currently working on a project where I need to automate the process of fetching data from one MongoDB collection, transforming and enriching it, and then saving the transformed data into another collection. My existing solution is a Python script that loops indefinitely with a sleep(20) interval between each run. It queries the right data and tags it when it\u2019s done. This goes trough various collections (like a pipeline). \n1. raw data\n2. transformed / aggregated \n3. enriched \n\nData comes in over a FastAPI endpoint with about 300k inserts a day. \n\nWhile this setup works, I\u2019m aware it\u2019s not the most efficient or reliable method for production environments. I\u2019ve read about tools like Apache Airflow automation but haven\u2019t used any of these for database operations specifically. My Python script includes snippets that perform the transformation, enrichment, and aggregation of data, and I\u2019m looking for a way to integrate these Python snippets into a more robust system.\n\nHere are my main considerations:\n\n\t\u2022\tPython Integration: Since all my logic is already in Python, I\u2019d prefer a tool that allows me to use my existing code with minimal changes.\n\n\t\u2022\tScheduling and Monitoring: The ability to schedule tasks and monitor their success or failure is crucial. Increasing also the speed and not have sleep timers. \n\n\nWhat would you recommend based on your experiences? Prefect? \n\nAny advice, insights, or recommendations would be greatly appreciated. Thank you in advance for your help!", "author_fullname": "t2_tlpij7lf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool for MongoDB Data Transformation, Enrichment and Aggregation with Python (Airflow?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ash6zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708112739.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708111567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working on a project where I need to automate the process of fetching data from one MongoDB collection, transforming and enriching it, and then saving the transformed data into another collection. My existing solution is a Python script that loops indefinitely with a sleep(20) interval between each run. It queries the right data and tags it when it\u2019s done. This goes trough various collections (like a pipeline). \n1. raw data\n2. transformed / aggregated \n3. enriched &lt;/p&gt;\n\n&lt;p&gt;Data comes in over a FastAPI endpoint with about 300k inserts a day. &lt;/p&gt;\n\n&lt;p&gt;While this setup works, I\u2019m aware it\u2019s not the most efficient or reliable method for production environments. I\u2019ve read about tools like Apache Airflow automation but haven\u2019t used any of these for database operations specifically. My Python script includes snippets that perform the transformation, enrichment, and aggregation of data, and I\u2019m looking for a way to integrate these Python snippets into a more robust system.&lt;/p&gt;\n\n&lt;p&gt;Here are my main considerations:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2022 Python Integration: Since all my logic is already in Python, I\u2019d prefer a tool that allows me to use my existing code with minimal changes.\n\n\u2022 Scheduling and Monitoring: The ability to schedule tasks and monitor their success or failure is crucial. Increasing also the speed and not have sleep timers. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;What would you recommend based on your experiences? Prefect? &lt;/p&gt;\n\n&lt;p&gt;Any advice, insights, or recommendations would be greatly appreciated. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ash6zc", "is_robot_indexable": true, "report_reasons": null, "author": "Additional-Relief109", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ash6zc/tool_for_mongodb_data_transformation_enrichment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ash6zc/tool_for_mongodb_data_transformation_enrichment/", "subreddit_subscribers": 161292, "created_utc": 1708111567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked in the data space for 14 years now, started off in SEO, moved into personalisation and CRM management, went through the MQTT and IOT phase, did the ELT vs ETL shitshow, datalake vs data lakehouse, DBT, CI/CD. DE was an exciting space to be in, and to an extent, still is.\n\nDo any other data professionals feel that we are now a one size fits all pair of trousers?\n\nWe are expected to do insights, analytics, pipeline builds, devops and stakeholder management?\n\nIt used to be that you had a sharp suit and pointy shoes and could deal with clients asking about ROI , then you worked in sales.\n\nYou wore a jumper your mum knitted you and knew how to write \"fuck you clive in sales you stole my wife\" in binary, you were a techy spodhead.\n\nNow I am not sure.\n\nDE was a space, now it is a commune with other great minds. is this a good thing or not. I think it is great from a personal point, but from a job perspective I am not so sure, especially when recruiters want 20 years experience with Flask.\n\n&amp;#x200B;", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DE an actual position anymore?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asa273", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708094835.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708094149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked in the data space for 14 years now, started off in SEO, moved into personalisation and CRM management, went through the MQTT and IOT phase, did the ELT vs ETL shitshow, datalake vs data lakehouse, DBT, CI/CD. DE was an exciting space to be in, and to an extent, still is.&lt;/p&gt;\n\n&lt;p&gt;Do any other data professionals feel that we are now a one size fits all pair of trousers?&lt;/p&gt;\n\n&lt;p&gt;We are expected to do insights, analytics, pipeline builds, devops and stakeholder management?&lt;/p&gt;\n\n&lt;p&gt;It used to be that you had a sharp suit and pointy shoes and could deal with clients asking about ROI , then you worked in sales.&lt;/p&gt;\n\n&lt;p&gt;You wore a jumper your mum knitted you and knew how to write &amp;quot;fuck you clive in sales you stole my wife&amp;quot; in binary, you were a techy spodhead.&lt;/p&gt;\n\n&lt;p&gt;Now I am not sure.&lt;/p&gt;\n\n&lt;p&gt;DE was a space, now it is a commune with other great minds. is this a good thing or not. I think it is great from a personal point, but from a job perspective I am not so sure, especially when recruiters want 20 years experience with Flask.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1asa273", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asa273/is_de_an_actual_position_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asa273/is_de_an_actual_position_anymore/", "subreddit_subscribers": 161292, "created_utc": 1708094149.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}