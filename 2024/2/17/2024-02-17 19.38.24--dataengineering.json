{"kind": "Listing", "data": {"after": "t3_1asmjjc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Leetcode SQL problems? Review concepts like window functions, joins, etc?\n\nI wrote tons of SQL in a past job but it was data modeling heavy and more \"practical\" than what these interview seem to be asking.", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you prep for SQL heavy technical rounds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asim63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708115119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Leetcode SQL problems? Review concepts like window functions, joins, etc?&lt;/p&gt;\n\n&lt;p&gt;I wrote tons of SQL in a past job but it was data modeling heavy and more &amp;quot;practical&amp;quot; than what these interview seem to be asking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1asim63", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asim63/how_do_you_prep_for_sql_heavy_technical_rounds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asim63/how_do_you_prep_for_sql_heavy_technical_rounds/", "subreddit_subscribers": 161427, "created_utc": 1708115119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I updated Geoglify on GitHub. It transforms real-world geospatial data into patterns or shapes. Currently supports 40 patterns, but can load more.\n\nhttps://github.com/geoglify/geoglify", "author_fullname": "t2_7svy5qp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geoglify: now supports visualizing geo data through patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1at0gkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_96.mp4", "dash_url": "https://v.redd.it/3ryx4ffyy4jc1/DASHPlaylist.mpd?a=1710790704%2CMDU1YWI1M2FiMzg3NjFiMmY2MWRhM2Q0NjIzZGQ5NDg1YTZiMzU3MWVmZmE3ODM4NjQ4ODQ4NmI5OWU3NWU4Yg%3D%3D&amp;v=1&amp;f=sd", "duration": 31, "hls_url": "https://v.redd.it/3ryx4ffyy4jc1/HLSPlaylist.m3u8?a=1710790704%2CODBmZjJhY2ZmMDFjZjFkOWEyZTMzMjc5MTdhNGJiNjE1YWNhNGNlNTgwM2I5OWY4NjU0YTZiOGEyYTk0MGNmMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=040417f63f365cd148d663615fcfa758dfbd2f54", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708171529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I updated Geoglify on GitHub. It transforms real-world geospatial data into patterns or shapes. Currently supports 40 patterns, but can load more.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/geoglify/geoglify\"&gt;https://github.com/geoglify/geoglify&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/3ryx4ffyy4jc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?format=pjpg&amp;auto=webp&amp;s=8ca885121e34f4432f561f532d7d666af7aee7e5", "width": 720, "height": 405}, "resolutions": [{"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=50a29c0e6d76bfb3516ee960e4e685224515fd78", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ccd209a17902d39fb6ce537639316785ff9afe34", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=60b6a5aa98cc75dc95c8515d09598ac58d221165", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=379574c1041f2be8125269b1f0d09f50c14d673a", "width": 640, "height": 360}], "variants": {}, "id": "NG1zMHQ2Ynl5NGpjMR-h2AYZ3CErHfcOpHTXE3iCE5O7fVMIXg2Lt9_-HuDE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1at0gkz", "is_robot_indexable": true, "report_reasons": null, "author": "leoneljdias", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at0gkz/geoglify_now_supports_visualizing_geo_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/3ryx4ffyy4jc1", "subreddit_subscribers": 161427, "created_utc": 1708171529.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/3ryx4ffyy4jc1/DASH_96.mp4", "dash_url": "https://v.redd.it/3ryx4ffyy4jc1/DASHPlaylist.mpd?a=1710790704%2CMDU1YWI1M2FiMzg3NjFiMmY2MWRhM2Q0NjIzZGQ5NDg1YTZiMzU3MWVmZmE3ODM4NjQ4ODQ4NmI5OWU3NWU4Yg%3D%3D&amp;v=1&amp;f=sd", "duration": 31, "hls_url": "https://v.redd.it/3ryx4ffyy4jc1/HLSPlaylist.m3u8?a=1710790704%2CODBmZjJhY2ZmMDFjZjFkOWEyZTMzMjc5MTdhNGJiNjE1YWNhNGNlNTgwM2I5OWY4NjU0YTZiOGEyYTk0MGNmMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I am a software developer that got put on some analytics tasking. I'm finding myself enjoying the process and enjoying the data side more than just web development. I am trying to \nDesign my own databases now for some ideas have. But I keep getting stuck on where the line is in terms of normalizing the DB. \n\nI.e. store has an address. Create an address table and store that ID in the store table. But addresses have a state field, should I just add the state there or would you create a new state table and store that ID in the address table as state Id?", "author_fullname": "t2_3m3oxyaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much normalization is too much normalization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ashy7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708113484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am a software developer that got put on some analytics tasking. I&amp;#39;m finding myself enjoying the process and enjoying the data side more than just web development. I am trying to \nDesign my own databases now for some ideas have. But I keep getting stuck on where the line is in terms of normalizing the DB. &lt;/p&gt;\n\n&lt;p&gt;I.e. store has an address. Create an address table and store that ID in the store table. But addresses have a state field, should I just add the state there or would you create a new state table and store that ID in the address table as state Id?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ashy7w", "is_robot_indexable": true, "report_reasons": null, "author": "darin_thompson", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ashy7w/how_much_normalization_is_too_much_normalization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ashy7w/how_much_normalization_is_too_much_normalization/", "subreddit_subscribers": 161427, "created_utc": 1708113484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will be focusing on SQL as the first tech stack because SQL is the core of DE and anything related to data. This blog will cover what you need to learn, the resources to learn as well as where you need to practice it.\n\n**Step 1**: If you are completely new to SQL and want to understand the basics you can [refer](https://www.w3schools.com/sql/). Next refer [this playlist](https://www.youtube.com/watch?v=7GVFYt6_ZFM&amp;list=PL08903FB7ACA1C2FB) to help you in understanding things like select, from, where, joins, indexes, window functions, stored procedures, functions, triggers.\n\n**Step 2**: Next, you need to understand how the execution order of SQL is, as long as you do not understand the flow of your data in your queries you will not be able to visualize how to write queries or at least efficient ones. [This video](https://youtu.be/uEmAvzuA7u8) should help you with that.\n\n**Step 3**: I feel that you should understand how queries are written, what should be the approach to solve a problem and for this you need to develop an intuition. I will recommend first going through the channel I mentioned in Step 2, [link](https://www.youtube.com/@ankitbansal6).\n\nHe has 4 playlists, which has really taken my query writing skills to next level. I will recommend watching them in below order:\n\n1. [SQL Tips and Tricks](https://www.youtube.com/watch?v=4xPxGX4mfb4&amp;list=PLBTZqjSKn0IcR6DhoLUibOG8frnWbZdSH)\n2. [SQL Medium Complex Interview Problems](https://www.youtube.com/watch?v=dOLBRfwzYcU&amp;list=PLBTZqjSKn0IfuIqbMIqzS-waofsPHMS0E)\n3. [Leetcode SQL Hard Problems](https://www.youtube.com/watch?v=tDfAo7uw-3w&amp;list=PLBTZqjSKn0IfULLRo9Tm4lESxYMAG7fUQ)\n4. [Complex SQL questions for Interview Preparation](https://www.youtube.com/watch?v=qyAgWL066Vo&amp;list=PLBTZqjSKn0IeKBQDjLmzisazhqQy4iGkb)\n\nI will recommend after watching the above playlists, to get your hands dirty. So far you have gained all the theoretical knowledge required now you need to start practicing it on a daily basis. The channel has the DDL and DML statements for almost all his videos in the description, if you do not have a local setup, you can use [this](https://sqliteonline.com/) and run the commands over there and start writing queries.\n\nNow that you are through his playlists twice the questions will feel repetitive and you have gained the querying experience needed to handle problems on your own which brings us to step 4.\n\n**Step 4**: Start Practicing.\n\nLearning by just watching will help clear your concepts but you will need to continuously apply them to be really comfortable with it and SQL query writing practice should be done daily even if it's just 1 or 2 problems everyday. After you are done practicing the Ankit Bansal playlists you can start with below:\n\n1. Hackerrank.\n2. Go to leetcode and solve all the free questions from easy, medium and then hard level.\n3. Stratascratch has good questions so you can solve the free questions over there too.\n\nBy now, you should be really good at writing complex queries. If you have followed properly then by step 3 itself you should be good at joins, windows functions, group by, cte, etc.\n\n**Step 5**: Understand some concepts mentioned below.\n\nYou can do this step in parallel with other steps. Below are some of the concepts you need to be aware about and I will be adding pointers and updating links if and as I find them.\n\n1. SQL Indexes, types, uses\n2. [SCD Types](https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types)\n3. OLAP vs OLTP\n4. Normalization vs Denormalization\n5. Normalization Forms\n6. Data Warehouse Concepts \\[[English](https://youtu.be/h0j0QN2b57M?list=PL_c9BZzLwBRK0Pc28IdvPQizD2mJlgoID), [Hindi](https://www.youtube.com/watch?v=UiTvqSd52ak&amp;list=PLTsNSGeIpGnGP8A74Ie1PgqHhewsqD3fv)\\]\n\n[Link to Table of Content Post](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nThank You..!! Please do let me know in comments if you liked the blog, if there is anything else you want to ask related to SQL or if there are any constructive criticism you would like to give.\n\nNote: This blog has focused on relational SQL, NoSQL is something I was aware of on a very very high level but I was honest with interviewers and told them I had no experience with NoSQL and they were okay with it (most did not bother asking about it anyways).", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog 2 - Learning SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asxqdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708175667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708160709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will be focusing on SQL as the first tech stack because SQL is the core of DE and anything related to data. This blog will cover what you need to learn, the resources to learn as well as where you need to practice it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: If you are completely new to SQL and want to understand the basics you can &lt;a href=\"https://www.w3schools.com/sql/\"&gt;refer&lt;/a&gt;. Next refer &lt;a href=\"https://www.youtube.com/watch?v=7GVFYt6_ZFM&amp;amp;list=PL08903FB7ACA1C2FB\"&gt;this playlist&lt;/a&gt; to help you in understanding things like select, from, where, joins, indexes, window functions, stored procedures, functions, triggers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Next, you need to understand how the execution order of SQL is, as long as you do not understand the flow of your data in your queries you will not be able to visualize how to write queries or at least efficient ones. &lt;a href=\"https://youtu.be/uEmAvzuA7u8\"&gt;This video&lt;/a&gt; should help you with that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: I feel that you should understand how queries are written, what should be the approach to solve a problem and for this you need to develop an intuition. I will recommend first going through the channel I mentioned in Step 2, &lt;a href=\"https://www.youtube.com/@ankitbansal6\"&gt;link&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;He has 4 playlists, which has really taken my query writing skills to next level. I will recommend watching them in below order:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=4xPxGX4mfb4&amp;amp;list=PLBTZqjSKn0IcR6DhoLUibOG8frnWbZdSH\"&gt;SQL Tips and Tricks&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=dOLBRfwzYcU&amp;amp;list=PLBTZqjSKn0IfuIqbMIqzS-waofsPHMS0E\"&gt;SQL Medium Complex Interview Problems&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=tDfAo7uw-3w&amp;amp;list=PLBTZqjSKn0IfULLRo9Tm4lESxYMAG7fUQ\"&gt;Leetcode SQL Hard Problems&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=qyAgWL066Vo&amp;amp;list=PLBTZqjSKn0IeKBQDjLmzisazhqQy4iGkb\"&gt;Complex SQL questions for Interview Preparation&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I will recommend after watching the above playlists, to get your hands dirty. So far you have gained all the theoretical knowledge required now you need to start practicing it on a daily basis. The channel has the DDL and DML statements for almost all his videos in the description, if you do not have a local setup, you can use &lt;a href=\"https://sqliteonline.com/\"&gt;this&lt;/a&gt; and run the commands over there and start writing queries.&lt;/p&gt;\n\n&lt;p&gt;Now that you are through his playlists twice the questions will feel repetitive and you have gained the querying experience needed to handle problems on your own which brings us to step 4.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Start Practicing.&lt;/p&gt;\n\n&lt;p&gt;Learning by just watching will help clear your concepts but you will need to continuously apply them to be really comfortable with it and SQL query writing practice should be done daily even if it&amp;#39;s just 1 or 2 problems everyday. After you are done practicing the Ankit Bansal playlists you can start with below:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hackerrank.&lt;/li&gt;\n&lt;li&gt;Go to leetcode and solve all the free questions from easy, medium and then hard level.&lt;/li&gt;\n&lt;li&gt;Stratascratch has good questions so you can solve the free questions over there too.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;By now, you should be really good at writing complex queries. If you have followed properly then by step 3 itself you should be good at joins, windows functions, group by, cte, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Understand some concepts mentioned below.&lt;/p&gt;\n\n&lt;p&gt;You can do this step in parallel with other steps. Below are some of the concepts you need to be aware about and I will be adding pointers and updating links if and as I find them.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL Indexes, types, uses&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types\"&gt;SCD Types&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;OLAP vs OLTP&lt;/li&gt;\n&lt;li&gt;Normalization vs Denormalization&lt;/li&gt;\n&lt;li&gt;Normalization Forms&lt;/li&gt;\n&lt;li&gt;Data Warehouse Concepts [&lt;a href=\"https://youtu.be/h0j0QN2b57M?list=PL_c9BZzLwBRK0Pc28IdvPQizD2mJlgoID\"&gt;English&lt;/a&gt;, &lt;a href=\"https://www.youtube.com/watch?v=UiTvqSd52ak&amp;amp;list=PLTsNSGeIpGnGP8A74Ie1PgqHhewsqD3fv\"&gt;Hindi&lt;/a&gt;]&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Link to Table of Content Post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank You..!! Please do let me know in comments if you liked the blog, if there is anything else you want to ask related to SQL or if there are any constructive criticism you would like to give.&lt;/p&gt;\n\n&lt;p&gt;Note: This blog has focused on relational SQL, NoSQL is something I was aware of on a very very high level but I was honest with interviewers and told them I had no experience with NoSQL and they were okay with it (most did not bother asking about it anyways).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?auto=webp&amp;s=4fee18cfd89cec5ab0520d892b2ce5eb0b9be1f7", "width": 436, "height": 228}, "resolutions": [{"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=740ea9501022e21c257895ff17338b1b33a9a989", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e941fd0c01e8d91f712bca4241a2196e537fcad2", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/KGD-cZWXegNAkY5AinRBvpXp7Ue6NDEaqKhy2ml5Dqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=750005e89b1a398842805c884bbf0fe3131ebd71", "width": 320, "height": 167}], "variants": {}, "id": "UjnWFWLTELxtFI-XmIDZdeUNDhicy5GUBXJWF3AfpVU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1asxqdx", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/", "subreddit_subscribers": 161427, "created_utc": 1708160709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am thinking about doing either the new DP-600 (Fabric Analytics Engineer) or DP-203 (Azure Data Engineer). Would the DP-600 exam be a good choice since Microsoft seems to be trying to push Fabric over Synapse? Or would the DP-203 be a better choice? Would like to hear your opinions.\n", "author_fullname": "t2_3iljgzjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP-600 vs DP-203: Which one would be better choice? Need advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ast2g5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1708145723.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708143428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am thinking about doing either the new DP-600 (Fabric Analytics Engineer) or DP-203 (Azure Data Engineer). Would the DP-600 exam be a good choice since Microsoft seems to be trying to push Fabric over Synapse? Or would the DP-203 be a better choice? Would like to hear your opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1ast2g5", "is_robot_indexable": true, "report_reasons": null, "author": "bass581", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ast2g5/dp600_vs_dp203_which_one_would_be_better_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ast2g5/dp600_vs_dp203_which_one_would_be_better_choice/", "subreddit_subscribers": 161427, "created_utc": 1708143428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "These are some questions that I have been thinking about for some time now. This all centers around what to do in the data warehouse vs what to do in the BI Layer. For the BI layer, I am sure this applies to all BI tools but I am primarily referring to PBI.   \n**I am giving very simple examples here but in reality, the data has many granularities, more metrics, and more reports need to be built.**\n\n&amp;#x200B;\n\n1. Do you create YRAGO, 2YRAGO metrics in DBT?\n   1. Let's say you have a table of Date, Item, and Sale Amount (*Appendix Table A*). Would it make sense to create another column for YRAGO sale amount for that item on that date? I typically do not like to do that and have it done in the PBI layer. What are your thoughts?\n2. Do you create aggregate tables for each major granularity?\n   1. Lets say you have a table of Date, Store, Item, and Sale Amount (*Appendix Table B*). You need to create 3 reports for Sales by Item, Store, and Date. Would you make views/tables that aggregate to each granularity?\n\n**Things I like to optimize for:**\n\n\\- Keeping the BI layer as light as possible, reusability in the DW, not keeping logic in the BI layer.\n\n&amp;#x200B;\n\nAppendix:\n\nTable A:\n\n|Date|Item ID|Sale Amount|\n|:-|:-|:-|\n|1/1/2023|a|100|\n|1/1/2023|b|50|\n|1/2/2023|a|76|\n\nTable B:\n\n|Date|Store ID|Item ID|Sale Amount|\n|:-|:-|:-|:-|\n|1/1/2023|z|a|100|\n|1/1/2023|q|b|50|\n|1/2/2023|w|a|76|\n\n&amp;#x200B;", "author_fullname": "t2_uww96dnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling: What logic to put in DBT vs in BI Layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asq126", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708134277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These are some questions that I have been thinking about for some time now. This all centers around what to do in the data warehouse vs what to do in the BI Layer. For the BI layer, I am sure this applies to all BI tools but I am primarily referring to PBI.&lt;br/&gt;\n&lt;strong&gt;I am giving very simple examples here but in reality, the data has many granularities, more metrics, and more reports need to be built.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you create YRAGO, 2YRAGO metrics in DBT?\n\n&lt;ol&gt;\n&lt;li&gt;Let&amp;#39;s say you have a table of Date, Item, and Sale Amount (&lt;em&gt;Appendix Table A&lt;/em&gt;). Would it make sense to create another column for YRAGO sale amount for that item on that date? I typically do not like to do that and have it done in the PBI layer. What are your thoughts?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Do you create aggregate tables for each major granularity?\n\n&lt;ol&gt;\n&lt;li&gt;Lets say you have a table of Date, Store, Item, and Sale Amount (&lt;em&gt;Appendix Table B&lt;/em&gt;). You need to create 3 reports for Sales by Item, Store, and Date. Would you make views/tables that aggregate to each granularity?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Things I like to optimize for:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Keeping the BI layer as light as possible, reusability in the DW, not keeping logic in the BI layer.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Appendix:&lt;/p&gt;\n\n&lt;p&gt;Table A:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Item ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Sale Amount&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/2/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Table B:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Store ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Item ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Sale Amount&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;z&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;q&lt;/td&gt;\n&lt;td align=\"left\"&gt;b&lt;/td&gt;\n&lt;td align=\"left\"&gt;50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/2/2023&lt;/td&gt;\n&lt;td align=\"left\"&gt;w&lt;/td&gt;\n&lt;td align=\"left\"&gt;a&lt;/td&gt;\n&lt;td align=\"left\"&gt;76&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asq126", "is_robot_indexable": true, "report_reasons": null, "author": "anon_data_person", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asq126/data_modeling_what_logic_to_put_in_dbt_vs_in_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asq126/data_modeling_what_logic_to_put_in_dbt_vs_in_bi/", "subreddit_subscribers": 161427, "created_utc": 1708134277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spent the last few months collecting and analyzing historical data from the NBA API. It contains high-quality, real-world data that's both interesting to analyze and great to practice with. \n\nThe experience has been so fun that I turned the project into a publicly available competition!\n\nHere's how the competition works: Participants utilize real NBA data to craft SQL queries, develop dbt\u2122 models, and derive insights, all for a chance to win a $1,500 Amazon gift card.\u00a0  \n\n\nFor more details, check out my corny video below, and register to participate [here](https://www.paradime.io/dbt-data-modeling-challenge-nba-edition)!  \n\n\nhttps://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Data Modeling Competition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fyjdyy9w00jc1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/1asi2e5/asset/fyjdyy9w00jc1/DASHPlaylist.mpd?a=1710790704%2CZWIzMDM0N2Q0MmU4MTJmNmNlYTU4ZDZiNzVkMGRjZTlkMmUxN2RkMWEzYzJiNmRhZjM3MTE5YTMzN2RkOWEwYg%3D%3D&amp;v=1&amp;f=sd", "x": 1920, "y": 1080, "hlsUrl": "https://v.redd.it/link/1asi2e5/asset/fyjdyy9w00jc1/HLSPlaylist.m3u8?a=1710790704%2CNDQ3NmE4N2UwZmU2ODNhNTI1YmNmMTNiYjgzYjQ3NDM3MDJhMmZiYzFiNDIxOGMwZDZlMDUxM2VjMWM3MWUwZg%3D%3D&amp;v=1&amp;f=sd", "id": "fyjdyy9w00jc1", "isGif": false}}, "name": "t3_1asi2e5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LB1zzxQ7TXcjIPuXMaifM8Hd7JWBsoXtu5fUnZ_t8BI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1708113753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spent the last few months collecting and analyzing historical data from the NBA API. It contains high-quality, real-world data that&amp;#39;s both interesting to analyze and great to practice with. &lt;/p&gt;\n\n&lt;p&gt;The experience has been so fun that I turned the project into a publicly available competition!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how the competition works: Participants utilize real NBA data to craft SQL queries, develop dbt\u2122 models, and derive insights, all for a chance to win a $1,500 Amazon gift card.\u00a0  &lt;/p&gt;\n\n&lt;p&gt;For more details, check out my corny video below, and register to participate &lt;a href=\"https://www.paradime.io/dbt-data-modeling-challenge-nba-edition\"&gt;here&lt;/a&gt;!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player\"&gt;https://reddit.com/link/1asi2e5/video/fyjdyy9w00jc1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?auto=webp&amp;s=0e6a69a173bc7a517e4426fa77b5ce63ca357210", "width": 1376, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa9033a0520a160ebc57d0e3354dcf7483f121d1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d671d735ac9482c5e580511d7090f11baa4e01f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d53e2037de8a232ed72eb0082d7d4dca2f4c4adb", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0df826dfb886bdf8ffa209deb21e0b8c7574fc56", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6aac07197f3a4df7afdbf3bcb69112354ee0e9ab", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/9By_UFWyZz8XMYi0Tu3O7y7eT0rAoABUI9-0aNq8V2Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efa695d775899cd3edee7829becbbc9d4107af68", "width": 1080, "height": 565}], "variants": {}, "id": "gMycMBUMB23ezrHe91vhkZqeB9cNGVRk3G4g0CuHC7Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1asi2e5", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asi2e5/dbt_data_modeling_competition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asi2e5/dbt_data_modeling_competition/", "subreddit_subscribers": 161427, "created_utc": 1708113753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When do you use which one?\n\nWhich is better?\n\nUsing auto-generating PK:\n\n`CREATE TABLE fact_orderlines (`  \n`order_id INTEGER IDENTITY(1,1) PRIMARY KEY, -- SERIAL not supported use IDENTITY instead`  \n`product_id INTEGER,`  \n`category_id INTEGER,`  \n`date_id INTEGER,`  \n`market_id INTEGER,`  \n`customer_id INTEGER,`  \n`amount DECIMAL(10,2) \u00a0-- MONEY data type is not supported, use DECIMAL instead`  \n`);`\n\nOr using a composite key:\n\n`ALTER TABLE fact_orderlines ADD CONSTRAINT pk_fact_orderlines PRIMARY KEY (product_id, category_id, date_id, market_id, customer_id);`", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use surrogate key or composite key in fact tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at32dw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708179699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When do you use which one?&lt;/p&gt;\n\n&lt;p&gt;Which is better?&lt;/p&gt;\n\n&lt;p&gt;Using auto-generating PK:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE fact_orderlines (&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;order_id INTEGER IDENTITY(1,1) PRIMARY KEY, -- SERIAL not supported use IDENTITY instead&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;product_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;category_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;date_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;market_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;customer_id INTEGER,&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;amount DECIMAL(10,2) \u00a0-- MONEY data type is not supported, use DECIMAL instead&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;);&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Or using a composite key:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE fact_orderlines ADD CONSTRAINT pk_fact_orderlines PRIMARY KEY (product_id, category_id, date_id, market_id, customer_id);&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at32dw", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at32dw/do_you_use_surrogate_key_or_composite_key_in_fact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at32dw/do_you_use_surrogate_key_or_composite_key_in_fact/", "subreddit_subscribers": 161427, "created_utc": 1708179699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nBeen working as a data engineer in the APS for a year now, getting about 79k AUD + 15% super (so about 90k total compensation).\n\nMy question is, should I be looking to switch to the private sector and what kind of salary would i realistically be looking at? Or, should I stay in my current position and gain more experience before thinking of moving to the private sector. Located in Perth.\n\nThanks!", "author_fullname": "t2_nsy66b5tt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Australia data engineering salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asw21j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708153964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Been working as a data engineer in the APS for a year now, getting about 79k AUD + 15% super (so about 90k total compensation).&lt;/p&gt;\n\n&lt;p&gt;My question is, should I be looking to switch to the private sector and what kind of salary would i realistically be looking at? Or, should I stay in my current position and gain more experience before thinking of moving to the private sector. Located in Perth.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1asw21j", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-z", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asw21j/australia_data_engineering_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asw21j/australia_data_engineering_salary/", "subreddit_subscribers": 161427, "created_utc": 1708153964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello \n\nI am confused about this technology. \n\nWe are a heavy snowflake and AWS shop. \n\nWhat are some use cases of Spark on such an org if any?  Are there things that could be done nor efficiently or made us more productive?\n\nI understand it\u2019s hard to give advices without the specifics. Please ask me questions and I\u2019ll try to answer without getting intro trouble at work. \n\nThank you. ", "author_fullname": "t2_lcc2eyxje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Question - Use cases of Spark if we are on Snowflake, AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at1rel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708175834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;/p&gt;\n\n&lt;p&gt;I am confused about this technology. &lt;/p&gt;\n\n&lt;p&gt;We are a heavy snowflake and AWS shop. &lt;/p&gt;\n\n&lt;p&gt;What are some use cases of Spark on such an org if any?  Are there things that could be done nor efficiently or made us more productive?&lt;/p&gt;\n\n&lt;p&gt;I understand it\u2019s hard to give advices without the specifics. Please ask me questions and I\u2019ll try to answer without getting intro trouble at work. &lt;/p&gt;\n\n&lt;p&gt;Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at1rel", "is_robot_indexable": true, "report_reasons": null, "author": "20231027", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at1rel/beginner_question_use_cases_of_spark_if_we_are_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at1rel/beginner_question_use_cases_of_spark_if_we_are_on/", "subreddit_subscribers": 161427, "created_utc": 1708175834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy. Does anybody have any technical books related to DE that they particularly like?\n\nFor me, it\u2019s Data Pipelines with Apache Airflow; Docker Up and Running; Learning Spark; and Kafka: The Definitive Guide.", "author_fullname": "t2_elthwsuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favourite technical books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at2ir9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708178135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy. Does anybody have any technical books related to DE that they particularly like?&lt;/p&gt;\n\n&lt;p&gt;For me, it\u2019s Data Pipelines with Apache Airflow; Docker Up and Running; Learning Spark; and Kafka: The Definitive Guide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at2ir9", "is_robot_indexable": true, "report_reasons": null, "author": "Low-Sandwich-7607", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at2ir9/favourite_technical_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at2ir9/favourite_technical_books/", "subreddit_subscribers": 161427, "created_utc": 1708178135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im looking to implement an OLAP db in my company. The idea is to pull data from our MongoDB, ETL that shit with mage.ai and push it to an OLAP db where BI can consume it.\n\nData is not really big. Less than 1GB total. May be bigger in the future but not by much.\n\nWhich provider would fit given the costs and without being overkill? Should I just stick with BigQuery?", "author_fullname": "t2_4rpxclpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help deciding an OLAP db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aspdhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708132421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking to implement an OLAP db in my company. The idea is to pull data from our MongoDB, ETL that shit with mage.ai and push it to an OLAP db where BI can consume it.&lt;/p&gt;\n\n&lt;p&gt;Data is not really big. Less than 1GB total. May be bigger in the future but not by much.&lt;/p&gt;\n\n&lt;p&gt;Which provider would fit given the costs and without being overkill? Should I just stick with BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aspdhk", "is_robot_indexable": true, "report_reasons": null, "author": "RedBlueWhiteBlack", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aspdhk/need_help_deciding_an_olap_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aspdhk/need_help_deciding_an_olap_db/", "subreddit_subscribers": 161427, "created_utc": 1708132421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After careful consideration and listening to your feedback, we've decided to no longer allow interview-related posts because they take away focus from our community's main purpose.\n\nIn the past, although they usually weren't directly related to data engineering we've allowed interview posts like \"What are interviews like at XYZ company?\" or \"What should I prepare/study for XYZ position?\"\n\nThese questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.\n\nThank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update to interview posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1at7mdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708191888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After careful consideration and listening to your feedback, we&amp;#39;ve decided to no longer allow interview-related posts because they take away focus from our community&amp;#39;s main purpose.&lt;/p&gt;\n\n&lt;p&gt;In the past, although they usually weren&amp;#39;t directly related to data engineering we&amp;#39;ve allowed interview posts like &amp;quot;What are interviews like at XYZ company?&amp;quot; or &amp;quot;What should I prepare/study for XYZ position?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;These questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.&lt;/p&gt;\n\n&lt;p&gt;Thank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1at7mdg", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/1at7mdg/update_to_interview_posts/", "subreddit_subscribers": 161427, "created_utc": 1708191888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nI am deploying clickhouse in production, total keeper 2 nodes, clickhouse 3 nodes. Read a lot here about problems scaling it on production, shards and configuration management.  \n\n\nWould love to hear about what to watch out for and any tips.  \n\n\nCurrent infra is aws and clickhouse will be deployed inside private vpc.  Using  \n1. Clickhouse Keeper (instead of zookeeper) (with docker) - 2 nodes - t2.medium  \n2. Clickhouse docker images with attached EBS volume (in case to scale or add extra storage if needed ) 3 nodes - t3.xlarge - 500GB volume each  \n3. Grafana for monitoring.  \n4. Looking at ELB for connecting to kafka for data pipeline  \n5. Openssl for certificates for hosts for clickhouse  \nThanks", "author_fullname": "t2_gyoai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Clickhouse in production - what to watch out for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at0xbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708173097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am deploying clickhouse in production, total keeper 2 nodes, clickhouse 3 nodes. Read a lot here about problems scaling it on production, shards and configuration management.  &lt;/p&gt;\n\n&lt;p&gt;Would love to hear about what to watch out for and any tips.  &lt;/p&gt;\n\n&lt;p&gt;Current infra is aws and clickhouse will be deployed inside private vpc.  Using&lt;br/&gt;\n1. Clickhouse Keeper (instead of zookeeper) (with docker) - 2 nodes - t2.medium&lt;br/&gt;\n2. Clickhouse docker images with attached EBS volume (in case to scale or add extra storage if needed ) 3 nodes - t3.xlarge - 500GB volume each&lt;br/&gt;\n3. Grafana for monitoring.&lt;br/&gt;\n4. Looking at ELB for connecting to kafka for data pipeline&lt;br/&gt;\n5. Openssl for certificates for hosts for clickhouse&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at0xbv", "is_robot_indexable": true, "report_reasons": null, "author": "abhishekgahlot", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at0xbv/deploying_clickhouse_in_production_what_to_watch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at0xbv/deploying_clickhouse_in_production_what_to_watch/", "subreddit_subscribers": 161427, "created_utc": 1708173097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like I may be going down the wrong route to I'm here to ask. I have a lot of situations where I'd like to encrypt a file before storing it on s3. I went ahead and wrote a fastapi endpoint that takes a file upload and a recipient email then returns an encrypted file. This would be available internally as a service over https. \n\nIs there an easier way of centrally managing keys / encryption?", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encryption micro service?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asnwgy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708128441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I may be going down the wrong route to I&amp;#39;m here to ask. I have a lot of situations where I&amp;#39;d like to encrypt a file before storing it on s3. I went ahead and wrote a fastapi endpoint that takes a file upload and a recipient email then returns an encrypted file. This would be available internally as a service over https. &lt;/p&gt;\n\n&lt;p&gt;Is there an easier way of centrally managing keys / encryption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1asnwgy", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asnwgy/encryption_micro_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asnwgy/encryption_micro_service/", "subreddit_subscribers": 161427, "created_utc": 1708128441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!  \nI need a bit of help figuring out how to run a docker container in a manner that I used to, but carelessly never documented. As it turns out, it used to be the perfect way to experiment with various Spark jobs.  \nEnvironment: docker on WSL, Windows used for IDE  \nWhat I know for sure is that I used the apache/spark-py image and the container settings allowed me to interact with the spark UI for jobs and everything similar. The container could be run indefinitely detached. It could network with a local postgres database also run in a docker container with an attached volume.\n\nThe setup was a single node cluster, ran exclusively on the wsl docker without docker compose or explicit worker-master relationships.\n\nRunning the spark container produced the spark logo, however my latest attempt does not.\n\nIf anyone out there has any clue about this, please let me know!", "author_fullname": "t2_n75h7s91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot recreate PySpark docker container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asj9op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708116733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI need a bit of help figuring out how to run a docker container in a manner that I used to, but carelessly never documented. As it turns out, it used to be the perfect way to experiment with various Spark jobs.&lt;br/&gt;\nEnvironment: docker on WSL, Windows used for IDE&lt;br/&gt;\nWhat I know for sure is that I used the apache/spark-py image and the container settings allowed me to interact with the spark UI for jobs and everything similar. The container could be run indefinitely detached. It could network with a local postgres database also run in a docker container with an attached volume.&lt;/p&gt;\n\n&lt;p&gt;The setup was a single node cluster, ran exclusively on the wsl docker without docker compose or explicit worker-master relationships.&lt;/p&gt;\n\n&lt;p&gt;Running the spark container produced the spark logo, however my latest attempt does not.&lt;/p&gt;\n\n&lt;p&gt;If anyone out there has any clue about this, please let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1asj9op", "is_robot_indexable": true, "report_reasons": null, "author": "Cheeky-owlet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asj9op/cannot_recreate_pyspark_docker_container/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asj9op/cannot_recreate_pyspark_docker_container/", "subreddit_subscribers": 161427, "created_utc": 1708116733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometime back, I came across this [post](https://www.reddit.com/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/). I did mention in the [reply](https://www.reddit.com/r/dataengineering/comments/1apkw7y/comment/kq74hfy/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) that I had an unfinished blog on DE and would try to complete it.\n\nSo I started to write and instead of a blog, I decided to make it an open book.\n\n[https://paperplaneflyr.gitbook.io/data-engineering-first-contact/](https://paperplaneflyr.gitbook.io/data-engineering-first-contact/)\n\nMy agenda was to make this for anyone who is just starting in Data Engineering, a seasoned software developer who was pushed into Data Engineering, or students who want to become Data Engineers.\n\nYou may find tons of information on the internet about DE, but how much of it applies to your project? Of course, there is always a give and take of tools and methods but at the core, some principles remain the same.\n\nThis book is about those principles and a bit on the tools. Tools always have documentation, principles are created with experience.\n\nIt's not complete, suggestions are welcome. ", "author_fullname": "t2_aryc45smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering - First Contact", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aswgfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1708155541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometime back, I came across this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1apkw7y/resources_for_data_modeling_and_building_a_data/\"&gt;post&lt;/a&gt;. I did mention in the &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1apkw7y/comment/kq74hfy/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;reply&lt;/a&gt; that I had an unfinished blog on DE and would try to complete it.&lt;/p&gt;\n\n&lt;p&gt;So I started to write and instead of a blog, I decided to make it an open book.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://paperplaneflyr.gitbook.io/data-engineering-first-contact/\"&gt;https://paperplaneflyr.gitbook.io/data-engineering-first-contact/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My agenda was to make this for anyone who is just starting in Data Engineering, a seasoned software developer who was pushed into Data Engineering, or students who want to become Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;You may find tons of information on the internet about DE, but how much of it applies to your project? Of course, there is always a give and take of tools and methods but at the core, some principles remain the same.&lt;/p&gt;\n\n&lt;p&gt;This book is about those principles and a bit on the tools. Tools always have documentation, principles are created with experience.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not complete, suggestions are welcome. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?auto=webp&amp;s=1b3d15e2eab54160d1eb8341a8e8a8f8d488df87", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3f121b8a75d093d8ca58faee2a92edcb1ac215d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9a3839d812d45bc7418fef80aff696f5b5d48c8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=092cde4fa2061aa37e79e35e3bf665e47aeff21f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=970aca83490e5c0d671932eee55fa0ae5be9bc0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61ef971cbb4e56f2eaa692513120d5944ac87e86", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/B2hfw9u0t0HEb5qQGUBnpsGvE6nL7WFTPSJqm3e0jy8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62b2aca4584a495a22733bd20205e93294b0200e", "width": 1080, "height": 567}], "variants": {}, "id": "Zypq0-kjIoHeIOLgkPEic9po2a5RmVdTkDYH8npVrHY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1aswgfv", "is_robot_indexable": true, "report_reasons": null, "author": "Paperplaneflyr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aswgfv/data_engineering_first_contact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aswgfv/data_engineering_first_contact/", "subreddit_subscribers": 161427, "created_utc": 1708155541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.\n\nI may have missed some important pieces and some of your favorite ones, please remind me in the comments.\n\nAdded some comments and thoughts that will be helpful: https://www.junaideffendi.com/p/end-to-end-data-engineering", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to End Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_1at8a9q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfEs7-UDmzuPpDmkUrSJTdayf3C3Cnz_rbuhCsgoHAI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1708193612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "junaideffendi.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Covering core Data Engineering concepts with end to end data tech stacks from the modern times. Leverage it to build your own path.&lt;/p&gt;\n\n&lt;p&gt;I may have missed some important pieces and some of your favorite ones, please remind me in the comments.&lt;/p&gt;\n\n&lt;p&gt;Added some comments and thoughts that will be helpful: &lt;a href=\"https://www.junaideffendi.com/p/end-to-end-data-engineering\"&gt;https://www.junaideffendi.com/p/end-to-end-data-engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?auto=webp&amp;s=1ed846a57b8f80325a7a2e15e0097e32eaa423f3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27190432bd95df1e73b8119755399dab8c18f679", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f2e1528d7b338d331fbbfb64f4d3b6c0607a304", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18abef8c17aa632e8d49b11b8f77045c6cd0a811", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cff001fc4237ad505221528f4994bf6427b1e37", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebb983bf58d7cf9d528b7d7efd422a2dc7203535", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_q33xe9NOX8HeV5eYiNrlntQD4h35zSVawSRrk8_r-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5b18e9de6272ac01cd6f0945539d2043effe601", "width": 1080, "height": 540}], "variants": {}, "id": "bokfuc0QawGL9Kh2VpDq4Cqd6CsLSESH7Y0mqj5BFaY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1at8a9q", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8a9q/end_to_end_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.junaideffendi.com/p/end-to-end-data-engineering", "subreddit_subscribers": 161427, "created_utc": 1708193612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_8u4sc3aj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good sources to learn Scala and Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at5kpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708186647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1at5kpz", "is_robot_indexable": true, "report_reasons": null, "author": "Present-Yogurt-1998", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at5kpz/what_are_some_good_sources_to_learn_scala_and/", "subreddit_subscribers": 161427, "created_utc": 1708186647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently been hired to be the sole data engineer for a public database that has built up a lot of tech debt. Everything is stored in Postgres with foreign data wrappers, with custom scrapers built in Python. We\u2019re looking to ingest some new data into a table, but want to include columns that are based on what is already in the database (if IDs match, comparing timestamps, etc). What\u2019s the best tool for this in Python? Is this just querying the database in Python and using that return to augment our data? Is this a duckdb use case? Trying to figure it out, any help would be appreciated!\n", "author_fullname": "t2_4xul99ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to ingest data that is based on data already in the database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at262v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708177079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently been hired to be the sole data engineer for a public database that has built up a lot of tech debt. Everything is stored in Postgres with foreign data wrappers, with custom scrapers built in Python. We\u2019re looking to ingest some new data into a table, but want to include columns that are based on what is already in the database (if IDs match, comparing timestamps, etc). What\u2019s the best tool for this in Python? Is this just querying the database in Python and using that return to augment our data? Is this a duckdb use case? Trying to figure it out, any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at262v", "is_robot_indexable": true, "report_reasons": null, "author": "RichOkra", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at262v/best_way_to_ingest_data_that_is_based_on_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at262v/best_way_to_ingest_data_that_is_based_on_data/", "subreddit_subscribers": 161427, "created_utc": 1708177079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i'd manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.\n\nAs I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.\n\nCan iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?", "author_fullname": "t2_hgupt3800", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About iceberg tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1at8bft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708193702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company will change ERP systems net year (after 20 years). Together with this change will come a change in data architecture. Where previously i&amp;#39;d manage an azure stack, the new stack will be AWS+Snowflake. A big requirement of the stack is to be able to time travel. Therefor they want to turn a daily full load into iceberg tables and do a catalog integration with snowflake.&lt;/p&gt;\n\n&lt;p&gt;As I have some experience with delta lakes, I had a discussion with our data architect. My argument was that trying to use iceberg tables without any maintenance for timetraveling lets say 5 years would probably be terrible for storage cost and performance, if not impossible. He said that it was no problem whatsoever.&lt;/p&gt;\n\n&lt;p&gt;Can iceberg tables be used to timetravel without limit? If so what would be the implication on storage volumes over time? Is there a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at8bft", "is_robot_indexable": true, "report_reasons": null, "author": "Annual_Scratch7181", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at8bft/about_iceberg_tables/", "subreddit_subscribers": 161427, "created_utc": 1708193702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have a bit of a conundrum:\n\nI'm a data scientist and I've got the opportunity to turn a monster of an Excel workbook that's crucial for operations and KPI tracking into a database - or at least into a more efficient solution. There's a ton of business processes intertwined with this workbook and no one actually knows how many processes rely on it - or to what level detail. I'm interested in doing this as I think this is what the business needs, because I want to learn more data engineering tools and I want to learn more about that aspect of our operations.\n\nMy manager is not too technical so I have to deliver judgement calls on the more technical aspects but I have his support to delve into the topic for a couple of months and map out what I can and what I cannot do, and whether it's feasible for our small team to develop a full solution.\n\nIf the result is a \"no\" then that would be acceptable for him, too (obviously I'd work out the next steps and use the lessons learned to \"outsource\" and continue the project).\n\nSo far, so good. I talked with the colleague who developed the excel workbook over the past 5-10 years and he was of the very strong opinion of not developing a database solution ourselves but to get a professional solution for three reasons:\n\n* we'd be 'stuck' with maintenance down the road and we're actually too small a team for that\n* he thinks that we should look at the much, much bigger picture (i.e. involving more sites and departments and looking at the business processes way more holistically), and consequently buy the \"infrastructure\" to drive home the point of how crucial this technology is. Also it would make access to data silos potentially easier.\n* he also fears that higher management (who's aware that this is a crucial bit of infrastructure) will then be happy with whatever \"small\" solution we come up with, and then won't be willing to pay for the \"big\" solution that would actually be necessary. The latter solution would take a couple of years to get done.\n\nMy manager is playing with the idea that we can entertain two scenarios, a short-to- midterm solution that we maybe develop ourselves and a mid-to-longterm scenario with a \"professional\" (maybe internal/central or maybe external) solution. So, I asked for another colleague and I to get a couple of months time to map the data flows and built a pilot/prototype data solution because I believe that an approach in small, reversible steps might be very valuable here - and no one has considered that yet over the last couple of years. (A couple of months because I want some time to ideate and I can't focus on this topic full-time).\n\nThe benefit of delivering a \"small\" solution would be that we'd be making a name for ourselves across the entire hierarchy. Disadvantage will be that accessing existing silos will be a slog and likely not easily available, so we'd still need some manual/workaround type of data handover to our new, independent system (at least initially).\n\nLastly, I talked with a senior data scientist from the central data team and he said that we should really move away from those \"small\" solutions and look at the bigger picture because they think that we should by default look at the bigger picture to move the company ahead more sustainably. After talking a bit he have me the advice of at least interviewing most of the stakeholders before even starting any coding.\n\nSo, I'm a bit confused. I just want to go ahead, get my hands dirty and get started doing things, rather than having these endless discussions. But I also see the benefits of looking at the bigger picture and I don't want to outright dismiss my (very experienced) colleagues' take on the matter.\n\nDo you have some advice for me? Have you been in a comparable situation - and if so, how did/would you approach it?\n\nTL;DR:\n\nI have the opportunity to develop an in-house database solution but I'm not sure if I'm thinking too small.\n\nMany thanks!", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a database project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1at7mp7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708191909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have a bit of a conundrum:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data scientist and I&amp;#39;ve got the opportunity to turn a monster of an Excel workbook that&amp;#39;s crucial for operations and KPI tracking into a database - or at least into a more efficient solution. There&amp;#39;s a ton of business processes intertwined with this workbook and no one actually knows how many processes rely on it - or to what level detail. I&amp;#39;m interested in doing this as I think this is what the business needs, because I want to learn more data engineering tools and I want to learn more about that aspect of our operations.&lt;/p&gt;\n\n&lt;p&gt;My manager is not too technical so I have to deliver judgement calls on the more technical aspects but I have his support to delve into the topic for a couple of months and map out what I can and what I cannot do, and whether it&amp;#39;s feasible for our small team to develop a full solution.&lt;/p&gt;\n\n&lt;p&gt;If the result is a &amp;quot;no&amp;quot; then that would be acceptable for him, too (obviously I&amp;#39;d work out the next steps and use the lessons learned to &amp;quot;outsource&amp;quot; and continue the project).&lt;/p&gt;\n\n&lt;p&gt;So far, so good. I talked with the colleague who developed the excel workbook over the past 5-10 years and he was of the very strong opinion of not developing a database solution ourselves but to get a professional solution for three reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;we&amp;#39;d be &amp;#39;stuck&amp;#39; with maintenance down the road and we&amp;#39;re actually too small a team for that&lt;/li&gt;\n&lt;li&gt;he thinks that we should look at the much, much bigger picture (i.e. involving more sites and departments and looking at the business processes way more holistically), and consequently buy the &amp;quot;infrastructure&amp;quot; to drive home the point of how crucial this technology is. Also it would make access to data silos potentially easier.&lt;/li&gt;\n&lt;li&gt;he also fears that higher management (who&amp;#39;s aware that this is a crucial bit of infrastructure) will then be happy with whatever &amp;quot;small&amp;quot; solution we come up with, and then won&amp;#39;t be willing to pay for the &amp;quot;big&amp;quot; solution that would actually be necessary. The latter solution would take a couple of years to get done.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My manager is playing with the idea that we can entertain two scenarios, a short-to- midterm solution that we maybe develop ourselves and a mid-to-longterm scenario with a &amp;quot;professional&amp;quot; (maybe internal/central or maybe external) solution. So, I asked for another colleague and I to get a couple of months time to map the data flows and built a pilot/prototype data solution because I believe that an approach in small, reversible steps might be very valuable here - and no one has considered that yet over the last couple of years. (A couple of months because I want some time to ideate and I can&amp;#39;t focus on this topic full-time).&lt;/p&gt;\n\n&lt;p&gt;The benefit of delivering a &amp;quot;small&amp;quot; solution would be that we&amp;#39;d be making a name for ourselves across the entire hierarchy. Disadvantage will be that accessing existing silos will be a slog and likely not easily available, so we&amp;#39;d still need some manual/workaround type of data handover to our new, independent system (at least initially).&lt;/p&gt;\n\n&lt;p&gt;Lastly, I talked with a senior data scientist from the central data team and he said that we should really move away from those &amp;quot;small&amp;quot; solutions and look at the bigger picture because they think that we should by default look at the bigger picture to move the company ahead more sustainably. After talking a bit he have me the advice of at least interviewing most of the stakeholders before even starting any coding.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m a bit confused. I just want to go ahead, get my hands dirty and get started doing things, rather than having these endless discussions. But I also see the benefits of looking at the bigger picture and I don&amp;#39;t want to outright dismiss my (very experienced) colleagues&amp;#39; take on the matter.&lt;/p&gt;\n\n&lt;p&gt;Do you have some advice for me? Have you been in a comparable situation - and if so, how did/would you approach it?&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;/p&gt;\n\n&lt;p&gt;I have the opportunity to develop an in-house database solution but I&amp;#39;m not sure if I&amp;#39;m thinking too small.&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1at7mp7", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at7mp7/need_help_with_a_database_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at7mp7/need_help_with_a_database_project/", "subreddit_subscribers": 161427, "created_utc": 1708191909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven't personally used Airbyte, on paper, it seems ideal.\n\nIt offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.\n\nAirbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.\n\nI'm eager to learn about others' experiences with Airbyte.\n\nAdditionally, I'm exploring other tools in the market. I'm in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:\n\n- Open-source and capable of being hosted independently (I don\u2019t want to pay for it)\n- The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi\n- The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.\n\nHistorically, I've set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I'm drawn to the concept of EL tools that offer a standardized approach to configuring these connections.", "author_fullname": "t2_tux1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte and similar EL tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1at76os", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708190772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently shared my thoughts in a previous discussion where there was some dissatisfaction mentioned regarding Airbyte. Although I haven&amp;#39;t personally used Airbyte, on paper, it seems ideal.&lt;/p&gt;\n\n&lt;p&gt;It offers a unified method for establishing your EL (Extract and Load) pipeline, recognizing that the majority of these pipelines\u2014connecting to standard data sources like databases, object storage, APIs, etc.\u2014follow a similar pattern with either scheduled batch or streaming workflows.&lt;/p&gt;\n\n&lt;p&gt;Airbyte appears to excel in this area, and the documentation suggests that setting up a completely new connector is relatively straightforward.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m eager to learn about others&amp;#39; experiences with Airbyte.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;m exploring other tools in the market. I&amp;#39;m in search of a solution that can seamlessly integrate data into my lakehouse, with the following criteria:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Open-source and capable of being hosted independently (I don\u2019t want to pay for it)&lt;/li&gt;\n&lt;li&gt;The ability to configure EL connections between sources and destinations as code, preferably in a declarative manner using Infrastructure as Code (IaC) tools like Terraform or Pulumi&lt;/li&gt;\n&lt;li&gt;The pipeline should automatically log crucial metadata, including extraction timestamps and file sizes etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Historically, I&amp;#39;ve set up these extraction processes manually using self-hosted servers or lambda functions. However, the complexity tends to increase with the number of connections. Hence, I&amp;#39;m drawn to the concept of EL tools that offer a standardized approach to configuring these connections.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1at76os", "is_robot_indexable": true, "report_reasons": null, "author": "caksters", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1at76os/airbyte_and_similar_el_tools/", "subreddit_subscribers": 161427, "created_utc": 1708190772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn AWS Kinesis for data engineering perspective and there aren't much resources there on the internet. \n\nMost of the udemy courses only cover theoretical aspects of it. I came across this course  [Ultimate Guide to Data Streaming with AWS Kinesis | Udemy](https://www.udemy.com/course/ultimate-guide-to-data-streaming-with-aws-kinesis/) but I dont know if this is a good one.\n\nMy main goal is to learn how to do ETL with streaming data using Kinesis and other tools. ", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resource for learning AWS Kinesis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1aszyi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708169645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn AWS Kinesis for data engineering perspective and there aren&amp;#39;t much resources there on the internet. &lt;/p&gt;\n\n&lt;p&gt;Most of the udemy courses only cover theoretical aspects of it. I came across this course  &lt;a href=\"https://www.udemy.com/course/ultimate-guide-to-data-streaming-with-aws-kinesis/\"&gt;Ultimate Guide to Data Streaming with AWS Kinesis | Udemy&lt;/a&gt; but I dont know if this is a good one.&lt;/p&gt;\n\n&lt;p&gt;My main goal is to learn how to do ETL with streaming data using Kinesis and other tools. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1aszyi3", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1aszyi3/resource_for_learning_aws_kinesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1aszyi3/resource_for_learning_aws_kinesis/", "subreddit_subscribers": 161427, "created_utc": 1708169645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI am looking for some guidance for my containerized Flask app API that I am hosting in Azure. Previously I have hosted internal APIs with Azure Functions and ACR/ACI but never an external API and had some questions. \n\nI am using bash in my console as a makeshift CICD pipeline until I get the infrastructure squared out. So far, once I commit, the script pushes to Docker Hub and Azure Container Registry (ACR) while tagging the image with the git commit ID. It then deletes the existing container group in Azure Container Instance (ACI) and recreates it using the same FQDN.\n\nI want to introduce Azure API Management (APIM) and Front Door (AFD) now that this will be an external facing IP. The idea here is that I can manage authentication/security/rate limits as well as use a custom DNS and point to a prod/test/dev API.\n\nMy issue is that I don't see a way to maintain a static IP on the ACI and of course if I delete and recreate it the IP will change. Then I will have to reconfigure in the APIM every time. Am I going about this wrong? It doesn't seem I can point to the FQDN in this case unfortunately. Ideally, I can rebuild the ACI without losing the IP. If not, I suppose the best practice would be to query the IP and update the corresponding setting in APIM. This won't be too difficult, but it felt like there may be a simpler way.\n\nI am also open to advice on splitting the prod/test/dev in this case. My initial thought was to split at the resource group level so each environment would have their own instance of each service i.e. ACR/ACI/APIM/AFD. This is what I do for internal APIs. ChatGPT is telling me a single instance of APIM and AFD would suffice but I am taking it with a grain of salt.\n\nI understand this may be more of a platform engineering question, but I am in one of those DE positions where I build the data services including the infrastructure and DevOps. Thank you all in advance!", "author_fullname": "t2_326jvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Container Instance IP CICD best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1asmjjc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1708124914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I am looking for some guidance for my containerized Flask app API that I am hosting in Azure. Previously I have hosted internal APIs with Azure Functions and ACR/ACI but never an external API and had some questions. &lt;/p&gt;\n\n&lt;p&gt;I am using bash in my console as a makeshift CICD pipeline until I get the infrastructure squared out. So far, once I commit, the script pushes to Docker Hub and Azure Container Registry (ACR) while tagging the image with the git commit ID. It then deletes the existing container group in Azure Container Instance (ACI) and recreates it using the same FQDN.&lt;/p&gt;\n\n&lt;p&gt;I want to introduce Azure API Management (APIM) and Front Door (AFD) now that this will be an external facing IP. The idea here is that I can manage authentication/security/rate limits as well as use a custom DNS and point to a prod/test/dev API.&lt;/p&gt;\n\n&lt;p&gt;My issue is that I don&amp;#39;t see a way to maintain a static IP on the ACI and of course if I delete and recreate it the IP will change. Then I will have to reconfigure in the APIM every time. Am I going about this wrong? It doesn&amp;#39;t seem I can point to the FQDN in this case unfortunately. Ideally, I can rebuild the ACI without losing the IP. If not, I suppose the best practice would be to query the IP and update the corresponding setting in APIM. This won&amp;#39;t be too difficult, but it felt like there may be a simpler way.&lt;/p&gt;\n\n&lt;p&gt;I am also open to advice on splitting the prod/test/dev in this case. My initial thought was to split at the resource group level so each environment would have their own instance of each service i.e. ACR/ACI/APIM/AFD. This is what I do for internal APIs. ChatGPT is telling me a single instance of APIM and AFD would suffice but I am taking it with a grain of salt.&lt;/p&gt;\n\n&lt;p&gt;I understand this may be more of a platform engineering question, but I am in one of those DE positions where I build the data services including the infrastructure and DevOps. Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1asmjjc", "is_robot_indexable": true, "report_reasons": null, "author": "ShouldHaveWentBio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1asmjjc/azure_container_instance_ip_cicd_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1asmjjc/azure_container_instance_ip_cicd_best_practices/", "subreddit_subscribers": 161427, "created_utc": 1708124914.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}