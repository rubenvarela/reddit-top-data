{"kind": "Listing", "data": {"after": "t3_1bdpwxm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I came across some info graphic depicting common storage media and their size:\n\n* various generations of magnetic tape = 10TB to 100GB\n* BluRay = 25GB\n* DVD = 4.5GB\n* CD = 700MB\n* 3.5in floppy disk = 1.5MB\n\nwas there really such a huge jump from 3.5inch floppies to CDs? It almost skipped two orders of magnitude, 10MB and 100MB.  \nI did some research and found some special floppy disks that could hold 10MB to 100MB, but they seem rather rare.\n\n**Did i miss something or was there no popular physical media in that size range?**\n\nIs that just cherry picking the numbers? Worst floppies vs. best CDs\n\nGaming Consoles had a period of cartridges, was there something similar for PCs?\n\nWas swapping hard drives \"a thing\" in that time?\n\nWas there no need for a intermediate medium because floppies were just so cheap? So just using 3 to 40 floppies was cheaper than getting a new medium.\n\nWere CDs just so innovative in their design? Optical instead of magnetic, funding from the music industry", "author_fullname": "t2_u8ypxau6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Retro] Was the jump from 3.5in floppy to CD really that big? Were there no 10MB to 100MB storage media?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdwc2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 135, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 135, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710348671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across some info graphic depicting common storage media and their size:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;various generations of magnetic tape = 10TB to 100GB&lt;/li&gt;\n&lt;li&gt;BluRay = 25GB&lt;/li&gt;\n&lt;li&gt;DVD = 4.5GB&lt;/li&gt;\n&lt;li&gt;CD = 700MB&lt;/li&gt;\n&lt;li&gt;3.5in floppy disk = 1.5MB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;was there really such a huge jump from 3.5inch floppies to CDs? It almost skipped two orders of magnitude, 10MB and 100MB.&lt;br/&gt;\nI did some research and found some special floppy disks that could hold 10MB to 100MB, but they seem rather rare.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Did i miss something or was there no popular physical media in that size range?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is that just cherry picking the numbers? Worst floppies vs. best CDs&lt;/p&gt;\n\n&lt;p&gt;Gaming Consoles had a period of cartridges, was there something similar for PCs?&lt;/p&gt;\n\n&lt;p&gt;Was swapping hard drives &amp;quot;a thing&amp;quot; in that time?&lt;/p&gt;\n\n&lt;p&gt;Was there no need for a intermediate medium because floppies were just so cheap? So just using 3 to 40 floppies was cheaper than getting a new medium.&lt;/p&gt;\n\n&lt;p&gt;Were CDs just so innovative in their design? Optical instead of magnetic, funding from the music industry&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bdwc2k", "is_robot_indexable": true, "report_reasons": null, "author": "Robert_A2D0FF", "discussion_type": null, "num_comments": 300, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdwc2k/retro_was_the_jump_from_35in_floppy_to_cd_really/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdwc2k/retro_was_the_jump_from_35in_floppy_to_cd_really/", "subreddit_subscribers": 738310, "created_utc": 1710348671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://forums.thesims.com/en_US/discussion/1012877/an-important-update-on-the-future-of-these-forums/p1\n\nIt has been announced that the forums of thesims.com are closing and they are only archiving posts made between October 2022 and 12 March 2024 (i.e. yesterday). You can ask for a thread to be archived as well, but in the transition to the new EA.com forums, years and years and years of indexable posts will be lost. Can we do better than that?", "author_fullname": "t2_169tozec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TheSims.com forums are closing down and only archiving posts from Oct 2022 to 12 March 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdd2j2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 94, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 94, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710288464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://forums.thesims.com/en_US/discussion/1012877/an-important-update-on-the-future-of-these-forums/p1\"&gt;https://forums.thesims.com/en_US/discussion/1012877/an-important-update-on-the-future-of-these-forums/p1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It has been announced that the forums of thesims.com are closing and they are only archiving posts made between October 2022 and 12 March 2024 (i.e. yesterday). You can ask for a thread to be archived as well, but in the transition to the new EA.com forums, years and years and years of indexable posts will be lost. Can we do better than that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lIX8M4JjIK_H2ZXtpePSHzAYwOU2gCKatLF4pqB-B7s.jpg?auto=webp&amp;s=63beea08804db1ee7046cb6fb50ec9dddec8ed80", "width": 250, "height": 250}, "resolutions": [{"url": "https://external-preview.redd.it/lIX8M4JjIK_H2ZXtpePSHzAYwOU2gCKatLF4pqB-B7s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbed06c60b2312263d7c12f6e16e08d6e4b60034", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/lIX8M4JjIK_H2ZXtpePSHzAYwOU2gCKatLF4pqB-B7s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f4e5193a5dd52f97a113598c743da3fc0f96127", "width": 216, "height": 216}], "variants": {}, "id": "f58VoyuylrhqqDrgHKi9ZaOThATPaYsL_TJwtOUND7E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "23TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdd2j2", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticInitiative", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bdd2j2/thesimscom_forums_are_closing_down_and_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdd2j2/thesimscom_forums_are_closing_down_and_only/", "subreddit_subscribers": 738310, "created_utc": 1710288464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a pretty good collection of family photos that I have scanned over the years, and still many albums that I have not yet scanned, but as I get older in age I wonder what the point is of scanning everything into digital form. For one, neither my siblings nor myself have any children, and for two, being the sentimental one I'm mostly the only one even remotely interested in our childhood photos, photos from my parents' and grandparents' youth, etc.\n\nLately I've been asking myself what would happen to these photos if I suddenly died. I don't think anyone would care or even try to look for them. Are there repositories out there that will accept photos from random people? I have photos dating back to the 1910s and obviously up to just before the time digital cameras became prevalent. Would they be considered historically valuable?\n\nWhen I see the many photo albums that still need to be scanned, I feel overwhelmed. Like life itself, it seems hopeless or pointless to hold on to something that you know can't last forever. I wonder if I'm just being lazy, or if it really would be worthwhile to scan them, or if I should just be happy that I got to see and experience some of what is captured in these photos.\n\nSo... what would you do?", "author_fullname": "t2_rwdvvavce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanned and scanning family photos - what would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdkimx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710310764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pretty good collection of family photos that I have scanned over the years, and still many albums that I have not yet scanned, but as I get older in age I wonder what the point is of scanning everything into digital form. For one, neither my siblings nor myself have any children, and for two, being the sentimental one I&amp;#39;m mostly the only one even remotely interested in our childhood photos, photos from my parents&amp;#39; and grandparents&amp;#39; youth, etc.&lt;/p&gt;\n\n&lt;p&gt;Lately I&amp;#39;ve been asking myself what would happen to these photos if I suddenly died. I don&amp;#39;t think anyone would care or even try to look for them. Are there repositories out there that will accept photos from random people? I have photos dating back to the 1910s and obviously up to just before the time digital cameras became prevalent. Would they be considered historically valuable?&lt;/p&gt;\n\n&lt;p&gt;When I see the many photo albums that still need to be scanned, I feel overwhelmed. Like life itself, it seems hopeless or pointless to hold on to something that you know can&amp;#39;t last forever. I wonder if I&amp;#39;m just being lazy, or if it really would be worthwhile to scan them, or if I should just be happy that I got to see and experience some of what is captured in these photos.&lt;/p&gt;\n\n&lt;p&gt;So... what would you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdkimx", "is_robot_indexable": true, "report_reasons": null, "author": "jyyyyyyyyyyyyyyy", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdkimx/scanned_and_scanning_family_photos_what_would_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdkimx/scanned_and_scanning_family_photos_what_would_you/", "subreddit_subscribers": 738310, "created_utc": 1710310764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warner Bros. Discovery Disappears Games People Already Purchased", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2o3l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RhtuabX2AZtqmze1q9DPpMCkkIn4ceU3JAAZEoWeI4M.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710363635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be2o3l", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2o3l/warner_bros_discovery_disappears_games_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "subreddit_subscribers": 738310, "created_utc": 1710363635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ci9q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Publishing historic data from around year 2000?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 120, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdu64z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3XFGMGAH2-GlT7rIcF9v4jta1434UFAZkbdT4T7RwIw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710343426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ne0teadzc4oc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ne0teadzc4oc1.png?auto=webp&amp;s=f9f44a92c9a33863fefd90d5a06e01c593f46cad", "width": 1031, "height": 890}, "resolutions": [{"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11af8a72bb534f22b7c44c2fedb19c69be0e2a97", "width": 108, "height": 93}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c380d7c02e7ab686d84b7702b49c15b684348cc", "width": 216, "height": 186}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2f5c9dfd97523abeceb2c6192a7950ba730c18f", "width": 320, "height": 276}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a35e4d168d3820fe7e710c939bc67833fe86c899", "width": 640, "height": 552}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c2ddcb74149d818e55a9fe2e8030c3200fb4678", "width": 960, "height": 828}], "variants": {}, "id": "KsNaxLvy3LpavfH2h2KoYzMTDNVFrp2E549N_RergbI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdu64z", "is_robot_indexable": true, "report_reasons": null, "author": "NagateTanikaze", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdu64z/publishing_historic_data_from_around_year_2000/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ne0teadzc4oc1.png", "subreddit_subscribers": 738310, "created_utc": 1710343426.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good evening, fellow hoarders. I've been working on a custom webripper (written in C#) that can handle certain scenarios like Juicebox which can't be downloaded through simple HTTP requests. Usually this means the content is loaded through JavaScript or some similar method, so the typical downloader can't easily handle it and that's where my program comes in.\n\nThe problem: While I've built for and tested against several methods of dynamic content loading, I really only have a limited number of sites I've developed this for personally, so if I want to expand this further than the existing capabilities I'll need examples of tough sites to work with. \n\nSo if you have a certain website you'd like to download something from, please feel free to send me either a URL or the fullest source code you can get from the page and I'll incorporate that into the ripper and get back to you. \n\nObvious caveat: please don't send me anything illegal or borderline illegal, (in the United States) for the love of God. \n\nMods: I don't think this breaks any rules but I won't be offended if you remove it. \n\nEdit: the program does not currently mirror sites, it downloads assets. But now that it's been requested I'll start working on that. ", "author_fullname": "t2_oj9u8zdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request: difficult-to-rip sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdgn7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710349683.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710298200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening, fellow hoarders. I&amp;#39;ve been working on a custom webripper (written in C#) that can handle certain scenarios like Juicebox which can&amp;#39;t be downloaded through simple HTTP requests. Usually this means the content is loaded through JavaScript or some similar method, so the typical downloader can&amp;#39;t easily handle it and that&amp;#39;s where my program comes in.&lt;/p&gt;\n\n&lt;p&gt;The problem: While I&amp;#39;ve built for and tested against several methods of dynamic content loading, I really only have a limited number of sites I&amp;#39;ve developed this for personally, so if I want to expand this further than the existing capabilities I&amp;#39;ll need examples of tough sites to work with. &lt;/p&gt;\n\n&lt;p&gt;So if you have a certain website you&amp;#39;d like to download something from, please feel free to send me either a URL or the fullest source code you can get from the page and I&amp;#39;ll incorporate that into the ripper and get back to you. &lt;/p&gt;\n\n&lt;p&gt;Obvious caveat: please don&amp;#39;t send me anything illegal or borderline illegal, (in the United States) for the love of God. &lt;/p&gt;\n\n&lt;p&gt;Mods: I don&amp;#39;t think this breaks any rules but I won&amp;#39;t be offended if you remove it. &lt;/p&gt;\n\n&lt;p&gt;Edit: the program does not currently mirror sites, it downloads assets. But now that it&amp;#39;s been requested I&amp;#39;ll start working on that. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdgn7s", "is_robot_indexable": true, "report_reasons": null, "author": "cishet-camel-fucker", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdgn7s/request_difficulttorip_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdgn7s/request_difficulttorip_sites/", "subreddit_subscribers": 738310, "created_utc": 1710298200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Curious as to what you hoarders use for daily carry storage? I myself have an always on VPN back to my NAS and network. But am looking for clever ideas in terms of usb drives, nvme portable caddy\u2019s, travel setups, etc?", "author_fullname": "t2_t2xqj88b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daily Carry/Travel Kit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdd2dv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710288455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious as to what you hoarders use for daily carry storage? I myself have an always on VPN back to my NAS and network. But am looking for clever ideas in terms of usb drives, nvme portable caddy\u2019s, travel setups, etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdd2dv", "is_robot_indexable": true, "report_reasons": null, "author": "Signal_Inside3436", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdd2dv/daily_carrytravel_kit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdd2dv/daily_carrytravel_kit/", "subreddit_subscribers": 738310, "created_utc": 1710288455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For some reason I can't post in this thread: [https://www.reddit.com/r/DataHoarder/comments/1bcqjcg/recertified\\_helium\\_drives/](https://www.reddit.com/r/DataHoarder/comments/1bcqjcg/recertified_helium_drives/) But I was planning to create a new thread anyway. \n\nHere's what Seagate says about re-certifying drives. Note that there's nothing about replacing parts. They're either erasable or trashed. \n\nThank you to u/serverpartdeals for the link\n\n[***https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/support-content/warranty/\\_shared/Files/TP689.2-1606US-Media-Sanitization-Practices.pdf***](https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/support-content/warranty/_shared/Files/TP689.2-1606US-Media-Sanitization-Practices.pdf)\n\nSorry for the formatting, that's how it is in the PDF. \n\nAnd NO, I'm not a Seagate fanboi or any brand fanboi. I just remembered this when recertification was asked about the thread in the first sentence\n\n***Media Sanitization Practices***\n\n***During Product Return ProcessISO/IEC 27040***\n\n***ISO/IEC publication 27040, section 3.35, Terms and definitions:***\n\n***\u201cpurge - sanitize (3.38) using physical techniques that make***\n\n***recovery infeasible using state of the art laboratory techniques,***\n\n***but which preserves the storage media (3.48) in a potentially***\n\n***reusable state\u201d***\n\n***ISO/IEC publication 27040, section Annex A.1, Methods used***\n\n***to sanitize media:***\n\n***\u201cPurge - Degaussing, cryptographic erase (see A.3), and***\n\n***executing the appropriate ATA/SCSI firmware commands***\n\n***to use block erase operations on both logically addressable***\n\n***and logically non-addressable physical media are acceptable***\n\n***methods for purging. Degaussing is not applicable to devices***\n\n***that contain non-magnetic media (e.g. SSD or SSHD).\u201d***\n\n***ATA Secure Erase***\n\n***The AT Attachment 8 - ATA/ATAPI Command Set (ATA8-ACS)***\n\n***document defines the command SECURITY ERASE UNIT:***\n\n***\u201cWhen Normal Erase mode is specified, the SECURITY ERASE***\n\n***UNIT command shall write binary zeroes to all user data areas***\n\n***(as determined by READ NATIVE MAX or READ NATIVE MAX***\n\n***EXT).\u201d***\n\n***\u201cWhen Enhanced Erase mode is specified, the device shall write***\n\n***predetermined data patterns to all user data areas. In Enhanced***\n\n***Erase mode, all previously written user data shall be overwritten,***\n\n***including sectors that are no longer in use due to reallocation.\u201d***\n\n***The ATA Security Erase command, once initiated, runs entirely***\n\n***within the drive and reports busy until the command (full erasure)***\n\n***is complete.***\n\n***Seagate has verified that not only does its repair process***\n\n***overwrite user-addressable locations, but the process also***\n\n***overwrites the non-user accessible locations. Seagate uses***\n\n***random characters, high-frequency patterns and digital-zeros***\n\n***patterns to match the drive design technologies.***\n\n***What Is the Product Return Process?***\n\n***Seagate maintains several collection depots throughout the***\n\n***world for the purpose of receiving warranty-returned product.***\n\n***These sites are highly automated and optimized to screen the***\n\n***returned products into two fundamental groups. A significant***\n\n***percentage of drives returned to Seagate are determined to***\n\n***have No Trouble Found (NTF). These drives are separated from***\n\n***the rest for a faster recertification process. The rest of the drives***\n\n***are shipped back to Seagate factories for evaluation and repair.***\n\n***In the case of SATA interface NTF drives, Seagate uses the***\n\n***ATA SECURITY ERASE UNIT command, Enhanced Mode, as***\n\n***recommended by NIST 800-88 and ISO/IEC 27040. After media***\n\n***sanitization, the drives are relabeled and marked as Certified***\n\n***Repaired HDD drives.***\n\n***In the case of drives returned to the factory, these drives are***\n\n***reprocessed. When drives are manufactured, after the physical***\n\n***assembly of parts, the drives are processed: The drive is given***\n\n***an initial low-level format, servo calibrations and media defects***\n\n***assessment, and reallocation. New drives are fundamentally***\n\n***blank with regards to data. Reprocessed drives are blank in***\n\n***the same way. Reprocessing drives has the effect of full media***\n\n***sanitization and exceeds the ATA SECURITY ERASE UNIT***\n\n***command in thoroughness and coverage.***\n\n***All Seagate\u00ae recertified drives have a unique top-cover label with***\n\n***a green border to distinguish them from newly built products.***\n\n***Both NTF and reprocessed drives are given this unique label.***\n\n***Media Destruction on Failed Drives***\n\n***Drives that are deemed not repairable or have no repair demand***\n\n***are scrapped and recycled for their metals. The scrapping***\n\n***procedure begins with physical destruction of the entire head***\n\n***and disk assembly, which completely destroys the media.***\n\n***Destruction of media is the ultimate form of sanitization. These***\n\n***activities are carried out effectively and securely prior to sending***\n\n***for raw material reclamation.***\n\n***Seagate Self-Encrypting Drives (SED)***\n\n***Many Seagate drives are available with a self-encrypting capability.***\n\n***All data written to the media is AES-128 or AES-256 encrypted***\n\n***using a unique encryption key. No two drives have the same***\n\n***key, so no two SED drives write the same data patterns to***\n\n***the media when given the same data to write. For SED drives,***\n\n***the SECURITY ERASE Enhanced command causes the SED***\n\n***encryption key to change, instantly rendering unreadable and***\n\n***useless any previous data on the device. This includes any***\n\n***reallocated sectors and should conform to NIST 800-88 and***\n\n***ISO/IEC 27040. Some Seagate SED drives have the further***\n\n***distinction of having FIPS 140-2 Level 2 validation, a U.S.***\n\n***government standard. Seagate SED and FIPS SED drives are***\n\n***always reprocessed.***\n\n***Non-SATA Interfaces: SAS, SCSI and Fibre Channel***\n\n***An internal secure erase command is defined by the ANSI SCSI***\n\n***specifications. It is called Security Initialize and is functionally***\n\n***equivalent to the ANSI ATA specification. In addition, the***\n\n***Sanitize command set is available on many products, which***\n\n***provides a single-command offline purge (erase) that runs until***\n\n***it finishes.***\n\n***USB External Drives***\n\n***USB drives have a SATA drive contained within them. A small***\n\n***circuit board bridges and joins the SATA and USB interfaces.***\n\n***Some USB bridge cards restrict the ATA SECURITY ERASE***\n\n***command, while others allow it. Newer Seagate USB products***\n\n***are given full media sanitization using the ATA SECURITY***\n\n***ERASE. Products that do not allow the command are given a***\n\n***full pack block overwrite of the media***", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate's drive recertification process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdj7kx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710305934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason I can&amp;#39;t post in this thread: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1bcqjcg/recertified_helium_drives/\"&gt;https://www.reddit.com/r/DataHoarder/comments/1bcqjcg/recertified_helium_drives/&lt;/a&gt; But I was planning to create a new thread anyway. &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what Seagate says about re-certifying drives. Note that there&amp;#39;s nothing about replacing parts. They&amp;#39;re either erasable or trashed. &lt;/p&gt;\n\n&lt;p&gt;Thank you to &lt;a href=\"/u/serverpartdeals\"&gt;u/serverpartdeals&lt;/a&gt; for the link&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/support-content/warranty/_shared/Files/TP689.2-1606US-Media-Sanitization-Practices.pdf\"&gt;&lt;strong&gt;&lt;em&gt;https://www.seagate.com/content/dam/seagate/migrated-assets/www-content/support-content/warranty/_shared/Files/TP689.2-1606US-Media-Sanitization-Practices.pdf&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the formatting, that&amp;#39;s how it is in the PDF. &lt;/p&gt;\n\n&lt;p&gt;And NO, I&amp;#39;m not a Seagate fanboi or any brand fanboi. I just remembered this when recertification was asked about the thread in the first sentence&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Media Sanitization Practices&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;During Product Return ProcessISO/IEC 27040&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ISO/IEC publication 27040, section 3.35, Terms and definitions:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;\u201cpurge - sanitize (3.38) using physical techniques that make&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;recovery infeasible using state of the art laboratory techniques,&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;but which preserves the storage media (3.48) in a potentially&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;reusable state\u201d&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ISO/IEC publication 27040, section Annex A.1, Methods used&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;to sanitize media:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;\u201cPurge - Degaussing, cryptographic erase (see A.3), and&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;executing the appropriate ATA/SCSI firmware commands&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;to use block erase operations on both logically addressable&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;and logically non-addressable physical media are acceptable&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;methods for purging. Degaussing is not applicable to devices&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;that contain non-magnetic media (e.g. SSD or SSHD).\u201d&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ATA Secure Erase&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;The AT Attachment 8 - ATA/ATAPI Command Set (ATA8-ACS)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;document defines the command SECURITY ERASE UNIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;\u201cWhen Normal Erase mode is specified, the SECURITY ERASE&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;UNIT command shall write binary zeroes to all user data areas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;(as determined by READ NATIVE MAX or READ NATIVE MAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;EXT).\u201d&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;\u201cWhen Enhanced Erase mode is specified, the device shall write&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;predetermined data patterns to all user data areas. In Enhanced&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Erase mode, all previously written user data shall be overwritten,&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;including sectors that are no longer in use due to reallocation.\u201d&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;The ATA Security Erase command, once initiated, runs entirely&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;within the drive and reports busy until the command (full erasure)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;is complete.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Seagate has verified that not only does its repair process&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;overwrite user-addressable locations, but the process also&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;overwrites the non-user accessible locations. Seagate uses&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;random characters, high-frequency patterns and digital-zeros&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;patterns to match the drive design technologies.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;What Is the Product Return Process?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Seagate maintains several collection depots throughout the&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;world for the purpose of receiving warranty-returned product.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;These sites are highly automated and optimized to screen the&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;returned products into two fundamental groups. A significant&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;percentage of drives returned to Seagate are determined to&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;have No Trouble Found (NTF). These drives are separated from&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;the rest for a faster recertification process. The rest of the drives&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;are shipped back to Seagate factories for evaluation and repair.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;In the case of SATA interface NTF drives, Seagate uses the&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ATA SECURITY ERASE UNIT command, Enhanced Mode, as&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;recommended by NIST 800-88 and ISO/IEC 27040. After media&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;sanitization, the drives are relabeled and marked as Certified&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Repaired HDD drives.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;In the case of drives returned to the factory, these drives are&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;reprocessed. When drives are manufactured, after the physical&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;assembly of parts, the drives are processed: The drive is given&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;an initial low-level format, servo calibrations and media defects&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;assessment, and reallocation. New drives are fundamentally&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;blank with regards to data. Reprocessed drives are blank in&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;the same way. Reprocessing drives has the effect of full media&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;sanitization and exceeds the ATA SECURITY ERASE UNIT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;command in thoroughness and coverage.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;All Seagate\u00ae recertified drives have a unique top-cover label with&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;a green border to distinguish them from newly built products.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Both NTF and reprocessed drives are given this unique label.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Media Destruction on Failed Drives&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Drives that are deemed not repairable or have no repair demand&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;are scrapped and recycled for their metals. The scrapping&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;procedure begins with physical destruction of the entire head&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;and disk assembly, which completely destroys the media.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Destruction of media is the ultimate form of sanitization. These&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;activities are carried out effectively and securely prior to sending&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;for raw material reclamation.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Seagate Self-Encrypting Drives (SED)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Many Seagate drives are available with a self-encrypting capability.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;All data written to the media is AES-128 or AES-256 encrypted&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;using a unique encryption key. No two drives have the same&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;key, so no two SED drives write the same data patterns to&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;the media when given the same data to write. For SED drives,&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;the SECURITY ERASE Enhanced command causes the SED&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;encryption key to change, instantly rendering unreadable and&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;useless any previous data on the device. This includes any&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;reallocated sectors and should conform to NIST 800-88 and&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ISO/IEC 27040. Some Seagate SED drives have the further&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;distinction of having FIPS 140-2 Level 2 validation, a U.S.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;government standard. Seagate SED and FIPS SED drives are&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;always reprocessed.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Non-SATA Interfaces: SAS, SCSI and Fibre Channel&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;An internal secure erase command is defined by the ANSI SCSI&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;specifications. It is called Security Initialize and is functionally&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;equivalent to the ANSI ATA specification. In addition, the&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sanitize command set is available on many products, which&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;provides a single-command offline purge (erase) that runs until&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;it finishes.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;USB External Drives&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;USB drives have a SATA drive contained within them. A small&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;circuit board bridges and joins the SATA and USB interfaces.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Some USB bridge cards restrict the ATA SECURITY ERASE&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;command, while others allow it. Newer Seagate USB products&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;are given full media sanitization using the ATA SECURITY&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ERASE. Products that do not allow the command are given a&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;full pack block overwrite of the media&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdj7kx", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdj7kx/seagates_drive_recertification_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdj7kx/seagates_drive_recertification_process/", "subreddit_subscribers": 738310, "created_utc": 1710305934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "scenario is we have content producer creating up to 20GB a day of data.  \nThis data is captured from the source media (SSD, SDCard, whatever) via a Macbook, onto an OWC Thunderbay 8.\n\nThis data needs to then be de-identified, at which point it also becomes smaller, and once approved, we can ship it off somewhere else. This part isn't in contention..\n\nOnce this happens, the raw data on the source media can be deleted (but it remains on the OWC and will need to be kept for 7 years.) \n\nONLY de-identified data can be taken offsite in any way, shape or form. I'm trying to find a way to back up this raw data on a daily basis. Whatever it's backed up to will then need to be stored in a fireproof safe.  The idea being, if there's a fire and they lose all the equipment, at least the raw data is there. If they lose both the equipment and that raw data, well, that's a problem.\n\n&amp;#x200B;\n\nBased on this, I'm pretty sure LTO is the best way to go, not just because it's easier for the people on the site to switch out a tape, but also higher capacity. \n\n&amp;#x200B;\n\nMy plan is to get a Windows PC with LTO8 or 9, run Veeam B&amp;R and grab all the content off a file share on the Mac, stream it to tape, put it in the fire-proof safe. \n\n&amp;#x200B;\n\nIs there a better option, or alternatives I haven't considered?  \n", "author_fullname": "t2_11jy8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20+TB, 20GB a day new data, backup options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdl8km", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710313644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;scenario is we have content producer creating up to 20GB a day of data.&lt;br/&gt;\nThis data is captured from the source media (SSD, SDCard, whatever) via a Macbook, onto an OWC Thunderbay 8.&lt;/p&gt;\n\n&lt;p&gt;This data needs to then be de-identified, at which point it also becomes smaller, and once approved, we can ship it off somewhere else. This part isn&amp;#39;t in contention..&lt;/p&gt;\n\n&lt;p&gt;Once this happens, the raw data on the source media can be deleted (but it remains on the OWC and will need to be kept for 7 years.) &lt;/p&gt;\n\n&lt;p&gt;ONLY de-identified data can be taken offsite in any way, shape or form. I&amp;#39;m trying to find a way to back up this raw data on a daily basis. Whatever it&amp;#39;s backed up to will then need to be stored in a fireproof safe.  The idea being, if there&amp;#39;s a fire and they lose all the equipment, at least the raw data is there. If they lose both the equipment and that raw data, well, that&amp;#39;s a problem.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Based on this, I&amp;#39;m pretty sure LTO is the best way to go, not just because it&amp;#39;s easier for the people on the site to switch out a tape, but also higher capacity. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My plan is to get a Windows PC with LTO8 or 9, run Veeam B&amp;amp;R and grab all the content off a file share on the Mac, stream it to tape, put it in the fire-proof safe. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there a better option, or alternatives I haven&amp;#39;t considered?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdl8km", "is_robot_indexable": true, "report_reasons": null, "author": "lemachet", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdl8km/20tb_20gb_a_day_new_data_backup_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdl8km/20tb_20gb_a_day_new_data_backup_options/", "subreddit_subscribers": 738310, "created_utc": 1710313644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?\n\nI have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.", "author_fullname": "t2_1hrlp7qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R-Pi with basic install plus Kiwix VS I-I-A-B and Kiwix - Which is better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2wrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?&lt;/p&gt;\n\n&lt;p&gt;I have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2wrj", "is_robot_indexable": true, "report_reasons": null, "author": "Bruh-Nanaz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "subreddit_subscribers": 738310, "created_utc": 1710364197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi fellow data hoarders,\n\nI've been frustrated multiple times over the years by WhatsApp's limitations about exporting my conversations (1 conversation at a time, only 40.000 messages without media / 10.000 messages with \\~10 media). I suspect this is a deliberate limitation on their part, as I believe that in the past (before acquisition by Facebook) you could export your full conversation history without issue.\n\nThis should not be an issue, given that Facebook provides a complete messaging history when you ask to export your data.\n\nFor my fellow EU citizens, do you think this is in violation of the GDPR? Maybe spamming their GDPR mailbox with full export requests and asking the opinion of a GDPR lawyer could change things...", "author_fullname": "t2_gxsnh5ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GDPR action to get WhatsApp full chat exports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdn2gj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710321521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data hoarders,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been frustrated multiple times over the years by WhatsApp&amp;#39;s limitations about exporting my conversations (1 conversation at a time, only 40.000 messages without media / 10.000 messages with ~10 media). I suspect this is a deliberate limitation on their part, as I believe that in the past (before acquisition by Facebook) you could export your full conversation history without issue.&lt;/p&gt;\n\n&lt;p&gt;This should not be an issue, given that Facebook provides a complete messaging history when you ask to export your data.&lt;/p&gt;\n\n&lt;p&gt;For my fellow EU citizens, do you think this is in violation of the GDPR? Maybe spamming their GDPR mailbox with full export requests and asking the opinion of a GDPR lawyer could change things...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdn2gj", "is_robot_indexable": true, "report_reasons": null, "author": "Elastix7865", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdn2gj/gdpr_action_to_get_whatsapp_full_chat_exports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdn2gj/gdpr_action_to_get_whatsapp_full_chat_exports/", "subreddit_subscribers": 738310, "created_utc": 1710321521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.\n\nThis site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.", "author_fullname": "t2_364dp83u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring taringa.net", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2q6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710363763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.&lt;/p&gt;\n\n&lt;p&gt;This site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2q6y", "is_robot_indexable": true, "report_reasons": null, "author": "2muchnet42day", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "subreddit_subscribers": 738310, "created_utc": 1710363763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI am in search for some songs from an artist in the mp3 .com archive. I downloaded the skeleton and used that to retrieve some songs from the artist page, but was not able to find the rest of the songs on the album.\n\nThe album(CD) is listed on their page as a separate link. There is one link to listen to all of the songs(through the browser if i\u2019m understanding the webpage contents correctly) and then one to purchase the CD back when they had that service. Unfortunately, the album link is not directly on the internet archive and I assume there has to be some workarounds similar to how individual songs ought to be downloaded.\n\nRequest: Has there been any luck with getting entire albums, even through the m3u files, from this archive? I am downloading the first half of the dump (entire mp3s) at the moment to look myself, but this might take a couple days. I would really like to get the entire album or this m3u file to hopefully point to missing tracks that were not advertised on artist pages.\n\nThank you for your time", "author_fullname": "t2_55ob2k04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MP3 .com Archive M3U/Album Files Assistance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be1l3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710361036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am in search for some songs from an artist in the mp3 .com archive. I downloaded the skeleton and used that to retrieve some songs from the artist page, but was not able to find the rest of the songs on the album.&lt;/p&gt;\n\n&lt;p&gt;The album(CD) is listed on their page as a separate link. There is one link to listen to all of the songs(through the browser if i\u2019m understanding the webpage contents correctly) and then one to purchase the CD back when they had that service. Unfortunately, the album link is not directly on the internet archive and I assume there has to be some workarounds similar to how individual songs ought to be downloaded.&lt;/p&gt;\n\n&lt;p&gt;Request: Has there been any luck with getting entire albums, even through the m3u files, from this archive? I am downloading the first half of the dump (entire mp3s) at the moment to look myself, but this might take a couple days. I would really like to get the entire album or this m3u file to hopefully point to missing tracks that were not advertised on artist pages.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be1l3c", "is_robot_indexable": true, "report_reasons": null, "author": "ComplexMotives99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be1l3c/mp3_com_archive_m3ualbum_files_assistance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be1l3c/mp3_com_archive_m3ualbum_files_assistance/", "subreddit_subscribers": 738310, "created_utc": 1710361036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello everyone, I need help with Synology Photos ? Why it's so slow to view a video with Quickconnect or OpenVPN ? I don't understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I'm away, it's so slow, it's not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! ", "author_fullname": "t2_p67lkkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Synology photos expert here to find a fix (slow videos when outside of my LAN)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1be5ig8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710370407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I need help with Synology Photos ? Why it&amp;#39;s so slow to view a video with Quickconnect or OpenVPN ? I don&amp;#39;t understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I&amp;#39;m away, it&amp;#39;s so slow, it&amp;#39;s not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be5ig8", "is_robot_indexable": true, "report_reasons": null, "author": "d2racing911", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "subreddit_subscribers": 738310, "created_utc": 1710370407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there's a way to  scrape/bulk-download the CSV's for different charts?    ", "author_fullname": "t2_ewdcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping Spotify Charts CSV s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzth9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there&amp;#39;s a way to  scrape/bulk-download the CSV&amp;#39;s for different charts?    &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzth9", "is_robot_indexable": true, "report_reasons": null, "author": "Penguintim", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "subreddit_subscribers": 738310, "created_utc": 1710356811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Especially if you are apart of one, do you find value in being a member?\n\nOr are they resume boosters, with occasional sideline benefits?", "author_fullname": "t2_aqgp2v9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is joining a professional archive association worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdxbhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710351000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Especially if you are apart of one, do you find value in being a member?&lt;/p&gt;\n\n&lt;p&gt;Or are they resume boosters, with occasional sideline benefits?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdxbhj", "is_robot_indexable": true, "report_reasons": null, "author": "violet-doggo-2019", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdxbhj/is_joining_a_professional_archive_association/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdxbhj/is_joining_a_professional_archive_association/", "subreddit_subscribers": 738310, "created_utc": 1710351000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to add more drives to my unraid server but encountered a problem, i cant add a LSI SAS card because the large PCle slot is being used by the GPU for my Plex transcoding and offloading from the CPU.  Setup:\n\n* Fractal Desing Node 304 (Mini ITX or M-ATX)\n* Asrock M-ATX H310 DVS\n* CPU   i3-9100F \n* GPU NVIDIA Quadro p400\n* x1 PCle Slot SATA expansion card for +4 SATA ports\n\nWhat should i change to add an LSI SAS  x8 PCle Card?\n\nChange my motherboard for two x16 PCle slots?\n\nUse my CPU for transcoding as it has intel quick sync and change the GPU slot for the new LSI  SAS card?\n\ni did not anticipate this hardware limitation as i already have x2 Cache drives, x1 Parity and x5 Disks in the array but planning on adding a second parity and 3 more drives to the array.", "author_fullname": "t2_66ale9u9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware Dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdtr3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710342401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to add more drives to my unraid server but encountered a problem, i cant add a LSI SAS card because the large PCle slot is being used by the GPU for my Plex transcoding and offloading from the CPU.  Setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fractal Desing Node 304 (Mini ITX or M-ATX)&lt;/li&gt;\n&lt;li&gt;Asrock M-ATX H310 DVS&lt;/li&gt;\n&lt;li&gt;CPU   i3-9100F &lt;/li&gt;\n&lt;li&gt;GPU NVIDIA Quadro p400&lt;/li&gt;\n&lt;li&gt;x1 PCle Slot SATA expansion card for +4 SATA ports&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What should i change to add an LSI SAS  x8 PCle Card?&lt;/p&gt;\n\n&lt;p&gt;Change my motherboard for two x16 PCle slots?&lt;/p&gt;\n\n&lt;p&gt;Use my CPU for transcoding as it has intel quick sync and change the GPU slot for the new LSI  SAS card?&lt;/p&gt;\n\n&lt;p&gt;i did not anticipate this hardware limitation as i already have x2 Cache drives, x1 Parity and x5 Disks in the array but planning on adding a second parity and 3 more drives to the array.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdtr3p", "is_robot_indexable": true, "report_reasons": null, "author": "iweputo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdtr3p/hardware_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdtr3p/hardware_dilemma/", "subreddit_subscribers": 738310, "created_utc": 1710342401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody,\n\nI'm in a process of moving my storage/media/automation server to a dedicated rack in a closet.\n\nServer itself is no special thing - repurposed gen4 i7 desktop with several WD reds in ZFS, been stable running 24/7 for years, no issues whatsoever.\n\nHowever, due to limited space, I had to get a 16U shallow rack that has \\~16.1\"/41cm of depth for equipment. This severely limits my choices for a server case.\n\nOf course, I'd like a possibility to expand my storage in near future (only couple of TBs left free).\n\nSo what I think my options are:\n\n1. 4U case with at least 8 hdd bays. Haven't been successful in finding any that are &lt;16\" in depth and can host enough drives\n2. 2U case for server + LSI HBA + 8-12 slot disk shelf. No luck in finding &lt;16\" shelves.\n\nPersonally i'd like to go with option #2, since it is going to be easier to expand storage later on.\n\nAs it is a home ( or a closet) lab, price is always important.\n\nSoooo, any suggestions on a possible setup that could fit my 16\" deep rack?", "author_fullname": "t2_znxp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request: looking for advice on shallow-depth rack equipment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdr8r5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710335913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a process of moving my storage/media/automation server to a dedicated rack in a closet.&lt;/p&gt;\n\n&lt;p&gt;Server itself is no special thing - repurposed gen4 i7 desktop with several WD reds in ZFS, been stable running 24/7 for years, no issues whatsoever.&lt;/p&gt;\n\n&lt;p&gt;However, due to limited space, I had to get a 16U shallow rack that has ~16.1&amp;quot;/41cm of depth for equipment. This severely limits my choices for a server case.&lt;/p&gt;\n\n&lt;p&gt;Of course, I&amp;#39;d like a possibility to expand my storage in near future (only couple of TBs left free).&lt;/p&gt;\n\n&lt;p&gt;So what I think my options are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;4U case with at least 8 hdd bays. Haven&amp;#39;t been successful in finding any that are &amp;lt;16&amp;quot; in depth and can host enough drives&lt;/li&gt;\n&lt;li&gt;2U case for server + LSI HBA + 8-12 slot disk shelf. No luck in finding &amp;lt;16&amp;quot; shelves.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Personally i&amp;#39;d like to go with option #2, since it is going to be easier to expand storage later on.&lt;/p&gt;\n\n&lt;p&gt;As it is a home ( or a closet) lab, price is always important.&lt;/p&gt;\n\n&lt;p&gt;Soooo, any suggestions on a possible setup that could fit my 16&amp;quot; deep rack?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdr8r5", "is_robot_indexable": true, "report_reasons": null, "author": "sixfoxtails", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdr8r5/request_looking_for_advice_on_shallowdepth_rack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdr8r5/request_looking_for_advice_on_shallowdepth_rack/", "subreddit_subscribers": 738310, "created_utc": 1710335913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've had the 1 GB Lifetime for a couple of years.  This morning I received the following.  So much for Lifetime.  I did decide to spring for the $2.99/month.  It's seems to work.  I've never had a do a full recovery though.  I do an image backup to an external drive periodically.\n\n&amp;#x200B;\n\nWe hope this message finds you well. Today, we're reaching out with an important update that impacts our PolarBackup community, including you.\n\n**A Necessary Transition for Sustained Quality**\n\nIn the face of rising operational costs - from data storage to electricity - maintaining our lifetime plan model has become unsustainable. This challenge compels us to adapt, ensuring we can continue offering you the reliable, secure service you depend on.\n\n## Unbeatable Value:\n\n* Your Current Storage: You secured **1 TB of cloud** storage at a great one-time cost.\n* Industry Comparison: **Competitors charge around $12.99/month on average for 2 TB.**\n* **Embracing change**, we're boosting your value: **Get 2 TB** with PolarBackup for just **$2.99/mo ($****~~12.99/mo~~****)** or $29.99/year, enhancing the service to meet your needs.\n\n## \u00a0\u00a0Savings Over 5 Years:\n\n## Competitors: $780 + Hidden Fees\n\n## VS\n\n## Polarbackup: $150\n\n## No Hidden Fees\n\n## Why This Move?\n\n* **To Cover Increasing Costs:** Your small monthly fee helps us manage the rising costs while avoiding the discontinuation of services.\n\n\ud83d\udcf7\n\n*New Version Coming Soon*\n\n\ud83d\udcf7\n\n* **Continued Excellence:** Ensure faster speeds, regular updates, and enhanced security for your backups.\n\n## Act Now: 14-Day Window:\n\nTo transition smoothly and keep your account active, please consider this upgrade **within 14 days**. We're here to assist you in every step, guaranteeing the process is straightforward and beneficial.\n\n## Upgrade Instantly with One-Click Join!\n\n## 2 TB$2.99($12.99) /mo\n\n[Subscribe now](https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;)\n\n## 6 TB$6.99($29.99)/mo\n\n[Subscribe now](https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;)\n\n[Explore yearly plans and more options here](https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;)\n\n**If You Decide Not to Upgrade**\n\nWe understand this decision may not fit everyone's needs. Should you choose not to upgrade, we're offering a 14-day window to restore your data, ensuring you have the time and resources needed to secure your files.\n\nThis decision wasn't made lightly. Our team has worked hard to balance sustaining our service with the realities of our operational costs, striving to offer you an option that keeps your data secure at a fraction of the cost.\n\nWe deeply appreciate your understanding and continued support as we navigate these changes. Our team is here for any questions or concerns you may have.\n\nWarm regards,\n\nThe PolarBackup Team\n\nImportant Note: In accordance with our [**Terms and Conditions**](https://mautic.polarbackup.com/r/d6a5ba1ac09cbc7db2a1e7178?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;), failure to add the maintenance plan will result in the cancellation of your lifetime service.", "author_fullname": "t2_5f6z7a7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So Much for Lifetime", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdp32r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710329085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had the 1 GB Lifetime for a couple of years.  This morning I received the following.  So much for Lifetime.  I did decide to spring for the $2.99/month.  It&amp;#39;s seems to work.  I&amp;#39;ve never had a do a full recovery though.  I do an image backup to an external drive periodically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We hope this message finds you well. Today, we&amp;#39;re reaching out with an important update that impacts our PolarBackup community, including you.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A Necessary Transition for Sustained Quality&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In the face of rising operational costs - from data storage to electricity - maintaining our lifetime plan model has become unsustainable. This challenge compels us to adapt, ensuring we can continue offering you the reliable, secure service you depend on.&lt;/p&gt;\n\n&lt;h2&gt;Unbeatable Value:&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Your Current Storage: You secured &lt;strong&gt;1 TB of cloud&lt;/strong&gt; storage at a great one-time cost.&lt;/li&gt;\n&lt;li&gt;Industry Comparison: &lt;strong&gt;Competitors charge around $12.99/month on average for 2 TB.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Embracing change&lt;/strong&gt;, we&amp;#39;re boosting your value: &lt;strong&gt;Get 2 TB&lt;/strong&gt; with PolarBackup for just &lt;strong&gt;$2.99/mo ($&lt;/strong&gt;&lt;strong&gt;&lt;del&gt;12.99/mo&lt;/del&gt;&lt;/strong&gt;&lt;strong&gt;)&lt;/strong&gt; or $29.99/year, enhancing the service to meet your needs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;\u00a0\u00a0Savings Over 5 Years:&lt;/h2&gt;\n\n&lt;h2&gt;Competitors: $780 + Hidden Fees&lt;/h2&gt;\n\n&lt;h2&gt;VS&lt;/h2&gt;\n\n&lt;h2&gt;Polarbackup: $150&lt;/h2&gt;\n\n&lt;h2&gt;No Hidden Fees&lt;/h2&gt;\n\n&lt;h2&gt;Why This Move?&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;To Cover Increasing Costs:&lt;/strong&gt; Your small monthly fee helps us manage the rising costs while avoiding the discontinuation of services.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;\ud83d\udcf7&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;New Version Coming Soon&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf7&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Continued Excellence:&lt;/strong&gt; Ensure faster speeds, regular updates, and enhanced security for your backups.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Act Now: 14-Day Window:&lt;/h2&gt;\n\n&lt;p&gt;To transition smoothly and keep your account active, please consider this upgrade &lt;strong&gt;within 14 days&lt;/strong&gt;. We&amp;#39;re here to assist you in every step, guaranteeing the process is straightforward and beneficial.&lt;/p&gt;\n\n&lt;h2&gt;Upgrade Instantly with One-Click Join!&lt;/h2&gt;\n\n&lt;h2&gt;2 TB$2.99($12.99) /mo&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;amp;\"&gt;Subscribe now&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;6 TB$6.99($29.99)/mo&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;amp;\"&gt;Subscribe now&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://mautic.polarbackup.com/r/aa9c3a3a23b5d9ac1c8e2ddbf?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;amp;\"&gt;Explore yearly plans and more options here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;If You Decide Not to Upgrade&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We understand this decision may not fit everyone&amp;#39;s needs. Should you choose not to upgrade, we&amp;#39;re offering a 14-day window to restore your data, ensuring you have the time and resources needed to secure your files.&lt;/p&gt;\n\n&lt;p&gt;This decision wasn&amp;#39;t made lightly. Our team has worked hard to balance sustaining our service with the realities of our operational costs, striving to offer you an option that keeps your data secure at a fraction of the cost.&lt;/p&gt;\n\n&lt;p&gt;We deeply appreciate your understanding and continued support as we navigate these changes. Our team is here for any questions or concerns you may have.&lt;/p&gt;\n\n&lt;p&gt;Warm regards,&lt;/p&gt;\n\n&lt;p&gt;The PolarBackup Team&lt;/p&gt;\n\n&lt;p&gt;Important Note: In accordance with our &lt;a href=\"https://mautic.polarbackup.com/r/d6a5ba1ac09cbc7db2a1e7178?ct=YTo1OntzOjY6InNvdXJjZSI7YToyOntpOjA7czo1OiJlbWFpbCI7aToxO2k6MTM7fXM6NToiZW1haWwiO2k6MTM7czo0OiJzdGF0IjtzOjIyOiI2NWYxNmNkNWM4OWQxOTg3ODg1MTU3IjtzOjQ6ImxlYWQiO3M6NDoiNzk0OSI7czo3OiJjaGFubmVsIjthOjE6e3M6NToiZW1haWwiO2k6MTM7fX0%3D&amp;amp;\"&gt;&lt;strong&gt;Terms and Conditions&lt;/strong&gt;&lt;/a&gt;, failure to add the maintenance plan will result in the cancellation of your lifetime service.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdp32r", "is_robot_indexable": true, "report_reasons": null, "author": "dudenator69", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdp32r/so_much_for_lifetime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdp32r/so_much_for_lifetime/", "subreddit_subscribers": 738310, "created_utc": 1710329085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I'm trying to preserve a King Crimson DVD, it's mainly music with some live show videos, [here's ](https://imgur.com/a/XICSEfg) a picture of the booklet with the different audio files it has, what would be the best option to rip this?\nI already tried handbrake, the original album was in one video file, with a broken/corrupted image for the video, the bonus track was the same, all without chapters splitting the different parts, the live videos were all in a file, with around a minute of nothingness in the start, but with chapters, and the last file was the dvd intro and outro, can this be improved?\nThanks.", "author_fullname": "t2_s2xocq9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be the best approach for ripping this music DVD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdchke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710286994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m trying to preserve a King Crimson DVD, it&amp;#39;s mainly music with some live show videos, &lt;a href=\"https://imgur.com/a/XICSEfg\"&gt;here&amp;#39;s &lt;/a&gt; a picture of the booklet with the different audio files it has, what would be the best option to rip this?\nI already tried handbrake, the original album was in one video file, with a broken/corrupted image for the video, the bonus track was the same, all without chapters splitting the different parts, the live videos were all in a file, with around a minute of nothingness in the start, but with chapters, and the last file was the dvd intro and outro, can this be improved?\nThanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?auto=webp&amp;s=bf237c76b6dd0eb2845c8c7a8811c42029eccf5c", "width": 4080, "height": 3072}, "resolutions": [{"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d8386fbdce3510c674740776605feb0377bd58d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa47c857e1de19a401f39f1181d76396c298de62", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f933ae3dd33581a418e940567a9434f6941f88d3", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72d14e8c07b9439aeefcd7ac84b429041ca4d39d", "width": 640, "height": 481}, {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=734eca0686832899cdcacc5bb21d3468e90e5cb5", "width": 960, "height": 722}, {"url": "https://external-preview.redd.it/-LGESjQWcwrfySIVgy6qhMBty4z0jR-Rl359acI1sbo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aa63088f8265d23e4178273bcac94b8f68dee99", "width": 1080, "height": 813}], "variants": {}, "id": "APOsJi8wTb0rlKGhpIrzyRsqMcu_0__2XpbtCCKxCPg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdchke", "is_robot_indexable": true, "report_reasons": null, "author": "GBember", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdchke/what_would_be_the_best_approach_for_ripping_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdchke/what_would_be_the_best_approach_for_ripping_this/", "subreddit_subscribers": 738310, "created_utc": 1710286994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any application or some kind of tool/technique to where I can dedicate an SSD pool and dataset to ingest fast copy write data then move it to larger hdd pool automagically? ", "author_fullname": "t2_vsbo9omx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Truenas Cache?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzyfv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710357146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any application or some kind of tool/technique to where I can dedicate an SSD pool and dataset to ingest fast copy write data then move it to larger hdd pool automagically? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzyfv", "is_robot_indexable": true, "report_reasons": null, "author": "Kltpzyxmm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzyfv/truenas_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzyfv/truenas_cache/", "subreddit_subscribers": 738310, "created_utc": 1710357146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the market for a large capacity HDD 16-22tb but I'm noticing the prices are almost entirely random if not nonsensical depending what sites you look at. For example I can find 16tb ironwolf pros new for \u00a3170 all the way up to \u00a3400. Why such the disparity between prices and why would scan.co.uk sell the 16tb for \u00a3360 when MSRP is \u00a3180 apparently. Any insight would be helpful.\n", "author_fullname": "t2_7pmhqt55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MSRP and HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdgfe6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710297585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the market for a large capacity HDD 16-22tb but I&amp;#39;m noticing the prices are almost entirely random if not nonsensical depending what sites you look at. For example I can find 16tb ironwolf pros new for \u00a3170 all the way up to \u00a3400. Why such the disparity between prices and why would scan.co.uk sell the 16tb for \u00a3360 when MSRP is \u00a3180 apparently. Any insight would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdgfe6", "is_robot_indexable": true, "report_reasons": null, "author": "Constant_Ad1749", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdgfe6/msrp_and_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdgfe6/msrp_and_hdds/", "subreddit_subscribers": 738310, "created_utc": 1710297585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to gather hundreds of TB to store large datasets for AI experiments. Online solutions are completely crazy, in less than a year i spent the same money i'd spend to directly buy the disks.\n\nI'm actually buying used drives when they are sold for less than 10\u20ac/TB (I'm from Italy), but i was looking for an even cheaper solution because money is not infinite and this experiments will bring me money not very soon, so i ended up searching for the cheapest (but decent) tape storage solutions, and I've found this LTO ultrium 9 that is sold as new for about 5\u20ac/TB uncompressed (or an astonishing 2\u20ac/TB if compressed, but i don't know how much it's safe to store compressed data).  \n\n\nQuestion: is this the cheapest storage solution?  \nQuestion 2: i've seen that there are many tape drives, and some cost about 5000\u20ac while others 100\u20ac but i can't really understand how to check compatibility and i don't know if there are some reliableness issues with recording tapes with the cheapest drives (like, i don't know, weaker magnetic flux transitions). Can you please give me some advice?  \n", "author_fullname": "t2_dwt1zb8c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is LTO ultrium 9 the cheapest storage solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdekra", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710292511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to gather hundreds of TB to store large datasets for AI experiments. Online solutions are completely crazy, in less than a year i spent the same money i&amp;#39;d spend to directly buy the disks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m actually buying used drives when they are sold for less than 10\u20ac/TB (I&amp;#39;m from Italy), but i was looking for an even cheaper solution because money is not infinite and this experiments will bring me money not very soon, so i ended up searching for the cheapest (but decent) tape storage solutions, and I&amp;#39;ve found this LTO ultrium 9 that is sold as new for about 5\u20ac/TB uncompressed (or an astonishing 2\u20ac/TB if compressed, but i don&amp;#39;t know how much it&amp;#39;s safe to store compressed data).  &lt;/p&gt;\n\n&lt;p&gt;Question: is this the cheapest storage solution?&lt;br/&gt;\nQuestion 2: i&amp;#39;ve seen that there are many tape drives, and some cost about 5000\u20ac while others 100\u20ac but i can&amp;#39;t really understand how to check compatibility and i don&amp;#39;t know if there are some reliableness issues with recording tapes with the cheapest drives (like, i don&amp;#39;t know, weaker magnetic flux transitions). Can you please give me some advice?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdekra", "is_robot_indexable": true, "report_reasons": null, "author": "AstroGippi", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdekra/is_lto_ultrium_9_the_cheapest_storage_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdekra/is_lto_ultrium_9_the_cheapest_storage_solution/", "subreddit_subscribers": 738310, "created_utc": 1710292511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I currently run Plex on an old laptop. I'm looking to expand my storage but can't decide if it's worth it to start a NAS or get an enclosure with RAID. Any specific reason for one versus the other? ", "author_fullname": "t2_74z5vo8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS vs Bay Enclosure for Plex", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdegvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710292223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I currently run Plex on an old laptop. I&amp;#39;m looking to expand my storage but can&amp;#39;t decide if it&amp;#39;s worth it to start a NAS or get an enclosure with RAID. Any specific reason for one versus the other? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdegvw", "is_robot_indexable": true, "report_reasons": null, "author": "QueenKaia", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdegvw/nas_vs_bay_enclosure_for_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdegvw/nas_vs_bay_enclosure_for_plex/", "subreddit_subscribers": 738310, "created_utc": 1710292223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI've been trying to scrape some data from Twitter for a personal project, but I've hit a roadblock because I don't have access to Twitter's API. I've tried a few methods, but none seem to work effectively.\n\nDoes anyone have experience or recommendations for tools or methods to scrape tweets from Twitter without relying on the API? I'm looking for something reliable and preferably free, but I'm willing to consider paid options if they get the job done.\n\nAny suggestions or advice would be greatly appreciated! Thanks in advance.", "author_fullname": "t2_w3cofqjhm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Scraping Tweets from Twitter Without Using the API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdpwxm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710331852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to scrape some data from Twitter for a personal project, but I&amp;#39;ve hit a roadblock because I don&amp;#39;t have access to Twitter&amp;#39;s API. I&amp;#39;ve tried a few methods, but none seem to work effectively.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience or recommendations for tools or methods to scrape tweets from Twitter without relying on the API? I&amp;#39;m looking for something reliable and preferably free, but I&amp;#39;m willing to consider paid options if they get the job done.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or advice would be greatly appreciated! Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdpwxm", "is_robot_indexable": true, "report_reasons": null, "author": "NebulaNom13", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdpwxm/need_help_scraping_tweets_from_twitter_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdpwxm/need_help_scraping_tweets_from_twitter_without/", "subreddit_subscribers": 738310, "created_utc": 1710331852.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}