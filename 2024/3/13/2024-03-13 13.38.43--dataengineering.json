{"kind": "Listing", "data": {"after": "t3_1bd8k71", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It\u2019s happening guys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd5wv8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 437, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 437, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a74WmaMoBJWdXjOc5lN_oAw6cl-ZpX12mnDZTZx8nDg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710271207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/x2u0nprdeync1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?auto=webp&amp;s=7951e286519849d36881db89b9f736171e631a3d", "width": 1170, "height": 2532}, "resolutions": [{"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e544c49f7f0b9018a52397ce2691791d9f50a79", "width": 108, "height": 216}, {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dcaf31155b0240163bd0a934a6a4360b92c792e0", "width": 216, "height": 432}, {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d1f498dead47a709422a080d2dce5f9c539b8b8", "width": 320, "height": 640}, {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=39cfe0c1bc03ac5f92031532267ae741609d63f1", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9c0cbf05a30219f48a8381a2efcf54f4b714fb8", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/x2u0nprdeync1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e29cb1e3855ca64e8184c10a52a612cf734d9230", "width": 1080, "height": 2160}], "variants": {}, "id": "jADU1Bmpqn51dvqLJa9gq3LlR2IVPhPc3jXKMELUDM0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bd5wv8", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 128, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd5wv8/its_happening_guys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/x2u0nprdeync1.jpeg", "subreddit_subscribers": 168330, "created_utc": 1710271207.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. I have worked my way up in my organization from Service Desk Analyst to the title of a Data Engineernand have been with this company for almost 7 years. My primary tasks are maintening an AWS Glue pipeline into our data lake and bringing new tables from various sources into it. I know python somewhat well (I can pass some medium leetcode problems as a reference), my SQL skills are very basic, I feel like my organization is so far behind things techwise I am getting away with it and I can use chatgpt well enough to figure out anything I don't know. \n\nTyping this out I feel like I'm describing myself more competent than I feel. I have no formal education besides highschool and a boatload of AWS certifications I always have to study my butt off for. \n\nI'm not sure what the point of this post is, I just kinda needed to rant and get it off my chest to someone and maybe get some outside perspective.\n\nHow do you guys deal with thoughts like these? Do my skills line up with what's expected of a DE? ", "author_fullname": "t2_kaic0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel like such a fraud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd500u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710269114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I have worked my way up in my organization from Service Desk Analyst to the title of a Data Engineernand have been with this company for almost 7 years. My primary tasks are maintening an AWS Glue pipeline into our data lake and bringing new tables from various sources into it. I know python somewhat well (I can pass some medium leetcode problems as a reference), my SQL skills are very basic, I feel like my organization is so far behind things techwise I am getting away with it and I can use chatgpt well enough to figure out anything I don&amp;#39;t know. &lt;/p&gt;\n\n&lt;p&gt;Typing this out I feel like I&amp;#39;m describing myself more competent than I feel. I have no formal education besides highschool and a boatload of AWS certifications I always have to study my butt off for. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what the point of this post is, I just kinda needed to rant and get it off my chest to someone and maybe get some outside perspective.&lt;/p&gt;\n\n&lt;p&gt;How do you guys deal with thoughts like these? Do my skills line up with what&amp;#39;s expected of a DE? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bd500u", "is_robot_indexable": true, "report_reasons": null, "author": "Jaggedfel2142", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd500u/i_feel_like_such_a_fraud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd500u/i_feel_like_such_a_fraud/", "subreddit_subscribers": 168330, "created_utc": 1710269114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen some pretty crazy SQL queries during my time working on data platforms. I've recently been dealing with SQL queries that consists of hundreds of thousands of union statements. \n\nI've also seen the abuse of dbt's ephemeral models which result in megabytes of massive SQL statements that are impossible to debug.\n\nRecently, I've had to create automated functions to translate spark explode -&gt; bigquery, it produces some pretty ugly code.\n\n```SELECT POSEXPLODE(ARRAY(2, 3)), EXPLODE(ARRAY(4, 5, 6)) FROM tbl```\n\nSELECT\n\n  IF(_u.pos = _u_2.pos_2, _u_2.col) AS col,\n\n  IF(_u.pos = _u_2.pos_2, _u_2.pos_2) AS pos_2,\n\n  IF(_u.pos = _u_3.pos_3, _u_3.col_2) AS col_2\n\nFROM tbl\n\nCROSS JOIN UNNEST(SEQUENCE(1, GREATEST(CARDINALITY(ARRAY[2, 3]), CARDINALITY(ARRAY[4, 5, 6])))) AS _u(pos)\n\nCROSS JOIN UNNEST(ARRAY[2, 3]) WITH ORDINALITY AS _u_2(col, pos_2)\n\nCROSS JOIN UNNEST(ARRAY[4, 5, 6]) WITH ORDINALITY AS _u_3(col_2, pos_3)\n\nWHERE\n\n  (\n\n    _u.pos = _u_2.pos_2\n\n    OR (\n\n      _u.pos &gt; CARDINALITY(ARRAY[2, 3]) AND _u_2.pos_2 = CARDINALITY(ARRAY[2, 3])\n\n    )\n\n  )\n\n  AND (\n\n    _u.pos = _u_3.pos_3\n\n    OR (\n\n      _u.pos &gt; CARDINALITY(ARRAY[4, 5, 6]) AND _u_3.pos_3 = CARDINALITY(ARRAY[4, 5, 6])\n\n    )\n\n  )\n\n\nWhat are some of the worst SQL queries you've encountered?", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gnarliest SQL queries you've ever seen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd32bx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710264607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen some pretty crazy SQL queries during my time working on data platforms. I&amp;#39;ve recently been dealing with SQL queries that consists of hundreds of thousands of union statements. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also seen the abuse of dbt&amp;#39;s ephemeral models which result in megabytes of massive SQL statements that are impossible to debug.&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve had to create automated functions to translate spark explode -&amp;gt; bigquery, it produces some pretty ugly code.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT POSEXPLODE(ARRAY(2, 3)), EXPLODE(ARRAY(4, 5, 6)) FROM tbl&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;SELECT&lt;/p&gt;\n\n&lt;p&gt;IF(_u.pos = _u_2.pos_2, _u_2.col) AS col,&lt;/p&gt;\n\n&lt;p&gt;IF(_u.pos = _u_2.pos_2, _u_2.pos_2) AS pos_2,&lt;/p&gt;\n\n&lt;p&gt;IF(_u.pos = _u_3.pos_3, _u_3.col_2) AS col_2&lt;/p&gt;\n\n&lt;p&gt;FROM tbl&lt;/p&gt;\n\n&lt;p&gt;CROSS JOIN UNNEST(SEQUENCE(1, GREATEST(CARDINALITY(ARRAY[2, 3]), CARDINALITY(ARRAY[4, 5, 6])))) AS _u(pos)&lt;/p&gt;\n\n&lt;p&gt;CROSS JOIN UNNEST(ARRAY[2, 3]) WITH ORDINALITY AS _u_2(col, pos_2)&lt;/p&gt;\n\n&lt;p&gt;CROSS JOIN UNNEST(ARRAY[4, 5, 6]) WITH ORDINALITY AS _u_3(col_2, pos_3)&lt;/p&gt;\n\n&lt;p&gt;WHERE&lt;/p&gt;\n\n&lt;p&gt;(&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;_u.pos = _u_2.pos_2\n\nOR (\n\n  _u.pos &amp;gt; CARDINALITY(ARRAY[2, 3]) AND _u_2.pos_2 = CARDINALITY(ARRAY[2, 3])\n\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;AND (&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;_u.pos = _u_3.pos_3\n\nOR (\n\n  _u.pos &amp;gt; CARDINALITY(ARRAY[4, 5, 6]) AND _u_3.pos_3 = CARDINALITY(ARRAY[4, 5, 6])\n\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;What are some of the worst SQL queries you&amp;#39;ve encountered?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bd32bx", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd32bx/gnarliest_sql_queries_youve_ever_seen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd32bx/gnarliest_sql_queries_youve_ever_seen/", "subreddit_subscribers": 168330, "created_utc": 1710264607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I've been into data engineering ever since I graduate from my bachelors in CS in June 2023. So I've gained skills in SQL and experience with Azure. But since I graduated from CS, I was wondering... what would make me stand out from other data engineers having studied CS? Like I did projects related to web development and would say have a decent coding skills. So what would make me a stronger candidate than others? Having a strong knowledge in frontend dev. or backend dev?", "author_fullname": "t2_chpvvtuts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strong Data Engineering Profile", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd1geu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710260835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve been into data engineering ever since I graduate from my bachelors in CS in June 2023. So I&amp;#39;ve gained skills in SQL and experience with Azure. But since I graduated from CS, I was wondering... what would make me stand out from other data engineers having studied CS? Like I did projects related to web development and would say have a decent coding skills. So what would make me a stronger candidate than others? Having a strong knowledge in frontend dev. or backend dev?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bd1geu", "is_robot_indexable": true, "report_reasons": null, "author": "Sea_Pen_1356", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd1geu/strong_data_engineering_profile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd1geu/strong_data_engineering_profile/", "subreddit_subscribers": 168330, "created_utc": 1710260835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, Microsoft is offering a free exam voucher for those who complete any of the Microsoft Learn AI Skills Challenges below:\n\n[Build next generation apps with Azure OpenAI](https://learn.microsoft.com/training/challenges?id=da09d3ca-a2bb-47dc-ba42-bea77b386a3d&amp;WT.mc_id=cloudskillschallenge_da09d3ca-a2bb-47dc-ba42-bea77b386a3d) \n\n[Azure AI Fundamentals](https://learn.microsoft.com/training/challenges?id=3ef5d197-cdef-49bc-a8bc-954bcd9e88cc&amp;WT.mc_id=cloudskillschallenge_3ef5d197-cdef-49bc-a8bc-954bcd9e88cc)\n\n[Fabric Analytics Engineer](https://learn.microsoft.com/training/challenges?id=b696c18d-7201-4aff-9c7d-d33014d93b25&amp;WT.mc_id=cloudskillschallenge_b696c18d-7201-4aff-9c7d-d33014d93b25)\n\n[Azure Machine Learning](https://learn.microsoft.com/training/challenges?id=764de7ca-8b6d-4b3a-a491-1942af389d8c&amp;WT.mc_id=cloudskillschallenge_764de7ca-8b6d-4b3a-a491-1942af389d8c)\n\nOnce you complete the challenge, you'll be given a free exam voucher which can be used for any of the exams listed below:\n\n[AI-102 Designing and Implementing a Microsoft Azure AI Solution](https://learn.microsoft.com/en-us/credentials/certifications/azure-ai-engineer/#certification-exams?ocid=aisc24_cloudskillschallenge_webpage_cnl)\n\n[AI-900 Microsoft Azure AI Fundamentals](https://learn.microsoft.com/en-us/credentials/certifications/exams/ai-900/?ocid=aisc24_cloudskillschallenge_webpage_cnl)\n\n[DP-600 Implementing Analytics Solutions Using Microsoft Fabric](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-600/?ocid=aisc24_cloudskillschallenge_webpage_cnl)\n\n[DP-100 Designing and Implementing a Data Science Soluition on Azure](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-100/?ocid=aisc24_cloudskillschallenge_webpage_cnl)\n\n&amp;#x200B;\n\nYou can read the links for all other details, you got a month to complete any of the challenges and you will get a voucher for any from the 4 exams listed. It would make more sense if you do the challenge that relates to the exam you plan on sitting for but it is not mandatory. You have to complete the challenge between March 19th and April 19th.\n\nWhich one are you choosing? I am thinking between AI-900 and DP-600.\n\nCheers!", "author_fullname": "t2_7gvgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Microsoft Exam Vouchers By Completing a Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdhw74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710301781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, Microsoft is offering a free exam voucher for those who complete any of the Microsoft Learn AI Skills Challenges below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/training/challenges?id=da09d3ca-a2bb-47dc-ba42-bea77b386a3d&amp;amp;WT.mc_id=cloudskillschallenge_da09d3ca-a2bb-47dc-ba42-bea77b386a3d\"&gt;Build next generation apps with Azure OpenAI&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/training/challenges?id=3ef5d197-cdef-49bc-a8bc-954bcd9e88cc&amp;amp;WT.mc_id=cloudskillschallenge_3ef5d197-cdef-49bc-a8bc-954bcd9e88cc\"&gt;Azure AI Fundamentals&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/training/challenges?id=b696c18d-7201-4aff-9c7d-d33014d93b25&amp;amp;WT.mc_id=cloudskillschallenge_b696c18d-7201-4aff-9c7d-d33014d93b25\"&gt;Fabric Analytics Engineer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/training/challenges?id=764de7ca-8b6d-4b3a-a491-1942af389d8c&amp;amp;WT.mc_id=cloudskillschallenge_764de7ca-8b6d-4b3a-a491-1942af389d8c\"&gt;Azure Machine Learning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Once you complete the challenge, you&amp;#39;ll be given a free exam voucher which can be used for any of the exams listed below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/azure-ai-engineer/#certification-exams?ocid=aisc24_cloudskillschallenge_webpage_cnl\"&gt;AI-102 Designing and Implementing a Microsoft Azure AI Solution&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/exams/ai-900/?ocid=aisc24_cloudskillschallenge_webpage_cnl\"&gt;AI-900 Microsoft Azure AI Fundamentals&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-600/?ocid=aisc24_cloudskillschallenge_webpage_cnl\"&gt;DP-600 Implementing Analytics Solutions Using Microsoft Fabric&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-100/?ocid=aisc24_cloudskillschallenge_webpage_cnl\"&gt;DP-100 Designing and Implementing a Data Science Soluition on Azure&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can read the links for all other details, you got a month to complete any of the challenges and you will get a voucher for any from the 4 exams listed. It would make more sense if you do the challenge that relates to the exam you plan on sitting for but it is not mandatory. You have to complete the challenge between March 19th and April 19th.&lt;/p&gt;\n\n&lt;p&gt;Which one are you choosing? I am thinking between AI-900 and DP-600.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bdhw74", "is_robot_indexable": true, "report_reasons": null, "author": "salihveseli", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdhw74/free_microsoft_exam_vouchers_by_completing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdhw74/free_microsoft_exam_vouchers_by_completing_a/", "subreddit_subscribers": 168330, "created_utc": 1710301781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm a data engineering consultant specializing in working with startups to develop robust data platforms. Currently, I'm collaborating with a Swedish company whose entire infrastructure is based on Postgres and Django. They've tasked me with either building upon or improving their existing platform to make it more manageable and scalable.\n\nHowever, I've noticed that they heavily rely on Django objects for the ETL process rather than utilizing SQL. As a data engineer, I firmly believe in the effectiveness of SQL/Python Dataframes methodology over Django models for such tasks.\n\nHow do you suggest I approach convincing them of this? Or do you think I might be mistaken in my approach? Open to your insights and suggestions!", "author_fullname": "t2_80ixgpf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Seeking Advice: Convincing a Startup to Embrace SQL/Python Dataframes over Django Models\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdjkaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710307126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineering consultant specializing in working with startups to develop robust data platforms. Currently, I&amp;#39;m collaborating with a Swedish company whose entire infrastructure is based on Postgres and Django. They&amp;#39;ve tasked me with either building upon or improving their existing platform to make it more manageable and scalable.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;ve noticed that they heavily rely on Django objects for the ETL process rather than utilizing SQL. As a data engineer, I firmly believe in the effectiveness of SQL/Python Dataframes methodology over Django models for such tasks.&lt;/p&gt;\n\n&lt;p&gt;How do you suggest I approach convincing them of this? Or do you think I might be mistaken in my approach? Open to your insights and suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdjkaj", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic-Emergency75", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdjkaj/seeking_advice_convincing_a_startup_to_embrace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdjkaj/seeking_advice_convincing_a_startup_to_embrace/", "subreddit_subscribers": 168330, "created_utc": 1710307126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started to learn Python as a data engineering tool a few months ago. Since I come from a SQL background, I have gained good grasp of Pandas and Airflow pretty fast. Recently I heard about other ETL tools such as bonobo, PygramEtL and Petl. Are they still relevant? I tried to find youtube videos to learn more but most videos are quite old (more than 3 years old). I am not sure if I need to spend my energy learning these tools if they are not used often any more. If you are a data engineer, would you please tell me what python tools are you using? If you are still using bonobo, PygramEtL and Petl, would you please share some use cases? Thanks a lot!", "author_fullname": "t2_9od6j04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning Python for data engineering for finance. Is bonobo, PygramEtL and Petl still relevant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bddw39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710290645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started to learn Python as a data engineering tool a few months ago. Since I come from a SQL background, I have gained good grasp of Pandas and Airflow pretty fast. Recently I heard about other ETL tools such as bonobo, PygramEtL and Petl. Are they still relevant? I tried to find youtube videos to learn more but most videos are quite old (more than 3 years old). I am not sure if I need to spend my energy learning these tools if they are not used often any more. If you are a data engineer, would you please tell me what python tools are you using? If you are still using bonobo, PygramEtL and Petl, would you please share some use cases? Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bddw39", "is_robot_indexable": true, "report_reasons": null, "author": "Tall-Skin5800", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bddw39/learning_python_for_data_engineering_for_finance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bddw39/learning_python_for_data_engineering_for_finance/", "subreddit_subscribers": 168330, "created_utc": 1710290645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I\u2019m developing a web app that will get shown on user devices. So the system would look something like this:\n\nUser Device &lt;-&gt; Streamlit Frontend -&gt; FastAPI Logic -&gt; SQLAlchemy Data Access &lt;-&gt; Postgres Data Store\n\nWould this result in:\n\n* Application Layer: Streamlit\n* Business Logic Layer: FastAPI\n* Data Access Layer: SQLAlchemy\n* Data Layer: Postgres\n\nOr am I misunderstanding anything?\n", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making sure I understand the 3 tier architecture\u2026 Is this it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bcyqug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710254137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I\u2019m developing a web app that will get shown on user devices. So the system would look something like this:&lt;/p&gt;\n\n&lt;p&gt;User Device &amp;lt;-&amp;gt; Streamlit Frontend -&amp;gt; FastAPI Logic -&amp;gt; SQLAlchemy Data Access &amp;lt;-&amp;gt; Postgres Data Store&lt;/p&gt;\n\n&lt;p&gt;Would this result in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Application Layer: Streamlit&lt;/li&gt;\n&lt;li&gt;Business Logic Layer: FastAPI&lt;/li&gt;\n&lt;li&gt;Data Access Layer: SQLAlchemy&lt;/li&gt;\n&lt;li&gt;Data Layer: Postgres&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Or am I misunderstanding anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bcyqug", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bcyqug/making_sure_i_understand_the_3_tier_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bcyqug/making_sure_i_understand_the_3_tier_architecture/", "subreddit_subscribers": 168330, "created_utc": 1710254137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://dagster.io/blog/dagster-openai](https://dagster.io/blog/dagster-openai)\n\nThe new dagster-openai integration lets you tap into the power of LLMs in a cost-efficient way.\n\nhttps://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=a7ecab5ae764f9a3ce77612374c19fa2442aa8b0", "author_fullname": "t2_c45yywox7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster's Open AI integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ovkzr6rjoxnc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2b5fa07a548a4241016e5574ec40dd1be9b42fc"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f756c46d79281ac646f745a834f3a2c150d8076"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7db258eb2a117848a241620ec0f31178f8c2ddc3"}, {"y": 336, "x": 640, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5640258c75bab8e6d7e14c3e1199cfefcfa8248f"}, {"y": 504, "x": 960, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09be2968a5be5d97ee06f679bacc7dc22590866d"}, {"y": 567, "x": 1080, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bf29066b69a4a3ddd9ae43e82b0d4e36b4d3247"}], "s": {"y": 630, "x": 1200, "u": "https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=a7ecab5ae764f9a3ce77612374c19fa2442aa8b0"}, "id": "ovkzr6rjoxnc1"}}, "name": "t3_1bd25o2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/CwcvahbzbTHlIv2YnRP4yV4EnweUcOGhl7TvGAVfqG0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710262472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://dagster.io/blog/dagster-openai\"&gt;https://dagster.io/blog/dagster-openai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The new dagster-openai integration lets you tap into the power of LLMs in a cost-efficient way.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a7ecab5ae764f9a3ce77612374c19fa2442aa8b0\"&gt;https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a7ecab5ae764f9a3ce77612374c19fa2442aa8b0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bd25o2", "is_robot_indexable": true, "report_reasons": null, "author": "dagster-io", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd25o2/dagsters_open_ai_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd25o2/dagsters_open_ai_integration/", "subreddit_subscribers": 168330, "created_utc": 1710262472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we rebuilt our streaming SQL engine on Arrow and DataFusion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd1ldx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SP7NuGNJCrGyOCFEDJ-ShUTComq-Wx19oDx4gknWiYE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710261151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/why-arrow-and-datafusion", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?auto=webp&amp;s=e7959007dda0dbb9cf030c20ddac353c861c09fa", "width": 2688, "height": 1792}, "resolutions": [{"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7d7d58332e0cbb07dea5da384e001c7d7176f27", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5966c776f1ca2cdcf06a241d07578197870315b", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46d58404c44429d0727987e7a25048654edf70e4", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f1ded20f4d4bafc79d841c95bf9467f0c70e1ba", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9c33cdda23e774d862952924083bd6044c55f42", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/M1LT7sPAemCZ40f5TyqpqambSAM-0LY1QRUY5vGlA7Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73070affd9bbc91c9314dcd1e8f6ed879045e9c8", "width": 1080, "height": 720}], "variants": {}, "id": "UNxPUV_rhw7H5oI0SODFEM6Izqt3u_t6LNuihNml6Jc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bd1ldx", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd1ldx/why_we_rebuilt_our_streaming_sql_engine_on_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/why-arrow-and-datafusion", "subreddit_subscribers": 168330, "created_utc": 1710261151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've got a master's degree in data and done several internships in data engineering. I'd like to start my career in the same field but I haven't had the opportunity to learn much about certain technologies. Are there any good exercises or projects on Kaggle to learn how to use certain combined technologies like Python, SQL,  ELK, Kafka, Airflow, Spark, terraform, with docker, Bash for example. If there aren't any Kaggle projects or that sort of thing, would there be appropriate datasets for the data sources? Maybe projects using GNS3 for the configuration part or some kind of github repository as a guided project? I don't know. Above all, I'm looking for a project that will enable me to learn or discover all these technologies. I know that a lot of these technologies can work together in a company, so I suppose it's not impossible to get a lot of them involved in this project.\n\nAs I'm considered to have little experience, I'd like to work on all these skills in my own time to see how everything fits together and to know what I'm talking about. I'm prepared to devote a lot of time to it so that I can be deployed more quickly in companies. If you can help me or even give me advice on the technology, for example, that would be really nice. Any feedback is also very welcome.", "author_fullname": "t2_q95335vn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn data engineering in a non-professional context?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd1bsf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710260528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a master&amp;#39;s degree in data and done several internships in data engineering. I&amp;#39;d like to start my career in the same field but I haven&amp;#39;t had the opportunity to learn much about certain technologies. Are there any good exercises or projects on Kaggle to learn how to use certain combined technologies like Python, SQL,  ELK, Kafka, Airflow, Spark, terraform, with docker, Bash for example. If there aren&amp;#39;t any Kaggle projects or that sort of thing, would there be appropriate datasets for the data sources? Maybe projects using GNS3 for the configuration part or some kind of github repository as a guided project? I don&amp;#39;t know. Above all, I&amp;#39;m looking for a project that will enable me to learn or discover all these technologies. I know that a lot of these technologies can work together in a company, so I suppose it&amp;#39;s not impossible to get a lot of them involved in this project.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;m considered to have little experience, I&amp;#39;d like to work on all these skills in my own time to see how everything fits together and to know what I&amp;#39;m talking about. I&amp;#39;m prepared to devote a lot of time to it so that I can be deployed more quickly in companies. If you can help me or even give me advice on the technology, for example, that would be really nice. Any feedback is also very welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bd1bsf", "is_robot_indexable": true, "report_reasons": null, "author": "Aquilae2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd1bsf/how_to_learn_data_engineering_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd1bsf/how_to_learn_data_engineering_in_a/", "subreddit_subscribers": 168330, "created_utc": 1710260528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nPlease feel free to stop me in my tracks if I\u2019m going down a bad path. I want to develop my data input forms as webapps with Streamlit. They won\u2019t be dashboards as is typical, but data collection tools. I love the minimal look of Streamlit and it\u2019s quite easy to put out a working product quickly with it. However, that ease makes me concerned a bit- is it safe for deployment?\n\nSome background:\n\nI\u2019ll probably want to deploy several replicas of the application and distribute load among them for high-availability. Perhaps only 2-3 instances is fine though, as this isn\u2019t for high load.\n\nUsers will access the app from remote locations, potentially up to 10-15 at a time. Their internet connection may be poor at best, but if their internet goes out it\u2019s not technically my problem anymore. I\u2019ll just want to make sure (for now) that I can perform well under medium latency.\n\nI\u2019ll launch it was k3s to automate a lot of the infrastructure management. FastAPI will sit between Streamlit and the database.\n\nI guess if nobody knows, I\u2019ll give it a shot and let you guys know how it goes. Can anyone offer some insight ahead of time though?\n\nThanks!", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone deployed Streamlit for some intense workload?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdcdex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710286700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Please feel free to stop me in my tracks if I\u2019m going down a bad path. I want to develop my data input forms as webapps with Streamlit. They won\u2019t be dashboards as is typical, but data collection tools. I love the minimal look of Streamlit and it\u2019s quite easy to put out a working product quickly with it. However, that ease makes me concerned a bit- is it safe for deployment?&lt;/p&gt;\n\n&lt;p&gt;Some background:&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll probably want to deploy several replicas of the application and distribute load among them for high-availability. Perhaps only 2-3 instances is fine though, as this isn\u2019t for high load.&lt;/p&gt;\n\n&lt;p&gt;Users will access the app from remote locations, potentially up to 10-15 at a time. Their internet connection may be poor at best, but if their internet goes out it\u2019s not technically my problem anymore. I\u2019ll just want to make sure (for now) that I can perform well under medium latency.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll launch it was k3s to automate a lot of the infrastructure management. FastAPI will sit between Streamlit and the database.&lt;/p&gt;\n\n&lt;p&gt;I guess if nobody knows, I\u2019ll give it a shot and let you guys know how it goes. Can anyone offer some insight ahead of time though?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bdcdex", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdcdex/has_anyone_deployed_streamlit_for_some_intense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdcdex/has_anyone_deployed_streamlit_for_some_intense/", "subreddit_subscribers": 168330, "created_utc": 1710286700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to stream data from SNS into an iceberg table on S3. However I am a beginner and I feel quite overwhelmed with the amount of possible ways of how to do that.\n\nSNS has around 100 thousand messaged per day (ca. 1/s) which I would like to stream to an Iceberg table. I was thinking Data Firehose however, I think it can not push directly to an iceberg table on S3.   \nDoes it make sense to first put data onto a landingzone in s3 and then run a streaming glue job in spark to stream into the table?   \nFurthermore the problem is that the table should be somewhat up to date (lets say 1 to 5 minutes delay) because customers use it for querys in Athena. However, as the streaming throughput is that small and the table is partitioned for every customer, it leaves me with the small file problem in the table. (the messages about every second can be from different customers)   \nI think running a compaction job every hour could be overkill if you have to do that for hundreds of customers with always just some little files.   \n\n\nI'm thankful for every pointer in the right direction and I can answer more details if I forgot anything as I am still a beginner. ", "author_fullname": "t2_kv5ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to stream from SNS to Iceberg table on AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd7bfo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710274558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to stream data from SNS into an iceberg table on S3. However I am a beginner and I feel quite overwhelmed with the amount of possible ways of how to do that.&lt;/p&gt;\n\n&lt;p&gt;SNS has around 100 thousand messaged per day (ca. 1/s) which I would like to stream to an Iceberg table. I was thinking Data Firehose however, I think it can not push directly to an iceberg table on S3.&lt;br/&gt;\nDoes it make sense to first put data onto a landingzone in s3 and then run a streaming glue job in spark to stream into the table?&lt;br/&gt;\nFurthermore the problem is that the table should be somewhat up to date (lets say 1 to 5 minutes delay) because customers use it for querys in Athena. However, as the streaming throughput is that small and the table is partitioned for every customer, it leaves me with the small file problem in the table. (the messages about every second can be from different customers)&lt;br/&gt;\nI think running a compaction job every hour could be overkill if you have to do that for hundreds of customers with always just some little files.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thankful for every pointer in the right direction and I can answer more details if I forgot anything as I am still a beginner. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bd7bfo", "is_robot_indexable": true, "report_reasons": null, "author": "Larsimoto", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd7bfo/how_to_stream_from_sns_to_iceberg_table_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd7bfo/how_to_stream_from_sns_to_iceberg_table_on_aws/", "subreddit_subscribers": 168330, "created_utc": 1710274558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks, dlt co-founder here!\n\nWe're on a mission to shake things up in data engineering by moving away from the repetitive and into the innovative, especially with source generation. Wondering about the future of connectors and traditional ETL tools? We see pipeline generation not just as a possibility but as our current reality, supported by the case study in the article.\n\nThe real question now isn't if we're generating sources, but how extensively we're doing it. Building a single data pipeline involves numerous decisions, and we're harnessing automation, LLMs, and the OpenAPI spec to streamline this process. While relying solely on automation tends to lead to compounded errors, a collaborative approach with human oversight is proving effective.\n\nWhat's the future of ETL libraries in this evolving landscape? Our experience and user feedback suggest a significant shift. With 500 private sources created last month alone by the dlt community without any code generation, we hope that empowering the community with better tooling and growing the community will 100x this process within the year.\n\nWant to dive deeper? Check out our [latest blog post](https://dlthub.com/docs/blog/code-vs-buy) and the related case study. \n\nYou can try the code generator we created last year (in the post), but you should probably hold off for our update towards the end of the month.  \n\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The future of Data Engineering: Beyond traditional ETL \"Connectors\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdp3jf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710329128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, dlt co-founder here!&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re on a mission to shake things up in data engineering by moving away from the repetitive and into the innovative, especially with source generation. Wondering about the future of connectors and traditional ETL tools? We see pipeline generation not just as a possibility but as our current reality, supported by the case study in the article.&lt;/p&gt;\n\n&lt;p&gt;The real question now isn&amp;#39;t if we&amp;#39;re generating sources, but how extensively we&amp;#39;re doing it. Building a single data pipeline involves numerous decisions, and we&amp;#39;re harnessing automation, LLMs, and the OpenAPI spec to streamline this process. While relying solely on automation tends to lead to compounded errors, a collaborative approach with human oversight is proving effective.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the future of ETL libraries in this evolving landscape? Our experience and user feedback suggest a significant shift. With 500 private sources created last month alone by the dlt community without any code generation, we hope that empowering the community with better tooling and growing the community will 100x this process within the year.&lt;/p&gt;\n\n&lt;p&gt;Want to dive deeper? Check out our &lt;a href=\"https://dlthub.com/docs/blog/code-vs-buy\"&gt;latest blog post&lt;/a&gt; and the related case study. &lt;/p&gt;\n\n&lt;p&gt;You can try the code generator we created last year (in the post), but you should probably hold off for our update towards the end of the month.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?auto=webp&amp;s=89baf6cd5beccc2ef436f6c5fd2bf1b1d206cb32", "width": 1780, "height": 994}, "resolutions": [{"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f7670b7c98fe4b83ea3de40d3e2e9046a886096", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0532458503ce22d0b009efd3ed3b0b7bb6b029ba", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=68e0562d5f34ce8a12ad3c477cb49a50cccef51a", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b99612b4585f2153ce648471fbef74ec45cb6ce", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a21ae84c23b97582b017045fe6241cafa3e455ba", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/c1KXyf8czxazHd26d9uciupuICcfYeQpljzPAn5jZGE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8179bfc0a98adacdd1fbe84e92dab79336661918", "width": 1080, "height": 603}], "variants": {}, "id": "dNitETZjWStuGD_7fRr5BIR4unHLIjEoCTLF_W7F4wA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bdp3jf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdp3jf/the_future_of_data_engineering_beyond_traditional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdp3jf/the_future_of_data_engineering_beyond_traditional/", "subreddit_subscribers": 168330, "created_utc": 1710329128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stop Hiring Senior Data Consultants: Leverage Expertise Instead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdodd2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sLS2Ogslp2yL5Q7XFOKf7SZv_XMfuluXS4X9spmtb6M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710326504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/stop-hiring-senior-data-consultants-leverage-expertise-instead/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?auto=webp&amp;s=29aeca12975e8deccaac6dd0e1aa52e6bc093179", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1ee205f60be083ecb442e24508df31d4f8033ef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0813d1970658c97c153a3b7f234657fa2d7295b1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=58af6e89465150c74e8cf15d8ee96c4fdbde498b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=913f7edd234fcc00a2b12cce1cede009494aee56", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce652497e787859b4e3e3de0ad20bee4dc43e5c8", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Kk-oqOXEhnTTaVYZlpJzGhy_rmLkBcxWfjSOY10Fx_s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3d63dba46210708fbfb8808ab42cd08c303dba0", "width": 1080, "height": 607}], "variants": {}, "id": "DKcXFibICoxVBhWZY1S1amaaIgEnLZ5N9_dmY3e_HYU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bdodd2", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdodd2/stop_hiring_senior_data_consultants_leverage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/stop-hiring-senior-data-consultants-leverage-expertise-instead/", "subreddit_subscribers": 168330, "created_utc": 1710326504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nThe company I work at is currently in a system transition. We are transferring to a low code solution (CMiC) that seemingly offers little to no API support.\n\nAfter transitioning systems, the company plans on integrating data visualization software to help supplement the short-comings of the system selected. CMiC offers some data visualization capabilities, but it does not come close to full fledged data viz softwares available. It does have the capability to output tabular data, so I\u2019m thinking it will be a limited ETL process if I design the standard reports appropriately. \n\nCompany would include 20+ business users that would need access to these reports. \n\nNumber of rows can be expected to be 10,000+. \n\nMy question is: Would Power BI or Tableau be a better solution for this situation? With the limited capabilities to export data, I\u2019m thinking it would be a better solution to use Power BI since it seems like we\u2019ll have to manually export the reports anyway. \n\nPlease let me know if this is the wrong sub and I\u2019ll happily report to another. Thanks!", "author_fullname": "t2_h3u82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ERP System to Data Visualization Tool ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdbeqc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710284320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;The company I work at is currently in a system transition. We are transferring to a low code solution (CMiC) that seemingly offers little to no API support.&lt;/p&gt;\n\n&lt;p&gt;After transitioning systems, the company plans on integrating data visualization software to help supplement the short-comings of the system selected. CMiC offers some data visualization capabilities, but it does not come close to full fledged data viz softwares available. It does have the capability to output tabular data, so I\u2019m thinking it will be a limited ETL process if I design the standard reports appropriately. &lt;/p&gt;\n\n&lt;p&gt;Company would include 20+ business users that would need access to these reports. &lt;/p&gt;\n\n&lt;p&gt;Number of rows can be expected to be 10,000+. &lt;/p&gt;\n\n&lt;p&gt;My question is: Would Power BI or Tableau be a better solution for this situation? With the limited capabilities to export data, I\u2019m thinking it would be a better solution to use Power BI since it seems like we\u2019ll have to manually export the reports anyway. &lt;/p&gt;\n\n&lt;p&gt;Please let me know if this is the wrong sub and I\u2019ll happily report to another. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdbeqc", "is_robot_indexable": true, "report_reasons": null, "author": "oimgoingin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdbeqc/erp_system_to_data_visualization_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdbeqc/erp_system_to_data_visualization_tool/", "subreddit_subscribers": 168330, "created_utc": 1710284320.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " For those of you who use PySpark consistently, \n\ndo you have code coverage for PySpark UDF/RDD? \n\nWhich tools do you use?", "author_fullname": "t2_a7j25p8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pyspark coverage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bdql0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710333976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who use PySpark consistently, &lt;/p&gt;\n\n&lt;p&gt;do you have code coverage for PySpark UDF/RDD? &lt;/p&gt;\n\n&lt;p&gt;Which tools do you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdql0m", "is_robot_indexable": true, "report_reasons": null, "author": "SnooDoubts9729", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdql0m/pyspark_coverage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdql0m/pyspark_coverage/", "subreddit_subscribers": 168330, "created_utc": 1710333976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Power of Perseverance: Starburst\u2019s Journey from Open Source Vision to $3.35 Billion Valuation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 99, "top_awarded_type": null, "hide_score": true, "name": "t3_1bdppvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1jxAAqmyJWFtTyQP0L1_CJT0tEkpk9YNJef_OBIZkoM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710331239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "svenbalnojan.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://svenbalnojan.medium.com/the-power-of-perseverance-starbursts-journey-from-open-source-vision-to-3-35-billion-valuation-767834e4c73b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?auto=webp&amp;s=7ed8d812ded288d8ed80c74d66a17f4f5f64af55", "width": 1200, "height": 857}, "resolutions": [{"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e5e6d4c50a0fee4fb75a6fc4b24e2d087497d81", "width": 108, "height": 77}, {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e98f4312f0ece12d6f97c06057e91ccd8709dd5", "width": 216, "height": 154}, {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=094c362b911db6013771cfcda5de85c45f99467d", "width": 320, "height": 228}, {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ac4cc8b05964bc10734dae8a8aae8fbdb9fd110", "width": 640, "height": 457}, {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9492423b0228874a96086a9ef1da1dfa0ae44b7f", "width": 960, "height": 685}, {"url": "https://external-preview.redd.it/Bz006bqkySGhRy3Us37hKlgQsZaGAsxASZvAd1VSoR0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1245c671f851e57da6484ccd1e8b7d24eb9aa9f6", "width": 1080, "height": 771}], "variants": {}, "id": "2V_dwqF9P-dmJX0TzhAUzTkLDZfFAFXidtmYe2uMLk0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bdppvx", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdppvx/the_power_of_perseverance_starbursts_journey_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://svenbalnojan.medium.com/the-power-of-perseverance-starbursts-journey-from-open-source-vision-to-3-35-billion-valuation-767834e4c73b", "subreddit_subscribers": 168330, "created_utc": 1710331239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello there,\n\nI look for advice on how to connect a powerbi to a trino(oss) to query trino datasets.\n\n(In a corporation environment)\n\nIs there someone to share any tools, proposal, advice,or RetEX on a such subject ?\n\nThanks a lot by advance.", "author_fullname": "t2_244osg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": " How to read Trino datas from PowerBi &amp;|or SSAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdp93l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710329654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;I look for advice on how to connect a powerbi to a trino(oss) to query trino datasets.&lt;/p&gt;\n\n&lt;p&gt;(In a corporation environment)&lt;/p&gt;\n\n&lt;p&gt;Is there someone to share any tools, proposal, advice,or RetEX on a such subject ?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot by advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdp93l", "is_robot_indexable": true, "report_reasons": null, "author": "vainamoinen_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdp93l/how_to_read_trino_datas_from_powerbi_or_ssas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdp93l/how_to_read_trino_datas_from_powerbi_or_ssas/", "subreddit_subscribers": 168330, "created_utc": 1710329654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I tested the code that Daniel Palma created for Running Doom inside Snowflake Container Services. In this video I'll show case the same in video format and explore a little bit what Container Services are and what are their use cases.\n\n[https://www.youtube.com/watch?v=Zd81nydtgX4](https://www.youtube.com/watch?v=Zd81nydtgX4)", "author_fullname": "t2_sxd6cnuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Doom in Snowflake Container Services (again)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdovx9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710328577.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710328378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested the code that Daniel Palma created for Running Doom inside Snowflake Container Services. In this video I&amp;#39;ll show case the same in video format and explore a little bit what Container Services are and what are their use cases.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=Zd81nydtgX4\"&gt;https://www.youtube.com/watch?v=Zd81nydtgX4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DURfMc7ju_r56ifCirHdmFWYRefndJFD8A19NBQmA2E.jpg?auto=webp&amp;s=afbcfde278fdf6f2f100bacfcfb80da367dc5d4c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DURfMc7ju_r56ifCirHdmFWYRefndJFD8A19NBQmA2E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=525eda73b654ebd15290c6b9d187c977c2a133ec", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DURfMc7ju_r56ifCirHdmFWYRefndJFD8A19NBQmA2E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc219c0229e4e211fed5eb54c89866c06157f0fa", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DURfMc7ju_r56ifCirHdmFWYRefndJFD8A19NBQmA2E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19112e2acdafd3c6ff33454a4582f1f98477777a", "width": 320, "height": 240}], "variants": {}, "id": "ChKNgWkPA8nGKkFnTBiHTPOdr_9M-xnMaoB34UiAtTg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bdovx9", "is_robot_indexable": true, "report_reasons": null, "author": "Recordly_MHeino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdovx9/running_doom_in_snowflake_container_services_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdovx9/running_doom_in_snowflake_container_services_again/", "subreddit_subscribers": 168330, "created_utc": 1710328378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey y\u2019all! \n\nI\u2019m trying to join two data sources; one for the customer subscriptions and the second is for the finance to monitor the payments and transactions with the subscription period. \n\nFirst tool have the following: \nCompany name \nContact info \nSubscription info\n\nSecond tool: \nCompany name (stored in another language) \nTransactions \n\nThe solution came to mind is to force the finance to add contact email and join if the email domain is the same and gave it a surrogate key. \nHowever, not all clients are using their work email, so a lot of them are using their gmail account. \n\nI\u2019m not finding any common data/columns to join on\u2026 any ideas that might help? \n\n", "author_fullname": "t2_nainn7cv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining two data sources without common columns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdlq97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710315742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y\u2019all! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to join two data sources; one for the customer subscriptions and the second is for the finance to monitor the payments and transactions with the subscription period. &lt;/p&gt;\n\n&lt;p&gt;First tool have the following: \nCompany name \nContact info \nSubscription info&lt;/p&gt;\n\n&lt;p&gt;Second tool: \nCompany name (stored in another language) \nTransactions &lt;/p&gt;\n\n&lt;p&gt;The solution came to mind is to force the finance to add contact email and join if the email domain is the same and gave it a surrogate key. \nHowever, not all clients are using their work email, so a lot of them are using their gmail account. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not finding any common data/columns to join on\u2026 any ideas that might help? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdlq97", "is_robot_indexable": true, "report_reasons": null, "author": "Vast-Hold2849", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdlq97/joining_two_data_sources_without_common_columns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdlq97/joining_two_data_sources_without_common_columns/", "subreddit_subscribers": 168330, "created_utc": 1710315742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone know of a service that cleans up addresses not only of USA but also other countries. ", "author_fullname": "t2_b7d1hftq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data cleanup for USA addresses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdcmb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710287334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know of a service that cleans up addresses not only of USA but also other countries. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdcmb1", "is_robot_indexable": true, "report_reasons": null, "author": "LesTabBlue", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdcmb1/data_cleanup_for_usa_addresses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdcmb1/data_cleanup_for_usa_addresses/", "subreddit_subscribers": 168330, "created_utc": 1710287334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on building a stitched-together ERP system, where I want actions taken and data created/received in one piece of software be reflected in the other ones. (Ex: make a purchase order in one piece of software =&gt; can see same purchase order in another). Most data pipeline and integration services I've seen have syncs on a daily basis or some other time-frame, they aren't event driven. Anyone have recommendations for something like this?", "author_fullname": "t2_9cov0jfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for Multidirectional Real-Time Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdaodb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710282499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on building a stitched-together ERP system, where I want actions taken and data created/received in one piece of software be reflected in the other ones. (Ex: make a purchase order in one piece of software =&amp;gt; can see same purchase order in another). Most data pipeline and integration services I&amp;#39;ve seen have syncs on a daily basis or some other time-frame, they aren&amp;#39;t event driven. Anyone have recommendations for something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bdaodb", "is_robot_indexable": true, "report_reasons": null, "author": "LongjumpingDebate959", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdaodb/recommendations_for_multidirectional_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdaodb/recommendations_for_multidirectional_realtime/", "subreddit_subscribers": 168330, "created_utc": 1710282499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which one is better? \n\nConsider below points:\n-more read operation vs write operation \n-due to late arriving data, we may have to update some of the transactions or table partitions. \n-schema evolution; e.g. int to bigint, new columns being added in the table. \n-rollback to previous states when something goes wrong during data processing \n-storage optimisation \n\nI would love to know how you guys are using it in production? Highlight some major disadvantages in both of cases. ", "author_fullname": "t2_amg2wh1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg vs Delta table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdah3x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710282012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which one is better? &lt;/p&gt;\n\n&lt;p&gt;Consider below points:\n-more read operation vs write operation \n-due to late arriving data, we may have to update some of the transactions or table partitions. \n-schema evolution; e.g. int to bigint, new columns being added in the table. \n-rollback to previous states when something goes wrong during data processing \n-storage optimisation &lt;/p&gt;\n\n&lt;p&gt;I would love to know how you guys are using it in production? Highlight some major disadvantages in both of cases. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bdah3x", "is_robot_indexable": true, "report_reasons": null, "author": "andalibansari", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdah3x/iceberg_vs_delta_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdah3x/iceberg_vs_delta_table/", "subreddit_subscribers": 168330, "created_utc": 1710282012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We run different batch jobs via Qubole/Airflow setup which run at different cadences for different sources. We also have a datadog integration available for observability.   \nFor eg, there will be a pipeline which is running every 3 hours and then there will be one which is running once every 24 hours.   \n\n\nI need to map the data delay that the upstream is having. For eg, if I am expecting the upstream to update their data by 11am PST, but the data is delayed and arrives at 11:30am PST. I will run my ETL/batch process after 11:30am. Need to track this 30 minute delay on datadog.   \n\n\nWhat is the best way of doing it?", "author_fullname": "t2_bjtd8b2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help mapping freshness as a metric for upstream data sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bd8k71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710277518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We run different batch jobs via Qubole/Airflow setup which run at different cadences for different sources. We also have a datadog integration available for observability.&lt;br/&gt;\nFor eg, there will be a pipeline which is running every 3 hours and then there will be one which is running once every 24 hours.   &lt;/p&gt;\n\n&lt;p&gt;I need to map the data delay that the upstream is having. For eg, if I am expecting the upstream to update their data by 11am PST, but the data is delayed and arrives at 11:30am PST. I will run my ETL/batch process after 11:30am. Need to track this 30 minute delay on datadog.   &lt;/p&gt;\n\n&lt;p&gt;What is the best way of doing it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bd8k71", "is_robot_indexable": true, "report_reasons": null, "author": "whiteclay9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bd8k71/need_help_mapping_freshness_as_a_metric_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bd8k71/need_help_mapping_freshness_as_a_metric_for/", "subreddit_subscribers": 168330, "created_utc": 1710277518.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}