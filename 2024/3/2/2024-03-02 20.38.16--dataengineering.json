{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been wondering why there are so many ETL tools out there when we already have Python and SQL. What do these tools offer that Python and SQL don't? Would love to hear your thoughts and experiences on this.\n\nAnd yes, as a junior I\u2019m completely open to the idea I\u2019m wrong about this\ud83d\ude02", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are there so many ETL tools when we have SQL and Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b44v59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 214, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 214, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709336581.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709325572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering why there are so many ETL tools out there when we already have Python and SQL. What do these tools offer that Python and SQL don&amp;#39;t? Would love to hear your thoughts and experiences on this.&lt;/p&gt;\n\n&lt;p&gt;And yes, as a junior I\u2019m completely open to the idea I\u2019m wrong about this\ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b44v59", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 123, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b44v59/why_are_there_so_many_etl_tools_when_we_have_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b44v59/why_are_there_so_many_etl_tools_when_we_have_sql/", "subreddit_subscribers": 165242, "created_utc": 1709325572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This though suddenly came up when I saw job postings for experienced individuals paying $25 an hour. This is not the norm, but still scary that companies even have such low expectations. Compared to most jobs out there, although the type of work may be fulfilling, And more often employers slap the \"Analyst\" title and get these people for a bargain. I feel that people in this industry are often underpaid compared to the work/skills they bring to the table. What do you all think?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Engineers underpaid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4v030", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709404052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This though suddenly came up when I saw job postings for experienced individuals paying $25 an hour. This is not the norm, but still scary that companies even have such low expectations. Compared to most jobs out there, although the type of work may be fulfilling, And more often employers slap the &amp;quot;Analyst&amp;quot; title and get these people for a bargain. I feel that people in this industry are often underpaid compared to the work/skills they bring to the table. What do you all think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b4v030", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4v030/are_data_engineers_underpaid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4v030/are_data_engineers_underpaid/", "subreddit_subscribers": 165242, "created_utc": 1709404052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just as the title says, how much web scraping would someone be able to do on twitter.com before it flags it as some bot/automated process and either ip bans or blocks the connection in the way they do?", "author_fullname": "t2_39utoy9a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How aggressive is Twitter against web scrapers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b45r0c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709327659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just as the title says, how much web scraping would someone be able to do on twitter.com before it flags it as some bot/automated process and either ip bans or blocks the connection in the way they do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b45r0c", "is_robot_indexable": true, "report_reasons": null, "author": "scuffed12s", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b45r0c/how_aggressive_is_twitter_against_web_scrapers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b45r0c/how_aggressive_is_twitter_against_web_scrapers/", "subreddit_subscribers": 165242, "created_utc": 1709327659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has two separate main business functions. One involving distribution of food products, and the other being the selling of custom merchandise and apparel. There\u2019s been some discussion on if in the future it would be beneficial to have one single data warehouse that is shared by both companies or having two separate systems for both. \n\nThe companies currently operate separately now, as one company was acquired and never entirely brought under the same umbrella as the parent from a technology standpoint. Both currently have separate systems and databases, but will be upgraded over the course of the next few years (one before the other).\n\nDoes it make sense to design a warehouse that can accommodate both under one roof? Or would it be wiser to keep these two divisions separate from a data warehouse perspective?", "author_fullname": "t2_3sghijoz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One data warehouse or two for different divisions of the same company ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4pxjb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709390903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has two separate main business functions. One involving distribution of food products, and the other being the selling of custom merchandise and apparel. There\u2019s been some discussion on if in the future it would be beneficial to have one single data warehouse that is shared by both companies or having two separate systems for both. &lt;/p&gt;\n\n&lt;p&gt;The companies currently operate separately now, as one company was acquired and never entirely brought under the same umbrella as the parent from a technology standpoint. Both currently have separate systems and databases, but will be upgraded over the course of the next few years (one before the other).&lt;/p&gt;\n\n&lt;p&gt;Does it make sense to design a warehouse that can accommodate both under one roof? Or would it be wiser to keep these two divisions separate from a data warehouse perspective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4pxjb", "is_robot_indexable": true, "report_reasons": null, "author": "ManiaMcG33_", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4pxjb/one_data_warehouse_or_two_for_different_divisions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4pxjb/one_data_warehouse_or_two_for_different_divisions/", "subreddit_subscribers": 165242, "created_utc": 1709390903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to ask how important it would be to learn (theoretically / practically) the following topics with in mind a data engineering future career:\n\nDistributed framework Apache Spark.\n\nClustering for data analysis and summarization.\n\nAnalysis of data streams.\n\nSimilarity search.\n\nAssociation Analysis.\n\n\nThanks!!", "author_fullname": "t2_a43b8wju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data Computing in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4p1t1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709388370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to ask how important it would be to learn (theoretically / practically) the following topics with in mind a data engineering future career:&lt;/p&gt;\n\n&lt;p&gt;Distributed framework Apache Spark.&lt;/p&gt;\n\n&lt;p&gt;Clustering for data analysis and summarization.&lt;/p&gt;\n\n&lt;p&gt;Analysis of data streams.&lt;/p&gt;\n\n&lt;p&gt;Similarity search.&lt;/p&gt;\n\n&lt;p&gt;Association Analysis.&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4p1t1", "is_robot_indexable": true, "report_reasons": null, "author": "Aiecco", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4p1t1/big_data_computing_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4p1t1/big_data_computing_in_data_engineering/", "subreddit_subscribers": 165242, "created_utc": 1709388370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3qpsfp70", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mistral AI, Klarna AI customer support agent, extract and load still unsolved", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4n50k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WEJPt2FzuFZnoXNluIHC-2TMSyaaGkmUB4VJd8cZGvI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709382380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blef.fr", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.blef.fr/data-news-week-24-09/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?auto=webp&amp;s=2c582ffb719685be75072e2ad3165658a96e0e7c", "width": 2500, "height": 1667}, "resolutions": [{"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff3a6beeb29af9738698f43405c5ef7c03630cb3", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f48117d3f5966230ea0f1006394aad85b2551e4", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbdd4e39c0c3ca26d2e72aa2c67fb44231f12b68", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=332c5b5d7e8f6cc6dec5cd676adb963f3aee3efb", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7985ea0e8108b3f58c869fc531dbf572b6cebc69", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/QcyJ4wO9_WFnEQEVrN7WrWq5xYKvhng7duKs7-45k38.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=823626cdd2dd3679a122dbc651c511446ea8cd5d", "width": 1080, "height": 720}], "variants": {}, "id": "_8UH2w59fyHSlLr2L65k-Njp35XnnrWQYTWT62hAk2Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b4n50k", "is_robot_indexable": true, "report_reasons": null, "author": "mrocral", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4n50k/mistral_ai_klarna_ai_customer_support_agent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.blef.fr/data-news-week-24-09/", "subreddit_subscribers": 165242, "created_utc": 1709382380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been wondering if anyone's using their Lakehouse for stuff beyond the usual the reporting and BI use-cases in the gold layer. It feels like everyone refers to the to the gold layer as being just for facts and dimensions, pretty much just standard Kimball warehouse stuff. But what about other uses?\n\nSpecifically, I'm looking into serving data for analytical web apps. These are not just simple reports but full-blown applications with a UI, an API, and a back-end running in Docker Containers on K8s. They would rely heavily on data in the Lakehouse, mostly from the silver layer, and wouldn't really generate much data on their own. There may be some exceptions where user input is written, but it would be a very little amount of data.\n\nI can't imagine that they best approach for this would keeping all this data in the Lakehouse, like in Delta Format. Wouldn't it make more sense to move this data into something like a Postgres DB, or even MongoDB, especially for serving it up through an API to the front-end?\n\nWould love to hear if anyone is currently supporting these types of use-cases on their platform.\n\nThanks!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beyond Reporting in a Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b46sye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709330174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering if anyone&amp;#39;s using their Lakehouse for stuff beyond the usual the reporting and BI use-cases in the gold layer. It feels like everyone refers to the to the gold layer as being just for facts and dimensions, pretty much just standard Kimball warehouse stuff. But what about other uses?&lt;/p&gt;\n\n&lt;p&gt;Specifically, I&amp;#39;m looking into serving data for analytical web apps. These are not just simple reports but full-blown applications with a UI, an API, and a back-end running in Docker Containers on K8s. They would rely heavily on data in the Lakehouse, mostly from the silver layer, and wouldn&amp;#39;t really generate much data on their own. There may be some exceptions where user input is written, but it would be a very little amount of data.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t imagine that they best approach for this would keeping all this data in the Lakehouse, like in Delta Format. Wouldn&amp;#39;t it make more sense to move this data into something like a Postgres DB, or even MongoDB, especially for serving it up through an API to the front-end?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear if anyone is currently supporting these types of use-cases on their platform.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b46sye", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b46sye/beyond_reporting_in_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b46sye/beyond_reporting_in_a_lakehouse/", "subreddit_subscribers": 165242, "created_utc": 1709330174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i have being giving interviews for azure data engg. Most of them ask me scenario based questions. The problem of scenario based questions is there may be multiple ways of achieving the solution. \n\nBelow are some scenarios. Please do share your insiggts if you might have came across in your work\n\n#1. You have multiple csv files in adls. You want to apply transformation on them and generate a single file. How would you acheive it.\n\nSol 1 - using adf mapping dataflows can take multiple source, do  the transformation and generate one file with one partition using sink transformation\n\nSol2 - using metadata activity to get the list of files and then using copy activity to get all the files from source and sink as one file by setting copy behaviour to merge\n\n\n\n#2. You have to store data on adls. How do you store the data in optimised way. Like partitioning indexing etc\n(Dont know about this. Please provide your input here)\n\n\n#3. How will you incrementally load data from file in adls.\nSol - probably using a metadata activity by getting all the last modified date \n\n\n#4. How will you copy data from onprem sql server to adls\nSol-  setup a Self hosted IR and then using a copy activity. But what if i dont want to copy all the data and just a section of data how do i do that ??\n\n\n#5. How do you Transform data without using a dataflow. \nSol - probably copy activity. But as far as i know it can only map source and dest cols and cant do any complex transformations\n\n\n#6- how do you connect to a on pem SQL server from a synaspse notebook without using adf pipeline\n-i dont have any idea abt this\n\n\nWould be great if experts of ADF share their insights", "author_fullname": "t2_e2e7dtpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you solve these adf scenarios ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4ti4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709400291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i have being giving interviews for azure data engg. Most of them ask me scenario based questions. The problem of scenario based questions is there may be multiple ways of achieving the solution. &lt;/p&gt;\n\n&lt;p&gt;Below are some scenarios. Please do share your insiggts if you might have came across in your work&lt;/p&gt;\n\n&lt;h1&gt;1. You have multiple csv files in adls. You want to apply transformation on them and generate a single file. How would you acheive it.&lt;/h1&gt;\n\n&lt;p&gt;Sol 1 - using adf mapping dataflows can take multiple source, do  the transformation and generate one file with one partition using sink transformation&lt;/p&gt;\n\n&lt;p&gt;Sol2 - using metadata activity to get the list of files and then using copy activity to get all the files from source and sink as one file by setting copy behaviour to merge&lt;/p&gt;\n\n&lt;h1&gt;2. You have to store data on adls. How do you store the data in optimised way. Like partitioning indexing etc&lt;/h1&gt;\n\n&lt;p&gt;(Dont know about this. Please provide your input here)&lt;/p&gt;\n\n&lt;h1&gt;3. How will you incrementally load data from file in adls.&lt;/h1&gt;\n\n&lt;p&gt;Sol - probably using a metadata activity by getting all the last modified date &lt;/p&gt;\n\n&lt;h1&gt;4. How will you copy data from onprem sql server to adls&lt;/h1&gt;\n\n&lt;p&gt;Sol-  setup a Self hosted IR and then using a copy activity. But what if i dont want to copy all the data and just a section of data how do i do that ??&lt;/p&gt;\n\n&lt;h1&gt;5. How do you Transform data without using a dataflow.&lt;/h1&gt;\n\n&lt;p&gt;Sol - probably copy activity. But as far as i know it can only map source and dest cols and cant do any complex transformations&lt;/p&gt;\n\n&lt;h1&gt;6- how do you connect to a on pem SQL server from a synaspse notebook without using adf pipeline&lt;/h1&gt;\n\n&lt;p&gt;-i dont have any idea abt this&lt;/p&gt;\n\n&lt;p&gt;Would be great if experts of ADF share their insights&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4ti4e", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult_Ad_426", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4ti4e/how_do_you_solve_these_adf_scenarios/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4ti4e/how_do_you_solve_these_adf_scenarios/", "subreddit_subscribers": 165242, "created_utc": 1709400291.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am trying to read from Mongo using spark mongo connector trying to load 10M rows\n\nAnyone know how I can ignore rows if there is datatype mismatch using my predefined schema. There are some date field which are timestamp but someone inserted String type. There are similar issues with other field. \n\nI want to know how we can handle datatype mismatch, The DROPMALFORMED functionality is not available in Spark Mongo Connector.\n\nAny advice or pointer will be helpful.", "author_fullname": "t2_6klazicf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Spark and Mongo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b4w7pv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709407109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am trying to read from Mongo using spark mongo connector trying to load 10M rows&lt;/p&gt;\n\n&lt;p&gt;Anyone know how I can ignore rows if there is datatype mismatch using my predefined schema. There are some date field which are timestamp but someone inserted String type. There are similar issues with other field. &lt;/p&gt;\n\n&lt;p&gt;I want to know how we can handle datatype mismatch, The DROPMALFORMED functionality is not available in Spark Mongo Connector.&lt;/p&gt;\n\n&lt;p&gt;Any advice or pointer will be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4w7pv", "is_robot_indexable": true, "report_reasons": null, "author": "asfifa14", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4w7pv/need_help_with_spark_and_mongo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4w7pv/need_help_with_spark_and_mongo/", "subreddit_subscribers": 165242, "created_utc": 1709407109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context: I have some work experience and have freelanced in this field - Built data pipelines(data engineeiring) and BI dashboards. Mostly inclined towards Azure. I have some other clients as well that are related to that of a software architect. I am considering my options to see if a masters can give me a significant jump. \n\nDoes it necessarily give me an edge? Alternatively, what's the next best thing to do if not a Masters? Would it be working on projects and building my portfolio?", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it worth Getting going for a Masters in Data Science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4r198", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709394218.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709393906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: I have some work experience and have freelanced in this field - Built data pipelines(data engineeiring) and BI dashboards. Mostly inclined towards Azure. I have some other clients as well that are related to that of a software architect. I am considering my options to see if a masters can give me a significant jump. &lt;/p&gt;\n\n&lt;p&gt;Does it necessarily give me an edge? Alternatively, what&amp;#39;s the next best thing to do if not a Masters? Would it be working on projects and building my portfolio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4r198", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4r198/is_it_worth_getting_going_for_a_masters_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4r198/is_it_worth_getting_going_for_a_masters_in_data/", "subreddit_subscribers": 165242, "created_utc": 1709393906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've inherited a large data warehouse that has many different SQL Server Databases.\n\nI'm trying to understand the structure and relationships of the objects, but there isn't a data model I can refer to.  The system has grown over many years and has inconsistent names etc. which makes it more difficult to follow.\n\nThe largest issue, is that there are an abundance of nested views which, in turn, reference objects in other databases.  When you dig down through the views, you may even end up back in your original database.\n\nI'm thinking that I need to reverse engineer it in to a data model so I can see the dependencies and work out what is/isnt required when implementing its replacement.\n\nHas anyone worked with data modelling tools that handle multi-database dependencies?  I briefly looked at ER/Studio, but it seems to only handle dependencies within the same database when reverse engineering objects.  Even when adding the objects manually, I couldn't see how the database could be specified for an object, as it didn't seem to go above schema level.\n\nAnyone worked with a modelling tool that can reverse engineer dependencies like this?\n\n&amp;#x200B;", "author_fullname": "t2_2z1ud2z1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse Engineering a multi-database model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4pj29", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709389751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve inherited a large data warehouse that has many different SQL Server Databases.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to understand the structure and relationships of the objects, but there isn&amp;#39;t a data model I can refer to.  The system has grown over many years and has inconsistent names etc. which makes it more difficult to follow.&lt;/p&gt;\n\n&lt;p&gt;The largest issue, is that there are an abundance of nested views which, in turn, reference objects in other databases.  When you dig down through the views, you may even end up back in your original database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that I need to reverse engineer it in to a data model so I can see the dependencies and work out what is/isnt required when implementing its replacement.&lt;/p&gt;\n\n&lt;p&gt;Has anyone worked with data modelling tools that handle multi-database dependencies?  I briefly looked at ER/Studio, but it seems to only handle dependencies within the same database when reverse engineering objects.  Even when adding the objects manually, I couldn&amp;#39;t see how the database could be specified for an object, as it didn&amp;#39;t seem to go above schema level.&lt;/p&gt;\n\n&lt;p&gt;Anyone worked with a modelling tool that can reverse engineer dependencies like this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4pj29", "is_robot_indexable": true, "report_reasons": null, "author": "TeflonJacket", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4pj29/reverse_engineering_a_multidatabase_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4pj29/reverse_engineering_a_multidatabase_model/", "subreddit_subscribers": 165242, "created_utc": 1709389751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \nSoon I will be joining a new project, seen the board and there is an user story of \"Verify and test spark connectivity\", with no further description. It will take place on Synapse. How would you approach this task? I know I can ask but I've just passed client technical review and wont necessarily go with such question day 1 what did he meant or so. I mean I'll probably go but after doing something on my own and then report that I did this and that and what else he except of me in regards of this user story but I have no idea what to do.. The admin most likely will deploy Synapse spark pool and what, should I just create a notebook in development tab and see if I can execute dataframe creation, read and write using pyspark? This user story has at least 2 story points if I remember correctly which is 1 day, so It cannot be that quick, can it? Thank you in advance for your help!\n\n  \nTL;DR: How to you make sure you have good connectivity of spark in Synapse, how do you test and verify that?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Verify and test spark connectivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4mxaf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709384265.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709381640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nSoon I will be joining a new project, seen the board and there is an user story of &amp;quot;Verify and test spark connectivity&amp;quot;, with no further description. It will take place on Synapse. How would you approach this task? I know I can ask but I&amp;#39;ve just passed client technical review and wont necessarily go with such question day 1 what did he meant or so. I mean I&amp;#39;ll probably go but after doing something on my own and then report that I did this and that and what else he except of me in regards of this user story but I have no idea what to do.. The admin most likely will deploy Synapse spark pool and what, should I just create a notebook in development tab and see if I can execute dataframe creation, read and write using pyspark? This user story has at least 2 story points if I remember correctly which is 1 day, so It cannot be that quick, can it? Thank you in advance for your help!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: How to you make sure you have good connectivity of spark in Synapse, how do you test and verify that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4mxaf", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4mxaf/verify_and_test_spark_connectivity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4mxaf/verify_and_test_spark_connectivity/", "subreddit_subscribers": 165242, "created_utc": 1709381640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone share their experience and tech stacks used dealing with these three above. Automation experience will be a plus! Really interested to know what\u2019s the workaround ppl are using to streamline the ETL process to BI reporting / dashboards.", "author_fullname": "t2_vp3nqhfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP ECC, S/4 HANA and ARIBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4k4hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709370691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone share their experience and tech stacks used dealing with these three above. Automation experience will be a plus! Really interested to know what\u2019s the workaround ppl are using to streamline the ETL process to BI reporting / dashboards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4k4hr", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway-765431", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4k4hr/sap_ecc_s4_hana_and_ariba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4k4hr/sap_ecc_s4_hana_and_ariba/", "subreddit_subscribers": 165242, "created_utc": 1709370691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any recommendations for materials, e.g., blog/textbook, to help with resource planning and design/architecture for cloud migration?", "author_fullname": "t2_6cc9wqbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recs for materials to help with cloud migration resource planning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4gdam", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709356620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any recommendations for materials, e.g., blog/textbook, to help with resource planning and design/architecture for cloud migration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4gdam", "is_robot_indexable": true, "report_reasons": null, "author": "Cultured_dude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4gdam/recs_for_materials_to_help_with_cloud_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4gdam/recs_for_materials_to_help_with_cloud_migration/", "subreddit_subscribers": 165242, "created_utc": 1709356620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can you all please suggest what are some good sources to understand salary trends for data scientist, engineering or even data analysts? - preferably with over the years data as well.\n\nMany thanks for all your suggestions.", "author_fullname": "t2_143jjw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salary reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4fngj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709354301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you all please suggest what are some good sources to understand salary trends for data scientist, engineering or even data analysts? - preferably with over the years data as well.&lt;/p&gt;\n\n&lt;p&gt;Many thanks for all your suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b4fngj", "is_robot_indexable": true, "report_reasons": null, "author": "Jay12a", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4fngj/salary_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4fngj/salary_reports/", "subreddit_subscribers": 165242, "created_utc": 1709354301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Better yet, what am I actually looking for here?\n\nI\u2019m designing a system that handles schedules in a novel way that, based on our review, isn\u2019t offered elsewhere. Honestly the magic boils down to just a handful of presentation choices and feature integrations.\n\nI\u2019m using a relational database and the data, for the most part, fits the model well. One thing that stands out is tasks though\u2026 The tasks have a serial order to them meaning that one gets started only after the prior finishes.\n\nI need to support a query that, given a task and a location, the response is the number of incomplete upstream tasks. This is done by getting all incomplete tasks with an expected start date of less than the given task, within the given location, and returning the count of records.\n\nHowever, tasks are global\u2026 Every single space has the same tasks. Therefore, the serial order for tasks is also global. It\u2019s wasted effort to slice to tasks within the given location, recalculating which are upstream every time as based on a date column. \n\nI could just artificially maintain a serial order column within a tasks table. Values would just count upward, and I as the user would just insert them this way. This is an implicitly maintained rule that\u2019s critical for the system\u2019s functionality though\u2026 I think not.\n\nThen there are trigger functions which likely can be used very effectively. However, they carry a negative connotation due to undesired side effects. It can be difficult to reason about the synergic behavior of relational and functional capabilities at large scale, and humans bite the bullet here eventually. I\u2019d like to avoid triggers if possible.\n\nAre there data models that support this kind of precedence between entities more naturally? \n\nAre there data models that blur the line between relational and what I\u2019m looking for, potentially solving both needs at the same time?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do relational databases offer a way to require serial order from manually entered data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4e5fn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709349550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Better yet, what am I actually looking for here?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m designing a system that handles schedules in a novel way that, based on our review, isn\u2019t offered elsewhere. Honestly the magic boils down to just a handful of presentation choices and feature integrations.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m using a relational database and the data, for the most part, fits the model well. One thing that stands out is tasks though\u2026 The tasks have a serial order to them meaning that one gets started only after the prior finishes.&lt;/p&gt;\n\n&lt;p&gt;I need to support a query that, given a task and a location, the response is the number of incomplete upstream tasks. This is done by getting all incomplete tasks with an expected start date of less than the given task, within the given location, and returning the count of records.&lt;/p&gt;\n\n&lt;p&gt;However, tasks are global\u2026 Every single space has the same tasks. Therefore, the serial order for tasks is also global. It\u2019s wasted effort to slice to tasks within the given location, recalculating which are upstream every time as based on a date column. &lt;/p&gt;\n\n&lt;p&gt;I could just artificially maintain a serial order column within a tasks table. Values would just count upward, and I as the user would just insert them this way. This is an implicitly maintained rule that\u2019s critical for the system\u2019s functionality though\u2026 I think not.&lt;/p&gt;\n\n&lt;p&gt;Then there are trigger functions which likely can be used very effectively. However, they carry a negative connotation due to undesired side effects. It can be difficult to reason about the synergic behavior of relational and functional capabilities at large scale, and humans bite the bullet here eventually. I\u2019d like to avoid triggers if possible.&lt;/p&gt;\n\n&lt;p&gt;Are there data models that support this kind of precedence between entities more naturally? &lt;/p&gt;\n\n&lt;p&gt;Are there data models that blur the line between relational and what I\u2019m looking for, potentially solving both needs at the same time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4e5fn", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4e5fn/do_relational_databases_offer_a_way_to_require/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4e5fn/do_relational_databases_offer_a_way_to_require/", "subreddit_subscribers": 165242, "created_utc": 1709349550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My client in healthcare uses a hardware device that generates data and saves it into the VM of the hardware manufacturer. The generated data is about 10gb per file. Is there a way to automatically move these files to my own computer? The next step would be to monitor a folder in my computer to upload the file to S3 using multipart upload, but first, the files need to be there in the first place. Any advice would be appreciated. \n", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automate data extraction from external VM to my computer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4dnze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709348099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My client in healthcare uses a hardware device that generates data and saves it into the VM of the hardware manufacturer. The generated data is about 10gb per file. Is there a way to automatically move these files to my own computer? The next step would be to monitor a folder in my computer to upload the file to S3 using multipart upload, but first, the files need to be there in the first place. Any advice would be appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b4dnze", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4dnze/how_to_automate_data_extraction_from_external_vm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4dnze/how_to_automate_data_extraction_from_external_vm/", "subreddit_subscribers": 165242, "created_utc": 1709348099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all - I'll cut right to the chase: I am working on mentoring a mid-level data dev on skills development, and I could use some advice on where to focus. They mentioned gaining skill in the areas of using python (presumably in the context of notebooks and such) as well as sql. They are also pursuing a snowflake certification.\n\nTo give some context on me: I am a very experienced full stack developer and application/cloud architect on large-scale systems. I am conversant in data, and have built some basic ETL/ELTs, Azure Data Pipelines, relational query tuning, and some work with data lakes. That's enough to know what I don't know about data, if that makes sense.\n\nAny tips on where to focus as it relates to the above skills? Any other thoughts welcome as well - TIA!", "author_fullname": "t2_dnexpjr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Training advice needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b44616", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709323913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all - I&amp;#39;ll cut right to the chase: I am working on mentoring a mid-level data dev on skills development, and I could use some advice on where to focus. They mentioned gaining skill in the areas of using python (presumably in the context of notebooks and such) as well as sql. They are also pursuing a snowflake certification.&lt;/p&gt;\n\n&lt;p&gt;To give some context on me: I am a very experienced full stack developer and application/cloud architect on large-scale systems. I am conversant in data, and have built some basic ETL/ELTs, Azure Data Pipelines, relational query tuning, and some work with data lakes. That&amp;#39;s enough to know what I don&amp;#39;t know about data, if that makes sense.&lt;/p&gt;\n\n&lt;p&gt;Any tips on where to focus as it relates to the above skills? Any other thoughts welcome as well - TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b44616", "is_robot_indexable": true, "report_reasons": null, "author": "leetcde", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b44616/training_advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b44616/training_advice_needed/", "subreddit_subscribers": 165242, "created_utc": 1709323913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would love to create content about data engineering but do not know what you would like or miss. :) ", "author_fullname": "t2_uuylu9emi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data engineering content do you miss on social media?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b4kxs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709373966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would love to create content about data engineering but do not know what you would like or miss. :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b4kxs0", "is_robot_indexable": true, "report_reasons": null, "author": "LinasData", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b4kxs0/what_data_engineering_content_do_you_miss_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b4kxs0/what_data_engineering_content_do_you_miss_on/", "subreddit_subscribers": 165242, "created_utc": 1709373966.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}