{"kind": "Listing", "data": {"after": "t3_1bj9ua8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_t535h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "F1 team Williams used Excel as their database to track the car components (hundreds of thousands of different components)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1bix81r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 177, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 177, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i4e37SD8XA3IyggLwyo8sH5a_emcTeUSb61sqGyodkY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710886869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "the-race.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.the-race.com/formula-1/shocking-details-behind-painful-williams-f1-revolution/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?auto=webp&amp;s=2b7e70ac6d3f2a40f7b7cf33acd46b67bb09f4b2", "width": 1200, "height": 801}, "resolutions": [{"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=012270937cdbb17ee5a046bb7a36a26942be0100", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc41a73feb1ad9af8989ff0aa9414f1085d8c733", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26556524a81cea8c034b5a3f2a3a65a61ec5c279", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59ab3f3334809d3ece23a9a6dfb8b843c6d9c6f0", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fca2911fa0919e151955a92241189464a5042fba", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/SlVO78FlnZpFCIGKGJLgOpdIdiKS3jI3KxxsD4yChOE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4552d28a32ae4b0577c392562b5e19a73b0c2011", "width": 1080, "height": 720}], "variants": {}, "id": "sYpbiPOv1gfX8p4U2iMf8N5oMWagShFRnkY2-_F9LBQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1bix81r", "is_robot_indexable": true, "report_reasons": null, "author": "Tape56", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bix81r/f1_team_williams_used_excel_as_their_database_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.the-race.com/formula-1/shocking-details-behind-painful-williams-f1-revolution/", "subreddit_subscribers": 170409, "created_utc": 1710886869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as \"Data Engineer\" and ended up getting a DE job.  \n\n\nDoes that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. ", "author_fullname": "t2_tpf6owzl8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stretching the truth about being a \"Data Engineer\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bit81p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710877335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as &amp;quot;Data Engineer&amp;quot; and ended up getting a DE job.  &lt;/p&gt;\n\n&lt;p&gt;Does that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bit81p", "is_robot_indexable": true, "report_reasons": null, "author": "DarkPaladin67", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bit81p/stretching_the_truth_about_being_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bit81p/stretching_the_truth_about_being_a_data_engineer/", "subreddit_subscribers": 170409, "created_utc": 1710877335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This comes up a lot in random posts. Snowflake is a data warehouse. BigQuery is a data warehouse. PostgreSQL, MySQL, and SQL Server are not. We have let companies like Snowflake, Oracle, etc. redefine data warehouse from it's data-centric meaning, to a platform-centric one. \n\nA data warehouse is a collection of disparate sources modeled to provide efficient querying. Just about any DB system can be part a data warehouse solution, but the platform itself is not the data warehouse. Snowflake is a great solution for larger use cases where it saves significant engineering resources. For some tiny DW with rows in the low millions, it is probably going to be very expensive compared to other platforms. \n\nI know this sounds pedantic, but as data engineers, we should be precise with our terms. Doing anything else leads to confusion and misunderstandings. In the end, we should perform analysis and choose the best tool for the job. It very well might be one of the advertised \"data warehouses\". It may be Postgres. It may be something else. It's our job to find the right solution with hard data, not marketing hype.", "author_fullname": "t2_8ov8i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can We Stop Using Marketing Terms to Define Data Warehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjcybi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710939902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This comes up a lot in random posts. Snowflake is a data warehouse. BigQuery is a data warehouse. PostgreSQL, MySQL, and SQL Server are not. We have let companies like Snowflake, Oracle, etc. redefine data warehouse from it&amp;#39;s data-centric meaning, to a platform-centric one. &lt;/p&gt;\n\n&lt;p&gt;A data warehouse is a collection of disparate sources modeled to provide efficient querying. Just about any DB system can be part a data warehouse solution, but the platform itself is not the data warehouse. Snowflake is a great solution for larger use cases where it saves significant engineering resources. For some tiny DW with rows in the low millions, it is probably going to be very expensive compared to other platforms. &lt;/p&gt;\n\n&lt;p&gt;I know this sounds pedantic, but as data engineers, we should be precise with our terms. Doing anything else leads to confusion and misunderstandings. In the end, we should perform analysis and choose the best tool for the job. It very well might be one of the advertised &amp;quot;data warehouses&amp;quot;. It may be Postgres. It may be something else. It&amp;#39;s our job to find the right solution with hard data, not marketing hype.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bjcybi", "is_robot_indexable": true, "report_reasons": null, "author": "leogodin217", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjcybi/can_we_stop_using_marketing_terms_to_define_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjcybi/can_we_stop_using_marketing_terms_to_define_data/", "subreddit_subscribers": 170409, "created_utc": 1710939902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have recently started working as a data analyst in a start-up company. We have a web-based application. Currently, we have only Google Analytics and Zoho CRM connected to our website. We are planning to add more connections to our website and we are going to need a data warehouse (I suppose). So, our data is very small due to our business model. We are never going to have hundreds of users. 1 month's worth of Zoho CRM data is around 100k rows. I think using bigquery or snowflake is an overkill for us. What should I do?", "author_fullname": "t2_7y78l90c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am planning to use Postgre as a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjbdv3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710934829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have recently started working as a data analyst in a start-up company. We have a web-based application. Currently, we have only Google Analytics and Zoho CRM connected to our website. We are planning to add more connections to our website and we are going to need a data warehouse (I suppose). So, our data is very small due to our business model. We are never going to have hundreds of users. 1 month&amp;#39;s worth of Zoho CRM data is around 100k rows. I think using bigquery or snowflake is an overkill for us. What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bjbdv3", "is_robot_indexable": true, "report_reasons": null, "author": "Dodomeki16", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjbdv3/i_am_planning_to_use_postgre_as_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjbdv3/i_am_planning_to_use_postgre_as_a_data_warehouse/", "subreddit_subscribers": 170409, "created_utc": 1710934829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering a career pivot and discovered analytics engineering\u2014which to my knowledge is pretty similar to DE but with more business context/less technical skills needed?). Was wondering to all analytics engineers out there what your job looks like and what problems you deal with on a day-to-day?", "author_fullname": "t2_edit41t1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the biggest problems/painpoints in Analytics Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bixoc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710887963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering a career pivot and discovered analytics engineering\u2014which to my knowledge is pretty similar to DE but with more business context/less technical skills needed?). Was wondering to all analytics engineers out there what your job looks like and what problems you deal with on a day-to-day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bixoc5", "is_robot_indexable": true, "report_reasons": null, "author": "Admirable-Roll-7108", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bixoc5/what_are_the_biggest_problemspainpoints_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bixoc5/what_are_the_biggest_problemspainpoints_in/", "subreddit_subscribers": 170409, "created_utc": 1710887963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought developing an API using Python/FastAPI to create a sql session and execute bunch of queries does not sound complex.\n\nWhat am I missing here ?\n\nIf you are someone that develops API, design databases, can you drop a resource that I could see/read about the complexity of developing an API?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Developing API\u2019s as part of being a DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1biyz7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710891221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought developing an API using Python/FastAPI to create a sql session and execute bunch of queries does not sound complex.&lt;/p&gt;\n\n&lt;p&gt;What am I missing here ?&lt;/p&gt;\n\n&lt;p&gt;If you are someone that develops API, design databases, can you drop a resource that I could see/read about the complexity of developing an API?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biyz7f", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biyz7f/developing_apis_as_part_of_being_a_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biyz7f/developing_apis_as_part_of_being_a_de/", "subreddit_subscribers": 170409, "created_utc": 1710891221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context : \n\n* Completed my bachelors in CSE (November 2023)\n* No prior experience with cloud computing and Azure\n* Did not write DP-900 or AZ-900 or any other certification exam before this one\n\nMost of the people on this subreddit have cleared this exam with some prior experience on cloud computing,I was unemployed(LOL ,I still am but I will start working from April onwards) and couldn't get placed from my college,so one of my uncle told that they  had a vacancy for a 'Azure Data Engineer Associate' and so I decided to **clear DP - 203 and ended up scoring 900/1000** \n\n**I feel this post will help people like me who have no prior experience with Azure or Cloud or SQL**\n\nI found the exam to be moderately difficult started studying on approximately 17th January and gave my exam on 17th March (If you study daily for around 2-3 hours daily,**consistently** it might take you around 40 - 45 days)\n\nFirst things first : You must have some theoretical and practical on SQL because the exam will test your knowledge on Transact-SQL (if you understand SQL it will not take time) \n\nSo I researched and many people suggested that I start preparing from Alan Rodrigues's course (Bought it for just 449 INR) [https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/](https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/) , **initially found the course overwhelming** (because I did not study for DP - 900) so I spent a lot of time on ChatGPT understanding the basics like *Batch Processing,ETL,ELT,Stream and Reference Data,Telemetry Data,Power BI,Polybase,HADOOP,Apache Spark,Azure Data Lake Storage,Parquet,JSON etc.* (while preparing for this exam I took a look at DP - 900's syllabus and found a youtuber that explained the basics clearly,**according to me you don't have to clear DP-900 to clear this exam but atleast understand the basics**,watched some videos from his playlist on 1.5X [https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb](https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb) ) \n\nSo after I completed Alan's course,took a practice test and ended up scoring around 20% (I would still recommend his course to understand *T-SQL queries,Synpase,DataFactory,Databricks,Pipelines,Data Flows,Azure Monitor etc.*)the overall explanation was good **but the mistake I made was** : focused too much on how to execute the services and getting hands on experience with the platform rather than getting an overall understanding of the concepts,but a little bit hands on experience will be helpful. \n\nSo I started to understand  the paper pattern and the type of questions that frequently come on the exam,around 60 - 70 % of the questions came from this playlist [https://www.youtube.com/watch?v=mbo43UgIkYc&amp;list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&amp;index=3](https://www.youtube.com/watch?v=mbo43UgIkYc&amp;list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&amp;index=3) \n\nI would also recommend this youtuber as he explains all the questions clearly [https://www.youtube.com/@studyingasyouwere/playlists](https://www.youtube.com/@studyingasyouwere/playlists) \n\nI would also recommend these 2 channels to revise your preparation \n\n[https://www.youtube.com/watch?v=RTlZZMDA7qw&amp;list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y](https://www.youtube.com/watch?v=RTlZZMDA7qw&amp;list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y) \n\n[https://www.youtube.com/watch?v=6deS7pKBEGM](https://www.youtube.com/watch?v=6deS7pKBEGM) \n\nAnd towards the end I bought a course that had 6 question papers for 449 INR,but some answers were wrong so just google the question or read about the question on Microsoft Documentation\n\n[https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk\\_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/](https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/) \n\n**A tip I would recommend**,if you don't understand any topic refer to Microsoft Documentation and still if you don't understand use ChatGPT\n\n**And finally my exam experience** : Had a total of 43 questions (I thought the total number of questions were 65,I guess it changed recently) , had to answer a Case Study intially,then had Multiple choice single answer,Multiple choice Multiple answer,Drop down menus , **not a single question consisted of rearranging the sequence ,** total exam time : 140 minutes,completed mine in around 100 minutes \n\nI hope someone on this subreddit finds this information valuable,study well and all the best", "author_fullname": "t2_29svvuid", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passed DP-203 on 17th March 2024,without any prior cloud experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjb4f1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710940020.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710933858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context : &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Completed my bachelors in CSE (November 2023)&lt;/li&gt;\n&lt;li&gt;No prior experience with cloud computing and Azure&lt;/li&gt;\n&lt;li&gt;Did not write DP-900 or AZ-900 or any other certification exam before this one&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Most of the people on this subreddit have cleared this exam with some prior experience on cloud computing,I was unemployed(LOL ,I still am but I will start working from April onwards) and couldn&amp;#39;t get placed from my college,so one of my uncle told that they  had a vacancy for a &amp;#39;Azure Data Engineer Associate&amp;#39; and so I decided to &lt;strong&gt;clear DP - 203 and ended up scoring 900/1000&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I feel this post will help people like me who have no prior experience with Azure or Cloud or SQL&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I found the exam to be moderately difficult started studying on approximately 17th January and gave my exam on 17th March (If you study daily for around 2-3 hours daily,&lt;strong&gt;consistently&lt;/strong&gt; it might take you around 40 - 45 days)&lt;/p&gt;\n\n&lt;p&gt;First things first : You must have some theoretical and practical on SQL because the exam will test your knowledge on Transact-SQL (if you understand SQL it will not take time) &lt;/p&gt;\n\n&lt;p&gt;So I researched and many people suggested that I start preparing from Alan Rodrigues&amp;#39;s course (Bought it for just 449 INR) &lt;a href=\"https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/\"&gt;https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/&lt;/a&gt; , &lt;strong&gt;initially found the course overwhelming&lt;/strong&gt; (because I did not study for DP - 900) so I spent a lot of time on ChatGPT understanding the basics like &lt;em&gt;Batch Processing,ETL,ELT,Stream and Reference Data,Telemetry Data,Power BI,Polybase,HADOOP,Apache Spark,Azure Data Lake Storage,Parquet,JSON etc.&lt;/em&gt; (while preparing for this exam I took a look at DP - 900&amp;#39;s syllabus and found a youtuber that explained the basics clearly,&lt;strong&gt;according to me you don&amp;#39;t have to clear DP-900 to clear this exam but atleast understand the basics&lt;/strong&gt;,watched some videos from his playlist on 1.5X &lt;a href=\"https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb\"&gt;https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb&lt;/a&gt; ) &lt;/p&gt;\n\n&lt;p&gt;So after I completed Alan&amp;#39;s course,took a practice test and ended up scoring around 20% (I would still recommend his course to understand &lt;em&gt;T-SQL queries,Synpase,DataFactory,Databricks,Pipelines,Data Flows,Azure Monitor etc.&lt;/em&gt;)the overall explanation was good &lt;strong&gt;but the mistake I made was&lt;/strong&gt; : focused too much on how to execute the services and getting hands on experience with the platform rather than getting an overall understanding of the concepts,but a little bit hands on experience will be helpful. &lt;/p&gt;\n\n&lt;p&gt;So I started to understand  the paper pattern and the type of questions that frequently come on the exam,around 60 - 70 % of the questions came from this playlist &lt;a href=\"https://www.youtube.com/watch?v=mbo43UgIkYc&amp;amp;list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&amp;amp;index=3\"&gt;https://www.youtube.com/watch?v=mbo43UgIkYc&amp;amp;list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&amp;amp;index=3&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I would also recommend this youtuber as he explains all the questions clearly &lt;a href=\"https://www.youtube.com/@studyingasyouwere/playlists\"&gt;https://www.youtube.com/@studyingasyouwere/playlists&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I would also recommend these 2 channels to revise your preparation &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=RTlZZMDA7qw&amp;amp;list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y\"&gt;https://www.youtube.com/watch?v=RTlZZMDA7qw&amp;amp;list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=6deS7pKBEGM\"&gt;https://www.youtube.com/watch?v=6deS7pKBEGM&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;And towards the end I bought a course that had 6 question papers for 449 INR,but some answers were wrong so just google the question or read about the question on Microsoft Documentation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/\"&gt;https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A tip I would recommend&lt;/strong&gt;,if you don&amp;#39;t understand any topic refer to Microsoft Documentation and still if you don&amp;#39;t understand use ChatGPT&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;And finally my exam experience&lt;/strong&gt; : Had a total of 43 questions (I thought the total number of questions were 65,I guess it changed recently) , had to answer a Case Study intially,then had Multiple choice single answer,Multiple choice Multiple answer,Drop down menus , &lt;strong&gt;not a single question consisted of rearranging the sequence ,&lt;/strong&gt; total exam time : 140 minutes,completed mine in around 100 minutes &lt;/p&gt;\n\n&lt;p&gt;I hope someone on this subreddit finds this information valuable,study well and all the best&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bjb4f1", "is_robot_indexable": true, "report_reasons": null, "author": "re8r0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjb4f1/passed_dp203_on_17th_march_2024without_any_prior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjb4f1/passed_dp203_on_17th_march_2024without_any_prior/", "subreddit_subscribers": 170409, "created_utc": 1710933858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, in a bit of a very stressful choice to make here, hoping I can get some advice on how what would lead me into a data engineering or analytics engineering role the best\n\nI\u2019ve been a data analyst intern(mainly SQL and PowerBI) at my current company for 1.5 years, and they offered me a full time position for 67k. But I got a job offer for a more business enablement focused data analytics position for 75k.\n\n The issue is, the business enablement company doesn\u2019t necessarily have any plans to get me involved in any ETL processes of the data  (they legit told me the position is far from any engineering in the interviews as there is 0 transformation of the data done). It\u2019s strictly SQL and PowerBI and whatever sources PowerBI dataflows could use.\n\nMy current company knows my career goals, but couldn\u2019t get me an actual engineering position said there will be projects in the future where I will be closer to the data source, and mentioned Python. Is this grooming? I\u2019m honestly not sure, I\u2019ve never done those projects before at this company, but going to full time typically gets our team members involved in a variety of projects, and if they know my goals maybe it would be worth staying?\n\nTLDR: Company I interned at offers 67K and more POSSIBLE potential to expand skillset and transition into an engineering role down the line(there\u2019s a chance it\u2019s a smokescreen). Other offer, 75k, much more business enablement focused, doesn\u2019t really want me going into the engineering path, would likely be much harder to leverage position in the future. Turn down 8k for a chance to get into engineering(and significantly higher pay) in the future?\n\n", "author_fullname": "t2_55fytx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Take less pay but more potential for a future in engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bj2psw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710902977.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710901397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, in a bit of a very stressful choice to make here, hoping I can get some advice on how what would lead me into a data engineering or analytics engineering role the best&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been a data analyst intern(mainly SQL and PowerBI) at my current company for 1.5 years, and they offered me a full time position for 67k. But I got a job offer for a more business enablement focused data analytics position for 75k.&lt;/p&gt;\n\n&lt;p&gt;The issue is, the business enablement company doesn\u2019t necessarily have any plans to get me involved in any ETL processes of the data  (they legit told me the position is far from any engineering in the interviews as there is 0 transformation of the data done). It\u2019s strictly SQL and PowerBI and whatever sources PowerBI dataflows could use.&lt;/p&gt;\n\n&lt;p&gt;My current company knows my career goals, but couldn\u2019t get me an actual engineering position said there will be projects in the future where I will be closer to the data source, and mentioned Python. Is this grooming? I\u2019m honestly not sure, I\u2019ve never done those projects before at this company, but going to full time typically gets our team members involved in a variety of projects, and if they know my goals maybe it would be worth staying?&lt;/p&gt;\n\n&lt;p&gt;TLDR: Company I interned at offers 67K and more POSSIBLE potential to expand skillset and transition into an engineering role down the line(there\u2019s a chance it\u2019s a smokescreen). Other offer, 75k, much more business enablement focused, doesn\u2019t really want me going into the engineering path, would likely be much harder to leverage position in the future. Turn down 8k for a chance to get into engineering(and significantly higher pay) in the future?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bj2psw", "is_robot_indexable": true, "report_reasons": null, "author": "ToothPickLegs", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bj2psw/take_less_pay_but_more_potential_for_a_future_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bj2psw/take_less_pay_but_more_potential_for_a_future_in/", "subreddit_subscribers": 170409, "created_utc": 1710901397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on an email course for Analytics team leads on product management. But since I'm super slow, I could release what I got so far for free.\n\nThe idea is this: I've seen that the most challenging part for analytics PMs is discovery, and not really the prototyping or the solution finding, but really digging into who the internal customers for their team really are, and what they really want. \n\n&amp;#x200B;\n\nSo that's what those five short emails are about.   \n\n\n1. Figuring out that users of BI systems are not the only internal customers, and likely not the most important ones\n2. Finding key decision makers inside the company that your team doesn't serve yet  \n\n3. Finding key decision makers for whom you've previously only had an indirect relation (e.g. through an analyst using the BI systems)  \n\n4. Finding a good line of questioning to get to the bottom of the requirements.  \n\n\nFiguring out that users of BI systems are not the only internal customers and likely not the most important ones. Here's my outline for questions I ask in discovery calls with internal customers:   \n***Problem-Centric Questions***\n\n* *Walk me through how you use this X.*\n* *What do you do with it then? (Export?)*\n* *How does this data help you make better decisions?*  \n\n   * *A less aggressive version: How does this data help you area to make better decisions?*  \n*Or: How does your area make decisions with data? Tell me all the steps.* \n* *If it is not you who makes better decisions, who is it? (also pretty aggressive, best used as follow-up with context)*\n* *Can you give me an example?*\n* *How does this help you move towards our company strategy?*\n* *How do you get this data right now? Manually? CSV, some other tool?*\n* *What does your workflow look like when preparing for X (sales meeting)?*\n\n*I\u2019ll follow up most of these questions by adapting them, extending them, letting them provide me with an example, or asking about one I have in mind that already exists. I\u2019ll often make sure I cover, at the very least, half of the meeting with problem-centric questions.*\n\n*Then, I move on to solution-centric questions.*\n\n**Solution-centric Questions**\n\n*I will always open the discussion of solutions by saying, \u201cNothing is set in stone; I\u2019ll come back to you once I discussed everything with the team, but what I can say is we have different ways of implementing this, and I\u2019m not the expert on it, the team is.\u201d*\n\n* *What would you do if we cannot build X? (literally) Both regarding the impact on your work, and what would you do to find a workaround?*\n* *If not an X (your product feature), how would it best be delivered to you? Explain how you\u2019d integrate X into your work to make/help make decisions.*\n* *If we build this as X, how would your workflow change?*\n* *Describe to me how this would change the impact on decision-making.*\n* *If you could only get one of the features described, what\u2019s the one that would have the most impact on decision-making?*\n\n  \nIf that makes sense, please take a look at it here: [https://www.theanalyticspm.com/discovery-course](https://www.theanalyticspm.com/discovery-course) and don't forget to give me some feedback! ", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Course On Analytics Product Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bj8kvh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710923136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on an email course for Analytics team leads on product management. But since I&amp;#39;m super slow, I could release what I got so far for free.&lt;/p&gt;\n\n&lt;p&gt;The idea is this: I&amp;#39;ve seen that the most challenging part for analytics PMs is discovery, and not really the prototyping or the solution finding, but really digging into who the internal customers for their team really are, and what they really want. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s what those five short emails are about.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Figuring out that users of BI systems are not the only internal customers, and likely not the most important ones&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finding key decision makers inside the company that your team doesn&amp;#39;t serve yet  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finding key decision makers for whom you&amp;#39;ve previously only had an indirect relation (e.g. through an analyst using the BI systems)  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finding a good line of questioning to get to the bottom of the requirements.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Figuring out that users of BI systems are not the only internal customers and likely not the most important ones. Here&amp;#39;s my outline for questions I ask in discovery calls with internal customers:&lt;br/&gt;\n&lt;strong&gt;&lt;em&gt;Problem-Centric Questions&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Walk me through how you use this X.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;What do you do with it then? (Export?)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;How does this data help you make better decisions?&lt;/em&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;A less aggressive version: How does this data help you area to make better decisions?&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;Or: How does your area make decisions with data? Tell me all the steps.&lt;/em&gt; &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;If it is not you who makes better decisions, who is it? (also pretty aggressive, best used as follow-up with context)&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;Can you give me an example?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;How does this help you move towards our company strategy?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;How do you get this data right now? Manually? CSV, some other tool?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;What does your workflow look like when preparing for X (sales meeting)?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;I\u2019ll follow up most of these questions by adapting them, extending them, letting them provide me with an example, or asking about one I have in mind that already exists. I\u2019ll often make sure I cover, at the very least, half of the meeting with problem-centric questions.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Then, I move on to solution-centric questions.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solution-centric Questions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I will always open the discussion of solutions by saying, \u201cNothing is set in stone; I\u2019ll come back to you once I discussed everything with the team, but what I can say is we have different ways of implementing this, and I\u2019m not the expert on it, the team is.\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;What would you do if we cannot build X? (literally) Both regarding the impact on your work, and what would you do to find a workaround?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;If not an X (your product feature), how would it best be delivered to you? Explain how you\u2019d integrate X into your work to make/help make decisions.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;If we build this as X, how would your workflow change?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Describe to me how this would change the impact on decision-making.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;If you could only get one of the features described, what\u2019s the one that would have the most impact on decision-making?&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If that makes sense, please take a look at it here: &lt;a href=\"https://www.theanalyticspm.com/discovery-course\"&gt;https://www.theanalyticspm.com/discovery-course&lt;/a&gt; and don&amp;#39;t forget to give me some feedback! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?auto=webp&amp;s=9e298d81e5cfb8892539f2b4a9d80e67bba1f638", "width": 5184, "height": 3456}, "resolutions": [{"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8999d61beadcfd59df649167decacac81b728732", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=431bf58be79cbeaf374fb8157de7bd2eea38541e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=455354eb7e0821d15efd1cbc65e9ba05707e2990", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=325d94e6972ebfe522e9d0f625c884fcb5edd4fd", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78393cbba7b7882ecbcff0bf2ffb87b4c89fdd9a", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/pegH9_UZOiP5FF6R6Dlp02VMvzGpDzbmsSCOxbTFgmo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b2c8033d391a40a46d9f4bef31f68ca55b03e87f", "width": 1080, "height": 720}], "variants": {}, "id": "8p3XRmkWno4hl8QimLdtm6LAjl1MrJJVHZO-E8POKOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bj8kvh", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bj8kvh/free_course_on_analytics_product_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bj8kvh/free_course_on_analytics_product_management/", "subreddit_subscribers": 170409, "created_utc": 1710923136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company currently has separate dev/test/prod environments for our data warehouse. Using one data source as an example:\n\nWe have scripts on a test VM that extract data from the data source's test server and loads the data to our test environment. We have the same scripts on a prod VM that extracts data from the data source's prod server and loads to our prod environment.\n\nProblem: \n\n1. The test server on the data source isn't routinely updated, and is thus out of sync with prod and makes a 1 to 1 validation of data for new dev work difficult when users look at the prod data on the data source application. \n2. It seems redundant to have scripts extract data from both test and prod servers, when we could alternatively extract just from prod data source server into our prod data warehouse and then push straight from our prod environment to our test environment so that minimal resources are used for ETL and test data aligns perfectly with prod data.\n\nQuestion:\n\nHow do you guys typically manage ETL for your test environment? Am I right that it would be better to only load data into the prod data warehouse and push from there into the test data warehouse?", "author_fullname": "t2_j3ecksk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Load Test Environment Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bisp92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently has separate dev/test/prod environments for our data warehouse. Using one data source as an example:&lt;/p&gt;\n\n&lt;p&gt;We have scripts on a test VM that extract data from the data source&amp;#39;s test server and loads the data to our test environment. We have the same scripts on a prod VM that extracts data from the data source&amp;#39;s prod server and loads to our prod environment.&lt;/p&gt;\n\n&lt;p&gt;Problem: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The test server on the data source isn&amp;#39;t routinely updated, and is thus out of sync with prod and makes a 1 to 1 validation of data for new dev work difficult when users look at the prod data on the data source application. &lt;/li&gt;\n&lt;li&gt;It seems redundant to have scripts extract data from both test and prod servers, when we could alternatively extract just from prod data source server into our prod data warehouse and then push straight from our prod environment to our test environment so that minimal resources are used for ETL and test data aligns perfectly with prod data.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;How do you guys typically manage ETL for your test environment? Am I right that it would be better to only load data into the prod data warehouse and push from there into the test data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bisp92", "is_robot_indexable": true, "report_reasons": null, "author": "SellGameRent", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bisp92/how_to_load_test_environment_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bisp92/how_to_load_test_environment_data/", "subreddit_subscribers": 170409, "created_utc": 1710876069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a Analytics Engineer/Data Engineer for a year and been solving Leetcode problems lately (to prepare for new opportunities). The following questions just came up in my mind regarding Leetcode and how to approach solving technical problems in general.\n\n**1. About intellectual capability to program:** I was able to crack some Easy level LC problems, but this one (https://leetcode.com/problems/subsets/) made me doubt my ability. I came up with a general solution (pseudo-code) and a runnable program (using recursion) for that problem within an afternoon, but only until the next day did I find a way to write it without using recursion. I do not have a CS background, but I know that interviews are very time-intensive and my colleagues with CS background can solve this kind of problem in minutes. With my age approaching 30, am I screwed with regards to my ability to program well and effectively? I'm really afraid that I'm not intellectually capable of programming *effectively* (i.e., coming up with efficient and smart solutions to problems).\n\n**2. About approaching technical problems:** For me, it takes quite some time to solve LC problems and lots of tweaking/debugging to pass all test cases. I feel like if I come back to those problems, I won't produce the exact answers. How would you organize the ideas that you learned from solving technical problems like LC? Do you recommend remembering all the details/edge cases of the code or just remember the general approach to each problem or the kind of the problem? Should I read algorithm textbooks as well to invigorate the ideas learnt?\n\nThank you for reading.", "author_fullname": "t2_ksuox3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leetcode, programming capability and approaching technical problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bj6pp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710915052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a Analytics Engineer/Data Engineer for a year and been solving Leetcode problems lately (to prepare for new opportunities). The following questions just came up in my mind regarding Leetcode and how to approach solving technical problems in general.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. About intellectual capability to program:&lt;/strong&gt; I was able to crack some Easy level LC problems, but this one (&lt;a href=\"https://leetcode.com/problems/subsets/\"&gt;https://leetcode.com/problems/subsets/&lt;/a&gt;) made me doubt my ability. I came up with a general solution (pseudo-code) and a runnable program (using recursion) for that problem within an afternoon, but only until the next day did I find a way to write it without using recursion. I do not have a CS background, but I know that interviews are very time-intensive and my colleagues with CS background can solve this kind of problem in minutes. With my age approaching 30, am I screwed with regards to my ability to program well and effectively? I&amp;#39;m really afraid that I&amp;#39;m not intellectually capable of programming &lt;em&gt;effectively&lt;/em&gt; (i.e., coming up with efficient and smart solutions to problems).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. About approaching technical problems:&lt;/strong&gt; For me, it takes quite some time to solve LC problems and lots of tweaking/debugging to pass all test cases. I feel like if I come back to those problems, I won&amp;#39;t produce the exact answers. How would you organize the ideas that you learned from solving technical problems like LC? Do you recommend remembering all the details/edge cases of the code or just remember the general approach to each problem or the kind of the problem? Should I read algorithm textbooks as well to invigorate the ideas learnt?&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bj6pp2", "is_robot_indexable": true, "report_reasons": null, "author": "vietzerg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bj6pp2/leetcode_programming_capability_and_approaching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bj6pp2/leetcode_programming_capability_and_approaching/", "subreddit_subscribers": 170409, "created_utc": 1710915052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, i\u00b4m trying to model my data from a scraper that i made. I\u00b4m able to retrieve around 26 columns from an api and put togheter in my dwh in bigquery, obviusly the first table that i have is one OBT table but i realize that along the time the prices maybe change and if the owner make renovations some characteristics around the property also can change, the status if it\u00b4s sell can change.\n\nhttps://preview.redd.it/2rvqo23o1dpc1.jpg?width=701&amp;format=pjpg&amp;auto=webp&amp;s=16d470f03ebffc901b4beeeeefe526318242598c\n\nIn the obt is hard to mantain these logic and i want to use scd type 2 to track this changes in the dimensions. So the first thing that i need to do is model my data as you can see in the image.\n\nBut i\u00b4m a few undecided if my schema it\u00b4s OK or something can change.\n\n* In the location table, i see that if is a condo maybe have the same adress and geographical data, but i\u00b4m not sure if i can put the adress and geo data into the fact table\n* for the attributes dimension also i only think that is 1 to 1 relation because there is no chance that 2 property listings have the same property\\_id and url. So i don\u00b4t know how to aproach this dimension\n* the other dimensions i think are ok\n\nWhat do you think about my schema and if i need to do some improvements?\n\nthanks for your support folks.", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Property listings data modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 138, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2rvqo23o1dpc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/2rvqo23o1dpc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9aa33ebf1ba7ea53fc3e6ca5d2fcad3d8b5281b"}, {"y": 212, "x": 216, "u": "https://preview.redd.it/2rvqo23o1dpc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e607712836c29ff93fedb5c5b235a8211902d317"}, {"y": 315, "x": 320, "u": "https://preview.redd.it/2rvqo23o1dpc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a808fa8875141a9c7ae010b7a753752d6d11bde"}, {"y": 630, "x": 640, "u": "https://preview.redd.it/2rvqo23o1dpc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=443d78d3ce77994725d7400bd3e7c0232520ba76"}], "s": {"y": 691, "x": 701, "u": "https://preview.redd.it/2rvqo23o1dpc1.jpg?width=701&amp;format=pjpg&amp;auto=webp&amp;s=16d470f03ebffc901b4beeeeefe526318242598c"}, "id": "2rvqo23o1dpc1"}}, "name": "t3_1biw65l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/g9ud-Jg0GADZsQDDRlLOEqGQWeEfs-EATU5KqiaRtpk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710884401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, i\u00b4m trying to model my data from a scraper that i made. I\u00b4m able to retrieve around 26 columns from an api and put togheter in my dwh in bigquery, obviusly the first table that i have is one OBT table but i realize that along the time the prices maybe change and if the owner make renovations some characteristics around the property also can change, the status if it\u00b4s sell can change.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2rvqo23o1dpc1.jpg?width=701&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=16d470f03ebffc901b4beeeeefe526318242598c\"&gt;https://preview.redd.it/2rvqo23o1dpc1.jpg?width=701&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=16d470f03ebffc901b4beeeeefe526318242598c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the obt is hard to mantain these logic and i want to use scd type 2 to track this changes in the dimensions. So the first thing that i need to do is model my data as you can see in the image.&lt;/p&gt;\n\n&lt;p&gt;But i\u00b4m a few undecided if my schema it\u00b4s OK or something can change.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In the location table, i see that if is a condo maybe have the same adress and geographical data, but i\u00b4m not sure if i can put the adress and geo data into the fact table&lt;/li&gt;\n&lt;li&gt;for the attributes dimension also i only think that is 1 to 1 relation because there is no chance that 2 property listings have the same property_id and url. So i don\u00b4t know how to aproach this dimension&lt;/li&gt;\n&lt;li&gt;the other dimensions i think are ok&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What do you think about my schema and if i need to do some improvements?&lt;/p&gt;\n\n&lt;p&gt;thanks for your support folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biw65l", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biw65l/property_listings_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biw65l/property_listings_data_modeling/", "subreddit_subscribers": 170409, "created_utc": 1710884401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Kafka connector to send kafka topics data to 200+ destinations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1biqsae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Dcu6VBDsnTInxOse8U-R-5-yKyByVMnBqCVSKZnCVVI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710871426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/rudderlabs/rudder-kafka-sink-connector", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?auto=webp&amp;s=2236b72c38c1ffdb3ef00bc2c4dae5fbd4a116cb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab3990f924bd9bf0da88fb79a89075e6cad33a01", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4df0cc65b132eb2a2f60895b37c4f59f705b5cd7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8847d43ea3f647be0e9a2cb39a1ebc6a8277a728", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e6253a85bf7e130c92e6bf21615e7bf385246f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c854fd5dd1329ce2cb94a4e60d5d3324969aeb54", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97c03bdc700f1d9114a39bb80c449226c6882b10", "width": 1080, "height": 540}], "variants": {}, "id": "HHCapiHt6RX89jBT8ceKmiW73oQ_kfiILwHJ-WwYTqg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1biqsae", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biqsae/open_source_kafka_connector_to_send_kafka_topics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/rudderlabs/rudder-kafka-sink-connector", "subreddit_subscribers": 170409, "created_utc": 1710871426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Airflow is so darn heavy, has so much unnecessary over engineering and it makes it so necessary to adapt your scripts to it rather than the other way around \u2014 which in my opinion should be how it should work.\n\nTo be honest, maybe Im using Airflow wrong but no one on my team seems to be privy to more knowledge nor can I find much online. \n\nIs there a lightweight orchestrator that\u2019s out there? Something simple, that does everything like Airflow minus the endless configuration. Something simple like CRON with a web ui for task status?", "author_fullname": "t2_2l4y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lightweight Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjfvir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710947715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Airflow is so darn heavy, has so much unnecessary over engineering and it makes it so necessary to adapt your scripts to it rather than the other way around \u2014 which in my opinion should be how it should work.&lt;/p&gt;\n\n&lt;p&gt;To be honest, maybe Im using Airflow wrong but no one on my team seems to be privy to more knowledge nor can I find much online. &lt;/p&gt;\n\n&lt;p&gt;Is there a lightweight orchestrator that\u2019s out there? Something simple, that does everything like Airflow minus the endless configuration. Something simple like CRON with a web ui for task status?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bjfvir", "is_robot_indexable": true, "report_reasons": null, "author": "5678", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjfvir/lightweight_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjfvir/lightweight_airflow/", "subreddit_subscribers": 170409, "created_utc": 1710947715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been asked to make a plan/road map to migrate my job\u2019s pipelines off airflow + Spark + Redshift to airflow + dbt core + Redshift\n\nCurrently our keep raw data  in S3 to use DBT I would need to load that raw data into using the COPY command in Redshift.\n\nSo I\u2019ve been tinkering with DBT but it doesn\u2019t let me run CREATE, COPY, and UNLOAD anywhere except the pre-hook and post-hook of models, it expects everything to be a SELECT statement.\n\nThis seems like a terrible practice because the SQL \u201chooks\u201d in these needs to be a quoted string meaning you get no sql linting,no syntax highlighting or checking etc.\n\nIs there any way to schedule arbitrary SQL that isn\u2019t models with dbt?\n\n I can roll my own but it just seems like a tool like this that\u2019s literally for running SQL should have something more than these pre and post hooks to run DDL SQL commands.\n\n**tl,dr:** We have over 200+ raw data tables in S3. Making empty models with only pre and post hook to use the CREAT, COPY, and UNLOAD commands to load the data into Redshift seems like a bad idea. Is there a way to run DDL from DBT that\u2019s not pre and post hooks.\n", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DDL for COPY and UNLOAD commands in DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1biukdk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710880824.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710880575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been asked to make a plan/road map to migrate my job\u2019s pipelines off airflow + Spark + Redshift to airflow + dbt core + Redshift&lt;/p&gt;\n\n&lt;p&gt;Currently our keep raw data  in S3 to use DBT I would need to load that raw data into using the COPY command in Redshift.&lt;/p&gt;\n\n&lt;p&gt;So I\u2019ve been tinkering with DBT but it doesn\u2019t let me run CREATE, COPY, and UNLOAD anywhere except the pre-hook and post-hook of models, it expects everything to be a SELECT statement.&lt;/p&gt;\n\n&lt;p&gt;This seems like a terrible practice because the SQL \u201chooks\u201d in these needs to be a quoted string meaning you get no sql linting,no syntax highlighting or checking etc.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to schedule arbitrary SQL that isn\u2019t models with dbt?&lt;/p&gt;\n\n&lt;p&gt;I can roll my own but it just seems like a tool like this that\u2019s literally for running SQL should have something more than these pre and post hooks to run DDL SQL commands.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl,dr:&lt;/strong&gt; We have over 200+ raw data tables in S3. Making empty models with only pre and post hook to use the CREAT, COPY, and UNLOAD commands to load the data into Redshift seems like a bad idea. Is there a way to run DDL from DBT that\u2019s not pre and post hooks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biukdk", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biukdk/ddl_for_copy_and_unload_commands_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biukdk/ddl_for_copy_and_unload_commands_in_dbt/", "subreddit_subscribers": 170409, "created_utc": 1710880575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Guys! I need help\n\nIn my business, we typically don't rely on concrete facts, but rather on daily snapshots. We operate within the educational system, focusing on aspects like \"Students enrolled in courses on date X\". These student records may remain constant (I lack a specific date column for updates, only a cancellation date). Therefore, every day we capture a snapshot at 11:59 PM, which results in a significant amount of duplicate data, given the minimal changes from day to day. Perhaps one or two cancellations and two or three new enrollments might occur.\n\nIts something like 50K rows per day, it is not so much but we are on prem \n\nHow can i handle this type of fact?  Usually directors request to me like \"How many enrolled students we have on day X\"", "author_fullname": "t2_8jc0mwfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problem to design a fact table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1biua2g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710879883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Guys! I need help&lt;/p&gt;\n\n&lt;p&gt;In my business, we typically don&amp;#39;t rely on concrete facts, but rather on daily snapshots. We operate within the educational system, focusing on aspects like &amp;quot;Students enrolled in courses on date X&amp;quot;. These student records may remain constant (I lack a specific date column for updates, only a cancellation date). Therefore, every day we capture a snapshot at 11:59 PM, which results in a significant amount of duplicate data, given the minimal changes from day to day. Perhaps one or two cancellations and two or three new enrollments might occur.&lt;/p&gt;\n\n&lt;p&gt;Its something like 50K rows per day, it is not so much but we are on prem &lt;/p&gt;\n\n&lt;p&gt;How can i handle this type of fact?  Usually directors request to me like &amp;quot;How many enrolled students we have on day X&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biua2g", "is_robot_indexable": true, "report_reasons": null, "author": "Andremallmann", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biua2g/problem_to_design_a_fact_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biua2g/problem_to_design_a_fact_table/", "subreddit_subscribers": 170409, "created_utc": 1710879883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm currently tasked with setting up an open-source ingestion tool for two main use cases within my organization:\n1. Self-service capabilities: for our non-technical users to easily ingest data.\n2. ELT data pipeline: construction to streamline our data processing workflows.\n\nWe were initially considering **Airbyte** as the primary tool due to its versatility and broad connector support. However, we've encountered a significant compatibility issue with our current architecture, which heavily relies on **Delta Lake tables** and **Spark** for data processing. Specifically, we're facing challenges with Airbyte's integration with this setup, as discussed in this GitHub issue: https://github.com/airbytehq/airbyte/issues/16322\n\nGiven this context, I'm reaching out to the community for advice:\n- Has anyone successfully found a workaround for integrating Airbyte with Delta Lake and Spark, as per the mentioned issue?\n- Alternatively, are there any other open-source tools you would recommend that could meet our needs and seamlessly fit into our Delta Lake and Spark-centric architecture?\n\nAny insights, experiences, or suggestions you could share would be immensely appreciated. Our goal is to find a reliable, open-source solution that can accommodate our specific requirements without compromising on functionality or ease of use.\n\nThank you in advance for your help and looking forward to your recommendations!\n", "author_fullname": "t2_hffs9quu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Open Source Ingestion Tool Compatible with Delta Lake and Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1biu10m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710879253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently tasked with setting up an open-source ingestion tool for two main use cases within my organization:\n1. Self-service capabilities: for our non-technical users to easily ingest data.\n2. ELT data pipeline: construction to streamline our data processing workflows.&lt;/p&gt;\n\n&lt;p&gt;We were initially considering &lt;strong&gt;Airbyte&lt;/strong&gt; as the primary tool due to its versatility and broad connector support. However, we&amp;#39;ve encountered a significant compatibility issue with our current architecture, which heavily relies on &lt;strong&gt;Delta Lake tables&lt;/strong&gt; and &lt;strong&gt;Spark&lt;/strong&gt; for data processing. Specifically, we&amp;#39;re facing challenges with Airbyte&amp;#39;s integration with this setup, as discussed in this GitHub issue: &lt;a href=\"https://github.com/airbytehq/airbyte/issues/16322\"&gt;https://github.com/airbytehq/airbyte/issues/16322&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Given this context, I&amp;#39;m reaching out to the community for advice:\n- Has anyone successfully found a workaround for integrating Airbyte with Delta Lake and Spark, as per the mentioned issue?\n- Alternatively, are there any other open-source tools you would recommend that could meet our needs and seamlessly fit into our Delta Lake and Spark-centric architecture?&lt;/p&gt;\n\n&lt;p&gt;Any insights, experiences, or suggestions you could share would be immensely appreciated. Our goal is to find a reliable, open-source solution that can accommodate our specific requirements without compromising on functionality or ease of use.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help and looking forward to your recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?auto=webp&amp;s=40569cb9bca9b656aab79a7caaf2aa15cecb1528", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=316467b66d8db1e6a7898aacaca13779a4ea3b08", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25f37ba73d36d0d9037a0fcd4924c33469d1d770", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da0f588fcc80b504fc9396d7b2ab057b97a3e4de", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d1e94e73dedf8b3380a50400e5aef29a6266815", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=970f88a9bdd47a0ef61e9ea80d52ce826e0d3a86", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef746c904e388dc56ef058a6d4e645f17e9caacf", "width": 1080, "height": 540}], "variants": {}, "id": "tVtrXdHx-ZiNTu5lwWtBhHdKSdGFT2DM2lQxn6Ffg38"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biu10m", "is_robot_indexable": true, "report_reasons": null, "author": "Ecstatic-Zucchini-53", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biu10m/seeking_advice_on_open_source_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biu10m/seeking_advice_on_open_source_ingestion_tool/", "subreddit_subscribers": 170409, "created_utc": 1710879253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello data engineers,\n\nI'm seeking advice on migrating our on-premises Oracle data warehouse to the azure cloud. Our on-premises oracle db is 80tb with thousands of etl information jobs . \n\n * What was your approach (business-driven vs. replicating on-premises processes)?\n\n * Any recommended migration strategies or tools?\nOur current approach feels scattered. \n\nAny tips or lessons learned would be greatly appreciated!\n", "author_fullname": "t2_7f74h3uy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Legacy Oracle Data Warehouse to Azure Cloud Migration Strategies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bistcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on migrating our on-premises Oracle data warehouse to the azure cloud. Our on-premises oracle db is 80tb with thousands of etl information jobs . &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What was your approach (business-driven vs. replicating on-premises processes)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any recommended migration strategies or tools?\nOur current approach feels scattered. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any tips or lessons learned would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bistcf", "is_robot_indexable": true, "report_reasons": null, "author": "CountNo9037", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bistcf/legacy_oracle_data_warehouse_to_azure_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bistcf/legacy_oracle_data_warehouse_to_azure_cloud/", "subreddit_subscribers": 170409, "created_utc": 1710876341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking out for any website/platform which allows users to interact and users will be able to work together on the project. If it's not present, I'm proposing following:\n\nA platform or website which have functionality to support following workflow: \n\n  \n**1**. **Ideation Pitch In** \\- Anyone can come and add their project/task ideas in the posts. For ex. Need to build a dashboard from this [dataset](https://voaratinglists.blob.core.windows.net/html/rlidata.htm). \n\n**2**. **Peer Gathering:** People gonna see the post and gonna add the enhancement that can be done on this. ex. in dashboard, can we add this feature?\n\n**3**. **Task Division:** OP can divide the tasks initially once the project is finalized (ofc they can add enhacements in between).\n\n**4**. **Task Ownership:** Peers who wanna pitch in can take these tasks and own them (if that owner drops off, someone else from community can pick that up)\n\n*Advantage*: They can show up these projects in their portfolio as they are openly available in public Github, Also if some ideas are really useful to everyone, they can easily showcase them. \n\nLet me know what you guys think of it. Any feedback is appreciated.  \n\n\n&amp;#x200B;", "author_fullname": "t2_c4kloony", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Peer Driven Projects for Experience and Learning Purpose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjd57h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710940459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking out for any website/platform which allows users to interact and users will be able to work together on the project. If it&amp;#39;s not present, I&amp;#39;m proposing following:&lt;/p&gt;\n\n&lt;p&gt;A platform or website which have functionality to support following workflow: &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. &lt;strong&gt;Ideation Pitch In&lt;/strong&gt; - Anyone can come and add their project/task ideas in the posts. For ex. Need to build a dashboard from this &lt;a href=\"https://voaratinglists.blob.core.windows.net/html/rlidata.htm\"&gt;dataset&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. &lt;strong&gt;Peer Gathering:&lt;/strong&gt; People gonna see the post and gonna add the enhancement that can be done on this. ex. in dashboard, can we add this feature?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. &lt;strong&gt;Task Division:&lt;/strong&gt; OP can divide the tasks initially once the project is finalized (ofc they can add enhacements in between).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. &lt;strong&gt;Task Ownership:&lt;/strong&gt; Peers who wanna pitch in can take these tasks and own them (if that owner drops off, someone else from community can pick that up)&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Advantage&lt;/em&gt;: They can show up these projects in their portfolio as they are openly available in public Github, Also if some ideas are really useful to everyone, they can easily showcase them. &lt;/p&gt;\n\n&lt;p&gt;Let me know what you guys think of it. Any feedback is appreciated.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bjd57h", "is_robot_indexable": true, "report_reasons": null, "author": "triesegment", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bjd57h/peer_driven_projects_for_experience_and_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjd57h/peer_driven_projects_for_experience_and_learning/", "subreddit_subscribers": 170409, "created_utc": 1710940459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy folks. The team I\u2019m currently on has a plethora of Informatica processes built up that are being considered for divestment out of Informatica. Currently those processes use a hashing function in Informatica that creates a hash off predefined attributes, checks that hash against the current table, and if there isn\u2019t a match: load the record.\n\nThe problem: the hash function is a black box. Attempting to recreate the hash, even by the namesake of the hashing algorithm (think MD5 and other hashing algos), the same result is not reproduced. There seems to be some baked in logic to the Informatica logic that modifies the algo one way or another.\n\nSo with that in mind, divesting from Informatica will cause every row to be new. The compute and storage of an entirely new dataset across all of the processes is out of scope.\n\nIf anyone\u2019s been down a similar road, curious on the solution you used. One thought is SCDM2 and indicate the columns being used as the hash input today as the changing dimensions for SCDM2. In theory, simply scanning those columns and timestamping changes ought to provide a solution that mirrors the legacy dataset and not cause an entirely new dataset. ", "author_fullname": "t2_708ooj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Divesting from Informatica &amp; Hashing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjbsqu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710936231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy folks. The team I\u2019m currently on has a plethora of Informatica processes built up that are being considered for divestment out of Informatica. Currently those processes use a hashing function in Informatica that creates a hash off predefined attributes, checks that hash against the current table, and if there isn\u2019t a match: load the record.&lt;/p&gt;\n\n&lt;p&gt;The problem: the hash function is a black box. Attempting to recreate the hash, even by the namesake of the hashing algorithm (think MD5 and other hashing algos), the same result is not reproduced. There seems to be some baked in logic to the Informatica logic that modifies the algo one way or another.&lt;/p&gt;\n\n&lt;p&gt;So with that in mind, divesting from Informatica will cause every row to be new. The compute and storage of an entirely new dataset across all of the processes is out of scope.&lt;/p&gt;\n\n&lt;p&gt;If anyone\u2019s been down a similar road, curious on the solution you used. One thought is SCDM2 and indicate the columns being used as the hash input today as the changing dimensions for SCDM2. In theory, simply scanning those columns and timestamping changes ought to provide a solution that mirrors the legacy dataset and not cause an entirely new dataset. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bjbsqu", "is_robot_indexable": true, "report_reasons": null, "author": "ExistentialFajitas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bjbsqu/divesting_from_informatica_hashing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjbsqu/divesting_from_informatica_hashing/", "subreddit_subscribers": 170409, "created_utc": 1710936231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some commonly used tools for integrating and transforming data with different GTM SaaS?   \nFor instance, I built a SaaS product that helps to construct PnL for B2B SaaS. I want to extract data from Stripe and Hubspot, perform some metric calculations across two systems, and then send it back to the original systems; what tools should I use? I considered using Airbyte for ETL, DBT for transformation, and some API wrapped to communicate the results back to the systems. \n\nDoes this approach sound reasonable or I'm looking in the wrong direction? ", "author_fullname": "t2_evy8q46", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL setup for the backend for the data products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjbmk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710935667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some commonly used tools for integrating and transforming data with different GTM SaaS?&lt;br/&gt;\nFor instance, I built a SaaS product that helps to construct PnL for B2B SaaS. I want to extract data from Stripe and Hubspot, perform some metric calculations across two systems, and then send it back to the original systems; what tools should I use? I considered using Airbyte for ETL, DBT for transformation, and some API wrapped to communicate the results back to the systems. &lt;/p&gt;\n\n&lt;p&gt;Does this approach sound reasonable or I&amp;#39;m looking in the wrong direction? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bjbmk7", "is_robot_indexable": true, "report_reasons": null, "author": "zkid18", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjbmk7/etl_setup_for_the_backend_for_the_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjbmk7/etl_setup_for_the_backend_for_the_data_products/", "subreddit_subscribers": 170409, "created_utc": 1710935667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have $500 to use as part of L&amp;D from my company.\n\nJust wondering what are the best resources to invest in.\n\nI read most of my books on Kindle and my computer, so not really a big of physical books.\n\nThinking of educative.io membership?\n\nAny suggestions?\n\nThanks!", "author_fullname": "t2_hnhd87tx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use learning budget?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bisyjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have $500 to use as part of L&amp;amp;D from my company.&lt;/p&gt;\n\n&lt;p&gt;Just wondering what are the best resources to invest in.&lt;/p&gt;\n\n&lt;p&gt;I read most of my books on Kindle and my computer, so not really a big of physical books.&lt;/p&gt;\n\n&lt;p&gt;Thinking of educative.io membership?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bisyjw", "is_robot_indexable": true, "report_reasons": null, "author": "StoicResearcher", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bisyjw/how_to_use_learning_budget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bisyjw/how_to_use_learning_budget/", "subreddit_subscribers": 170409, "created_utc": 1710876684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a Data Engineer at a startup and we are using Redash as a platform to query from multiple database sources, schedule queries and link result of queries with Google sheet.\nI am looking for a tool that can handle all these use cases, please suggest some alternatives of Redash. Currently I am exploring datagrip. \n\n", "author_fullname": "t2_3p3vfvzt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternate of Redash", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjdyd2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710942731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a Data Engineer at a startup and we are using Redash as a platform to query from multiple database sources, schedule queries and link result of queries with Google sheet.\nI am looking for a tool that can handle all these use cases, please suggest some alternatives of Redash. Currently I am exploring datagrip. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bjdyd2", "is_robot_indexable": true, "report_reasons": null, "author": "Dr_Fida", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjdyd2/alternate_of_redash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjdyd2/alternate_of_redash/", "subreddit_subscribers": 170409, "created_utc": 1710942731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, short intro: We are a small team of students that are doing a project with a company. The goal of the project is to automate reports given data.\n\nWhat we developed so far, locally: (1) A script that does the full processing of the data, (2) A script that takes the processed data, user input (through an excel file) and generates a custom word document that can be easily adjusted by an engineer.\n\nThe company apparently uses microsoft Azure and has its data hosted there. The company suggested to use Synapse Analytics to host our code. They have the following vision: An engineer gets new data, uploads it to Azure, adjusts some variables in the excel sheet, clicks a button, and a new document gets generated.\n\nNow the question: Is this possible? Our company contact does not really know himself how to do this and so far has only showed us how to run a notebook in the \"Develop\" section. We have been spending some time trying to just get familiar with synapse and we still don't really know how to host a codebase as if you had it locally.", "author_fullname": "t2_mxisf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Student project: Company wants us to use Synapse Analytics but we are not sure if that makes sense", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bjbafg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710934476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, short intro: We are a small team of students that are doing a project with a company. The goal of the project is to automate reports given data.&lt;/p&gt;\n\n&lt;p&gt;What we developed so far, locally: (1) A script that does the full processing of the data, (2) A script that takes the processed data, user input (through an excel file) and generates a custom word document that can be easily adjusted by an engineer.&lt;/p&gt;\n\n&lt;p&gt;The company apparently uses microsoft Azure and has its data hosted there. The company suggested to use Synapse Analytics to host our code. They have the following vision: An engineer gets new data, uploads it to Azure, adjusts some variables in the excel sheet, clicks a button, and a new document gets generated.&lt;/p&gt;\n\n&lt;p&gt;Now the question: Is this possible? Our company contact does not really know himself how to do this and so far has only showed us how to run a notebook in the &amp;quot;Develop&amp;quot; section. We have been spending some time trying to just get familiar with synapse and we still don&amp;#39;t really know how to host a codebase as if you had it locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bjbafg", "is_robot_indexable": true, "report_reasons": null, "author": "Im_jayco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bjbafg/student_project_company_wants_us_to_use_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bjbafg/student_project_company_wants_us_to_use_synapse/", "subreddit_subscribers": 170409, "created_utc": 1710934476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is more of a Data Science problem, but I found surprisingly little information on it and I thought it would be a good idea to ask the epxerts.\n\nI have a large dataset of strings and corresponding embeddings (fixed-size torch tensors). I would like to save them in some kind of tabular format, to be re-used when experimenting with hyperparameters, model architectures, etc. If possible I would like to avoid signing up for cloud vector DBs or setting up DB servers (like Postgres).\n\nThe solutions I see now:\n\n- store as `.npy`, `.pt` or `.hd5`: straightforward &amp; efficient. Cons: not so straightforward retrieval without link to the corresponding string, no built-in appending / batching solutions.\n- PyArrow has a `FixedShapeTensorArray` extension format which you can save into Parquet. Con: it does not seem to be interoperable with tools I would use to retrieve embeddings for a particular set of strings, like Polars or DuckDB.\n- DuckDB &amp; Parquet have a 1D fixed-length array type. Con: would require reshaping before saving and after retrieval.\n- Finally, Parquet could also store a serialized bytes column. Con: again, requires (de)serialization on read and write.\n\nI would appreciate any insights.", "author_fullname": "t2_59m5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File storage format for tensors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bj9ua8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710928904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is more of a Data Science problem, but I found surprisingly little information on it and I thought it would be a good idea to ask the epxerts.&lt;/p&gt;\n\n&lt;p&gt;I have a large dataset of strings and corresponding embeddings (fixed-size torch tensors). I would like to save them in some kind of tabular format, to be re-used when experimenting with hyperparameters, model architectures, etc. If possible I would like to avoid signing up for cloud vector DBs or setting up DB servers (like Postgres).&lt;/p&gt;\n\n&lt;p&gt;The solutions I see now:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;store as &lt;code&gt;.npy&lt;/code&gt;, &lt;code&gt;.pt&lt;/code&gt; or &lt;code&gt;.hd5&lt;/code&gt;: straightforward &amp;amp; efficient. Cons: not so straightforward retrieval without link to the corresponding string, no built-in appending / batching solutions.&lt;/li&gt;\n&lt;li&gt;PyArrow has a &lt;code&gt;FixedShapeTensorArray&lt;/code&gt; extension format which you can save into Parquet. Con: it does not seem to be interoperable with tools I would use to retrieve embeddings for a particular set of strings, like Polars or DuckDB.&lt;/li&gt;\n&lt;li&gt;DuckDB &amp;amp; Parquet have a 1D fixed-length array type. Con: would require reshaping before saving and after retrieval.&lt;/li&gt;\n&lt;li&gt;Finally, Parquet could also store a serialized bytes column. Con: again, requires (de)serialization on read and write.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would appreciate any insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bj9ua8", "is_robot_indexable": true, "report_reasons": null, "author": "satyrmode", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bj9ua8/file_storage_format_for_tensors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bj9ua8/file_storage_format_for_tensors/", "subreddit_subscribers": 170409, "created_utc": 1710928904.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}