{"kind": "Listing", "data": {"after": "t3_1bnskfq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company recently launched their data engineering team, and I'm their newest hire (a self-taught fresh grad, &lt;1 month in the company). I have two seniors, and we're the only data engineers here.\n\nWe use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We're weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we're gonna use Astronomer's Astro CLI and Cosmos to set up dbt with Airflow.\n\nHas anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you've encountered? How's Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I've heard many had success with it, and cost is the only concern. We're trying to minimize costs, so as much as possible we're opting for open-source solutions (I believe Astro CLI and Cosmos are free if you're not using their cloud/IDE version).\n\nAny insights/advice would be highly appreciated. TYIA!\n\n", "author_fullname": "t2_ts2pktzo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating dbt Core with Apache Airflow: MWAA vs Self-Hosted EC2 Deployment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnebbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711415330.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711375051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company recently launched their data engineering team, and I&amp;#39;m their newest hire (a self-taught fresh grad, &amp;lt;1 month in the company). I have two seniors, and we&amp;#39;re the only data engineers here.&lt;/p&gt;\n\n&lt;p&gt;We use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We&amp;#39;re weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we&amp;#39;re gonna use Astronomer&amp;#39;s Astro CLI and Cosmos to set up dbt with Airflow.&lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you&amp;#39;ve encountered? How&amp;#39;s Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I&amp;#39;ve heard many had success with it, and cost is the only concern. We&amp;#39;re trying to minimize costs, so as much as possible we&amp;#39;re opting for open-source solutions (I believe Astro CLI and Cosmos are free if you&amp;#39;re not using their cloud/IDE version).&lt;/p&gt;\n\n&lt;p&gt;Any insights/advice would be highly appreciated. TYIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnebbr", "is_robot_indexable": true, "report_reasons": null, "author": "zmxavier", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "subreddit_subscribers": 171761, "created_utc": 1711375051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? ", "author_fullname": "t2_7chglbbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to ask senior data engineers during interviews?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn843b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711353211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn843b", "is_robot_indexable": true, "report_reasons": null, "author": "Computingss", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "subreddit_subscribers": 171761, "created_utc": 1711353211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.\n\n&amp;#x200B;\n\nCode for reference:\n\n    import csv\n    import random\n    from faker import Faker\n    \n    # Initialize Faker to generate fake data\n    fake = Faker()\n    \n    # Define the number of rows to generate\n    num_rows = 1000000\n    \n    # Define field names for the CSV\n    field_names = ['ID', 'Name', 'Age', 'Email', 'Address']\n    \n    # Function to generate random data for a row\n    def generate_row():\n        return {\n            'ID': random.randint(1, 1000000),\n            'Name': fake.name(),\n            'Age': random.randint(18, 80),\n            'Email': fake.email(),\n            'Address': fake.address()\n        }\n    \n    # Generate and write data to CSV file\n    with open('dummy_data.csv', 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n        writer.writeheader()\n        for _ in range(num_rows):\n            writer.writerow(generate_row())\n    \n    print(\"CSV file generation complete.\")\n    \n    ##########################################\n    \n    import pandas as pd\n    \n    # Read CSV file into pandas DataFrame\n    df = pd.read_csv('dummy_data.csv')\n    \n    # Write DataFrame to Parquet file\n    df.to_parquet('dummy_data.parquet', index=False)\n    \n    print(\"Parquet file generation complete.\")\n    \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My parquet file takes up MORE space than original csv. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnps6k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Code for reference:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import csv\nimport random\nfrom faker import Faker\n\n# Initialize Faker to generate fake data\nfake = Faker()\n\n# Define the number of rows to generate\nnum_rows = 1000000\n\n# Define field names for the CSV\nfield_names = [&amp;#39;ID&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;Age&amp;#39;, &amp;#39;Email&amp;#39;, &amp;#39;Address&amp;#39;]\n\n# Function to generate random data for a row\ndef generate_row():\n    return {\n        &amp;#39;ID&amp;#39;: random.randint(1, 1000000),\n        &amp;#39;Name&amp;#39;: fake.name(),\n        &amp;#39;Age&amp;#39;: random.randint(18, 80),\n        &amp;#39;Email&amp;#39;: fake.email(),\n        &amp;#39;Address&amp;#39;: fake.address()\n    }\n\n# Generate and write data to CSV file\nwith open(&amp;#39;dummy_data.csv&amp;#39;, &amp;#39;w&amp;#39;, newline=&amp;#39;&amp;#39;) as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n    writer.writeheader()\n    for _ in range(num_rows):\n        writer.writerow(generate_row())\n\nprint(&amp;quot;CSV file generation complete.&amp;quot;)\n\n##########################################\n\nimport pandas as pd\n\n# Read CSV file into pandas DataFrame\ndf = pd.read_csv(&amp;#39;dummy_data.csv&amp;#39;)\n\n# Write DataFrame to Parquet file\ndf.to_parquet(&amp;#39;dummy_data.parquet&amp;#39;, index=False)\n\nprint(&amp;quot;Parquet file generation complete.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnps6k", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "subreddit_subscribers": 171761, "created_utc": 1711402498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Someone I know who is into DE role &gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  \n\n\nIn DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? ", "author_fullname": "t2_8i81bbqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much of you guys have to deal with algorithms or ML algorithms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnda5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711378244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711372229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone I know who is into DE role &amp;gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  &lt;/p&gt;\n\n&lt;p&gt;In DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnda5x", "is_robot_indexable": true, "report_reasons": null, "author": "trafalgar28", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "subreddit_subscribers": 171761, "created_utc": 1711372229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).\n\nLet me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. \n\nBut at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with \"everything else\". I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.\n\nAnyone else been or currently in this kinda situation? What did you do?", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you draw the line at the nebulous job title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnqqvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711404953.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711404685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).&lt;/p&gt;\n\n&lt;p&gt;Let me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. &lt;/p&gt;\n\n&lt;p&gt;But at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with &amp;quot;everything else&amp;quot;. I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.&lt;/p&gt;\n\n&lt;p&gt;Anyone else been or currently in this kinda situation? What did you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnqqvl", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "subreddit_subscribers": 171761, "created_utc": 1711404685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm mostly a python guy but at one point I wrote c#. Lately I've been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. \n\nMy job is mostly writing backend APIs and automation in fastapi. I've done a few wildly basic akka endpoints over the years and really like that syntax. \n\nThing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. \n\nIt's fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. \n\nWhy don't YOU personally use scala?", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on scala? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnupia", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m mostly a python guy but at one point I wrote c#. Lately I&amp;#39;ve been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. &lt;/p&gt;\n\n&lt;p&gt;My job is mostly writing backend APIs and automation in fastapi. I&amp;#39;ve done a few wildly basic akka endpoints over the years and really like that syntax. &lt;/p&gt;\n\n&lt;p&gt;Thing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. &lt;/p&gt;\n\n&lt;p&gt;Why don&amp;#39;t YOU personally use scala?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnupia", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "subreddit_subscribers": 171761, "created_utc": 1711414574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . \n\nNow the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.\n\nNow we have to convey the SLA's to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. \n\n\nI have brainstormed several approaches but didn't get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db's are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . \n\n\nNow how to tackle this problem and convey the freshness and SLA's to downstream systems.\n\nAny suggestions or external tools will be helpful. \n\nThanks in advance ", "author_fullname": "t2_bpwbtfnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data freshness and completeness", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn74g3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711348926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . &lt;/p&gt;\n\n&lt;p&gt;Now the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.&lt;/p&gt;\n\n&lt;p&gt;Now we have to convey the SLA&amp;#39;s to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. &lt;/p&gt;\n\n&lt;p&gt;I have brainstormed several approaches but didn&amp;#39;t get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db&amp;#39;s are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . &lt;/p&gt;\n\n&lt;p&gt;Now how to tackle this problem and convey the freshness and SLA&amp;#39;s to downstream systems.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or external tools will be helpful. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bn74g3", "is_robot_indexable": true, "report_reasons": null, "author": "oofla_mey_goofla", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "subreddit_subscribers": 171761, "created_utc": 1711348926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.\n\nHowever, my experience has been a bit of a mess across several different disciplines, so I'm really not sure what I'm qualified to go for or what's expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.\n\nHowever, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.\n\nIs Palantir Foundry an \"easier\" version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?\n\nThe work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven't actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.", "author_fullname": "t2_iemkebmei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Palantir Foundry Count as Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbg99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711366574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.&lt;/p&gt;\n\n&lt;p&gt;However, my experience has been a bit of a mess across several different disciplines, so I&amp;#39;m really not sure what I&amp;#39;m qualified to go for or what&amp;#39;s expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.&lt;/p&gt;\n\n&lt;p&gt;However, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.&lt;/p&gt;\n\n&lt;p&gt;Is Palantir Foundry an &amp;quot;easier&amp;quot; version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?&lt;/p&gt;\n\n&lt;p&gt;The work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven&amp;#39;t actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnbg99", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous_lurker_01", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "subreddit_subscribers": 171761, "created_utc": 1711366574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? ", "author_fullname": "t2_w54oah5ij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance on Advancing Skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnnfq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711397087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I&amp;#39;ve been working hard to learn these, but I&amp;#39;m stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven&amp;#39;t seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnnfq8", "is_robot_indexable": true, "report_reasons": null, "author": "abhannan980", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "subreddit_subscribers": 171761, "created_utc": 1711397087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello everyone,\n\nI have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?", "author_fullname": "t2_ic15ya1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "snowflake+dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnkubv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711391040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnkubv", "is_robot_indexable": true, "report_reasons": null, "author": "satwikchandra", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnkubv/snowflakedbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnkubv/snowflakedbt/", "subreddit_subscribers": 171761, "created_utc": 1711391040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.\n\n I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. \n\nCurrently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different partitions of same table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnuiuv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.&lt;/p&gt;\n\n&lt;p&gt;I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. &lt;/p&gt;\n\n&lt;p&gt;Currently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnuiuv", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "subreddit_subscribers": 171761, "created_utc": 1711414068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.\n\nThere\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:\n\n- What will be the biggest shock/change?\n- How long did it take to feel comfortable in the new role?\n- What did/could you have done that made the transition smoother? \n\n", "author_fullname": "t2_2o4st1yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics to Big Data Engineer Tips", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnt8o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711410774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What will be the biggest shock/change?&lt;/li&gt;\n&lt;li&gt;How long did it take to feel comfortable in the new role?&lt;/li&gt;\n&lt;li&gt;What did/could you have done that made the transition smoother? &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnt8o9", "is_robot_indexable": true, "report_reasons": null, "author": "el527", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "subreddit_subscribers": 171761, "created_utc": 1711410774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you call the curated dataset that you use to validate or unit test your transformation?\n\nExample, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV's galore, or something like that and you are automating all the business rules and making it run daily. Now, you're going to validate if your pipeline recreates exactly what the users used to do in Excel.\n\nHow do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?", "author_fullname": "t2_rpj2htsu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Truth vs Gold Standard vs Baseline vs ???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnm4w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711394092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you call the curated dataset that you use to validate or unit test your transformation?&lt;/p&gt;\n\n&lt;p&gt;Example, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV&amp;#39;s galore, or something like that and you are automating all the business rules and making it run daily. Now, you&amp;#39;re going to validate if your pipeline recreates exactly what the users used to do in Excel.&lt;/p&gt;\n\n&lt;p&gt;How do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnm4w0", "is_robot_indexable": true, "report_reasons": null, "author": "taciom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "subreddit_subscribers": 171761, "created_utc": 1711394092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. \n\nOne of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. \n\nThe prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don't want to move anything to a new storage. So I'm a little worried if we are biting more than we can chew, but I don't want to lose a business that is already at the doorsteps. \n\nWhat are some implementation techniques you guys used during virtualization to address speed and performance issues?", "author_fullname": "t2_a1jxqx9hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential Issues with Data Virtualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnis0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711386134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. &lt;/p&gt;\n\n&lt;p&gt;One of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. &lt;/p&gt;\n\n&lt;p&gt;The prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don&amp;#39;t want to move anything to a new storage. So I&amp;#39;m a little worried if we are biting more than we can chew, but I don&amp;#39;t want to lose a business that is already at the doorsteps. &lt;/p&gt;\n\n&lt;p&gt;What are some implementation techniques you guys used during virtualization to address speed and performance issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnis0n", "is_robot_indexable": true, "report_reasons": null, "author": "turboline-ai", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "subreddit_subscribers": 171761, "created_utc": 1711386134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I've researched the topic a good amount and still haven't been able to verify whether my idea is possible and is the right approach or not.\n\n**The problem:** at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.\n\nOne (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.\n\nWhat I'd really like to do is implement a more CDC-based ETL pipeline. I don't have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.\n\nThis would only be used on certain tables, all of which don't have millions of daily changes.\n\nI've found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I'm confused as to how to use a change stream-based sensor operator?", "author_fullname": "t2_gcq3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trigger airflow dag based on real-time changes to MongoDB database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bne5wy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711374657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I&amp;#39;ve researched the topic a good amount and still haven&amp;#39;t been able to verify whether my idea is possible and is the right approach or not.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.&lt;/p&gt;\n\n&lt;p&gt;One (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d really like to do is implement a more CDC-based ETL pipeline. I don&amp;#39;t have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.&lt;/p&gt;\n\n&lt;p&gt;This would only be used on certain tables, all of which don&amp;#39;t have millions of daily changes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I&amp;#39;m confused as to how to use a change stream-based sensor operator?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bne5wy", "is_robot_indexable": true, "report_reasons": null, "author": "johnsonfrusciante", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "subreddit_subscribers": 171761, "created_utc": 1711374657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)\n\nIt is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?\n\nIf i do that my dim table has only the city and  neighborhood granularity and have around 10k rows\n\n&amp;#x200B;\n\nThanks for your time", "author_fullname": "t2_wgg22vwi4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geographic info is stored in FACT or DIM table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnunsa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)&lt;/p&gt;\n\n&lt;p&gt;It is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?&lt;/p&gt;\n\n&lt;p&gt;If i do that my dim table has only the city and  neighborhood granularity and have around 10k rows&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnunsa", "is_robot_indexable": true, "report_reasons": null, "author": "fr-profile1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "subreddit_subscribers": 171761, "created_utc": 1711414443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. \n\nIt hasn't done a good job learning how the different tables join or what I'm most likely to select or fields I'm returning. Has anyone used this with Visual Studio Code or Azure Data Studio?", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Github Copilot for SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnstzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. &lt;/p&gt;\n\n&lt;p&gt;It hasn&amp;#39;t done a good job learning how the different tables join or what I&amp;#39;m most likely to select or fields I&amp;#39;m returning. Has anyone used this with Visual Studio Code or Azure Data Studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnstzw", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "subreddit_subscribers": 171761, "created_utc": 1711409739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Too specific of a question but would like to know the nuances", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the difference between exadata database vs exadata warehouse? Also, can we migrate both of them to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnljan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Too specific of a question but would like to know the nuances&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnljan", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "subreddit_subscribers": 171761, "created_utc": 1711392678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Python package to help Databricks Unity Catalog users read and query Delta Lake tables with Polars, DuckDb, or PyArrow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnf67w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DRcjTpTyb9GkuKMvyPZE89uYrxg2Ons7iy7JhO_rnWI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711377212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/danielbeach/lakescum", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?auto=webp&amp;s=6d21a764643609310ba3389f80673261cc8025e5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c996873227f58c286c4db36ef4d7cfd67c76e37c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4e02ecb0c65b217928fff4c91e98189640791a5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a9aad197a1c0cd0477a498cae2ab9a8fdf370fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02b63710d92488c40bbc932cfbf13f1f7dc4db67", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af8cec4826bda3fabf081007c22a673572befb61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b89038e7fcb5beaa1bef04c9cf48c2ee7e3c930", "width": 1080, "height": 540}], "variants": {}, "id": "u586jNjYI52ADVaOP025fVjfx6Pn53ZVWD_9GRGNRBg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bnf67w", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnf67w/a_python_package_to_help_databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/danielbeach/lakescum", "subreddit_subscribers": 171761, "created_utc": 1711377212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nAs many of you reached out to me and said to continue the blogs I have decided to get back at it. However, 1 glaring issue was that all my blogs would be lost with time on reddit hence I have decided to post them on YouTube going forward. This will ensure all of you and the future folks can access it whenever you want as well as I can teach with more creative freedom than being restricted to writing.\n\nI have posted my first video today which is about Spark Architecture: [https://youtu.be/8JPu1CECtPE](https://youtu.be/8JPu1CECtPE)\n\nNote: As this is my first video there are some problems which I am aware of like maybe sound, getting stuck for a short time while explaining at times, flow may seem a bit off, rest assured I will keep improving, for now goal is to get these videos out to world and improve myself in the process rather than waiting for perfection and never getting started :) ", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbtne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711367855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;As many of you reached out to me and said to continue the blogs I have decided to get back at it. However, 1 glaring issue was that all my blogs would be lost with time on reddit hence I have decided to post them on YouTube going forward. This will ensure all of you and the future folks can access it whenever you want as well as I can teach with more creative freedom than being restricted to writing.&lt;/p&gt;\n\n&lt;p&gt;I have posted my first video today which is about Spark Architecture: &lt;a href=\"https://youtu.be/8JPu1CECtPE\"&gt;https://youtu.be/8JPu1CECtPE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: As this is my first video there are some problems which I am aware of like maybe sound, getting stuck for a short time while explaining at times, flow may seem a bit off, rest assured I will keep improving, for now goal is to get these videos out to world and improve myself in the process rather than waiting for perfection and never getting started :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?auto=webp&amp;s=e9aeb3c75eb54cbdbeee58e530a54f4ce8c1023d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6deffc0caf1132ee83fa142820772ed0cbceb572", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b299507ca0c917622c0a9deb22a983f8eb8452bb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e96b444650aa0a272f5b45780a325b79b504c05a", "width": 320, "height": 240}], "variants": {}, "id": "Y-PsXGrq_V2X6IBudqfiOtlS8pTQQJ3l6onGNzrLGYk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bnbtne", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbtne/spark_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbtne/spark_architecture/", "subreddit_subscribers": 171761, "created_utc": 1711367855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not having used DBT for anything other than demos, this is likely a 'basic mindset' question.\n\nIs it normal to have a destination final target in your data warehouse that is defined outside of dbt?  \n\nIn other words: would it be the \"dbt way\" to for an organization to say: \"the schema of ACCOUNTS\\_DB.HVAC.PERRY is maintained by some team other than DEs, and DEs use dbt to update that table, but dbt user does not have sufficient grants to modify the table's metadata\"\n\n&amp;#x200B;", "author_fullname": "t2_abfpw4qq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt basics - final model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbtkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711367847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not having used DBT for anything other than demos, this is likely a &amp;#39;basic mindset&amp;#39; question.&lt;/p&gt;\n\n&lt;p&gt;Is it normal to have a destination final target in your data warehouse that is defined outside of dbt?  &lt;/p&gt;\n\n&lt;p&gt;In other words: would it be the &amp;quot;dbt way&amp;quot; to for an organization to say: &amp;quot;the schema of ACCOUNTS_DB.HVAC.PERRY is maintained by some team other than DEs, and DEs use dbt to update that table, but dbt user does not have sufficient grants to modify the table&amp;#39;s metadata&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnbtkc", "is_robot_indexable": true, "report_reasons": null, "author": "levintennine", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbtkc/dbt_basics_final_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbtkc/dbt_basics_final_model/", "subreddit_subscribers": 171761, "created_utc": 1711367847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.\n\nSo, my question is, what's the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don't necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.", "author_fullname": "t2_7j05ermei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnb4p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711365473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.&lt;/p&gt;\n\n&lt;p&gt;So, my question is, what&amp;#39;s the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don&amp;#39;t necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnb4p5", "is_robot_indexable": true, "report_reasons": null, "author": "careerrant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnb4p5/tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnb4p5/tools/", "subreddit_subscribers": 171761, "created_utc": 1711365473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  \n\n\nHave you found it necessary for your team, and if so, how does the general RBAC design look like from your side?\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_7erz2gzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran RBAC - Terraform or REST API based implementation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn9edz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711358843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  &lt;/p&gt;\n\n&lt;p&gt;Have you found it necessary for your team, and if so, how does the general RBAC design look like from your side?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn9edz", "is_robot_indexable": true, "report_reasons": null, "author": "TinkermanN7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "subreddit_subscribers": 171761, "created_utc": 1711358843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a bigquery datawarehouse on 2 environments dev and prod. the datawarehouse is on medallion modelization. (Bronze - Silver - Gold)   \n\n\n  \nWhat could be the drawbacks and advantages of ceasing the extraction of source data into the bronze layer across both the development (dev) and production (prod) environments, opting instead to exclusively gather data in the production environment? Additionally, how does utilizing a service account in dev to extract data for transformation in the silver and gold layers impact this process, considering the use of dbt for transformation?", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning Data Sourcing: Pros and Cons of Shifting to Production-Exclusive Approach with Service Account Integration for Transformation in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bntcf2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711411044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a bigquery datawarehouse on 2 environments dev and prod. the datawarehouse is on medallion modelization. (Bronze - Silver - Gold)   &lt;/p&gt;\n\n&lt;p&gt;What could be the drawbacks and advantages of ceasing the extraction of source data into the bronze layer across both the development (dev) and production (prod) environments, opting instead to exclusively gather data in the production environment? Additionally, how does utilizing a service account in dev to extract data for transformation in the silver and gold layers impact this process, considering the use of dbt for transformation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bntcf2", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bntcf2/transitioning_data_sourcing_pros_and_cons_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bntcf2/transitioning_data_sourcing_pros_and_cons_of/", "subreddit_subscribers": 171761, "created_utc": 1711411044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). \n\nJust wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.\n\nWhat do I need to do to become a senior or better my self? Skills ? Experience ? \n\nThanks in advance.", "author_fullname": "t2_6l52lijc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnskfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). &lt;/p&gt;\n\n&lt;p&gt;Just wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.&lt;/p&gt;\n\n&lt;p&gt;What do I need to do to become a senior or better my self? Skills ? Experience ? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnskfq", "is_robot_indexable": true, "report_reasons": null, "author": "Tookie2x", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnskfq/career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnskfq/career_advice/", "subreddit_subscribers": 171761, "created_utc": 1711409103.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}